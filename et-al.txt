Chapter 1 Data Mining
In this intoductory chapter we begin with the essence of data mining and a dis- cussion of how data mining is treated by the various disciplines that contribute to this field. We cover “Bonferroni’s Principle,” which is really a warning about overusing the ability to mine data. This chapter is also the place where we summarize a few useful ideas that are not data mining but are useful in un- derstanding some important data-mining concepts. These include the TF.IDF measure of word importance, behavior of hash functions and indexes, and iden- tities involving e, the base of natural logarithms. Finally, we give an outline of the topics covered in the balance of the book.
1.1 What is Data Mining?
The most commonly accepted definition of “data mining” is the discovery of “models” for data. A “model,” however, can be one of several things. We mention below the most important directions in modeling.
1.1.1 Statistical Modeling
Statisticians were the first to use the term “data mining.” Originally, “data mining” or “data dredging” was a derogatory term referring to attempts to extract information that was not supported by the data. Section 1.2 illustrates the sort of errors one can make by trying to extract what really isn’t in the data. Today, “data mining” has taken on a positive meaning. Now, statisticians view data mining as the construction of a statistical model, that is, an underlying distribution from which the visible data is drawn.
Example 1.1: Suppose our data is a set of numbers. This data is much simpler than data that would be data-mined, but it will serve as an example. A statistician might decide that the data comes from a Gaussian distribution and use a formula to compute the most likely parameters of this Gaussian. The mean
1
2 CHAPTER 1. DATA MINING and standard deviation of this Gaussian distribution completely characterize the
distribution and would become the model of the data. ✷ 1.1.2 Machine Learning
There are some who regard data mining as synonymous with machine learning. There is no question that some data mining appropriately uses algorithms from machine learning. Machine-learning practitioners use the data as a training set, to train an algorithm of one of the many types used by machine-learning prac- titioners, such as Bayes nets, support-vector machines, decision trees, hidden Markov models, and many others.
There are situations where using data in this way makes sense. The typical case where machine learning is a good approach is when we have little idea of what we are looking for in the data. For example, it is rather unclear what it is about movies that makes certain movie-goers like or dislike it. Thus, in answering the “Netflix challenge” to devise an algorithm that predicts the ratings of movies by users, based on a sample of their responses, machine- learning algorithms have proved quite successful. We shall discuss a simple form of this type of algorithm in Section 9.4.
On the other hand, machine learning has not proved successful in situations where we can describe the goals of the mining more directly. An interesting case in point is the attempt by WhizBang! Labs1 to use machine learning to locate people’s resumes on the Web. It was not able to do better than algorithms designed by hand to look for some of the obvious words and phrases that appear in the typical resume. Since everyone who has looked at or written a resume has a pretty good idea of what resumes contain, there was no mystery about what makes a Web page a resume. Thus, there was no advantage to machine-learning over the direct design of an algorithm to discover resumes.
1.1.3 Computational Approaches to Modeling
More recently, computer scientists have looked at data mining as an algorithmic problem. In this case, the model of the data is simply the answer to a complex query about it. For instance, given the set of numbers of Example 1.1, we might compute their average and standard deviation. Note that these values might not be the parameters of the Gaussian that best fits the data, although they will almost certainly be very close if the size of the data is large.
There are many different approaches to modeling data. We have already mentioned the possibility of constructing a statistical process whereby the data could have been generated. Most other approaches to modeling can be described as either
1. Summarizing the data succinctly and approximately, or
1This startup attempted to use machine learning to mine large-scale data, and hired many of the top machine-learning people to do so. Unfortunately, it was not able to survive.
 
1.1. WHAT IS DATA MINING? 3 2. Extracting the most prominent features of the data and ignoring the rest.
We shall explore these two approaches in the following sections.
1.1.4 Summarization
One of the most interesting forms of summarization is the PageRank idea, which made Google successful and which we shall cover in Chapter 5. In this form of Web mining, the entire complex structure of the Web is summarized by a single number for each page. This number, the “PageRank” of the page, is (oversimplifying somewhat) the probability that a random walker on the graph would be at that page at any given time. The remarkable property this ranking has is that it reflects very well the “importance” of the page – the degree to which typical searchers would like that page returned as an answer to their search query.
Another important form of summary – clustering – will be covered in Chap- ter 7. Here, data is viewed as points in a multidimensional space. Points that are “close” in this space are assigned to the same cluster. The clusters themselves are summarized, perhaps by giving the centroid of the cluster and the average distance from the centroid of points in the cluster. These cluster summaries become the summary of the entire data set.
Example 1.2 : A famous instance of clustering to solve a problem took place longagoinLondon,anditwasdoneentirelywithoutcomputers.2 Thephysician John Snow, dealing with a Cholera outbreak plotted the cases on a map of the city. A small illustration suggesting the process is shown in Fig. 1.1.
                                                                                      Figure 1.1: Plotting cholera cases on a map of London
2 See http://en.wikipedia.org/wiki/1854 Broad Street cholera outbreak.
     
4 CHAPTER 1. DATA MINING
The cases clustered around some of the intersections of roads. These inter- sections were the locations of wells that had become contaminated; people who lived nearest these wells got sick, while people who lived nearer to wells that had not been contaminated did not get sick. Without the ability to cluster the data, the cause of Cholera would not have been discovered. ✷
1.1.5 Feature Extraction
The typical feature-based model looks for the most extreme examples of a phe- nomenon and represents the data by these examples. If you are familiar with Bayes nets, a branch of machine learning and a topic we do not cover in this book, you know how a complex relationship between objects is represented by finding the strongest statistical dependencies among these objects and using only those in representing all statistical connections. Some of the important kinds of feature extraction from large-scale data that we shall study are:
1. Frequent Itemsets. This model makes sense for data that consists of “bas- kets” of small sets of items, as in the market-basket problem that we shall discuss in Chapter 6. We look for small sets of items that appear together in many baskets, and these “frequent itemsets” are the characterization of the data that we seek. The original application of this sort of mining was true market baskets: the sets of items, such as hamburger and ketchup, that people tend to buy together when checking out at the cash register of a store or super market.
2. Similar Items. Often, your data looks like a collection of sets, and the objective is to find pairs of sets that have a relatively large fraction of their elements in common. An example is treating customers at an on- line store like Amazon as the set of items they have bought. In order for Amazon to recommend something else they might like, Amazon can look for “similar” customers and recommend something many of these customers have bought. This process is called “collaborative filtering.” If customers were single-minded, that is, they bought only one kind of thing, then clustering customers might work. However, since customers tend to have interests in many different things, it is more useful to find, for each customer, a small number of other customers who are similar in their tastes, and represent the data by these connections. We discuss similarity in Chapter 3.
1.2
Statistical Limits on Data Mining
A common sort of data-mining problem involves discovering unusual events hidden within massive amounts of data. This section is a discussion of the problem, including “Bonferroni’s Principle,” a warning against overzealous use of data mining.
1.2. STATISTICAL LIMITS ON DATA MINING 5
1.2.1 Total Information Awareness
In 2002, the Bush administration put forward a plan to mine all the data it could find, including credit-card receipts, hotel records, travel data, and many other kinds of information in order to track terrorist activity. This idea naturally caused great concern among privacy advocates, and the project, called TIA, or Total Information Awareness, was eventually killed by Congress, although it is unclear whether the project in fact exists under another name. It is not the purpose of this book to discuss the difficult issue of the privacy-security tradeoff. However, the prospect of TIA or a system like it does raise technical questions about its feasibility and the realism of its assumptions.
The concern raised by many is that if you look at so much data, and you try to find within it activities that look like terrorist behavior, are you not going to find many innocent activities – or even illicit activities that are not terrorism – that will result in visits from the police and maybe worse than just a visit? The answer is that it all depends on how narrowly you define the activities that you look for. Statisticians have seen this problem in many guises and have a theory, which we introduce in the next section.
1.2.2 Bonferroni’s Principle
Suppose you have a certain amount of data, and you look for events of a cer- tain type within that data. You can expect events of this type to occur, even if the data is completely random, and the number of occurrences of these events will grow as the size of the data grows. These occurrences are “bogus,” in the sense that they have no cause other than that random data will always have some number of unusual features that look significant but aren’t. A theorem of statistics, known as the Bonferroni correction gives a statistically sound way to avoid most of these bogus positive responses to a search through the data. Without going into the statistical details, we offer an informal version, Bon- ferroni’s principle, that helps us avoid treating random occurrences as if they were real. Calculate the expected number of occurrences of the events you are looking for, on the assumption that data is random. If this number is signifi- cantly larger than the number of real instances you hope to find, then you must expect almost anything you find to be bogus, i.e., a statistical artifact rather than evidence of what you are looking for. This observation is the informal statement of Bonferroni’s principle.
In a situation like searching for terrorists, where we expect that there are few terrorists operating at any one time, Bonferroni’s principle says that we may only detect terrorists by looking for events that are so rare that they are unlikely to occur in random data. We shall give an extended example in the next section.
6 CHAPTER 1. DATA MINING 1.2.3 An Example of Bonferroni’s Principle
Suppose there are believed to be some “evil-doers” out there, and we want to detect them. Suppose further that we have reason to believe that periodi- cally, evil-doers gather at a hotel to plot their evil. Let us make the following assumptions about the size of the problem:
1. There are one billion people who might be evil-doers.
2. Everyone goes to a hotel one day in 100.
3. A hotel holds 100 people. Hence, there are 100,000 hotels – enough to hold the 1% of a billion people who visit a hotel on any given day.
4. We shall examine hotel records for 1000 days.
To find evil-doers in this data, we shall look for people who, on two different days, were both at the same hotel. Suppose, however, that there really are no evil-doers. That is, everyone behaves at random, deciding with probability 0.01 to visit a hotel on any given day, and if so, choosing one of the 105 hotels at random. Would we find any pairs of people who appear to be evil-doers?
We can do a simple approximate calculation as follows. The probability of any two people both deciding to visit a hotel on any given day is .0001. The chance that they will visit the same hotel is this probability divided by 105, the number of hotels. Thus, the chance that they will visit the same hotel on one given day is 10−9. The chance that they will visit the same hotel on two different given days is the square of this number, 10−18. Note that the hotels can be different on the two days.
Now, we must consider how many events will indicate evil-doing. An “event” in this sense is a pair of people and a pair of days, such that the two people were at the same hotel on each of the two days. To simplify the arithmetic, note that for large n,  n2  is about n2/2. We shall use this approximation in what
follows. Thus, the number of pairs of people is  109  = 5 × 1017. The number 2
of pairs of days is  1000  = 5 × 105. The expected number of events that look 2
like evil-doing is the product of the number of pairs of people, the number of pairs of days, and the probability that any one pair of people and pair of days is an instance of the behavior we are looking for. That number is
5×1017 ×5×105 ×10−18 =250,000
That is, there will be a quarter of a million pairs of people who look like evil- doers, even though they are not.
Now, suppose there really are 10 pairs of evil-doers out there. The police will need to investigate a quarter of a million other pairs in order to find the real evil-doers. In addition to the intrusion on the lives of half a million innocent people, the work involved is sufficiently great that this approach to finding evil-doers is probably not feasible.
1.3. THINGS USEFUL TO KNOW 7 1.2.4 Exercises for Section 1.2
Exercise 1.2.1 : Using the information from Section 1.2.3, what would be the number of suspected pairs if the following changes were made to the data (and all other numbers remained as they were in that section)?
(a) The number of days of observation was raised to 2000.
(b) The number of people observed was raised to 2 billion (and there were therefore 200,000 hotels).
(c) We only reported a pair as suspect if they were at the same hotel at the same time on three different days.
! Exercise 1.2.2: Suppose we have information about the supermarket pur- chases of 100 million people. Each person goes to the supermarket 100 times in a year and buys 10 of the 1000 items that the supermarket sells. We believe that a pair of terrorists will buy exactly the same set of 10 items (perhaps the ingredients for a bomb?) at some time during the year. If we search for pairs of people who have bought the same set of items, would we expect that any such people found were truly terrorists?3
1.3 Things Useful to Know
In this section, we offer brief introductions to subjects that you may or may not have seen in your study of other courses. Each will be useful in the study of data mining. They include:
1. The TF.IDF measure of word importance.
2. Hash functions and their use.
3. Secondary storage (disk) and its effect on running time of algorithms. 4. The base e of natural logarithms and identities involving that constant. 5. Power laws.
1.3.1 Importance of Words in Documents
In several applications of data mining, we shall be faced with the problem of categorizing documents (sequences of words) by their topic. Typically, topics are identified by finding the special words that characterize documents about that topic. For instance, articles about baseball would tend to have many occurrences of words like “ball,” “bat,” “pitch,”, “run,” and so on. Once we
3That is, assume our hypothesis that terrorists will surely buy a set of 10 items in common at some time during the year. We don’t want to address the matter of whether or not terrorists would necessarily do so.
 
8 CHAPTER 1. DATA MINING
have classified documents to determine they are about baseball, it is not hard to notice that words such as these appear unusually frequently. However, until we have made the classification, it is not possible to identify these words as characteristic.
Thus, classification often starts by looking at documents, and finding the significant words in those documents. Our first guess might be that the words appearing most frequently in a document are the most significant. However, that intuition is exactly opposite of the truth. The most frequent words will most surely be the common words such as “the” or “and,” which help build ideas but do not carry any significance themselves. In fact, the several hundred most common words in English (called stop words) are often removed from documents before any attempt to classify them.
In fact, the indicators of the topic are relatively rare words. However, not all rare words are equally useful as indicators. There are certain words, for example “notwithstanding” or “albeit,” that appear rarely in a collection of documents, yet do not tell us anything useful. On the other hand, a word like “chukker” is probably equally rare, but tips us off that the document is about the sport of polo. The difference between rare words that tell us something and those that do not has to do with the concentration of the useful words in just a few documents. That is, the presence of a word like “albeit” in a document does not make it terribly more likely that it will appear multiple times. However, if an article mentions “chukker” once, it is likely to tell us what happened in the “first chukker,” then the “second chukker,” and so on. That is, the word is likely to be repeated if it appears at all.
The formal measure of how concentrated into relatively few documents are the occurrences of a given word is called TF.IDF (Term Frequency times In- verse Document Frequency). It is normally computed as follows. Suppose we have a collection of N documents. Define fij to be the frequency (number of occurrences) of term (word) i in document j. Then, define the term frequency TFij tobe:
TFij = fij maxk fkj
That is, the term frequency of term i in document j is fij normalized by dividing it by the maximum number of occurrences of any term (perhaps excluding stop words) in the same document. Thus, the most frequent term in document j gets a TF of 1, and other terms get fractions as their term frequency for this document.
The IDF for a term is defined as follows. Suppose term i appears in ni of the N documents in the collection. Then IDFi = log2(N/ni). The TF.IDF score for term i in document j is then defined to be TFij × IDFi. The terms with the highest TF.IDF score are often the terms that best characterize the topic of the document.
Example 1.3 : Suppose our repository consists of 220 = 1,048,576 documents. Suppose word w appears in 210 = 1024 of these documents. Then IDFw =
 
1.3. THINGS USEFUL TO KNOW 9
log2(220/210) = log 2(210) = 10. Consider a document j in which w appears 20 times, and that is the maximum number of times in which any word appears (perhaps after eliminating stop words). Then TF wj = 1, and the TF.IDF score for w in document j is 10.
Suppose that in document k, word w appears once, while the maximum number of occurrences of any word in this document is 20. Then TF wk = 1/20, and the TF.IDF score for w in document k is 1/2. ✷
1.3.2 Hash Functions
The reader has probably heard of hash tables, and perhaps used them in Java classes or similar packages. The hash functions that make hash tables feasible are also essential components in a number of data-mining algorithms, where the hash table takes an unfamiliar form. We shall review the basics here.
First, a hash function h takes a hash-key value as an argument and produces a bucket number as a result. The bucket number is an integer, normally in the range 0 to B − 1, where B is the number of buckets. Hash-keys can be of any type. There is an intuitive property of hash functions that they “randomize” hash-keys. To be precise, if hash-keys are drawn randomly from a reasonable population of possible hash-keys, then h will send approximately equal numbers of hash-keys to each of the B buckets. It would be impossible to do so if, for example, the population of possible hash-keys were smaller than B. Such a population would not be “reasonable.” However, there can be more subtle rea- sons why a hash function fails to achieve an approximately uniform distribution into buckets.
Example 1.4 : Suppose hash-keys are positive integers. A common and simple hash function is to pick h(x) = x mod B, that is, the remainder when x is divided by B. That choice works fine if our population of hash-keys is all positive integers. 1/Bth of the integers will be assigned to each of the buckets. However, suppose our population is the even integers, and B = 10. Then only buckets 0, 2, 4, 6, and 8 can be the value of h(x), and the hash function is distinctly nonrandom in its behavior. On the other hand, if we picked B = 11, then we would find that 1/11th of the even integers get sent to each of the 11 buckets, so the hash function would work very well. ✷
The generalization of Example 1.4 is that when hash-keys are integers, chos- ing B so it has any common factor with all (or even most of) the possible hash- keys will result in nonrandom distribution into buckets. Thus, it is normally preferred that we choose B to be a prime. That choice reduces the chance of nonrandom behavior, although we still have to consider the possibility that all hash-keys have B as a factor. Of course there are many other types of hash functions not based on modular arithmetic. We shall not try to summarize the options here, but some sources of information will be mentioned in the bibliographic notes.
10 CHAPTER 1. DATA MINING
What if hash-keys are not integers? In a sense, all data types have values that are composed of bits, and sequences of bits can always be interpreted as in- tegers. However, there are some simple rules that enable us to convert common types to integers. For example, if hash-keys are strings, convert each character to its ASCII or Unicode equivalent, which can be interpreted as a small inte- ger. Sum the integers before dividing by B. As long as B is smaller than the typical sum of character codes for the population of strings, the distribution into buckets will be relatively uniform. If B is larger, then we can partition the characters of a string into groups of several characters each. Treat the concate- nation of the codes for the characters of a group as a single integer. Sum the integers associated with all the groups of a string, and divide by B as before. For instance, if B is around a billion, or 230, then grouping characters four at a time will give us 32-bit integers. The sum of several of these will distribute fairly evenly into a billion buckets.
For more complex data types, we can extend the idea used for converting strings to integers, recursively.
• For a type that is a record, each of whose components has its own type, recursively convert the value of each component to an integer, using the algorithm appropriate for the type of that component. Sum the integers for the components, and convert the integer sum to buckets by dividing by B.
• For a type that is an array, set, or bag of elements of some one type, convert the values of the elements’ type to integers, sum the integers, and divide by B.
1.3.3 Indexes
An index is a data structure that makes it efficient to retrieve objects given the value of one or more elements of those objects. The most common situation is one where the objects are records, and the index is on one of the fields of that record. Given a value v for that field, the index lets us retrieve all the records with value v in that field. For example, we could have a file of (name, address, phone) triples, and an index on the phone field. Given a phone number, the index allows us to find quickly the record or records with that phone number.
There are many ways to implement indexes, and we shall not attempt to survey the matter here. The bibliographic notes give suggestions for further reading. However, a hash table is one simple way to build an index. The field or fields on which the index is based form the hash-key for a hash function. Records have the hash function applied to value of the hash-key, and the record itself is placed in the bucket whose number is determined by the hash function. The bucket could be a list of records in main-memory, or a disk block, for example.
1.3. THINGS USEFUL TO KNOW 11
Then, given a hash-key value, we can hash it, find the bucket, and need to search only that bucket to find the records with that value for the hash-key. If we choose the number of buckets B to be comparable to the number of records in the file, then there will be relatively few records in any bucket, and the search of a bucket takes little time.
  Sally Jones
  Maple St
800−555−1212
      h (800−555−1212)
0
17
B−1
Array of bucket headers
. . .
             . . .
 Figure 1.2: A hash table used as an index; phone numbers are hashed to buckets, and the entire record is placed in the bucket whose number is the hash value of the phone
Example 1.5 : Figure 1.2 suggests what a main-memory index of records with name, address, and phone fields might look like. Here, the index is on the phone field, and buckets are linked lists. We show the phone 800-555-1212 hashed to bucket number 17. There is an array of bucket headers, whose ith element is the head of a linked list for the bucket numbered i. We show expanded one of the elements of the linked list. It contains a record with name, address, and phone fields. This record is in fact one with the phone number 800-555-1212. Other records in that bucket may or may not have this phone number. We only know that whatever phone number they have is a phone that hashes to 17. ✷
1.3.4 Secondary Storage
It is important, when dealing with large-scale data, that we have a good un- derstanding of the difference in time taken to perform computations when the data is initially on disk, as opposed to the time needed if the data is initially in main memory. The physical characteristics of disks is another subject on which we could say much, but shall say only a little and leave the interested reader to follow the bibliographic notes.
Disks are organized into blocks, which are the minimum units that the oper- ating system uses to move data between main memory and disk. For example,
Records with
h(phone) = 17
12 CHAPTER 1. DATA MINING
the Windows operating system uses blocks of 64K bytes (i.e., 216 = 65,536 bytes to be exact). It takes approximately ten milliseconds to access (move the disk head to the track of the block and wait for the block to rotate under the head) and read a disk block. That delay is at least five orders of magnitude (a factor of 105) slower than the time taken to read a word from main memory, so if all we want to do is access a few bytes, there is an overwhelming benefit to having data in main memory. In fact, if we want to do something simple to every byte of a disk block, e.g., treat the block as a bucket of a hash table and search for a particular value of the hash-key among all the records in that bucket, then the time taken to move the block from disk to main memory will be far larger than the time taken to do the computation.
By organizing our data so that related data is on a single cylinder (the collection of blocks reachable at a fixed radius from the center of the disk, and therefore accessible without moving the disk head), we can read all the blocks on the cylinder into main memory in considerably less than 10 milliseconds per block. You can assume that a disk cannot transfer data to main memory at more than a hundred million bytes per second, no matter how that data is organized. That is not a problem when your dataset is a megabyte. But a dataset of a hundred gigabytes or a terabyte presents problems just accessing it, let alone doing anything useful with it.
1.3.5 The Base of Natural Logarithms
The constant e = 2.7182818 · · · has a number of useful special properties. In particular, e is the limit of (1 + 1 )x as x goes to infinity. The values of this
x
expression for x = 1, 2, 3, 4 are approximately 2, 2.25, 2.37, 2.44, so you should find it easy to believe that the limit of this series is around 2.72.
Some algebra lets us obtain approximations to many seemingly complex
expressions. Consider (1 + a)b , where a is small. We can rewrite the expression
as (1+a)(1/a)(ab). Then substitute a = 1/x and 1/a = x, so we have (1+ 1 )x(ab),
x
which is
Since a is assumed small, x is large, so the subexpression (1 + 1 )x will be close
Similar identities hold when a is negative. That is, the limit as x goes to
    1+ 1 x ab x
  to the limiting value of e. We can thus approximate (1 + a) as e .
infinity of (1 − 1 )x is 1/e. It follows that the approximation (1 + a)b = eab xb
b x ab
 holds even when a is a small negative number. Put another way, (1 − a) is approximately e−ab when a is small and b is large.
Some other useful approximations follow from the Taylor expansion of ex. That is, ex =  ∞i=0 xi/i!, or ex = 1+x+x2/2+x3/6+x4/24+···. When x is large, the above series converges slowly, although it does converge because n! grows faster than xn for any constant x. However, when x is small, either positive or negative, the series converges rapidly, and only a few terms are necessary to get a good approximation.
1.3. THINGS USEFUL TO KNOW 13 Example 1.6 : Let x = 1/2. Then
e1/2=1+1+1+1+ 1 +··· 2 8 48 384
or approximately e1/2 = 1.64844. Let x = −1. Then
e−1=1−1+1−1+1− 1 + 1 − 1 +··· 2 6 24 120 720 5040
or approximately e−1 = 0.36786. ✷ 1.3.6 Power Laws
There are many phenomena that relate two variables by a power law, that is, a linear relationship between the logarithms of the variables. Figure 1.3 suggests such a relationship. If x is the horizontal axis and y is the vertical axis, then the relationship is log10 y = 6 − 2 log10 x.
10,000,000 1,000,000 100,000 10,000 1000 100 10
1
1 10 100 1000 10,000
Figure 1.3: A power law with a slope of −2
Example 1.7: We might examine book sales at Amazon.com, and let x rep- resent the rank of books by sales. Then y is the number of sales of the xth best-selling book over some period. The implication of the graph of Fig. 1.3 would be that the best-selling book sold 1,000,000 copies, the 10th best-selling book sold 10,000 copies, the 100th best-selling book sold 100 copies, and so on for all ranks between these numbers and beyond. The implication that above
                      
14 CHAPTER 1. DATA MINING
   The Matthew Effect
Often, the existence of power laws with values of the exponent higher than 1 are explained by the Matthew effect. In the biblical Book of Matthew, there is a verse about “the rich get richer.” Many phenomena exhibit this behavior, where getting a high value of some property causes that very property to increase. For example, if a Web page has many links in, then people are more likely to find the page and may choose to link to it from one of their pages as well. As another example, if a book is selling well on Amazon, then it is likely to be advertised when customers go to the Amazon site. Some of these people will choose to buy the book as well, thus increasing the sales of this book.
 rank 1000 the sales are a fraction of a book is too extreme, and we would in fact expect the line to flatten out for ranks much higher than 1000. ✷
The general form of a power law relating x and y is logy = b+alogx. If we raise the base of the logarithm (which doesn’t actually matter), say e, to the values on both sides of this equation, we get y = ebea log x = ebxa. Since eb is just “some constant,” let us replace it by constant c. Thus, a power law can be written as y = cxa for some constants a and c.
Example1.8: InFig.1.3weseethatwhenx=1,y=106,andwhenx= 1000, y = 1. Making the first substitution, we see 106 = c. The second substitution gives us 1 = c(1000)a. Since we now know c = 106, the second equation gives us 1 = 106(1000)a, from which we see a = −2. That is, the law expressed by Fig. 1.3 is y = 106x−2, or y = 106/x2. ✷
We shall meet in this book many ways that power laws govern phenomena. Here are some examples:
1. Node Degrees in the Web Graph: Order all pages by the number of in- links to that page. Let x be the position of a page in this ordering, and let y be the number of in-links to the xth page. Then y as a function of x looks very much like Fig. 1.3. The exponent a is slightly larger than the −2 shown there; it has been found closer to 2.1.
2. Sales of Products: Order products, say books at Amazon.com, by their sales over the past year. Let y be the number of sales of the xth most pop- ular book. Again, the function y(x) will look something like Fig. 1.3. we shall discuss the consequences of this distribution of sales in Section 9.1.2, where we take up the matter of the “long tail.”
3. Sizes of Web Sites: Count the number of pages at Web sites, and order sites by the number of their pages. Let y be the number of pages at the xth site. Again, the function y(x) follows a power law.
1.4. OUTLINE OF THE BOOK 15
4. Zipf’s Law: This power law originally referred to the frequency of words in a collection of documents. If you order words by frequency, and let y be the number of times the xth word in the order appears, then you get a power law, although with a much shallower slope than that of Fig. 1.3. Zipf’s observation was that y = cx−1/2. Interestingly, a number of other kinds of data follow this particular power law. For example, if we order states in the US by population and let y be the population of the xth most populous state, then x and y obey Zipf’s law approximately.
1.3.7 Exercises for Section 1.3
Exercise 1.3.1 : Suppose there is a repository of ten million documents. What (to the nearest integer) is the IDF for a word that appears in (a) 40 documents (b) 10,000 documents?
Exercise 1.3.2: Suppose there is a repository of ten million documents, and word w appears in 320 of them. In a particular document d, the maximum number of occurrences of a word is 15. Approximately what is the TF.IDF score for w if that word appears (a) once (b) five times?
! Exercise 1.3.3 : Suppose hash-keys are drawn from the population of all non- negative integers that are multiples of some constant c, and hash function h(x) is x mod 15. For what values of c will h be a suitable hash function, i.e., a large random choice of hash-keys will be divided roughly equally into buckets?
Exercise 1.3.4 : In terms of e, give approximations to (a) (1.01)500 (b) (1.05)1000 (c) (0.9)40
Exercise 1.3.5 : Use the Taylor expansion of ex to compute, to three decimal places: (a) e1/10 (b) e−1/10 (c) e2.
1.4 Outline of the Book
This section gives brief summaries of the remaining chapters of the book. Chapter 2 is not about data mining per se. Rather, it introduces us to the MapReduce methodology for exploiting parallelism in computing clouds (racks of interconnected processors). There is reason to believe that cloud computing, and MapReduce in particular, will become the normal way to compute when analysis of very large amounts of data is involved. A pervasive issue in later chapters will be the exploitation of the MapReduce methodology to implement
the algorithms we cover.
Chapter 3 is about finding similar items. Our starting point is that items
can be represented by sets of elements, and similar sets are those that have a large fraction of their elements in common. The key techniques of minhashing and locality-sensitive hashing are explained. These techniques have numerous
16 CHAPTER 1. DATA MINING
applications and often give surprisingly efficient solutions to problems that ap- pear impossible for massive data sets.
In Chapter 4, we consider data in the form of a stream. The difference between a stream and a database is that the data in a stream is lost if you do not do something about it immediately. Important examples of streams are the streams of search queries at a search engine or clicks at a popular Web site. In this chapter, we see several of the surprising applications of hashing that make management of stream data feasible.
Chapter 5 is devoted to a single application: the computation of PageRank. This computation is the idea that made Google stand out from other search engines, and it is still an essential part of how search engines know what pages the user is likely to want to see. Extensions of PageRank are also essential in the fight against spam (euphemistically called “search engine optimization”), and we shall examine the latest extensions of the idea for the purpose of combating spam.
Then, Chapter 6 introduces the market-basket model of data, and its canon- ical problems of association rules and finding frequent itemsets. In the market- basket model, data consists of a large collection of baskets, each of which con- tains a small set of items. We give a sequence of algorithms capable of finding all frequent pairs of items, that is pairs of items that appear together in many baskets. Another sequence of algorithms are useful for finding most of the frequent itemsets larger than pairs, with high efficiency.
Chapter 7 examines the problem of clustering. We assume a set of items with a distance measure defining how close or far one item is from another. The goal is to examine a large amount of data and partition it into subsets (clusters), each cluster consisting of items that are all close to one another, yet far from items in the other clusters.
Chapter 8 is devoted to on-line advertising and the computational problems it engenders. We introduce the notion of an on-line algorithm – one where a good response must be given immediately, rather than waiting until we have seen the entire dataset. The idea of competitive ratio is another important concept covered in this chapter; it is the ratio of the guaranteed performance of an on-line algorithm compared with the performance of the optimal algorithm that is allowed to see all the data before making any decisions. These ideas are used to give good algorithms that match bids by advertisers for the right to display their ad in response to a query against the search queries arriving at a search engine.
Chapter 9 is devoted to recommendation systems. Many Web applications involve advising users on what they might like. The Netflix challenge is one example, where it is desired to predict what movies a user would like, or Ama- zon’s problem of pitching a product to a customer based on information about what they might be interested in buying. There are two basic approaches to recommendation. We can characterize items by features, e.g., the stars of a movie, and recommend items with the same features as those the user is known to like. Or, we can look at other users with preferences similar to that of the
1.5. SUMMARY OF CHAPTER 1 17
user in question, and see what they liked (a technique known as collaborative filtering).
In Chapter 10, we study social networks and algorithms for their analysis. The canonical example of a social network is the graph of Facebook friends, where the nodes are people, and edges connect two people if they are friends. Directed graphs, such as followers on Twitter, can also be viewed as social networks. A common example of a problem to be addressed is identifying “communities,” that is, small sets of nodes with an unusually large number of edges among them. Other questions about social networks are general questions about graphs, such as computing the transitive closure or diameter of a graph, but are made more difficult by the size of typical networks.
Chapter 11 looks at dimensionality reduction. We are given a very large matrix, typically sparse. Think of the matrix as representing a relationship between two kinds of entities, e.g., ratings of movies by viewers. Intuitively, there are a small number of concepts, many fewer concepts than there are movies or viewers, that explain why certain viewers like certain movies. We offer several algorithms that simplify matrices by decomposing them into a product of matrices that are much smaller in one of the two dimensions. One matrix relates entities of one kind to the small number of concepts and another relates the concepts to the other kind of entity. If done correctly, the product of the smaller matrices will be very close to the original matrix.
Finally, Chapter 12 discusses algorithms for machine learning from very large datasets. Techniques covered include perceptrons, support-vector ma- chines, finding models by gradient descent, nearest-neighbor models, and deci- sion trees.
1.5 Summary of Chapter 1
✦ Data Mining: This term refers to the process of extracting useful models of data. Sometimes, a model can be a summary of the data, or it can be the set of most extreme features of the data.
✦ Bonferroni’s Principle: If we are willing to view as an interesting fea- ture of data something of which many instances can be expected to exist in random data, then we cannot rely on such features being significant. This observation limits our ability to mine data for features that are not sufficiently rare in practice.
✦ TF.IDF : The measure called TF.IDF lets us identify words in a collection of documents that are useful for determining the topic of each document. A word has high TF.IDF score in a document if it appears in relatively few documents, but appears in this one, and when it appears in a document it tends to appear many times.
✦ Hash Functions: A hash function maps hash-keys of some data type to integer bucket numbers. A good hash function distributes the possible
18
✦
✦
✦
1.6
CHAPTER 1. DATA MINING
hash-key values approximately evenly among buckets. Any data type can be the domain of a hash function.
Indexes: An index is a data structure that allows us to store and retrieve data records efficiently, given the value in one or more of the fields of the record. Hashing is one way to build an index.
Storage on Disk: When data must be stored on disk (secondary memory), it takes very much more time to access a desired data item than if the same data were stored in main memory. When data is large, it is important that algorithms strive to keep needed data in main memory.
Power Laws: Many phenomena obey a law that can be expressed as y = cxa for some power a, often around −2. Such phenomena include the sales of the xth most popular book, or the number of in-links to the xth most popular page.
References for Chapter 1
[7] is a clear introduction to the basics of data mining. [2] covers data mining principally from the point of view of machine learning and statistics.
For construction of hash functions and hash tables, see [4]. Details of the TF.IDF measure and other matters regarding document processing can be found in [5]. See [3] for more on managing indexes, hash tables, and data on disk.
Power laws pertaining to the Web were explored by [1]. The Matthew effect was first observed in [6].
1. A. Broder, R. Kumar, F. Maghoul, P. Raghavan, S. Rajagopalan, R. Stata, A. Tomkins, and J. Weiner, “Graph structure in the web,” Com- puter Networks 33:1–6, pp. 309–320, 2000.
2. M.M. Gaber, Scientific Data Mining and Knowledge Discovery — Prin- ciples and Foundations, Springer, New York, 2010.
3. H. Garcia-Molina, J.D. Ullman, and J. Widom, Database Systems: The Complete Book Second Edition, Prentice-Hall, Upper Saddle River, NJ, 2009.
4. D.E. Knuth, The Art of Computer Programming Vol. 3 (Sorting and Searching), Second Edition, Addison-Wesley, Upper Saddle River, NJ, 1998.
5. C.P. Manning, P. Raghavan, and H. Schu ̈tze, Introduction to Information Retrieval, Cambridge Univ. Press, 2008.
6. R.K. Merton, “The Matthew effect in science,” Science 159:3810, pp. 56– 63, Jan. 5, 1968.
1.6. REFERENCES FOR CHAPTER 1 19
7. P.-N. Tan, M. Steinbach, and V. Kumar, Introduction to Data Mining, Addison-Wesley, Upper Saddle River, NJ, 2005.
20 CHAPTER 1. DATA MINING
Chapter 2
MapReduce and the New Software Stack
Modern data-mining applications, often called “big-data” analysis, require us to manage immense amounts of data quickly. In many of these applications, the data is extremely regular, and there is ample opportunity to exploit parallelism. Important examples are:
1. The ranking of Web pages by importance, which involves an iterated matrix-vector multiplication where the dimension is many billions.
2. Searches in “friends” networks at social-networking sites, which involve graphs with hundreds of millions of nodes and many billions of edges.
To deal with applications such as these, a new software stack has evolved. These programming systems are designed to get their parallelism not from a “super- computer,” but from “computing clusters” – large collections of commodity hardware, including conventional processors (“compute nodes”) connected by Ethernet cables or inexpensive switches. The software stack begins with a new form of file system, called a “distributed file system,” which features much larger units than the disk blocks in a conventional operating system. Distributed file systems also provide replication of data or redundancy to protect against the frequent media failures that occur when data is distributed over thousands of low-cost compute nodes.
On top of these file systems, many different higher-level programming sys- tems have been developed. Central to the new software stack is a programming system called MapReduce. Implementations of MapReduce enable many of the most common calculations on large-scale data to be performed on computing clusters efficiently and in a way that is tolerant of hardware failures during the computation.
MapReduce systems are evolving and extending rapidly. Today, it is com- mon for MapReduce programs to be created from still higher-level programming
21
22 CHAPTER 2. MAPREDUCE AND THE NEW SOFTWARE STACK
systems, often an implementation of SQL. Further, MapReduce turns out to be a useful, but simple, case of more general and powerful ideas. We include in this chapter a discussion of generalizations of MapReduce, first to systems that support acyclic workflows and then to systems that implement recursive algorithms.
Our last topic for this chapter is the design of good MapReduce algorithms, a subject that often differs significantly from the matter of designing good parallel algorithms to be run on a supercomputer. When designing MapReduce algorithms, we often find that the greatest cost is in the communication. We thus investigate communication cost and what it tells us about the most efficient MapReduce algorithms. For several common applications of MapReduce we are able to give families of algorithms that optimally trade the communication cost against the degree of parallelism.
2.1 Distributed File Systems
Most computing is done on a single processor, with its main memory, cache, and local disk (a compute node). In the past, applications that called for parallel processing, such as large scientific calculations, were done on special-purpose parallel computers with many processors and specialized hardware. However, the prevalence of large-scale Web services has caused more and more computing to be done on installations with thousands of compute nodes operating more or less independently. In these installations, the compute nodes are commodity hardware, which greatly reduces the cost compared with special-purpose parallel machines.
These new computing facilities have given rise to a new generation of pro- gramming systems. These systems take advantage of the power of parallelism and at the same time avoid the reliability problems that arise when the comput- ing hardware consists of thousands of independent components, any of which could fail at any time. In this section, we discuss both the characteristics of these computing installations and the specialized file systems that have been developed to take advantage of them.
2.1.1 Physical Organization of Compute Nodes
The new parallel-computing architecture, sometimes called cluster computing, is organized as follows. Compute nodes are stored on racks, perhaps 8–64 on a rack. The nodes on a single rack are connected by a network, typically gigabit Ethernet. There can be many racks of compute nodes, and racks are connected by another level of network or a switch. The bandwidth of inter-rack communication is somewhat greater than the intrarack Ethernet, but given the number of pairs of nodes that might need to communicate between racks, this bandwidth may be essential. Figure 2.1 suggests the architecture of a large- scale computing system. However, there may be many more racks and many more compute nodes per rack.
2.1. DISTRIBUTED FILE SYSTEMS 23
 Switch
                                  Racks of compute nodes
Figure 2.1: Compute nodes are organized into racks, and racks are intercon- nected by a switch
It is a fact of life that components fail, and the more components, such as compute nodes and interconnection networks, a system has, the more frequently something in the system will not be working at any given time. For systems such as Fig. 2.1, the principal failure modes are the loss of a single node (e.g., the disk at that node crashes) and the loss of an entire rack (e.g., the network connecting its nodes to each other and to the outside world fails).
Some important calculations take minutes or even hours on thousands of compute nodes. If we had to abort and restart the computation every time one component failed, then the computation might never complete successfully. The solution to this problem takes two forms:
1. Files must be stored redundantly. If we did not duplicate the file at several compute nodes, then if one node failed, all its files would be unavailable until the node is replaced. If we did not back up the files at all, and the disk crashes, the files would be lost forever. We discuss file management in Section 2.1.2.
2. Computations must be divided into tasks, such that if any one task fails to execute to completion, it can be restarted without affecting other tasks. This strategy is followed by the MapReduce programming system that we introduce in Section 2.2.
2.1.2 Large-Scale File-System Organization
To exploit cluster computing, files must look and behave somewhat differently from the conventional file systems found on single computers. This new file system, often called a distributed file system or DFS (although this term has had other meanings in the past), is typically used as follows.
24 CHAPTER 2. MAPREDUCE AND THE NEW SOFTWARE STACK
   DFS Implementations
There are several distributed file systems of the type we have described that are used in practice. Among these:
1. The Google File System (GFS), the original of the class.
2. Hadoop Distributed File System (HDFS), an open-source DFS used with Hadoop, an implementation of MapReduce (see Section 2.2) and distributed by the Apache Software Foundation.
3. CloudStore, an open-source DFS originally developed by Kosmix.
 • Files can be enormous, possibly a terabyte in size. If you have only small files, there is no point using a DFS for them.
• Files are rarely updated. Rather, they are read as data for some calcula- tion, and possibly additional data is appended to files from time to time. For example, an airline reservation system would not be suitable for a DFS, even if the data were very large, because the data is changed so frequently.
Files are divided into chunks, which are typically 64 megabytes in size. Chunks are replicated, perhaps three times, at three different compute nodes. Moreover, the nodes holding copies of one chunk should be located on different racks, so we don’t lose all copies due to a rack failure. Normally, both the chunk size and the degree of replication can be decided by the user.
To find the chunks of a file, there is another small file called the master node or name node for that file. The master node is itself replicated, and a directory for the file system as a whole knows where to find its copies. The directory itself can be replicated, and all participants using the DFS know where the directory copies are.
2.2 MapReduce
MapReduce is a style of computing that has been implemented in several sys- tems, including Google’s internal implementation (simply called MapReduce) and the popular open-source implementation Hadoop which can be obtained, along with the HDFS file system from the Apache Foundation. You can use an implementation of MapReduce to manage many large-scale computations in a way that is tolerant of hardware faults. All you need to write are two functions, called Map and Reduce, while the system manages the parallel exe- cution, coordination of tasks that execute Map or Reduce, and also deals with
2.2. MAPREDUCE 25 the possibility that one of these tasks will fail to execute. In brief, a MapReduce
computation executes as follows:
1. Some number of Map tasks each are given one or more chunks from a distributed file system. These Map tasks turn the chunk into a sequence of key-value pairs. The way key-value pairs are produced from the input data is determined by the code written by the user for the Map function.
2. The key-value pairs from each Map task are collected by a master con- troller and sorted by key. The keys are divided among all the Reduce tasks, so all key-value pairs with the same key wind up at the same Re- duce task.
3. The Reduce tasks work on one key at a time, and combine all the val- ues associated with that key in some way. The manner of combination of values is determined by the code written by the user for the Reduce function.
Figure 2.2 suggests this computation.
Key−value pairs
Keys with all their values (k, [v, w,...])
     Input chunks
(k,v)
              Combined output
                 2.2.1
Figure 2.2: Schematic of a MapReduce computation
The Map Tasks
Map tasks
Group by keys
We view input files for a Map task as consisting of elements, which can be any type: a tuple or a document, for example. A chunk is a collection of elements, and no element is stored across two chunks. Technically, all inputs
Reduce tasks
26 CHAPTER 2. MAPREDUCE AND THE NEW SOFTWARE STACK
to Map tasks and outputs from Reduce tasks are of the key-value-pair form, but normally the keys of input elements are not relevant and we shall tend to ignore them. Insisting on this form for inputs and outputs is motivated by the desire to allow composition of several MapReduce processes.
The Map function takes an input element as its argument and produces zero or more key-value pairs. The types of keys and values are each arbitrary. Further, keys are not “keys” in the usual sense; they do not have to be unique. Rather a Map task can produce several key-value pairs with the same key, even from the same element.
Example 2.1 : We shall illustrate a MapReduce computation with what has become the standard example application: counting the number of occurrences for each word in a collection of documents. In this example, the input file is a repository of documents, and each document is an element. The Map function for this example uses keys that are of type String (the words) and values that are integers. The Map task reads a document and breaks it into its sequence of words w1, w2, . . . , wn. It then emits a sequence of key-value pairs where the value is always 1. That is, the output of the Map task for this document is the sequence of key-value pairs:
(w1,1), (w2,1),...,(wn,1)
Note that a single Map task will typically process many documents – all the documents in one or more chunks. Thus, its output will be more than the sequence for the one document suggested above. Note also that if a word w appears m times among all the documents assigned to that process, then there will be m key-value pairs (w, 1) among its output. An option, which we discuss in Section 2.2.4, is to combine these m pairs into a single pair (w, m), but we can only do that because, as we shall see, the Reduce tasks apply an associative and commutative operation, addition, to the values. ✷
2.2.2 Grouping by Key
As soon as the Map tasks have all completed successfully, the key-value pairs are grouped by key, and the values associated with each key are formed into a list of values. The grouping is performed by the system, regardless of what the Map and Reduce tasks do. The master controller process knows how many Reduce tasks there will be, say r such tasks. The user typically tells the MapReduce system what r should be. Then the master controller picks a hash function that applies to keys and produces a bucket number from 0 to r − 1. Each key that is output by a Map task is hashed and its key-value pair is put in one of r local files. Each file is destined for one of the Reduce tasks.1
1Optionally, users can specify their own hash function or other method for assigning keys to Reduce tasks. However, whatever algorithm is used, each key is assigned to one and only one Reduce task.
 
2.2. MAPREDUCE 27
To perform the grouping by key and distribution to the Reduce tasks, the master controller merges the files from each Map task that are destined for a particular Reduce task and feeds the merged file to that process as a se- quence of key-list-of-value pairs. That is, for each key k, the input to the Reduce task that handles key k is a pair of the form (k, [v1, v2, . . . , vn]), where (k,v1), (k,v2),...,(k,vn) are all the key-value pairs with key k coming from all the Map tasks.
2.2.3 The Reduce Tasks
The Reduce function’s argument is a pair consisting of a key and its list of associated values. The output of the Reduce function is a sequence of zero or more key-value pairs. These key-value pairs can be of a type different from those sent from Map tasks to Reduce tasks, but often they are the same type. We shall refer to the application of the Reduce function to a single key and its associated list of values as a reducer.
A Reduce task receives one or more keys and their associated value lists. That is, a Reduce task executes one or more reducers. The outputs from all the Reduce tasks are merged into a single file. Reducers may be partitioned among a smaller number of Reduce tasks is by hashing the keys and associating each Reduce task with one of the buckets of the hash function.
Example 2.2 : Let us continue with the word-count example of Example 2.1. The Reduce function simply adds up all the values. The output of a reducer consists of the word and the sum. Thus, the output of all the Reduce tasks is a sequence of (w, m) pairs, where w is a word that appears at least once among all the input documents and m is the total number of occurrences of w among all those documents. ✷
2.2.4 Combiners
Sometimes, a Reduce function is associative and commutative. That is, the values to be combined can be combined in any order, with the same result. The addition performed in Example 2.2 is an example of an associative and commutative operation. It doesn’t matter how we group a list of numbers v1,v2,...,vn; the sum will be the same.
When the Reduce function is associative and commutative, we can push some of what the reducers do to the Map tasks. For example, instead of the Map tasks in Example 2.1 producing many pairs (w,1), (w,1),..., we could apply the Reduce function within the Map task, before the output of the Map tasks is subject to grouping and aggregation. These key-value pairs would thus be replaced by one pair with key w and value equal to the sum of all the 1’s in all those pairs. That is, the pairs with key w generated by a single Map task would be replaced by a pair (w,m), where m is the number of times that w appears among the documents handled by this Map task. Note that it is still necessary to do grouping and aggregation and to pass the result to the Reduce
28 CHAPTER 2. MAPREDUCE AND THE NEW SOFTWARE STACK
   Reducers, Reduce Tasks, Compute Nodes, and Skew
If we want maximum parallelism, then we could use one Reduce task to execute each reducer, i.e., a single key and its associated value list. Further, we could execute each Reduce task at a different compute node, so they would all execute in parallel. This plan is not usually the best. One problem is that there is overhead associated with each task we create, so we might want to keep the number of Reduce tasks lower than the number of different keys. Moreover, often there are far more keys than there are compute nodes available, so we would get no benefit from a huge number of Reduce tasks.
Second, there is often significant variation in the lengths of the value lists for different keys, so different reducers take different amounts of time. If we make each reducer a separate Reduce task, then the tasks themselves will exhibit skew – a significant difference in the amount of time each takes. We can reduce the impact of skew by using fewer Reduce tasks than there are reducers. If keys are sent randomly to Reduce tasks, we can expect that there will be some averaging of the total time required by the different Reduce tasks. We can further reduce the skew by using more Reduce tasks than there are compute nodes. In that way, long Reduce tasks might occupy a compute node fully, while several shorter Reduce tasks might run sequentially at a single compute node.
 tasks, since there will typically be one key-value pair with key w coming from each of the Map tasks.
2.2.5 Details of MapReduce Execution
Let us now consider in more detail how a program using MapReduce is executed. Figure 2.3 offers an outline of how processes, tasks, and files interact. Taking advantage of a library provided by a MapReduce system such as Hadoop, the user program forks a Master controller process and some number of Worker processes at different compute nodes. Normally, a Worker handles either Map tasks (a Map worker) or Reduce tasks (a Reduce worker), but not both.
The Master has many responsibilities. One is to create some number of Map tasks and some number of Reduce tasks, these numbers being selected by the user program. These tasks will be assigned to Worker processes by the Master. It is reasonable to create one Map task for every chunk of the input file(s), but we may wish to create fewer Reduce tasks. The reason for limiting the number of Reduce tasks is that it is necessary for each Map task to create an intermediate file for each Reduce task, and if there are too many Reduce tasks the number of intermediate files explodes.
The Master keeps track of the status of each Map and Reduce task (idle,
2.2. MAPREDUCE
29
     fork
User Program
fork
Master
assign Map
fork
   assign Reduce
           Worker
Worker
Worker
Worker
Worker
                                   Input Data
Output File
  Intermediate Files
Figure 2.3: Overview of the execution of a MapReduce program
executing at a particular Worker, or completed). A Worker process reports to the Master when it finishes a task, and a new task is scheduled by the Master for that Worker process.
Each Map task is assigned one or more chunks of the input file(s) and executes on it the code written by the user. The Map task creates a file for each Reduce task on the local disk of the Worker that executes the Map task. The Master is informed of the location and sizes of each of these files, and the Reduce task for which each is destined. When a Reduce task is assigned by the Master to a Worker process, that task is given all the files that form its input. The Reduce task executes code written by the user and writes its output to a file that is part of the surrounding distributed file system.
2.2.6 Coping With Node Failures
The worst thing that can happen is that the compute node at which the Master is executing fails. In this case, the entire MapReduce job must be restarted. But only this one node can bring the entire process down; other failures will be managed by the Master, and the MapReduce job will complete eventually.
Suppose the compute node at which a Map worker resides fails. This fail- ure will be detected by the Master, because it periodically pings the Worker processes. All the Map tasks that were assigned to this Worker will have to be redone, even if they had completed. The reason for redoing completed Map
30 CHAPTER 2. MAPREDUCE AND THE NEW SOFTWARE STACK
tasks is that their output destined for the Reduce tasks resides at that compute node, and is now unavailable to the Reduce tasks. The Master sets the status of each of these Map tasks to idle and will schedule them on a Worker when one becomes available. The Master must also inform each Reduce task that the location of its input from that Map task has changed.
Dealing with a failure at the node of a Reduce worker is simpler. The Master simply sets the status of its currently executing Reduce tasks to idle. These will be rescheduled on another reduce worker later.
2.2.7 Exercises for Section 2.2
Exercise 2.2.1 : Suppose we execute the word-count MapReduce program de- scribed in this section on a large repository such as a copy of the Web. We shall use 100 Map tasks and some number of Reduce tasks.
(a)
(b)
! (c)
2.3
Suppose we do not use a combiner at the Map tasks. Do you expect there to be significant skew in the times taken by the various reducers to process their value list? Why or why not?
If we combine the reducers into a small number of Reduce tasks, say 10 tasks, at random, do you expect the skew to be significant? What if we instead combine the reducers into 10,000 Reduce tasks?
Suppose we do use a combiner at the 100 Map tasks. Do you expect skew to be significant? Why or why not?
Algorithms Using MapReduce
MapReduce is not a solution to every problem, not even every problem that profitably can use many compute nodes operating in parallel. As we mentioned in Section 2.1.2, the entire distributed-file-system milieu makes sense only when files are very large and are rarely updated in place. Thus, we would not expect to use either a DFS or an implementation of MapReduce for managing on- line retail sales, even though a large on-line retailer such as Amazon.com uses thousands of compute nodes when processing requests over the Web. The reason is that the principal operations on Amazon data involve responding to searches for products, recording sales, and so on, processes that involve relatively little calculation and that change the database.2 On the other hand, Amazon might use MapReduce to perform certain analytic queries on large amounts of data, such as finding for each user those users whose buying patterns were most similar.
The original purpose for which the Google implementation of MapReduce was created was to execute very large matrix-vector multiplications as are
2Remember that even looking at a product you don’t buy causes Amazon to remember that you looked at it.
 
2.3. ALGORITHMS USING MAPREDUCE 31
needed in the calculation of PageRank (See Chapter 5). We shall see that matrix-vector and matrix-matrix calculations fit nicely into the MapReduce style of computing. Another important class of operations that can use MapRe- duce effectively are the relational-algebra operations. We shall examine the MapReduce execution of these operations as well.
2.3.1 Matrix-Vector Multiplication by MapReduce
Suppose we have an n × n matrix M , whose element in row i and column j will be denoted mij. Suppose we also have a vector v of length n, whose jth element is vj. Then the matrix-vector product is the vector x of length n, whose ith element xi is given by
n
xi = mijvj j=1
If n = 100, we do not want to use a DFS or MapReduce for this calculation. But this sort of calculation is at the heart of the ranking of Web pages that goes on at search engines, and there, n is in the tens of billions.3 Let us first assume that n is large, but not so large that vector v cannot fit in main memory and thus be available to every Map task.
The matrix M and the vector v each will be stored in a file of the DFS. We assume that the row-column coordinates of each matrix element will be discov- erable, either from its position in the file, or because it is stored with explicit coordinates, as a triple (i, j, mij ). We also assume the position of element vj in the vector v will be discoverable in the analogous way.
The Map Function: The Map function is written to apply to one element of M. However, if v is not already read into main memory at the compute node executing a Map task, then v is first read, in its entirety, and subsequently will be available to all applications of the Map function performed at this Map task. Each Map task will operate on a chunk of the matrix M. From each matrix element mij it produces the key-value pair (i, mij vj ). Thus, all terms of the sum that make up the component xi of the matrix-vector product will get the same key, i.
The Reduce Function: The Reduce function simply sums all the values as- sociated with a given key i. The result will be a pair (i,xi).
2.3.2 If the Vector v Cannot Fit in Main Memory
However, it is possible that the vector v is so large that it will not fit in its entirety in main memory. It is not required that v fit in main memory at a compute node, but if it does not then there will be a very large number of
3The matrix is sparse, with on the average of 10 to 15 nonzero elements per row, since the matrix represents the links in the Web, with mij nonzero if and only if there is a link from page j to page i. Note that there is no way we could store a dense matrix whose side was 1010, since it would have 1020 elements.
 
32 CHAPTER 2. MAPREDUCE AND THE NEW SOFTWARE STACK
disk accesses as we move pieces of the vector into main memory to multiply components by elements of the matrix. Thus, as an alternative, we can divide the matrix into vertical stripes of equal width and divide the vector into an equal number of horizontal stripes, of the same height. Our goal is to use enough stripes so that the portion of the vector in one stripe can fit conveniently into main memory at a compute node. Figure 2.4 suggests what the partition looks like if the matrix and vector are each divided into five stripes.
          Matrix M Vector v
Figure 2.4: Division of a matrix and vector into five stripes
The ith stripe of the matrix multiplies only components from the ith stripe of the vector. Thus, we can divide the matrix into one file for each stripe, and do the same for the vector. Each Map task is assigned a chunk from one of the stripes of the matrix and gets the entire corresponding stripe of the vector. The Map and Reduce tasks can then act exactly as was described above for the case where Map tasks get the entire vector.
We shall take up matrix-vector multiplication using MapReduce again in Section 5.2. There, because of the particular application (PageRank calcula- tion), we have an additional constraint that the result vector should be part- itioned in the same way as the input vector, so the output may become the input for another iteration of the matrix-vector multiplication. We shall see there that the best strategy involves partitioning the matrix M into square blocks, rather than stripes.
2.3.3 Relational-Algebra Operations
There are a number of operations on large-scale data that are used in database queries. Many traditional database applications involve retrieval of small am- ounts of data, even though the database itself may be large. For example, a query may ask for the bank balance of one particular account. Such queries are not useful applications of MapReduce.
However, there are many operations on data that can be described easily in terms of the common database-query primitives, even if the queries themselves
2.3. ALGORITHMS USING MAPREDUCE 33
are not executed within a database management system. Thus, a good starting point for exploring applications of MapReduce is by considering the standard operations on relations. We assume you are familiar with database systems, the query language SQL, and the relational model, but to review, a relation is a table with column headers called attributes. Rows of the relation are called tuples. The set of attributes of a relation is called its schema. We often write an expression like R(A1, A2, . . . , An) to say that the relation name is R and its attributes are A1, A2, . . . , An.
From To
url1 url2 url1 url3 url2 url3 url2 url4 ··· ···
Figure 2.5: Relation Links consists of the set of pairs of URL’s, such that the first has one or more links to the second
Example 2.3 : In Fig. 2.5 we see part of the relation Links that describes the structure of the Web. There are two attributes, From and To. A row, or tuple, of the relation is a pair of URL’s, such that there is at least one link from the first URL to the second. For instance, the first row of Fig. 2.5 is the pair (url1,url2) that says the Web page url1 has a link to page url2. While we have shown only four tuples, the real relation of the Web, or the portion of it that would be stored by a typical search engine, has billions of tuples. ✷
A relation, however large, can be stored as a file in a distributed file system. The elements of this file are the tuples of the relation.
There are several standard operations on relations, often referred to as re- lational algebra, that are used to implement queries. The queries themselves usually are written in SQL. The relational-algebra operations we shall discuss are:
1. Selection: Apply a condition C to each tuple in the relation and produce as output only those tuples that satisfy C. The result of this selection is denoted σC(R).
2. Projection: For some subset S of the attributes of the relation, produce from each tuple only the components for the attributes in S. The result of this projection is denoted πS(R).
3. Union, Intersection, and Difference: These well-known set operations apply to the sets of tuples in two relations that have the same schema. There are also bag (multiset) versions of the operations in SQL, with
        
34
CHAPTER 2. MAPREDUCE AND THE NEW SOFTWARE STACK
somewhat unintuitive definitions, but we shall not go into the bag versions of these operations here.
4. Natural Join: Given two relations, compare each pair of tuples, one from each relation. If the tuples agree on all the attributes that are common to the two schemas, then produce a tuple that has components for each of the attributes in either schema and agrees with the two tuples on each attribute. If the tuples disagree on one or more shared attributes, then produce nothing from this pair of tuples. The natural join of relations R and S is denoted R ⊲⊳ S. While we shall discuss executing only the nat- ural join with MapReduce, all equijoins (joins where the tuple-agreement condition involves equality of attributes from the two relations that do not necessarily have the same name) can be executed in the same manner. We shall give an illustration in Example 2.4.
5. Grouping and Aggregation:4 Given a relation R, partition its tuples according to their values in one set of attributes G, called the grouping attributes. Then, for each group, aggregate the values in certain other at- tributes. The normally permitted aggregations are SUM, COUNT, AVG, MIN, and MAX, with the obvious meanings. Note that MIN and MAX require that the aggregated attributes have a type that can be compared, e.g., numbers or strings, while SUM and AVG require that the type allow arithmetic operations. We denote a grouping-and-aggregation operation on a relation R by γX (R), where X is a list of elements that are either
(a) A grouping attribute, or
(b) An expression θ(A), where θ is one of the five aggregation opera- tions such as SUM, and A is an attribute not among the grouping attributes.
The result of this operation is one tuple for each group. That tuple has a component for each of the grouping attributes, with the value common to tuples of that group. It also has a component for each aggregation, with the aggregated value for that group. We shall see an illustration in Example 2.5.
Example 2.4 : Let us try to find the paths of length two in the Web, using the relation Links of Fig. 2.5. That is, we want to find the triples of URL’s (u,v,w)suchthatthereisalinkfromutovandalinkfromvtow. We essentially want to take the natural join of Links with itself, but we first need to imagine that it is two relations, with different schemas, so we can describe the desired connection as a natural join. Thus, imagine that there are two copies of Links, namely L1(U1,U2) and L2(U2,U3). Now, if we compute L1 ⊲⊳ L2,
4Some descriptions of relational algebra do not include these operations, and indeed they were not part of the original definition of this algebra. However, these operations are so important in SQL, that modern treatments of relational algebra include them.
 
2.3. ALGORITHMS USING MAPREDUCE 35
we shall have exactly what we want. That is, for each tuple t1 of L1 (i.e., each tuple of Links) and each tuple t2 of L2 (another tuple of Links, possibly even the same tuple), see if their U2 components are the same. Note that these components are the second component of t1 and the first component of t2. If these two components agree, then produce a tuple for the result, with schema (U 1, U 2, U 3). This tuple consists of the first component of t1, the second component of t1 (which must equal the first component of t2), and the second component of t2.
We may not want the entire path of length two, but only want the pairs (u, w) of URL’s such that there is at least one path from u to w of length two. If so, we can project out the middle components by computing πU1,U3(L1 ⊲⊳ L2). ✷
Example 2.5 : Imagine that a social-networking site has a relation Friends(User, Friend)
This relation has tuples that are pairs (a, b) such that b is a friend of a. The site might want to develop statistics about the number of friends members have. Their first step would be to compute a count of the number of friends of each user. This operation can be done by grouping and aggregation, specifically
γUser,COUNT(Friend)(Friends)
This operation groups all the tuples by the value in their first component, so there is one group for each user. Then, for each group the count of the number of friends of that user is made. The result will be one tuple for each group, and a typical tuple would look like (Sally, 300), if user “Sally” has 300 friends. ✷
2.3.4 Computing Selections by MapReduce
Selections really do not need the full power of MapReduce. They can be done most conveniently in the map portion alone, although they could also be done in the reduce portion alone. Here is a MapReduce implementation of selection σC (R).
The Map Function: For each tuple t in R, test if it satisfies C. If so, produce the key-value pair (t, t). That is, both the key and value are t.
The Reduce Function: The Reduce function is the identity. It simply passes each key-value pair to the output.
Note that the output is not exactly a relation, because it has key-value pairs. However, a relation can be obtained by using only the value components (or only the key components) of the output.
36 CHAPTER 2. MAPREDUCE AND THE NEW SOFTWARE STACK 2.3.5 Computing Projections by MapReduce
Projection is performed similarly to selection, because projection may cause the same tuple to appear several times, the Reduce function must eliminate duplicates. We may compute πS (R) as follows.
The Map Function: For each tuple t in R, construct a tuple t′ by eliminating from t those components whose attributes are not in S. Output the key-value pair (t′, t′).
The Reduce Function: For each key t′ produced by any of the Map tasks, there will be one or more key-value pairs (t′,t′). The Reduce function turns (t′, [t′, t′, . . . , t′]) into (t′, t′), so it produces exactly one pair (t′, t′) for this key t′.
Observe that the Reduce operation is duplicate elimination. This operation is associative and commutative, so a combiner associated with each Map task can eliminate whatever duplicates are produced locally. However, the Reduce tasks are still needed to eliminate two identical tuples coming from different Map tasks.
2.3.6 Union, Intersection, and Difference by MapReduce
First, consider the union of two relations. Suppose relations R and S have the same schema. Map tasks will be assigned chunks from either R or S; it doesn’t matter which. The Map tasks don’t really do anything except pass their input tuples as key-value pairs to the Reduce tasks. The latter need only eliminate duplicates as for projection.
The Map Function: Turn each input tuple t into a key-value pair (t, t). The Reduce Function: Associated with each key t there will be either one or
two values. Produce output (t, t) in either case.
To compute the intersection, we can use the same Map function. However, the Reduce function must produce a tuple only if both relations have the tuple. If the key t has a list of two values [t,t] associated with it, then the Reduce task for t should produce (t, t). However, if the value-list associated with key t is just [t], then one of R and S is missing t, so we don’t want to produce a tuple for the intersection.
The Map Function: Turn each tuple t into a key-value pair (t, t).
The Reduce Function: If key t has value list [t, t], then produce (t, t). Oth-
erwise, produce nothing.
The Difference R − S requires a bit more thought. The only way a tuple t can appear in the output is if it is in R but not in S. The Map function can pass tuples from R and S through, but must inform the Reduce function whether the tuple came from R or S. We shall thus use the relation as the value associated with the key t. Here is a specification for the two functions.
2.3. ALGORITHMS USING MAPREDUCE 37
The Map Function: For a tuple t in R, produce key-value pair (t,R), and for a tuple t in S, produce key-value pair (t,S). Note that the intent is that the value is the name of R or S (or better, a single bit indicating whether the relation is R or S), not the entire relation.
The Reduce Function: For each key t, if the associated value list is [R], then produce (t, t). Otherwise, produce nothing.
2.3.7 Computing Natural Join by MapReduce
The idea behind implementing natural join via MapReduce can be seen if we look at the specific case of joining R(A,B) with S(B,C). We must find tuples that agree on their B components, that is the second component from tuples of R and the first component of tuples of S. We shall use the B-value of tuples from either relation as the key. The value will be the other component and the name of the relation, so the Reduce function can know where each tuple came from.
The Map Function: For each tuple (a,b) of R, produce the key-value pair  b, (R, a) . For each tuple (b, c) of S, produce the key-value pair  b, (S, c) .
The Reduce Function: Each key value b will be associated with a list of pairs that are either of the form (R, a) or (S, c). Construct all pairs consisting of one with first component R and the other with first component S, say (R,a) and (S, c). The output from this key and value list is a sequence of key-value pairs. The key is irrelevant. Each value is one of the triples (a, b, c) such that (R, a) and (S, c) are on the input list of values.
The same algorithm works if the relations have more than two attributes. You can think of A as representing all those attributes in the schema of R but not S. B represents the attributes in both schemas, and C represents attributes onlyintheschemaofS. ThekeyforatupleofRorSisthelistofvaluesinall the attributes that are in the schemas of both R and S. The value for a tuple of R is the name R together with the values of all the attributes belonging to R but not to S, and the value for a tuple of S is the name S together with the values of the attributes belonging to S but not R.
The Reduce function looks at all the key-value pairs with a given key and combines those values from R with those values of S in all possible ways. From each pairing, the tuple produced has the values from R, the key values, and the values from S.
2.3.8 Grouping and Aggregation by MapReduce
As with the join, we shall discuss the minimal example of grouping and aggrega- tion, where there is one grouping attribute and one aggregation. Let R(A, B, C) be a relation to which we apply the operator γA,θ(B)(R). Map will perform the grouping, while Reduce does the aggregation.
The Map Function: For each tuple (a, b, c) produce the key-value pair (a, b).
38 CHAPTER 2. MAPREDUCE AND THE NEW SOFTWARE STACK
The Reduce Function: Each key a represents a group. Apply the aggregation operator θ to the list [b1, b2, . . . , bn] of B-values associated with key a. The output is the pair (a,x), where x is the result of applying θ to the list. For example, if θ is SUM, then x = b1 +b2 +···+bn, and if θ is MAX, then x is the largest of b1, b2,...,bn.
If there are several grouping attributes, then the key is the list of the values of a tuple for all these attributes. If there is more than one aggregation, then the Reduce function applies each of them to the list of values associated with a given key and produces a tuple consisting of the key, including components for all grouping attributes if there is more than one, followed by the results of each of the aggregations.
2.3.9 Matrix Multiplication
If M is a matrix with element mij in row i and column j, and N is a matrix with element njk in row j and column k, then the product P = MN is the matrix P with element pik in row i and column k, where
pik = mijnjk j
It is required that the number of columns of M equals the number of rows of N, so the sum over j makes sense.
We can think of a matrix as a relation with three attributes: the row number, the column number, and the value in that row and column. Thus, we could view matrix M as a relation M(I,J,V), with tuples (i,j,mij), and we could view matrix N as a relation N(J,K,W), with tuples (j,k,njk). As large matrices are often sparse (mostly 0’s), and since we can omit the tuples for matrix elements that are 0, this relational representation is often a very good one for a large matrix. However, it is possible that i, j, and k are implicit in the position of a matrix element in the file that represents it, rather than written explicitly with the element itself. In that case, the Map function will have to be designed to construct the I, J, and K components of tuples from the position of the data.
The product MN is almost a natural join followed by grouping and ag- gregation. That is, the natural join of M(I,J,V) and N(J,K,W), having only attribute J in common, would produce tuples (i, j, k, v, w) from each tuple (i, j, v) in M and tuple (j, k, w) in N . This five-component tuple represents the pair of matrix elements (mij , njk ). What we want instead is the product of these elements, that is, the four-component tuple (i, j, k, v × w), because that represents the product mijnjk. Once we have this relation as the result of one MapReduce operation, we can perform grouping and aggregation, with I and K as the grouping attributes and the sum of V × W as the aggregation. That is, we can implement matrix multiplication as the cascade of two MapReduce operations, as follows. First:
The Map Function: For each matrix element mij, produce the key value pair  j,(M,i,mij) . Likewise, for each matrix element njk, produce the key value
2.3. ALGORITHMS USING MAPREDUCE 39
pair  j,(N,k,njk) . Note that M and N in the values are not the matrices themselves. Rather they are names of the matrices or (as we mentioned for the similar Map function used for natural join) better, a bit indicating whether the element comes from M or N.
The Reduce Function: For each key j, examine its list of associated values. For each value that comes from M, say (M,i,mij), and each value that comes from N, say (N, k, njk), produce a key-value pair with key equal to (i, k) and value equal to the product of these elements, mijnjk.
Now, we perform a grouping and aggregation by another MapReduce operation. The Map Function: This function is just the identity. That is, for every input
element with key (i, k) and value v, produce exactly this key-value pair.
The Reduce Function: For each key (i,k), produce the sum of the list of values associated with this key. The result is a pair  (i, k), v , where v is the value of the element in row i and column k of the matrix P = MN.
2.3.10 Matrix Multiplication with One MapReduce Step
There often is more than one way to use MapReduce to solve a problem. You may wish to use only a single MapReduce pass to perform matrix multiplication P = MN. 5 It is possible to do so if we put more work into the two functions. Start by using the Map function to create the sets of matrix elements that are needed to compute each element of the answer P. Notice that an element of M or N contributes to many elements of the result, so one input element will be turned into many key-value pairs. The keys will be pairs (i, k), where i is a row of M and k is a column of N. Here is a synopsis of the Map and Reduce functions.
The Map Function: For each element mij of M, produce all the key-value pairs  (i,k), (M,j,mij)  for k = 1,2,..., up to the number of columns of N. Similarly, for each element njk of N, produce all the key-value pairs  (i,k), (N,j,njk)  for i = 1,2,..., up to the number of rows of M. As be- fore, M and N are really bits to tell which of the two matrices a value comes from.
The Reduce Function: Each key (i,k) will have an associated list with all the values (M,j,mij) and (N,j,njk), for all possible values of j. The Reduce function needs to connect the two values on the list that have the same value of j, for each j. An easy way to do this step is to sort by j the values that begin with M and sort by j the values that begin with N, in separate lists. The jth values on each list must have their third components, mij and njk extracted and multiplied. Then, these products are summed and the result is paired with (i, k) in the output of the Reduce function.
5However, we show in Section 2.6.7 that two passes of MapReduce are usually better than one for matrix multiplication.
 
40 CHAPTER 2. MAPREDUCE AND THE NEW SOFTWARE STACK
You may notice that if a row of the matrix M or a column of the matrix N is so large that it will not fit in main memory, then the Reduce tasks will be forced to use an external sort to order the values associated with a given key (i, k). However, in that case, the matrices themselves are so large, perhaps 1020 elements, that it is unlikely we would attempt this calculation if the matrices were dense. If they are sparse, then we would expect many fewer values to be associated with any one key, and it would be feasible to do the sum of products in main memory.
2.3.11 Exercises for Section 2.3
Exercise 2.3.1: Design MapReduce algorithms to take a very large file of integers and produce as output:
(a) The largest integer.
(b) The average of all the integers.
(c) The same set of integers, but with each integer appearing only once. (d) The count of the number of distinct integers in the input.
Exercise 2.3.2 : Our formulation of matrix-vector multiplication assumed that the matrix M was square. Generalize the algorithm to the case where M is an r-by-c matrix for some number of rows r and columns c.
! Exercise 2.3.3: In the form of relational algebra implemented in SQL, rela- tions are not sets, but bags; that is, tuples are allowed to appear more than once. There are extended definitions of union, intersection, and difference for bags, which we shall define below. Write MapReduce algorithms for computing the following operations on bags R and S:
(a) Bag Union, defined to be the bag of tuples in which tuple t appears the sum of the numbers of times it appears in R and S.
(b) Bag Intersection, defined to be the bag of tuples in which tuple t appears the minimum of the numbers of times it appears in R and S.
(c) Bag Difference, defined to be the bag of tuples in which the number of times a tuple t appears is equal to the number of times it appears in R minus the number of times it appears in S. A tuple that appears more times in S than in R does not appear in the difference.
! Exercise 2.3.4 : Selection can also be performed on bags. Give a MapReduce implementation that produces the proper number of copies of each tuple t that passes the selection condition. That is, produce key-value pairs from which the correct result of the selection can be obtained easily from the values.
2.4. EXTENSIONS TO MAPREDUCE 41
Exercise 2.3.5 : The relational-algebra operation R(A, B) ⊲⊳ B<C S(C, D) produces all tuples (a, b, c, d) such that tuple (a, b) is in relation R, tuple (c, d) is in S, and b < c. Give a MapReduce implementation of this operation, assuming R and S are sets.
2.4 Extensions to MapReduce
MapReduce has proved so influential that it has spawned a number of extensions and modifications. These systems typically share a number of characteristics with MapReduce systems:
1. They are built on a distributed file system.
2. They manage very large numbers of tasks that are instantiations of a small number of user-written functions.
3. They incorporate a method for dealing with most of the failures that occur during the execution of a large job, without having to restart that job from the beginning.
In this section, we shall mention some of the interesting directions being ex- plored. References to the details of the systems mentioned can be found in the bibliographic notes for this chapter.
2.4.1 Workflow Systems
Two experimental systems called Clustera from the University of Wisconsin and Hyracks from the University of California at Irvine extend MapReduce from the simple two-step workflow (the Map function feeds the Reduce function) to any collection of functions, with an acyclic graph representing workflow among the functions. That is, there is an acyclic flow graph whose arcs a → b represent the fact that function a’s output is input to function b. A suggestion of what a workflow might look like is in Fig. 2.6. There, five functions, f through j, pass data from left to right in specific ways, so the flow of data is acyclic and no task needs to provide data out before its input is available. For instance, function h takes its input from a preexisting file of the distributed file system. Each of h’s output elements is passed to at least one of the functions i and j.
In analogy to Map and Reduce functions, each function of a workflow can be executed by many tasks, each of which is assigned a portion of the input to the function. A master controller is responsible for dividing the work among the tasks that implement a function, usually by hashing the input elements to decide on the proper task to receive an element. Thus, like Map tasks, each task implementing a function f has an output file of data destined for each of the tasks that implement the successor function(s) of f. These files are delivered by the Master at the appropriate time – after the task has completed its work.
42 CHAPTER 2. MAPREDUCE AND THE NEW SOFTWARE STACK
Figure 2.6: An example of a workflow that is more complex than Map feeding Reduce
The functions of a workflow, and therefore the tasks, share with MapReduce tasks the important property that they only deliver output after they complete. As a result, if a task fails, it has not delivered output to any of its successors in the flow graph. A master controller can therefore restart the failed task at another compute node, without worrying that the output of the restarted task will duplicate output that previously was passed to some other task.
Many applications of workflow systems such as Clustera or Hyracks are cascades of MapReduce jobs. An example would be the join of three relations, where one MapReduce job joins the first two relations, and a second MapReduce job joins the third relation with the result of joining the first two relations. Both jobs would use an algorithm like that of Section 2.3.7.
There is an advantage to implementing such cascades as a single workflow. For example, the flow of data among tasks, and its replication, can be managed by the master controller, without need to store the temporary file that is out- put of one MapReduce job in the distributed file system. By locating tasks at compute nodes that have a copy of their input, we can avoid much of the com- munication that would be necessary if we stored the result of one MapReduce job and then initiated a second MapReduce job (although Hadoop and other MapReduce systems also try to locate Map tasks where a copy of their input is already present).
2.4.2 Recursive Extensions to MapReduce
Many large-scale computations are really recursions. An important example is PageRank, which is the subject of Chapter 5. That computation is, in sim- ple terms, the computation of the fixedpoint of a matrix-vector multiplication. It is computed under MapReduce systems by the iterated application of the matrix-vector multiplication algorithm described in Section 2.3.1, or by a more complex strategy that we shall introduce in Section 5.2. The iteration typi- cally continues for an unknown number of steps, each step being a MapReduce job, until the results of two consecutive iterations are sufficiently close that we believe convergence has occurred.
The reason recursions are normally implemented by iterated MapReduce
  f
g
         j
        h
i
    
2.4. EXTENSIONS TO MAPREDUCE 43
jobs is that a true recursive task does not have the property necessary for independent restart of failed tasks. It is impossible for a collection of mutually recursive tasks, each of which has an output that is input to at least some of the other tasks, to produce output only at the end of the task. If they all followed that policy, no task would ever receive any input, and nothing could be accomplished. As a result, some mechanism other than simple restart of failed tasks must be implemented in a system that handles recursive workflows (flow graphs that are not acyclic). We shall start by studying an example of a recursion implemented as a workflow, and then discuss approaches to dealing with task failures.
Example 2.6 : Suppose we have a directed graph whose arcs are represented by the relation E(X, Y ), meaning that there is an arc from node X to node Y . We wish to compute the paths relation P (X, Y ), meaning that there is a path of length 1 or more from node X to node Y . That is, P is the transitive closure of E. A simple recursive algorithm to do so is:
1. StartwithP(X,Y)=E(X,Y).
2. While changes to the relation P occur, add to P all tuples in
π X , Y   P ( X , Z ) ⊲⊳ P ( Z , Y )  
That is, find pairs of nodes X and Y such that for some node Z there is
known to be a path from X to Z and also a path from Z to Y .
Figure 2.7 suggests how we could organize recursive tasks to perform this computation. There are two kinds of tasks: Join tasks and Dup-elim tasks. There are n Join tasks, for some n, and each corresponds to a bucket of a hash function h. A path tuple P(a,b), when it is discovered, becomes input to two Join tasks: those numbered h(a) and h(b). The job of the ith Join task, when it receives input tuple P(a,b), is to find certain other tuples seen previously (and stored locally by that task).
1. Store P (a, b) locally.
2. If h(a) = i then look for tuples P (x, a) and produce output tuple P (x, b). 3. If h(b) = i then look for tuples P (b, y) and produce output tuple P (a, y).
Note that in rare cases, we have h(a) = h(b), so both (2) and (3) are executed. But generally, only one of these needs to be executed for a given tuple.
There are also m Dup-elim tasks, and each corresponds to a bucket of a hash function g that takes two arguments. If P (c, d) is an output of some Join task, then it is sent to Dup-elim task j = g(c,d). On receiving this tuple, the jth Dup-elim task checks that it had not received it before, since its job is duplicate elimination. If previously received, the tuple is ignored. But if this tuple is new, it is stored locally and sent to two Join tasks, those numbered h(c) and h(d).
44
CHAPTER 2.
MAPREDUCE AND THE NEW SOFTWARE STACK
  Join task 0
 Join task 1
  . . .
. . .
P(c,d) if g(c,d) = j
. . .
. . .
To join task h(c)
P(c,d) if never seen before
   P(a,b) if h(a) = i or h(b) = i
   Figure 2.7: Implementation of transitive closure by a collection of recursive tasks
Every Join task has m output files – one for each Dup-elim task – and every Dup-elim task has n output files – one for each Join task. These files may be distributed according to any of several strategies. Initially, the E(a,b) tuples representing the arcs of the graph are distributed to the Dup-elim tasks, with E(a, b) being sent as P (a, b) to Dup-elim task g(a, b). The Master can wait until each Join task has processed its entire input for a round. Then, all output files are distributed to the Dup-elim tasks, which create their own output. That output is distributed to the Join tasks and becomes their input for the next round. Alternatively, each task can wait until it has produced enough output to justify transmitting its output files to their destination, even if the task has not consumed all its input. ✷
In Example 2.6 it is not essential to have two kinds of tasks. Rather, Join tasks could eliminate duplicates as they are received, since they must store their previously received inputs anyway. However, this arrangement has an advantage when we must recover from a task failure. If each task stores all the output files it has ever created, and we place Join tasks on different racks from the Dup-elim tasks, then we can deal with any single compute node or
Dup−elim task
0
 Dup−elim task
1
 Dup−elim task
j
 Join task i
To join task h(d)
2.4. EXTENSIONS TO MAPREDUCE 45
single rack failure. That is, a Join task needing to be restarted can get all the previously generated inputs that it needs from the Dup-elim tasks, and vice versa.
In the particular case of computing transitive closure, it is not necessary to prevent a restarted task from generating outputs that the original task gener- ated previously. In the computation of the transitive closure, the rediscovery of a path does not influence the eventual answer. However, many computations cannot tolerate a situation where both the original and restarted versions of a task pass the same output to another task. For example, if the final step of the computation were an aggregation, say a count of the number of nodes reached by each node in the graph, then we would get the wrong answer if we counted a path twice. In such a case, the master controller can record what files each task generated and passed to other tasks. It can then restart a failed task and ignore those files when the restarted version produces them a second time.
2.4.3 Pregel
Another approach to managing failures when implementing recursive algorithms on a computing cluster is represented by the Pregel system. This system views its data as a graph. Each node of the graph corresponds roughly to a task (although in practice many nodes of a large graph would be bundled into a single task, as in the Join tasks of Example 2.6). Each graph node generates output messages that are destined for other nodes of the graph, and each graph node processes the inputs it receives from other nodes.
Example 2.7: Suppose our data is a collection of weighted arcs of a graph, and we want to find, for each node of the graph, the length of the shortest path to each of the other nodes. Initially, each graph node a stores the set of pairs (b, w) such that there is an arc from a to b of weight w. These facts are initially sent to all other nodes, as triples (a, b, w).6 When the node a receives a triple (c, d, w), it looks up its current distance to c; that is, it finds the pair (c, v) stored locally, if there is one. It also finds the pair (d, u) if there is one. If w+v < u, then the pair (d,u) is replaced by (d,w+v), and if there was no pair (d, u), then the pair (d, w + v) is stored at the node a. Also, the other nodes are sent the message (a, d, w + v) in either of these two cases. ✷
6This algorithm uses much too much communication, but it will serve as a simple example of the Pregel computation model.
   Pregel and Giraph
Like MapReduce, Pregel was developed originally at Google. Also like MapReduce, there is an Apache, open-source equivalent, called Giraph.
  
46 CHAPTER 2. MAPREDUCE AND THE NEW SOFTWARE STACK
Computations in Pregel are organized into supersteps. In one superstep, all the messages that were received by any of the nodes at the previous superstep (or initially, if it is the first superstep) are processed, and then all the messages generated by those nodes are sent to their destination.
In case of a compute-node failure, there is no attempt to restart the failed tasks at that compute node. Rather, Pregel checkpoints its entire computation after some of the supersteps. A checkpoint consists of making a copy of the entire state of each task, so it can be restarted from that point if necessary. If any compute node fails, the entire job is restarted from the most recent checkpoint.
Although this recovery strategy causes many tasks that have not failed to redo their work, it is satisfactory in many situations. Recall that the reason MapReduce systems support restart of only the failed tasks is that we want assurance that the expected time to complete the entire job in the face of fail- ures is not too much greater than the time to run the job with no failures. Any failure-management system will have that property as long as the time to recover from a failure is much less than the average time between failures. Thus, it is only necessary that Pregel checkpoints its computation after a num- ber of supersteps such that the probability of a failure during that number of supersteps is low.
2.4.4 Exercises for Section 2.4
! Exercise 2.4.1 : Suppose a job consists of n tasks, each of which takes time t seconds. Thus, if there are no failures, the sum over all compute nodes of the time taken to execute tasks at that node is nt. Suppose also that the probability of a task failing is p per job per second, and when a task fails, the overhead of management of the restart is such that it adds 10t seconds to the total execution time of the job. What is the total expected execution time of the job?
! Exercise 2.4.2: Suppose a Pregel job has a probability p of a failure during any superstep. Suppose also that the execution time (summed over all compute nodes) of taking a checkpoint is c times the time it takes to execute a superstep. To minimize the expected execution time of the job, how many supersteps should elapse between checkpoints?
2.5 The Communication Cost Model
In this section we shall introduce a model for measuring the quality of algorithms implemented on a computing cluster of the type so far discussed in this chapter. We assume the computation is described by an acyclic workflow, as discussed in Section 2.4.1. For many applications, the bottleneck is moving data among tasks, such as transporting the outputs of Map tasks to their proper Reduce tasks. As an example, we explore the computation of multiway joins as single
2.5. THE COMMUNICATION COST MODEL 47 MapReduce jobs, and we see that in some situations, this approach is more
efficient than the straightforward cascade of 2-way joins.
2.5.1 Communication-Cost for Task Networks
Imagine that an algorithm is implemented by an acyclic network of tasks. These tasks could be Map tasks feeding Reduce tasks, as in a standard MapReduce algorithm, or they could be several MapReduce jobs cascaded, or a more general workflow structure, such as a collection of tasks each of which implements the workflowofFig.2.6.7 Thecommunicationcostofataskisthesizeoftheinput to the task. This size can be measured in bytes. However, since we shall be using relational database operations as examples, we shall often use the number of tuples as a measure of size.
The communication cost of an algorithm is the sum of the communication cost of all the tasks implementing that algorithm. We shall focus on the commu- nication cost as the way to measure the efficiency of an algorithm. In particular, we do not consider the amount of time it takes each task to execute when es- timating the running time of an algorithm. While there are exceptions, where execution time of tasks dominates, these exceptions are rare in practice. We can explain and justify the importance of communication cost as follows.
• The algorithm executed by each task tends to be very simple, often linear in the size of its input.
• The typical interconnect speed for a computing cluster is one gigabit per second. That may seem like a lot, but it is slow compared with the speed at which a processor executes instructions. Moreover, in many cluster architectures, there is competition for the interconnect when several com- pute nodes need to communicate at the same time. As a result, the compute node can do a lot of work on a received input element in the time it takes to deliver that element.
• Even if a task executes at a compute node that has a copy of the chunk(s) on which the task operates, that chunk normally will be stored on disk, and the time taken to move the data into main memory may exceed the time needed to operate on the data once it is available in memory.
Assuming that communication cost is the dominant cost, we might still ask why we count only input size, and not output size. The answer to this question involves two points:
1. If the output of one task τ is input to another task, then the size of τ’s output will be accounted for when measuring the input size for the receiv- ing task. Thus, there is no reason to count the size of any output except for those tasks whose output forms the result of the entire algorithm.
7Recall that this figure represented functions, not tasks. As a network of tasks, there would be, for example, many tasks implementing function f, each of which feeds data to each of the tasks for function g and each of the tasks for function i.
 
48 2.
CHAPTER 2. MAPREDUCE AND THE NEW SOFTWARE STACK
But in practice, the algorithm output is rarely large compared with the input or the intermediate data produced by the algorithm. The reason is that massive outputs cannot be used unless they are summarized or aggregated in some way. For example, although we talked in Example 2.6 of computing the entire transitive closure of a graph, in practice we would want something much simpler, such as the count of the number of nodes reachable from each node, or the set of nodes reachable from a single node.
Example 2.8 : Let us evaluate the communication cost for the join algorithm from Section 2.3.7. Suppose we are joining R(A,B) ⊲⊳ S(B,C), and the sizes of relations R and S are r and s, respectively. Each chunk of the files holding R and S is fed to one Map task, so the sum of the communication costs for all the Map tasks is r + s. Note that in a typical execution, the Map tasks will each be executed at a compute node holding a copy of the chunk to which it applies. Thus, no internode communication is needed for the Map tasks, but they still must read their data from disk. Since all the Map tasks do is make a simple transformation of each input tuple into a key-value pair, we expect that the computation cost will be small compared with the communication cost, regardless of whether the input is local to the task or must be transported to its compute node.
The sum of the outputs of the Map tasks is roughly as large as their in- puts. Each output key-value pair is sent to exactly one Reduce task, and it is unlikely that this Reduce task will execute at the same compute node. There- fore, communication from Map tasks to Reduce tasks is likely to be across the interconnect of the cluster, rather than memory-to-disk. This communication is O(r + s), so the communication cost of the join algorithm is O(r + s).
The Reduce tasks execute the reducer (application of the Reduce function to a key and its associated value list) for one or more values of attribute B. Each reducer takes the inputs it receives and divides them between tuples that came from R and those that came from S. Each tuple from R pairs with each tuple from S to produce one output. The output size for the join can be either larger or smaller than r + s, depending on how likely it is that a given R-tuple joins with a given S-tuple. For example, if there are many different B-values, we would expect the output to be small, while if there are few B-values, a large output is likely.
If the output is large, then the computation cost of generating all the outputs from a reducer could be much larger than O(r+s). However, we shall rely on our supposition that if the output of the join is large, then there is probably some aggregation being done to reduce the size of the output. It will be necessary to communicate the result of the join to another collection of tasks that perform this aggregation, and thus the communication cost will be at least proportional to the computation needed to produce the output of the join. ✷
2.5. THE COMMUNICATION COST MODEL 49 2.5.2 Wall-Clock Time
While communication cost often influences our choice of algorithm to use in a cluster-computing environment, we must also be aware of the importance of wall-clock time, the time it takes a parallel algorithm to finish. Using careless reasoning, one could minimize total communication cost by assigning all the work to one task, and thereby minimize total communication. However, the wall-clock time of such an algorithm would be quite high. The algorithms we suggest, or have suggested so far, have the property that the work is divided fairly among the tasks. Therefore, the wall-clock time would be approximately as small as it could be, given the number of compute nodes available.
2.5.3 Multiway Joins
To see how analyzing the communication cost can help us choose an algorithm in the cluster-computing environment, we shall examine carefully the case of a multiway join. There is a general theory in which we:
1. Select certain attributes of the relations involved in the natural join of three or more relations to have their values hashed, each to some number of buckets.
2. Select the number of buckets for each of these attributes, subject to the constraint that the product of the numbers of buckets for each attribute is k, the number of reducers that will be used.
3. Identify each of the k reducers with a vector of bucket numbers. These vectors have one component for each of the attributes selected at step (1).
4. Send tuples of each relation to all those reducers where it might find tuples to join with. That is, the given tuple t will have values for some of the attributes selected at step (1), so we can apply the hash function(s) to those values to determine certain components of the vector that identifies the reducers. Other components of the vector are unknown, so t must be sent to reducers for all vectors having any value in these unknown components.
Some examples of this general technique appear in the exercises.
Here, we shall look only at the join R(A,B) ⊲⊳ S(B,C) ⊲⊳ T(C,D) as an example. Suppose that the relations R, S, and T have sizes r, s, and t,
respectively, and for simplicity, suppose p is the probability that
1. An R-tuple and and S-tuple agree on B, and also the probability that 2. An S-tuple and a T-tuple agree on C.
If we join R and S first, using the MapReduce algorithm of Section 2.3.7, then the communication cost is O(r + s), and the size of the intermediate join
50 CHAPTER 2. MAPREDUCE AND THE NEW SOFTWARE STACK
R ⊲⊳ S is prs. When we join this result with T, the communication of this second MapReduce job is O(t + prs). Thus, the entire communication cost of the algorithm consisting of two 2-way joins is O(r + s + t + prs). If we instead join S and T first, and then join R with the result, we get another algorithm whose communication cost is O(r + s + t + pst).
A third way to take this join is to use a single MapReduce job that joins the three relations at once. Suppose that we plan to use k reducers for this job. Pick numbers b and c representing the number of buckets into which we shall hash B- and C-values, respectively. Let h be a hash function that sends B-values into b buckets, and let g be another hash function that sends C-values into c buckets. We require that bc = k; that is, each reducer corresponds to a pair of buckets, one for the B-value and one for the C-value. The reducer corresponding to bucket pair (i, j) is responsible for joining the tuples R(u, v), S(v, w), and T (w, x) whenever h(v) = i and g(w) = j.
As a result, the Map tasks that send tuples of R, S, and T to the reducers that need them must send R- and T -tuples to more than one reducer. For an S-tuple S(v, w), we know the B- and C-values, so we can send this tuple only to the reducer for  h(v), g(w) . However, consider an R-tuple R(u, v). We know it only needs to go to reducers that correspond to  h(v),y , for some y. But we don’t know y; the value of C could be anything as far as we know. Thus, we must send R(u,v) to c reducers, since y could be any of the c buckets for C-values. Similarly, we must send the T -tuple T (w, x) to each of the reducers  z, g(w)  for any z. There are b such reducers.
 g(T.C) = 1
g(C) = 0123
0
1 h(B) =
2
h(R.B) = 2
3
h(S.B) = 2 and g(S.C) = 1
                Figure 2.8: Sixteen reducers together perform a 3-way join
Example 2.9 : Suppose that b = c = 4, so k = 16. The sixteen reducers can be thought of as arranged in a rectangle, as suggested by Fig. 2.8. There, we see a hypothetical S-tuple S(v,w) for which h(v) = 2 and g(w) = 1. This tuple is sent by its Map task only to the reducer for key (2,1). We also see
2.5. THE COMMUNICATION COST MODEL 51
   Computation Cost of the 3-Way Join
Each of the reducers must join of parts of the three relations, and it is reasonable to ask whether this join can be taken in time that is linear in the size of the input to that Reduce task. While more complex joins might not be computable in linear time, the join of our running example can be executed at each Reduce process efficiently. First, create an index on R.B, to organize the R-tuples received. Likewise, create an index on T.C for the T-tuples. Then, consider each received S-tuple, S(v,w). Use the index on R.B to find all R-tuples with R.B = v and use the index on T.C to find all T-tuples with T.C = w.
 an R-tuple R(u, v). Since h(v) = 2, this tuple is sent to all reducers (2, y), for y = 1,2,3,4. Finally, we see a T-tuple T(w,x). Since g(w) = 1, this tuple is sent to all reducers (z, 1) for z = 1, 2, 3, 4. Notice that these three tuples join, and they meet at exactly one reducer, the reducer for key (2, 1). ✷
Now, suppose that the sizes of R, S, and T are different; recall we use r, s, and t, respectively, for those sizes. If we hash B-values to b buckets and C-values to c buckets, where bc = k, then the total communication cost for moving the tuples to the proper reducers is the sum of:
1. s to move each tuple S(v, w) once to the reducer  h(v), g(w) .
2. cr to move each tuple R(u, v) to the c reducers  h(v), y  for each of the c
possible values of y.
3. bt to move each tuple T (w, x) to the b reducers  z, g(w)  for each of the
b possible values of z.
There is also a cost r + s + t to make each tuple of each relation be input to one of the Map tasks. This cost is fixed, independent of b, c, and k.
We must select b and c, subject to the constraint bc = k, to minimize s + cr + bt. We shall use the technique of Lagrangean multipliers to find the place where the function s + cr + bt − λ(bc − k) has its derivatives with respect to b and c equal to 0. That is, we must solve the equations r−λb = 0 and t − λc = 0. Since r = λb and t = λc, we may multiply corresponding sides of these equations to get rt = λ2bc. Since bc = k, we get rt = λ2k, or λ =  rt/k. Thus, the minimum communication cost is obtained when c = t/λ =  kt/r, and b = r/λ =  kr/t. √
If we substitute these values into the formula s + cr + bt, we get s + 2 krt. That is the communication cost for the Reduce tasks, to which we must add the cost s + r + t for the communication cost of the Map tasks. The total
    
52 CHAPTER 2. MAPREDUCE AND THE NEW SOFTWARE STACK communication cost is thus r + 2s + t + 2√krt. In most circumstances, we can
 neglect r + t, because it will be less than 2√krt, usually by a factor of O(√k).
  Example 2.10 : Let us see under what circumstances the 3-way join has lower communication cost than the cascade of two 2-way joins. To make matters simple, let us assume that R, S, and T are all the same relation R, which represents the “friends” relation in a social network like Facebook. There are roughly a billion subscribers on Facebook, with an average of 300 friends each, so relation R has r = 3 × 1011 tuples. Suppose we want to compute R ⊲⊳ R ⊲⊳ R, perhaps as part of a calculation to find the number of friends of friends of friends each subscriber has, or perhaps just the person with the largest number of friends of friends of friends.8 The cost of the 3-way join of R with itself is 4r + 2r√k; 3r represents the cost of the Map tasks, and r + 2√kr2 is the cost of the Reduce tasks. Since we assume r = 3×1011, this cost is 1,2×1012 +6× 1011√k.
Now consider the communication cost of joining R with itself, and then joining the result with R again. The Map and Reduce tasks for the first join each have a cost of 2r, so the first join only has communication cost 4r = 1.2 × 1012. But the size of R ⊲⊳ R is large. We cannot say exactly how large, since friends tend to fall into cliques, and therefore a person with 300 friends will have many fewer than the maximum possible number of friends of friends, which is 90,000. Let us estimate conservatively that the size of R ⊲⊳ R is not 300r, but only 30r, or 9 × 1012. The communication cost for the second join of (R ⊲⊳ R) ⊲⊳ R is thus 1.8 × 1013 + 6 × 1011. The total cost of the two joins is therefore 1.2×1012 +1.8×1013 +6×1011 = 1.98×1013.
We must ask whether the cost of the 3-way join, which is 1.2 × 1012 + 6 × 1011√k
is less than 1.98 × 1013. That is so, provided 6 × 1011√k < 1.86 × 1013, or √k < 31. That is, the 3-way join will be preferable provided we use no more than 312 = 961 reducers. ✷
2.5.4 Exercises for Section 2.5
Exercise 2.5.1: What is the communication cost of each of the following algorithms, as a function of the size of the relations, matrices, or vectors to which they are applied?
(a) The matrix-vector multiplication algorithm of Section 2.3.2. (b) The union algorithm of Section 2.3.6.
(c) The aggregation algorithm of Section 2.3.8.
8This person, or more generally, people with large extended circles of friends, are good people to use to start a marketing campaign by giving them free samples.
       
2.5. THE COMMUNICATION COST MODEL 53
   Star Joins
A common structure for data mining of commercial data is the star join. For example, a chain store like Walmart keeps a fact table whose tu- ples each represent a single sale. This relation looks like F(A1,A2,...), where each attribute Ai is a key representing one of the important com- ponents of the sale, such as the purchaser, the item purchased, the store branch, or the date. For each key attribute there is a dimension table giving information about the participant. For instance, the dimension ta- ble D(A1,B11,B12,...) might represent purchasers. A1 is the purchaser ID, the key for this relation. The B1i’s might give the purchaser’s name, address, phone, and so on. Typically, the fact table is much larger than the dimension tables. For instance, there might be a fact table of a billion tuples and ten dimension tables of a million tuples each.
Analysts mine this data by asking analytic queries that typically join the fact table with several of the dimension tables (a “star join”) and then aggregate the result into a useful form. For instance, an analyst might ask “give me a table of sales of pants, broken down by region and color, for each month of 2012.” Under the communication-cost model of this section, joining the fact table and dimension tables by a multiway join is almost certain to be more efficient than joining the relations in pairs. In fact, it may make sense to store the fact table over however many compute nodes are available, and replicate the dimension tables permanently in exactly the same way as we would replicate them should we take the join of the fact table and all the dimension tables. In this special case, only the key attributes (the A’s above) are hashed to buckets, and the number of buckets for each key attribute is proportional to the size of its dimension table.
 (d) The matrix-multiplication algorithm of Section 2.3.10.
! Exercise 2.5.2: Suppose relations R, S, and T have sizes r, s, and t, respec- tively, and we want to take the 3-way join R(A,B) ⊲⊳ S(B,C) ⊲⊳ T(A,C), using k reducers. We shall hash values of attributes A, B, and C to a, b, and c buckets, respectively, where abc = k. Each reducer is associated with a vector of buckets, one for each of the three hash functions. Find, as a function of r, s, t, and k, the values of a, b, and c that minimize the communication cost of the algorithm.
! Exercise 2.5.3: Suppose we take a star join of a fact table F(A1,A2,...,Am) with dimension tables Di(Ai,Bi) for i = 1,2,...,m. Let there be k reducers, each associated with a vector of buckets, one for each of the key attributes A1,A2,...,Am. Suppose the number of buckets into which we hash Ai is ai.
54 CHAPTER 2. MAPREDUCE AND THE NEW SOFTWARE STACK
Naturally, a1a2 · · · am = k. Finally, suppose each dimension table Di has size di, and the size of the fact table is much larger than any of these sizes. Find the values of the ai’s that minimize the cost of taking the star join as one MapReduce operation.
2.6 Complexity Theory for MapReduce
Now, we shall explore the design of MapReduce algorithms in more detail. Sec- tion 2.5 introduced the idea that communication between the Map and Reduce tasks often accounts for the largest fraction of the time spent by these tasks. Here, we shall look at how the communication cost relates to other desiderata for MapReduce algorithms, in particular our desire to shrink the wall-clock time and to execute each reducer in main memory. Recall that a “reducer” is the execution of the Reduce function on a single key and its associated value list. The point of the exploration in this section is that for many problems there is a spectrum of MapReduce algorithms requiring different amounts of communica- tion. Moreover, the less communication an algorithm uses, the worse it may be in other respects, including wall-clock time and the amount of main memory it requires.
2.6.1 Reducer Size and Replication Rate
Let us now introduce the two parameters that characterize families of MapRe- duce algorithms. The first is the reducer size, which we denote by q. This parameter is the upper bound on the number of values that are allowed to ap- pear in the list associated with a single key. Reducer size can be selected with at least two goals in mind.
1. By making the reducer size small, we can force there to be many reducers, i.e., many different keys according to which the problem input is divided by the Map tasks. If we also create many Reduce tasks – even one for each reducer – then there will be a high degree of parallelism, and we can look forward to a low wall-clock time.
2. We can choose a reducer size sufficiently small that we are certain the computation associated with a single reducer can be executed entirely in the main memory of the compute node where its Reduce task is located. Regardless of the computation done by the reducers, the running time will be greatly reduced if we can avoid having to move data repeatedly between main memory and disk.
The second parameter is the replication rate, denoted r. We define r to be the number of key-value pairs produced by all the Map tasks on all the inputs, divided by the number of inputs. That is, the replication rate is the average communication from Map tasks to Reduce tasks (measured by counting key-value pairs) per input.
2.6. COMPLEXITY THEORY FOR MAPREDUCE 55
Example 2.11 : Let us consider the one-pass matrix-multiplication algorithm of Section 2.3.10. Suppose that all the matrices involved are n × n matrices. Then the replication rate r is equal to n. That fact is easy to see, since for each element mij, there are n key-value pairs produced; these have all keys of the form (i, k), for 1 ≤ k ≤ n. Likewise, for each element of the other matrix, say njk, we produce n key-value pairs, each having one of the keys (i,k), for 1 ≤ i ≤ n. In this case, not only is n the average number of key-value pairs produced for an input element, but each input produces exactly this number of pairs.
We also see that q, the required reducer size, is 2n. That is, for each key (i, k), there are n key-value pairs representing elements mij of the first matrix and another n key-value pairs derived from the elements njk of the second matrix. While this pair of values represents only one particular algorithm for one-pass matrix multiplication, we shall see that it is part of a spectrum of algorithms, and in fact represents an extreme point, where q is as small as can be, and r is at its maximum. More generally, there is a tradeoff between r and q, that can be expressed as qr ≥ 2n2. ✷
2.6.2 An Example: Similarity Joins
To see the tradeoff between r and q in a realistic situation, we shall examine a problem known as similarity join. In this problem, we are given a large set of elements X and a similarity measure s(x, y) that tells how similar two elements x and y of set X are. In Chapter 3 we shall learn about the most important notions of similarity and also learn some tricks that let us find similar pairs quickly. But here, we shall consider only the raw form of the problem, where we have to look at each pair of elements of X and determine their similarity by applying the function s. We assume that s is symmetric, so s(x, y) = s(y, x), but we assume nothing else about s. The output of the algorithm is those pairs whose similarity exceeds a given threshold t.
For example, let us suppose we have a collection of one million images, each of size one megabyte. Thus, the dataset has size one terabyte. We shall not try to describe the similarity function s, but it might, say, involve giving higher values when images have roughly the same distribution of colors or when images have corresponding regions with the same distribution of colors. The goal would be to discover pairs of images that show the same type of object or scene. This problem is extremely hard, but classifying by color distribution is generally of some help toward that goal.
Let us look at how we might do the computation using MapReduce to exploit the natural parallelism found in this problem. The input is key-value pairs (i,Pi), where i is an ID for the picture and Pi is the picture itself. We want to compare each pair of pictures, so let us use one key for each set of two ID’s {i, j}. There are approximately 5 × 1011 pairs of two ID’s. We want each key {i,j} to be associated with the two values Pi and Pj, so the input to the corresponding reducer will be ({i,j}, [Pi,Pj]). Then, the Reduce function can
56 CHAPTER 2. MAPREDUCE AND THE NEW SOFTWARE STACK
simply apply the similarity function s to the two pictures on its value list, that is, compute s(Pi,Pj), and decide whether the similarity of the two pictures is above threshold. The pair would be output if so.
Alas, this algorithm will fail completely. The reducer size is small, since no list has more than two values, or a total of 2MB of input. Although we don’t know exactly how the similarity function s operates, we can reasonably expect that it will not require more than the available main memory. However, the replication rate is 999,999, since for each picture we generate that number of key-value pairs, one for each of the other pictures in the dataset. The total number of bytes communicated from Map tasks to Reduce tasks is 1,000,000 (for the pictures) times 999,999 (for the replication), times 1,000,000 (for the size of each picture). That’s 1018 bytes, or one exabyte. To communicate this amount of data over gigabit Ethernet would take 1010 seconds, or about 300 years.9
Fortunately, this algorithm is only the extreme point in a spectrum of possi- ble algorithms. We can characterize these algorithms by grouping pictures into g groups, each of 106/g pictures.
The Map Function: Take an input element (i, Pi) and generate g − 1 key- value pairs. For each, the key is one of the sets {u, v}, where u is the group to which picture i belongs, and v is one of the other groups. The associated value is the pair (i,Pi).
The Reduce Function: Consider the key {u,v}. The associated value list will have the 2 × 106/g elements (j, Pj ), where j belongs to either group u or group v. The Reduce function takes each (i, Pi) and (j, Pj ) on this list, where i and j belong to different groups, and applies the similarity function s(Pi,Pj). In addition, we need to compare the pictures that belong to the same group, but we don’t want to do the same comparison at each of the g − 1 reducers whose key contains a given group number. There are many ways to handle this problem, but one way is as follows. Compare the members of group u at the reducer {u, u + 1}, where the “+1” is taken in the end-around sense. That is, if u = g (i.e., u is the last group), then u+1 is group 1. Otherwise, u+1 is the group whose number is one greater than u.
We can compute the replication rate and reducer size as a function of the number of groups g. Each input element is turned into g − 1 key-value pairs. That is, the replication rate is g − 1, or approximately r = g, since we suppose that the number of groups is still fairly large. The reducer size is 2×106/g, since that is the number of values on the list for each reducer. Each value is about a megabyte, so the number of bytes needed to store the input is 2 × 1012/g.
Example 2.12: If g is 1000, then the input consumes about 2GB. That’s enough to hold everything in a typical main memory. Moreover, the total
9In a typical cluster, there are many switches connecting subsets of the compute nodes, so all the data does not need to go across a single gigabit switch. However, the total available communication is still small enough that it is not feasible to implement this algorithm for the scale of data we have hypothesized.
 
2.6. COMPLEXITY THEORY FOR MAPREDUCE 57
number of bytes communicated is now 106 × 999 × 106, or about 1015 bytes. While that is still a huge amount of data to communicate, it is 1000 times less than that of the obvious algorithm. Moreover, there are still about half a million reducers. Since we are unlikely to have available that many compute nodes, we can divide all the reducers into a smaller number of Reduce tasks and still keep all the compute nodes busy; i.e., we can get as much parallelism as our computing cluster offers us. ✷
The computation cost for algorithms in this family is independent of the number of groups g, as long as the input to each reducer fits in main memory. The reason is that the bulk of the computation is the application of function s to the pairs of pictures. No matter what value g has, s is applied to each pair once and only once. Thus, although the work of algorithms in the family may be divided among reducers in widely different ways, all members of the family do the same computation.
2.6.3 A Graph Model for MapReduce Problems
In this section, we begin the study of a technique that will enable us to prove lower bounds on the replication rate, as a function of reducer size for a number of problems. Our first step is to introduce a graph model of problems. For each problem solvable by a MapReduce algorithm there is:
1. A set of inputs.
2. A set of outputs.
3. A many-many relationship between the inputs and outputs, which de- scribes which inputs are necessary to produce which outputs.
Example 2.13 : Figure 2.9 shows the graph for the similarity-join problem discussed in Section 2.6.2, if there were four pictures rather than a million. The inputs are the pictures, and the outputs are the six possible pairs of pictures. Each output is related to the two inputs that are members of its pair. ✷
Example 2.14 : Matrix multiplication presents a more complex graph. If we multiply n × n matrices M and N to get matrix P , then there are 2n2 inputs, mij and njk, and there are n2 outputs pik. Each output pik is related to 2n inputs: mi1, mi2, . . . , min and n1k, n2k, . . . , nnk. Moreover, each input is related to n outputs. For example, mij is related to pi1 , pi2 , . . . , pin . Figure 2.10 shows the input-output relationship for matrix multiplication for the simple case of 2 × 2 matrices, specifically
✷
 a b  e f = i j  cdghkl
58
CHAPTER 2.
MAPREDUCE AND THE NEW SOFTWARE STACK
 P1
P2
{P 1 , P 2 }
{P 1 , P 3 }
{P 1 , P 4 }
P3 {P2,P3}
{P 2 , P 4 }
P4
{P 3 , P 4 }
Figure 2.9: Input-output relationship for a similarity join
In the problems of Examples 2.13 and 2.14, the inputs and outputs were clearly all present. However, there are other problems where the inputs and/or outputs may not all be present in any instance of the problem. An example of such a problem is the natural join of R(A,B) and S(B,C) discussed in Section 2.3.7. We assume the attributes A, B, and C each have a finite domain, so there are only a finite number of possible inputs and outputs. The inputs are all possible R-tuples, those consisting of a value from the domain of A paired with a value from the domain of B, and all possible S-tuples – pairs from the domains of B and C. The outputs are all possible triples, with components from the domains of A, B, and C in that order. The output (a, b, c) is connected to two inputs, namely R(a, b) and S(b, c).
But in an instance of the join computation, only some of the possible inputs will be present, and therefore only some of the possible outputs will be produced. That fact does not influence the graph for the problem. We still need to know how every possible output relates to inputs, whether or not that output is produced in a given instance.
2.6.4 Mapping Schemas
Now that we see how to represent problems addressable by MapReduce as graphs, we can define the requirements for a MapReduce algorithm to solve a given problem. Each such algorithm must have a mapping schema, which expresses how outputs are produced by the various reducers used by the algo- rithm. That is, a mapping schema for a given problem with a given reducer size q is an assignment of inputs to one or more reducers, such that:
1. No reducer is assigned more than q inputs.
2.6. COMPLEXITY THEORY FOR MAPREDUCE 59 a
 b c d e f g h
i
j
k
l
Figure 2.10: Input-output relationship for matrix multiplication
2. For every output of the problem, there is at least one reducer that is assigned all the inputs that are related to that output. We say this reducer covers the output.
It can be argued that the existence of a mapping schema for any reducer size is what distinguishes problems that can be solved by a single MapReduce job from those that cannot.
Example 2.15 : Let us reconsider the “grouping” strategy we discussed in connection with the similarity join in Section 2.6.2. To generalize the problem, suppose the input is p pictures, which we place in g equal-sized groups of p/g inputs each. The number of outputs is  p2 , or approximately p2/2 outputs. A reducer will get the inputs from two groups – that is 2p/g inputs – so the reducer size we need is q = 2p/g. Each picture is sent to the reducers corresponding to the pairs consisting of its group and any of the g − 1 other groups. Thus, the replication rate is g − 1, or approximately g. If we replace g by the replication rate r in q = 2p/g, we conclude that r = 2p/q. That is, the replication rate is inversely proportional to the reducer size. That relationship is common; the smaller the reducer size, the larger the replication rate, and therefore the higher the communication.
This family of algorithms is described by a family of mapping schemas, one for each possible q. In the mapping schema for q = 2p/g, there are  g2 , or approximately g2/2 reducers. Each reducer corresponds to a pair of groups, and an input P is assigned to all the reducers whose pair includes the group of P. Thus, no reducer is assigned more than 2p/g inputs; in fact each reducer is assigned exactly that number. Moreover, every output is covered by some reducer. Specifically, if the output is a pair from two different groups u and v, then this output is covered by the reducer for the pair of groups {u, v}. If the
60 CHAPTER 2. MAPREDUCE AND THE NEW SOFTWARE STACK
output corresponds to inputs from only one group u, then the output is covered by several reducers – those corresponding to the set of groups {u,v} for any v ̸= u. Note that the algorithm we described has only one of these reducers computing the output, but any of them could compute it. ✷
The fact that an output depends on a certain input means that when that input is processed at the Map task, there will be at least one key-value pair generated to be used when computing that output. The value might not be exactly the input (as was the case in Example 2.15), but it is derived from that input. What is important is that for every related input and output there is a unique key-value pair that must be communicated. Note that there is technically never a need for more than one key-value pair for a given input and output, because the input could be transmitted to the reducer as itself, and whatever transformations on the input were applied by the Map function could instead be applied by the Reduce function at the reducer for that output.
2.6.5 When Not All Inputs Are Present
Example 2.15 describes a problem where we know every possible input is pre- sent, because we can define the input set to be those pictures that actually exist in the dataset. However, as discussed at the end of Section 2.6.3, there are problems like computing the join, where the graph of inputs and outputs describes inputs that might exist, and outputs that are only made when at least one of the inputs exists in the dataset. In fact, for the join, both inputs related to an output must exist if we are to make that output.
An algorithm for a problem where outputs can be missing still needs a mapping schema. The justification is that all inputs, or any subset of them, might be present, so an algorithm without a mapping schema would not be able to produce every possible output if all the inputs related to that output happened to be present, and yet no reducer covered that output.
The only way the absence of some inputs makes a difference is that we may wish to rethink the desired value of the reducer size q when we select an algorithm from the family of possible algorithms. Especially, if the value of q we select is that number such that we can be sure the input will just fit in main memory, then we may wish to increase q to take into account that some fraction of the inputs are not really there.
Example 2.16: Suppose that we know we can execute the Reduce function in main memory on a key and its associated list of q values. However, we also know that only 5% of the possible inputs are really present in the data set. Then a mapping schema for reducer size q will really send about q/20 of the inputs that exist to each reducer. Put another way, we could use the algorithm for reducer size 20q and expect that an average of q inputs will actually appear on the list for each reducer. We can thus choose 20q as the reducer size, or since there will be some randomness in the number of inputs actually appearing at
2.6. COMPLEXITY THEORY FOR MAPREDUCE 61 each reducer, we might wish to pick a slightly smaller value of reducer size, such
as 18q. ✷
2.6.6 Lower Bounds on Replication Rate
The family of similarity-join algorithms described in Example 2.15 lets us trade off communication against the reducer size, and through reducer size to trade communication against parallelism or against the ability to execute the Reduce function in main memory. How do we know we are getting the best possible tradeoff? We can only know we have the minimum possible communication if we can prove a matching lower bound. Using existence of a mapping schema as the starting point, we can often prove such a lower bound. Here is an outline of the technique.
1. Prove an upper bound on how many outputs a reducer with q inputs can cover. Call this bound g(q). This step can be difficult, but for examples like similarity join, it is actually quite simple.
2. Determine the total number of outputs produced by the problem.
3. Suppose that there are k reducers, and the ith reducer has qi < q inputs. Observe that  ki=1 g(qi) must be no less than the number of outputs computed in step (2).
4. Manipulate the inequality from (3) to get a lower bound on  ki=1 qi. Often, the trick used at this step is to replace some factors of qi by their upper bound q, but leave a single factor of qi in the term for i.
5. Since  ki=1 qi is the total communication from Map tasks to Reduce tasks, divide the lower bound from (4) on this quantity by the number of inputs. The result is a lower bound on the replication rate.
Example 2.17 : This sequence of steps may seem mysterious, but let us con- sider the similarity join as an example that we hope will make things clear. Recall that in Example 2.15 we gave an upper bound on the replication rate r of r ≤ 2p/q, where p was the number of inputs and q was the reducer size. We shall show a lower bound on r that is half that amount, which implies that, although improvements to the algorithm might be possible, any reduction in communication for a given reducer size will be by a factor of 2 at most.
For step (1), observe that if a reducer gets q inputs, it cannot cover more than  q2 , or approximately q2/2 outputs. For step (2), we know there are a total of  p2 , or approximately p2/2 outputs that each must be covered. The inequality constructed at step (3) is thus
k
  q i2 / 2 ≥ p 2 / 2 i=1
62 CHAPTER 2. MAPREDUCE AND THE NEW SOFTWARE STACK or, multiplying both sides by 2,
k
 qi2 ≥p2 (2.1) i=1
Now, we must do the manipulation of step (4). Following the hint, we note that there are two factors of qi in each term on the left of Equation (2.1), so we replace one factor by q and leave the other as qi. Since q ≥ qi, we can only increase the left side by doing so, and thus the inequality continues to hold:
or, dividing by q:
k
q qi ≥p2
i=1
k
 qi ≥p2/q i=1
The final step, which is step (5), is to divide both sides of Equation 2.2 by p, the number of inputs. As a result, the left side, which is ( ki=1 qi)/p is equal to the replication rate, and the right side becomes p/q. That is, we have proved the lower bound on r:
r ≥ p/q
As claimed, this shows that the family of algorithms from Example 2.15 all have
a replication rate that is at most twice the lowest possible replication rate. ✷ 2.6.7 Case Study: Matrix Multiplication
In this section we shall apply the lower-bound technique to one-pass matrix- multiplication algorithms. We saw one such algorithm in Section 2.3.10, but that is only an extreme case of a family of possible algorithms. In particular, for that algorithm, a reducer corresponds to a single element of the output matrix. Just as we grouped inputs in the similarity-join problem to reduce the communication at the expense of a larger reducer size, we can group rows and columns of the two input matrices into bands. Each pair consisting of a band of rows of the first matrix and a band of columns of the second matrix is used by one reducer to produce a square of elements of the output matrix. An example is suggested by Fig. 2.11.
In more detail, suppose we want to compute M N = P , and all three matrices are n × n. Group the rows of M into g bands of n/g rows each, and group the columns of N into g bands of n/g columns each. This grouping is as suggested by Fig. 2.11. Keys correspond to two groups (bands), one from M and one from N.
The Map Function: For each element of M, the Map function generates g key-value pairs. The value in each case is the element itself, together with its
(2.2)
2.6. COMPLEXITY THEORY FOR MAPREDUCE 63
=
Figure 2.11: Dividing matrices into bands to reduce communication
row and column number so it can be identified by the Reduce function. The key is the group to which the element belongs, paired with any of the groups of the matrix N. Similarly, for each element of N, the Map function generates g key-value pairs. The key is the group of that element paired with any of the groups of M, and the value is the element itself plus its row and column.
The Reduce Function: The reducer corresponding to the key (i,j), where i is a group of M and j is a group of N, gets a value list consisting of all the elements in the ith band of M and the jth band of N. It thus has all the values it needs to compute the elements of P whose row is one of those rows comprising the ith band of M and whose column is one of those comprising the jth band of N. For instance, Fig. 2.11 suggests the third group of M and the fourth group of N , combining to compute a square of P at the reducer (3, 4).
Each reducer gets n(n/g) elements from each of the two matrices, so q = 2n2/g. The replication rate is g, since each element of each matrix is sent to g reducers. That is, r = g. Combining r = g with q = 2n2/g we can conclude that r = 2n2/q. That is, just as for similarity join, the replication rate varies inversely with the reducer size.
It turns out that this upper bound on replication rate is also a lower bound. That is, we cannot do better than the family of algorithms we described above in a single round of MapReduce. Interestingly, we shall see that we can get a lower total communication for the same reducer size, if we use two passes of MapReduce as we discussed in Section 2.3.9. We shall not give the complete proof of the lower bound, but will suggest the important elements.
For step (1) we need to get an upper bound on how many outputs a reducer of size q can cover. First, notice that if a reducer gets some of the elements in a row of M, but not all of them, then the elements of that row are useless; the reducer cannot produce any output in that row of P. Similarly, if a reducer receives some but not all of a column of N, these inputs are also useless. Thus, we may assume that the best mapping schema will send to each reducer some number of full rows of M and some number of full columns of N. This reducer
                      
64 CHAPTER 2. MAPREDUCE AND THE NEW SOFTWARE STACK
is then capable of producing output element pik if and only if it has received the entire ith row of M and the entire kth column of N. The remainder of the argument for step (1) is to prove that the largest number of outputs are covered when the reducer receives the same number of rows as columns. We leave this part as an exercise.
However, assuming a reducer receives k rows of M and k columns of N, then q = 2nk, and k2 outputs are covered. That is, g(q), the maximum number of outputs covered by a reducer that receives q inputs, is q2/4n2.
For step (2), we know the number of outputs is n2. In step (3) we observe that if there are k reducers, with the ith reducer receiving qi ≤ q inputs, then
or
k
  q i2 / 4 n 2 ≥ n 2 i=1
k
  q i2 ≥ 4 n 4 i=1
From this inequality, you can derive
r ≥ 2n2/q
We leave the algebraic manipulation, which is similar to that in Example 2.17, as an exercise.
Now, let us consider the generalization of the two-pass matrix-multiplication algorithm that we described in Section 2.3.9. First, notice that we could have designed the first pass to use one reducer for each triple (i, j, k). This reducer would get only the two elements mij and njk. We can generalize this idea to use reducers that get larger sets of elements from each matrix; these sets of elements form squares within their respective matrices. The idea is suggested by Fig. 2.12. We may divide the rows and columns of both input matrices M and N into g groups of n/g rows or columns each. The intersections of the groups partition each matrix into g2 squares of n2/g2 elements each.
The square of M corresponding to set of rows I and set of columns J com- bines with the square of N corresponding to set of rows J and set of columns K. These two squares compute some of the terms that are needed to produce the square of the output matrix P that has set of rows I and set of columns K. However, these two squares do not compute the full value of these elements of P; rather they produce a contribution to the sum. Other pairs of squares, one from M and one from N, contribute to the same square of P. These contribu- tions are suggested in Fig. 2.12. There, we see how all the squares of M with a fixed value for set of rows I pair with all the squares of N that have a fixed value for the set of columns K by letting the set J vary.
So in the first pass, we compute the products of the square (I,J) of M with the square (J,K) of N, for all I, J, and K. Then, in the second pass, for each
2.6. COMPLEXITY THEORY FOR MAPREDUCE 65
                 =
Figure 2.12: Partitioning matrices into squares for a two-pass MapReduce al- gorithm
I and K we sum the products over all possible sets J. In more detail, the first MapReduce job does the following.
The Map Function: The keys are triples of sets of rows and/or column num- bers (I, J, K). Suppose the element mij belongs to group of rows I and group of columns J. Then from mij we generate g key-value pairs with value equal to mij, together with its row and column numbers, i and j, to identify the matrix element. There is one key-value pair for each key (I, J, K), where K can be any of the g groups of columns of N. Similarly, from element njk of N, if j belongs to group J and k to group K, the Map function generates g key-value pairs with value consisting of njk, j, and k, and with keys (I,J,K) for any group I.
The Reduce Function: The reducer corresponding to (I,J,K) receives as input all the elements mij where i is in I and j is in J, and it also receives all the elements njk, where j is in J and k is in K. It computes
xiJk =   mijnjk j in J
for all i in I and k in K.
Notice that the replication rate for the first MapReduce job is g, and the to- tal communication is therefore 2gn2. Also notice that each reducer gets 2n2/g2 inputs, so q = 2n2/g2. Equivalently, g = n√ 2/q.√Thus, the total communica- tion 2gn2 can be written in terms of q as 2 2n3/ q.
       
66 CHAPTER 2. MAPREDUCE AND THE NEW SOFTWARE STACK The second MapReduce job is simple; it sums up the xiJk’s over all sets J.
The Map Function: We assume that the Map tasks execute at whatever compute nodes executed the Reduce tasks of the previous job. Thus, no com- munication is needed between the jobs. The Map function takes as input one element xiJk, which we assume the previous reducers have left labeled with i and k so we know to what element of matrix P this term contributes. One key-value pair is generated. The key is (i,k) and the value is xiJk.
The Reduce Function: The Reduce function simply sums the values associ- ated with key (i,k) to compute the output element Pik.
The communication between the Map and Reduce tasks of the second job is gn2, since there are n possible values of i, n possible values of k, and g possible values of the set J, and each xiJk is communicated only once. If we recall from our analysis of the first MapReduce job that g = n 2/q, we can write the
 q. This amount is exactly communication for the second job as n2g = √2n3/√
  half the communication for the first job, so the total communication for the
q. Although we shall not examine this point two-pass algorithm is 3√2n3/√
  here, it turns out that we can do slightly better if we divide the matrices M and N not into squares but into rectangles that are twice as long on one side as on the other. In that case, we get the slightly smaller constant 4 in place of 3√2 = 4.24, and we get a two-pass algorithm with communication equal to
algorithm is 4n4/q. We may as well assume q is less than n2, or else we can just use a s√erial algorithm at one compute node and not use MapReduce at all. Thus, n3/ q is smaller than n4/q, and if q is close to its minimum possible value of 2n,1√0 then the two-pass algorithm beats the one-pass algorithm by a factor of O( n) in communication. Moreover, we can expect the difference in communication to be the significant cost difference. Both algorithms do the same O(n3) arithmetic operations. The two-pass method naturally has more overhead managing tasks than does the one-job method. On the other hand, the second pass of the two-pass algorithm applies a Reduce function that is associative and commutative. Thus, it might be possible to save some communication cost by using a combiner on that pass.
2.6.8 Exercises for Section 2.6
Exercise 2.6.1: Describe the graphs that model the following problems. (a) The multiplication of an n × n matrix by a vector of length n.
(b) The natural join of R(A,B) and S(B,C), where A, B, and C have do- mains of sizes a, b, and c, respectively.
10If q is less than 2n, then a reducer cannot get even one row and one column, and therefore cannot compute any outputs at all.
 4n3 /√
q.
Now, recall that the communication cost we computed for the one-pass
    
2.7. SUMMARY OF CHAPTER 2 67
(c) The grouping and aggregation on the relation R(A,B), where A is the grouping attribute and B is aggregated by the MAX operation. Assume A and B have domains of size a and b, respectively.
! Exercise 2.6.2: Provide the details of the proof that a one-pass matrix- multiplication algorithm requires replication rate at least r ≥ 2n2/q, including:
(a) The proof that, for a fixed reducer size, the maximum number of outputs are covered by a reducer when that reducer receives an equal number of rows of M and columns of N.
(b) The algebraic manipulation needed, starting with  ki=1 qi2 ≥ 4n4.
!! Exercise 2.6.3 : Suppose our inputs are bit strings of length b, and the outputs
correspond to pairs of strings at Hamming distance 1.11
(a) Prove that a reducer of size q can cover at most (q/2) log2 q outputs.
(b) Use part (a) to show the lower bound on replication rate: r ≥ b/ log2 q.
(c) Show that there are algorithms with replication rate as given by part (b) for the cases q = 2, q = 2b, and q = 2b/2.
2.7 Summary of Chapter 2
✦ Cluster Computing: A common architecture for very large-scale applica- tions is a cluster of compute nodes (processor chip, main memory, and disk). Compute nodes are mounted in racks, and the nodes on a rack are connected, typically by gigabit Ethernet. Racks are also connected by a high-speed network or switch.
✦ Distributed File Systems: An architecture for very large-scale file sys- tems has developed recently. Files are composed of chunks of about 64 megabytes, and each chunk is replicated several times, on different com- pute nodes or racks.
✦ MapReduce: This programming system allows one to exploit parallelism inherent in cluster computing, and manages the hardware failures that can occur during a long computation on many nodes. Many Map tasks and many Reduce tasks are managed by a Master process. Tasks on a failed compute node are rerun by the Master.
✦ The Map Function: This function is written by the user. It takes a collection of input objects and turns each into zero or more key-value pairs. Keys are not necessarily unique.
11Bit strings have Hamming distance 1 if they differ in exactly one bit position. You may look ahead to Section 3.5.6 for the general definition.
 
68
✦
✦
✦
✦
✦
✦
✦
✦
CHAPTER 2. MAPREDUCE AND THE NEW SOFTWARE STACK
The Reduce Function: A MapReduce programming system sorts all the key-value pairs produced by all the Map tasks, forms all the values asso- ciated with a given key into a list and distributes key-list pairs to Reduce tasks. Each Reduce task combines the elements on each list, by applying the function written by the user. The results produced by all the Reduce tasks form the output of the MapReduce process.
Reducers: It is often convenient to refer to the application of the Reduce function to a single key and its associated value list as a “reducer.”
Hadoop: This programming system is an open-source implementation of a distributed file system (HDFS, the Hadoop Distributed File System) and MapReduce (Hadoop itself). It is available through the Apache Founda- tion.
Managing Compute-Node Failures: MapReduce systems support restart of tasks that fail because their compute node, or the rack containing that node, fail. Because Map and Reduce tasks deliver their output only after they finish, it is possible to restart a failed task without concern for possible repetition of the effects of that task. It is necessary to restart the entire job only if the node at which the Master executes fails.
Applications of MapReduce: While not all parallel algorithms are suitable for implementation in the MapReduce framework, there are simple im- plementations of matrix-vector and matrix-matrix multiplication. Also, the principal operators of relational algebra are easily implemented in MapReduce.
Workflow Systems: MapReduce has been generalized to systems that sup- port any acyclic collection of functions, each of which can be instantiated by any number of tasks, each responsible for executing that function on a portion of the data.
Recursive Workflows: When implementing a recursive collection of func- tions, it is not always possible to preserve the ability to restart any failed task, because recursive tasks may have produced output that was con- sumed by another task before the failure. A number of schemes for check- pointing parts of the computation to allow restart of single tasks, or restart all tasks from a recent point, have been proposed.
Communication-Cost: Many applications of MapReduce or similar sys- tems do very simple things for each task. Then, the dominant cost is usually the cost of transporting data from where it is created to where it is used. In these cases, efficiency of a MapReduce algorithm can be estimated by calculating the sum of the sizes of the inputs to all the tasks.
2.8. REFERENCES FOR CHAPTER 2 69
✦ Multiway Joins: It is sometimes more efficient to replicate tuples of the relations involved in a join and have the join of three or more relations computed as a single MapReduce job. The technique of Lagrangean mul- tipliers can be used to optimize the degree of replication for each of the participating relations.
✦ Star Joins: Analytic queries often involve a very large fact table joined with smaller dimension tables. These joins can always be done efficiently by the multiway-join technique. An alternative is to distribute the fact table and replicate the dimension tables permanently, using the same strategy as would be used if we were taking the multiway join of the fact table and every dimension table.
✦ Replication Rate and Reducer Size: It is often convenient to measure communication by the replication rate, which is the communication per input. Also, the reducer size is the maximum number of inputs associated with any reducer. For many problems, it is possible to derive a lower bound on replication rate as a function of the reducer size.
✦ Representing Problems as Graphs: It is possible to represent many prob- lems that are amenable to MapReduce computation by a graph in which nodes represent inputs and outputs. An output is connected to all the inputs that are needed to compute that output.
✦ Mapping Schemas: Given the graph of a problem, and given a reducer size, a mapping schema is an assignment of the inputs to one or more reducers so that no reducer is assigned more inputs than the reducer size permits, and yet for every output there is some reducer that gets all the inputs needed to compute that output. The requirement that there be a mapping schema for any MapReduce algorithm is a good expression of what makes MapReduce algorithms different from general parallel computations.
✦ Matrix Multiplication by MapReduce: There is a family of one-pass Map- Reduce algorithms that performs multiplication of n × n matrices with the minimum possible replication rate r = 2n2/q, where q is the reducer size. On the other hand, a two-pass MapReduce algorithm for the same problem with the same reducer size can use up to a factor of n less com- munication.
2.8 References for Chapter 2
GFS, the Google File System, was described in [10]. The paper on Google’s MapReduce is [8]. Information about Hadoop and HDFS can be found at [11]. More detail on relations and relational algebra can be found in [16].
Clustera is covered in [9]. Hyracks (previously called Hyrax) is from [4]. The Dryad system [13] has similar capabilities, but requires user creation of
70 CHAPTER 2. MAPREDUCE AND THE NEW SOFTWARE STACK
parallel tasks. That responsibility was automated through the introduction of DryadLINQ [17]. For a discussion of cluster implementation of recursion, see [1]. Pregel is from [14].
A different approach to recursion was taken in Haloop [5]. There, recursion is seen as an iteration, with the output of one round being input to the next round. Efficiency is obtained by managing the location of the intermediate data and the tasks that implement each round.
There are a number of other systems built on a distributed file system and/or MapReduce, which have not been covered here, but may be worth knowing about. [6] describes BigTable, a Google implementation of an object store of very large size. A somewhat different direction was taken at Yahoo! with Pnuts [7]. The latter supports a limited form of transaction processing, for example.
PIG [15] is an implementation of relational algebra on top of Hadoop. Sim- ilarly, Hive [12] implements a restricted form of SQL on top of Hadoop.
The communication-cost model for MapReduce algorithms and the optimal implementations of multiway joins is from [3]. The material on replication rate, reducer size, and their relationship is from [2]. Solutions to Exercises 2.6.2 and 2.6.3 can be found there.
1. F.N. Afrati, V. Borkar, M. Carey, A. Polyzotis, and J.D. Ullman, “Clus- ter computing, recursion, and Datalog,” to appear in Proc. Datalog 2.0 Workshop, Elsevier, 2011.
2. F.N. Afrati, A. Das Sarma, S. Salihoglu, and J.D. Ullman, “Upper and lower bounds on the cost of a MapReduce computation.” to appear in Proc. Intl. Conf. on Very Large Databases, 2013. Also available as CoRR, abs/1206.4377.
3. F.N. Afrati and J.D. Ullman, “Optimizing joins in a MapReduce environ- ment,” Proc. Thirteenth Intl. Conf. on Extending Database Technology, 2010.
4. V. Borkar and M. Carey, “Hyrax: demonstrating a new foundation for data-parallel computation,”
http://asterix.ics.uci.edu/pub/hyraxdemo.pdf
Univ. of California, Irvine, 2010.
5. Y. Bu, B. Howe, M. Balazinska, and M. Ernst, “HaLoop: efficient iter- ative data processing on large clusters,” Proc. Intl. Conf. on Very Large Databases, 2010.
6. F. Chang, J. Dean, S. Ghemawat, W.C. Hsieh, D.A. Wallach, M. Burrows, T. Chandra, A. Fikes, and R.E. Gruber, “Bigtable: a distributed storage system for structured data,” ACM Transactions on Computer Systems 26:2, pp. 1–26, 2008.
2.8. REFERENCES FOR CHAPTER 2 71
7. B.F. Cooper, R. Ramakrishnan, U. Srivastava, A. Silberstein, P. Bohan- non, H.-A. Jacobsen, N. Puz, D. Weaver, and R. Yerneni, “Pnuts: Ya- hoo!’s hosted data serving platform,” PVLDB 1:2, pp. 1277–1288, 2008.
8. J. Dean and S. Ghemawat, “Mapreduce: simplified data processing on large clusters,” Comm. ACM 51:1, pp. 107–113, 2008.
9. D.J. DeWitt, E. Paulson, E. Robinson, J.F. Naughton, J. Royalty, S. Shankar, and A. Krioukov, “Clustera: an integrated computation and data management system,” PVLDB 1:1, pp. 28–41, 2008.
10. S. Ghemawat, H. Gobioff, and S.-T. Leung, “The Google file system,” 19th ACM Symposium on Operating Systems Principles, 2003.
11. hadoop.apache.org, Apache Foundation.
12. hadoop.apache.org/hive, Apache Foundation.
13. M. Isard, M. Budiu, Y. Yu, A. Birrell, and D. Fetterly. “Dryad: dis- tributed data-parallel programs from sequential building blocks,” Proceed- ings of the 2nd ACM SIGOPS/EuroSys European Conference on Com- puter Systems, pp. 59–72, ACM, 2007.
14. G. Malewicz, M.N. Austern, A.J.C. Sik, J.C. Denhert, H. Horn, N. Leiser, and G. Czajkowski, “Pregel: a system for large-scale graph processing,” Proc. ACM SIGMOD Conference, 2010.
15. C. Olston, B. Reed, U. Srivastava, R. Kumar, and A. Tomkins, “Pig latin: a not-so-foreign language for data processing,” Proc. ACM SIGMOD Con- ference, pp. 1099–1110, 2008.
16. J.D. Ullman and J. Widom, A First Course in Database Systems, Third Edition, Prentice-Hall, Upper Saddle River, NJ, 2008.
17. Y. Yu, M. Isard, D. Fetterly, M. Budiu, I. Erlingsson, P.K. Gunda, and J. Currey, “DryadLINQ: a system for general-purpose distributed data- parallel computing using a high-level language,” OSDI, pp. 1–14, USENIX Association, 2008.
72 CHAPTER 2. MAPREDUCE AND THE NEW SOFTWARE STACK
Chapter 3
Finding Similar Items
A fundamental data-mining problem is to examine data for “similar” items. We shall take up applications in Section 3.1, but an example would be looking at a collection of Web pages and finding near-duplicate pages. These pages could be plagiarisms, for example, or they could be mirrors that have almost the same content but differ in information about the host and about other mirrors.
We begin by phrasing the problem of similarity as one of finding sets with a relatively large intersection. We show how the problem of finding textually similar documents can be turned into such a set problem by the technique known as “shingling.” Then, we introduce a technique called “minhashing,” which compresses large sets in such a way that we can still deduce the similarity of the underlying sets from their compressed versions. Other techniques that work when the required degree of similarity is very high are covered in Section 3.9.
Another important problem that arises when we search for similar items of any kind is that there may be far too many pairs of items to test each pair for their degree of similarity, even if computing the similarity of any one pair can be made very easy. That concern motivates a technique called “locality-sensitive hashing,” for focusing our search on pairs that are most likely to be similar.
Finally, we explore notions of “similarity” that are not expressible as inter- section of sets. This study leads us to consider the theory of distance measures in arbitrary spaces. It also motivates a general framework for locality-sensitive hashing that applies for other definitions of “similarity.”
3.1 Applications of Near-Neighbor Search
We shall focus initially on a particular notion of “similarity”: the similarity of sets by looking at the relative size of their intersection. This notion of similarity is called “Jaccard similarity,” and will be introduced in Section 3.1.1. We then examine some of the uses of finding similar sets. These include finding textually similar documents and collaborative filtering by finding similar customers and similar products. In order to turn the problem of textual similarity of documents
73
74 CHAPTER 3. FINDING SIMILAR ITEMS into one of set intersection, we use a technique called “shingling,” which is
introduced in Section 3.2.
3.1.1 Jaccard Similarity of Sets
The Jaccard similarity of sets S and T is |S ∩ T|/|S ∪ T|, that is, the ratio of the size of the intersection of S and T to the size of their union. We shall denote the Jaccard similarity of S and T by SIM(S, T ).
Example 3.1 : In Fig. 3.1 we see two sets S and T . There are three elements in their intersection and a total of eight elements that appear in S or T or both. Thus, SIM(S, T ) = 3/8. ✷
     
   
S
 
   
 
   
   
        
   
T
 Figure 3.1: Two sets with Jaccard similarity 3/8
3.1.2 Similarity of Documents
An important class of problems that Jaccard similarity addresses well is that of finding textually similar documents in a large corpus such as the Web or a collection of news articles. We should understand that the aspect of similarity we are looking at here is character-level similarity, not “similar meaning,” which requires us to examine the words in the documents and their uses. That problem is also interesting but is addressed by other techniques, which we hinted at in Section 1.3.1. However, textual similarity also has important uses. Many of these involve finding duplicates or near duplicates. First, let us observe that testing whether two documents are exact duplicates is easy; just compare the two documents character-by-character, and if they ever differ then they are not the same. However, in many applications, the documents are not identical, yet they share large portions of their text. Here are some examples:
3.1. APPLICATIONS OF NEAR-NEIGHBOR SEARCH 75 Plagiarism
Finding plagiarized documents tests our ability to find textual similarity. The plagiarizer may extract only some parts of a document for his own. He may alter a few words and may alter the order in which sentences of the original appear. Yet the resulting document may still contain 50% or more of the original. No simple process of comparing documents character by character will detect a sophisticated plagiarism.
Mirror Pages
It is common for important or popular Web sites to be duplicated at a number of hosts, in order to share the load. The pages of these mirror sites will be quite similar, but are rarely identical. For instance, they might each contain information associated with their particular host, and they might each have links to the other mirror sites but not to themselves. A related phenomenon is the appropriation of pages from one class to another. These pages might include class notes, assignments, and lecture slides. Similar pages might change the name of the course, year, and make small changes from year to year. It is important to be able to detect similar pages of these kinds, because search engines produce better results if they avoid showing two pages that are nearly identical within the first page of results.
Articles from the Same Source
It is common for one reporter to write a news article that gets distributed, say through the Associated Press, to many newspapers, which then publish the article on their Web sites. Each newspaper changes the article somewhat. They may cut out paragraphs, or even add material of their own. They most likely will surround the article by their own logo, ads, and links to other articles at their site. However, the core of each newspaper’s page will be the original article. News aggregators, such as Google News, try to find all versions of such an article, in order to show only one, and that task requires finding when two Web pages are textually similar, although not identical.1
3.1.3 Collaborative Filtering as a Similar-Sets Problem
Another class of applications where similarity of sets is very important is called collaborative filtering, a process whereby we recommend to users items that were liked by other users who have exhibited similar tastes. We shall investigate collaborative filtering in detail in Section 9.3, but for the moment let us see some common examples.
1News aggregation also involves finding articles that are about the same topic, even though not textually similar. This problem too can yield to a similarity search, but it requires techniques other than Jaccard similarity of sets.
 
76 CHAPTER 3. FINDING SIMILAR ITEMS On-Line Purchases
Amazon.com has millions of customers and sells millions of items. Its database records which items have been bought by which customers. We can say two cus- tomers are similar if their sets of purchased items have a high Jaccard similarity. Likewise, two items that have sets of purchasers with high Jaccard similarity will be deemed similar. Note that, while we might expect mirror sites to have Jaccard similarity above 90%, it is unlikely that any two customers have Jac- card similarity that high (unless they have purchased only one item). Even a Jaccard similarity like 20% might be unusual enough to identify customers with similar tastes. The same observation holds for items; Jaccard similarities need not be very high to be significant.
Collaborative filtering requires several tools, in addition to finding similar customers or items, as we discuss in Chapter 9. For example, two Amazon customers who like science-fiction might each buy many science-fiction books, but only a few of these will be in common. However, by combining similarity- finding with clustering (Chapter 7), we might be able to discover that science- fiction books are mutually similar and put them in one group. Then, we can get a more powerful notion of customer-similarity by asking whether they made purchases within many of the same groups.
Movie Ratings
NetFlix records which movies each of its customers rented, and also the ratings assigned to those movies by the customers. We can see movies as similar if they were rented or rated highly by many of the same customers, and see customers as similar if they rented or rated highly many of the same movies. The same observations that we made for Amazon above apply in this situation: similarities need not be high to be significant, and clustering movies by genre will make things easier.
When our data consists of ratings rather than binary decisions (bought/did not buy or liked/disliked), we cannot rely simply on sets as representations of customers or items. Some options are:
1. Ignore low-rated customer/movie pairs; that is, treat these events as if the customer never watched the movie.
2. When comparing customers, imagine two set elements for each movie, “liked” and “hated.” If a customer rated a movie highly, put the “liked” for that movie in the customer’s set. If they gave a low rating to a movie, put “hated” for that movie in their set. Then, we can look for high Jaccard similarity among these sets. We can do a similar trick when comparing movies.
3. If ratings are 1-to-5-stars, put a movie in a customer’s set n times if they rated the movie n-stars. Then, use Jaccard similarity for bags when measuring the similarity of customers. The Jaccard similarity for bags
3.2. SHINGLING OF DOCUMENTS 77
B and C is defined by counting an element n times in the intersection if n is the minimum of the number of times the element appears in B and C. In the union, we count the element the sum of the number of times it appears in B and in C.2
Example 3.2 : The bag-similarity of bags {a, a, a, b} and {a, a, b, b, c} is 1/3. The intersection counts a twice and b once, so its size is 3. The size of the union of two bags is always the sum of the sizes of the two bags, or 9 in this case. Since the highest possible Jaccard similarity for bags is 1/2, the score of 1/3 indicates the two bags are quite similar, as should be apparent from an examination of their contents. ✷
3.1.4 Exercises for Section 3.1
Exercise 3.1.1 : Compute the Jaccard similarities of each pair of the following three sets: {1, 2, 3, 4}, {2, 3, 5, 7}, and {2, 4, 6}.
Exercise 3.1.2: Compute the Jaccard bag similarity of each pair of the fol- lowing three bags: {1, 1, 1, 2}, {1, 1, 2, 2, 3}, and {1, 2, 3, 4}.
!! Exercise 3.1.3: Suppose we have a universal set U of n elements, and we choose two subsets S and T at random, each with m of the n elements. What is the expected value of the Jaccard similarity of S and T ?
3.2 Shingling of Documents
The most effective way to represent documents as sets, for the purpose of iden- tifying lexically similar documents is to construct from the document the set of short strings that appear within it. If we do so, then documents that share pieces as short as sentences or even phrases will have many common elements in their sets, even if those sentences appear in different orders in the two docu- ments. In this section, we introduce the simplest and most common approach, shingling, as well as an interesting variation.
3.2.1 k-Shingles
A document is a string of characters. Define a k-shingle for a document to be
any substring of length k found within the document. Then, we may associate
2Although the union for bags is normally (e.g., in the SQL standard) defined to have the sum of the number of copies in the two bags, this definition causes some inconsistency with the Jaccard similarity for sets. Under this definition of bag union, the maximum Jaccard similarity is 1/2, not 1, since the union of a set with itself has twice as many elements as the intersection of the same set with itself. If we prefer to have the Jaccard similarity of a set with itself be 1, we can redefine the union of bags to have each element appear the maximum number of times it appears in either of the two bags. This change does not simply double the similarity in each case, but it also gives a reasonable measure of bag similarity.
 
78 CHAPTER 3. FINDING SIMILAR ITEMS with each document the set of k-shingles that appear one or more times within
that document.
Example 3.3 : Suppose our document D is the string abcdabd, and we pick k = 2. Then the set of 2-shingles for D is {ab,bc,cd,da,bd}.
Note that the substring ab appears twice within D, but appears only once as a shingle. A variation of shingling produces a bag, rather than a set, so each shingle would appear in the result as many times as it appears in the document. However, we shall not use bags of shingles here. ✷
There are several options regarding how white space (blank, tab, newline, etc.) is treated. It probably makes sense to replace any sequence of one or more white-space characters by a single blank. That way, we distinguish shingles that cover two or more words from those that do not.
Example 3.4 : If we use k = 9, but eliminate whitespace altogether, then we would see some lexical similarity in the sentences “The plane was ready for touch down”. and “The quarterback scored a touchdown”. However, if we retain the blanks, then the first has shingles touch dow and ouch down, while the second has touchdown. If we eliminated the blanks, then both would have touchdown. ✷
3.2.2 Choosing the Shingle Size
We can pick k to be any constant we like. However, if we pick k too small, then we would expect most sequences of k characters to appear in most documents. If so, then we could have documents whose shingle-sets had high Jaccard simi- larity, yet the documents had none of the same sentences or even phrases. As an extreme example, if we use k = 1, most Web pages will have most of the common characters and few other characters, so almost all Web pages will have high similarity.
How large k should be depends on how long typical documents are and how large the set of typical characters is. The important thing to remember is:
• k should be picked large enough that the probability of any given shingle appearing in any given document is low.
Thus, if our corpus of documents is emails, picking k = 5 should be fine. To see why, suppose that only letters and a general white-space character ap- pear in emails (although in practice, most of the printable ASCII characters can be expected to appear occasionally). If so, then there would be 275 = 14,348,907 possible shingles. Since the typical email is much smaller than 14 million characters long, we would expect k = 5 to work well, and indeed it does.
However, the calculation is a bit more subtle. Surely, more than 27 charac- ters appear in emails, However, all characters do not appear with equal proba- bility. Common letters and blanks dominate, while ”z” and other letters that
3.2. SHINGLING OF DOCUMENTS 79
have high point-value in Scrabble are rare. Thus, even short emails will have many 5-shingles consisting of common letters, and the chances of unrelated emails sharing these common shingles is greater than would be implied by the calculation in the paragraph above. A good rule of thumb is to imagine that there are only 20 characters and estimate the number of k-shingles as 20k. For large documents, such as research articles, choice k = 9 is considered safe.
3.2.3 Hashing Shingles
Instead of using substrings directly as shingles, we can pick a hash function that maps strings of length k to some number of buckets and treat the resulting bucket number as the shingle. The set representing a document is then the set of integers that are bucket numbers of one or more k-shingles that appear in the document. For instance, we could construct the set of 9-shingles for a document and then map each of those 9-shingles to a bucket number in the range 0 to 232 − 1. Thus, each shingle is represented by four bytes instead of nine. Not only has the data been compacted, but we can now manipulate (hashed) shingles by single-word machine operations.
Notice that we can differentiate documents better if we use 9-shingles and hash them down to four bytes than to use 4-shingles, even though the space used to represent a shingle is the same. The reason was touched upon in Section 3.2.2. If we use 4-shingles, most sequences of four bytes are unlikely or impossible to find in typical documents. Thus, the effective number of different shingles is much less than 232 − 1. If, as in Section 3.2.2, we assume only 20 characters are frequent in English text, then the number of different 4-shingles that are likely to occur is only (20)4 = 160,000. However, if we use 9-shingles, there are many more than 232 likely shingles. When we hash them down to four bytes, we can expect almost any sequence of four bytes to be possible, as was discussed in Section 1.3.2.
3.2.4 Shingles Built from Words
An alternative form of shingle has proved effective for the problem of identifying similar news articles, mentioned in Section 3.1.2. The exploitable distinction for this problem is that the news articles are written in a rather different style than are other elements that typically appear on the page with the article. News articles, and most prose, have a lot of stop words (see Section 1.3.1), the most common words such as “and,” “you,” “to,” and so on. In many applications, we want to ignore stop words, since they don’t tell us anything useful about the article, such as its topic.
However, for the problem of finding similar news articles, it was found that defining a shingle to be a stop word followed by the next two words, regardless of whether or not they were stop words, formed a useful set of shingles. The advantage of this approach is that the news article would then contribute more shingles to the set representing the Web page than would the surrounding ele-
80 CHAPTER 3. FINDING SIMILAR ITEMS
ments. Recall that the goal of the exercise is to find pages that had the same articles, regardless of the surrounding elements. By biasing the set of shingles in favor of the article, pages with the same article and different surrounding material have higher Jaccard similarity than pages with the same surrounding material but with a different article.
Example 3.5 : An ad might have the simple text “Buy Sudzo.” However, a news article with the same idea might read something like “A spokesperson for the Sudzo Corporation revealed today that studies have shown it is good for people to buy Sudzo products.” Here, we have italicized all the likely stop words, although there is no set number of the most frequent words that should be considered stop words. The first three shingles made from a stop word and the next two following are:
A spokesperson for for the Sudzo
the Sudzo Corporation
There are nine shingles from the sentence, but none from the “ad.” ✷ 3.2.5 Exercises for Section 3.2
Exercise 3.2.1 : What are the first ten 3-shingles in the first sentence of Sec- tion 3.2?
Exercise 3.2.2 : If we use the stop-word-based shingles of Section 3.2.4, and we take the stop words to be all the words of three or fewer letters, then what are the shingles in the first sentence of Section 3.2?
Exercise 3.2.3: What is the largest number of k-shingles a document of n bytes can have? You may assume that the size of the alphabet is large enough that the number of possible strings of length k is at least as n.
3.3 Similarity-Preserving Summaries of Sets
Sets of shingles are large. Even if we hash them to four bytes each, the space needed to store a set is still roughly four times the space taken by the document. If we have millions of documents, it may well not be possible to store all the shingle-sets in main memory.3
Our goal in this section is to replace large sets by much smaller represen- tations called “signatures.” The important property we need for signatures is that we can compare the signatures of two sets and estimate the Jaccard sim- ilarity of the underlying sets from the signatures alone. It is not possible that
3There is another serious concern: even if the sets fit in main memory, the number of pairs may be too great for us to evaluate the similarity of each pair. We take up the solution to this problem in Section 3.4.
 
3.3. SIMILARITY-PRESERVING SUMMARIES OF SETS 81
the signatures give the exact similarity of the sets they represent, but the esti- mates they provide are close, and the larger the signatures the more accurate the estimates. For example, if we replace the 200,000-byte hashed-shingle sets that derive from 50,000-byte documents by signatures of 1000 bytes, we can usually get within a few percent.
3.3.1 Matrix Representation of Sets
Before explaining how it is possible to construct small signatures from large sets, it is helpful to visualize a collection of sets as their characteristic matrix. The columns of the matrix correspond to the sets, and the rows correspond to elements of the universal set from which elements of the sets are drawn. There is a 1 in row r and column c if the element for row r is a member of the set for column c. Otherwise the value in position (r, c) is 0.
Element   S1 S2 S3 S4
a   1001 b   0010 c   0101 d   1011 e   0010
Figure 3.2: A matrix representing four sets
Example 3.6 : In Fig. 3.2 is an example of a matrix representing sets chosen from the universal set {a,b,c,d,e}. Here, S1 = {a,d}, S2 = {c}, S3 = {b,d,e}, and S4 = {a, c, d}. The top row and leftmost columns are not part of the matrix, but are present only to remind us what the rows and columns represent. ✷
It is important to remember that the characteristic matrix is unlikely to be the way the data is stored, but it is useful as a way to visualize the data. For one reason not to store data as a matrix, these matrices are almost always sparse (they have many more 0’s than 1’s) in practice. It saves space to represent a sparse matrix of 0’s and 1’s by the positions in which the 1’s appear. For another reason, the data is usually stored in some other format for other purposes.
As an example, if rows are products, and columns are customers, represented by the set of products they bought, then this data would really appear in a database table of purchases. A tuple in this table would list the item, the purchaser, and probably other details about the purchase, such as the date and the credit card used.
3.3.2 Minhashing
The signatures we desire to construct for sets are composed of the results of a large number of calculations, say several hundred, each of which is a “minhash”
                    
82 CHAPTER 3. FINDING SIMILAR ITEMS
of the characteristic matrix. In this section, we shall learn how a minhash is computed in principle, and in later sections we shall see how a good approxi- mation to the minhash is computed in practice.
To minhash a set represented by a column of the characteristic matrix, pick a permutation of the rows. The minhash value of any column is the number of the first row, in the permuted order, in which the column has a 1.
Example 3.7 : Let us suppose we pick the order of rows beadc for the matrix of Fig. 3.2. This permutation defines a minhash function h that maps sets to rows. Let us compute the minhash value of set S1 according to h. The first column, which is the column for set S1, has 0 in row b, so we proceed to row e, the second in the permuted order. There is again a 0 in the column for S1, so we proceed to row a, where we find a 1. Thus. h(S1) = a.
Element   S1 S2 S3 S4
b   0010 e   0010 a   1001 d   1011 c   0101
Figure 3.3: A permutation of the rows of Fig. 3.2
Although it is not physically possible to permute very large characteristic matrices, the minhash function h implicitly reorders the rows of the matrix of Fig. 3.2 so it becomes the matrix of Fig. 3.3. In this matrix, we can read off the values of h by scanning from the top until we come to a 1. Thus, we see thath(S2)=c,h(S3)=b,andh(S4)=a. ✷
3.3.3 Minhashing and Jaccard Similarity
There is a remarkable connection between minhashing and Jaccard similarity of the sets that are minhashed.
• The probability that the minhash function for a random permutation of rows produces the same value for two sets equals the Jaccard similarity of those sets.
To see why, we need to picture the columns for those two sets. If we restrict ourselves to the columns for sets S1 and S2, then rows can be divided into three classes:
1. Type X rows have 1 in both columns.
2. Type Y rows have 1 in one of the columns and 0 in the other.
                    
3.3. SIMILARITY-PRESERVING SUMMARIES OF SETS 83 3. Type Z rows have 0 in both columns.
Since the matrix is sparse, most rows are of type Z. However, it is the ratio of the numbers of type X and type Y rows that determine both SIM(S1,S2) and the probability that h(S1) = h(S2). Let there be x rows of type X and y rows of type Y . Then SIM(S1, S2) = x/(x + y). The reason is that x is the size ofS1 ∩S2 andx+yisthesizeofS1 ∪S2.
Now, consider the probability that h(S1) = h(S2). If we imagine the rows permuted randomly, and we proceed from the top, the probability that we shall meet a type X row before we meet a type Y row is x/(x+y). But if the first row from the top other than type Z rows is a type X row, then surely h(S1) = h(S2). On the other hand, if the first row other than a type Z row that we meet is a type Y row, then the set with a 1 gets that row as its minhash value. However the set with a 0 in that row surely gets some row further down the permuted list. Thus, we know h(S1) ̸= h(S2) if we first meet a type Y row. We conclude the probability that h(S1) = h(S2) is x/(x + y), which is also the Jaccard similarity of S1 and S2.
3.3.4 Minhash Signatures
Again think of a collection of sets represented by their characteristic matrix M . To represent sets, we pick at random some number n of permutations of the rows of M. Perhaps 100 permutations or several hundred permutations will do. Call the minhash functions determined by these permutations h1,h2,...,hn. From the column representing set S, construct the minhash signature for S, the vector [h1(S), h2(S), . . . , hn(S)]. We normally represent this list of hash-values as a column. Thus, we can form from matrix M a signature matrix, in which the ith column of M is replaced by the minhash signature for (the set of) the ith column.
Note that the signature matrix has the same number of columns as M but only n rows. Even if M is not represented explicitly, but in some compressed form suitable for a sparse matrix (e.g., by the locations of its 1’s), it is normal for the signature matrix to be much smaller than M.
3.3.5 Computing Minhash Signatures
It is not feasible to permute a large characteristic matrix explicitly. Even picking a random permutation of millions or billions of rows is time-consuming, and the necessary sorting of the rows would take even more time. Thus, permuted matrices like that suggested by Fig. 3.3, while conceptually appealing, are not implementable.
Fortunately, it is possible to simulate the effect of a random permutation by a random hash function that maps row numbers to as many buckets as there are rows. A hash function that maps integers 0, 1, . . . , k − 1 to bucket numbers 0 through k − 1 typically will map some pairs of integers to the same bucket and leave other buckets unfilled. However, the difference is unimportant as long as
84 CHAPTER 3. FINDING SIMILAR ITEMS
k is large and there are not too many collisions. We can maintain the fiction that our hash function h “permutes” row r to position h(r) in the permuted order.
Thus, instead of picking n random permutations of rows, we pick n randomly chosen hash functions h1, h2, . . . , hn on the rows. We construct the signature matrix by considering each row in their given order. Let SIG(i, c) be the element of the signature matrix for the ith hash function and column c. Initially, set SIG(i, c) to ∞ for all i and c. We handle row r by doing the following:
1. Computeh1(r),h2(r),...,hn(r).
2. For each column c do the following:
(a) If c has 0 in row r, do nothing.
(b) However, if c has 1 in row r, then for each i = 1,2,...,n set SIG(i,c) to the smaller of the current value of SIG(i, c) and hi(r).
Row     S1   S2   S3   S4     x + 1 mod 5   3x + 1 01001 1 1 10010 2 4 20101 3 2 31011 4 0 40010 0 3
mod 5
                                         Figure 3.4: Hash functions computed for the matrix of Fig. 3.2
Example 3.8 : Let us reconsider the characteristic matrix of Fig. 3.2, which we reproduce with some additional data as Fig. 3.4. We have replaced the letters naming the rows by integers 0 through 4. We have also chosen two hash functions: h1(x) = x+1 mod 5 and h2(x) = 3x+1 mod 5. The values of these two functions applied to the row numbers are given in the last two columns of Fig. 3.4. Notice that these simple hash functions are true permutations of the rows, but a true permutation is only possible because the number of rows, 5, is a prime. In general, there will be collisions, where two rows get the same hash value.
Now, let us simulate the algorithm for computing the signature matrix. Initially, this matrix consists of all ∞’s:
S1 S2 S3 S4 h1 ∞ ∞ ∞ ∞ h2 ∞ ∞ ∞ ∞
First, we consider row 0 of Fig. 3.4. We see that the values of h1(0) and h2(0) are both 1. The row numbered 0 has 1’s in the columns for sets S1 and
                
3.3. SIMILARITY-PRESERVING SUMMARIES OF SETS 85
S4, so only these columns of the signature matrix can change. As 1 is less than ∞, we do in fact change both values in the columns for S1 and S4. The current estimate of the signature matrix is thus:
S1 S2   S3 S4 h1 1 ∞   ∞ 1 h2 1 ∞   ∞ 1
Now, we move to the row numbered 1 in Fig. 3.4. This row has 1 only in S3, and its hash values are h1(1) = 2 and h2(1) = 4. Thus, we set SIG(1, 3) to 2 and SIG(2, 3) to 4. All other signature entries remain as they are because their columns have 0 in the row numbered 1. The new signature matrix:
S1 S2 S3 S4 h1 1 ∞ 2 1 h2 1 ∞ 4 1
The row of Fig. 3.4 numbered 2 has 1’s in the columns for S2 and S4, and its hash values are h1(2) = 3 and h2(2) = 2. We could change the values in the signature for S4, but the values in this column of the signature matrix, [1, 1], are each less than the corresponding hash values [3, 2]. However, since the column for S2 still has ∞’s, we replace it by [3, 2], resulting in:
S1 S2 S3 S4 h1 1 3 2 1 h2 1 2 4 1
Next comes the row numbered 3 in Fig. 3.4. Here, all columns but S2 have 1, and the hash values are h1(3) = 4 and h2(3) = 0. The value 4 for h1 exceeds what is already in the signature matrix for all the columns, so we shall not change any values in the first row of the signature matrix. However, the value 0 for h2 is less than what is already present, so we lower SIG(2, 1), SIG(2, 3) and SIG(2, 4) to 0. Note that we cannot lower SIG(2, 2) because the column for S2 in Fig. 3.4 has 0 in the row we are currently considering. The resulting signature matrix:
S1 S2 S3 S4 h1 1 3 2 1 h2 0 2 0 0
Finally, consider the row of Fig. 3.4 numbered 4. h1(4) = 0 and h2(4) = 3. Since row 4 has 1 only in the column for S3, we only compare the current signature column for that set, [2, 0] with the hash values [0, 3]. Since 0 < 2, we change SIG(1,3) to 0, but since 3 > 0 we do not change SIG(2,3). The final signature matrix is:
S1 S2 S3 S4 h1 1 3 0 1 h2 0 2 0 0
                                                                             
86 CHAPTER 3. FINDING SIMILAR ITEMS
We can estimate the Jaccard similarities of the underlying sets from this signature matrix. Notice that columns 1 and 4 are identical, so we guess that SIM(S1, S4) = 1.0. If we look at Fig. 3.4, we see that the true Jaccard similarity of S1 and S4 is 2/3. Remember that the fraction of rows that agree in the signature matrix is only an estimate of the true Jaccard similarity, and this example is much too small for the law of large numbers to assure that the estimates are close. For additional examples, the signature columns for S1 and S3 agree in half the rows (true similarity 1/4), while the signatures of S1 and S2 estimate 0 as their Jaccard similarity (the correct value). ✷
3.3.6 Exercises for Section 3.3
Exercise 3.3.1 : Verify the theorem from Section 3.3.3, which relates the Jac- card similarity to the probability of minhashing to equal values, for the partic- ular case of Fig. 3.2.
(a) Compute the Jaccard similarity of each of the pairs of columns in Fig. 3.2.
! (b) Compute, for each pair of columns of that figure, the fraction of the 120 permutations of the rows that make the two columns hash to the same value.
Exercise 3.3.2: Using the data from Fig. 3.4, add to the signatures of the columns the values of the following hash functions:
(a) h3(x)=2x+4 mod5. (b) h4(x)=3x−1 mod5.
Element   S1 S2 S3 S4
0   0101 1   0100 2   1001 3   0010 4   0011 5   1000
Figure 3.5: Matrix for Exercise 3.3.3
Exercise 3.3.3 : In Fig. 3.5 is a matrix with six rows.
(a) Compute the minhash signature for each column if we use the following three hash functions: h1(x) = 2x + 1 mod 6; h2(x) = 3x + 2 mod 6; h3(x)=5x+2 mod6.
                       
3.4. LOCALITY-SENSITIVE HASHING FOR DOCUMENTS 87 (b) Which of these hash functions are true permutations?
(c) HowclosearetheestimatedJaccardsimilaritiesforthesixpairsofcolumns to the true Jaccard similarities?
! Exercise 3.3.4 : Now that we know Jaccard similarity is related to the proba- bility that two sets minhash to the same value, reconsider Exercise 3.1.3. Can you use this relationship to simplify the problem of computing the expected Jaccard similarity of randomly chosen sets?
! Exercise 3.3.5 : Prove that if the Jaccard similarity of two columns is 0, then minhashing always gives a correct estimate of the Jaccard similarity.
!! Exercise 3.3.6: One might expect that we could estimate the Jaccard simi- larity of columns without using all possible permutations of rows. For example, we could only allow cyclic permutations; i.e., start at a randomly chosen row r, which becomes the first in the order, followed by rows r + 1, r + 2, and so on, down to the last row, and then continuing with the first row, second row, and so on, down to row r − 1. There are only n such permutations if there are n rows. However, these permutations are not sufficient to estimate the Jaccard similarity correctly. Give an example of a two-column matrix where averaging over all the cyclic permutations does not give the Jaccard similarity.
! Exercise 3.3.7 : Suppose we want to use a MapReduce framework to compute minhash signatures. If the matrix is stored in chunks that correspond to some columns, then it is quite easy to exploit parallelism. Each Map task gets some of the columns and all the hash functions, and computes the minhash signatures of its given columns. However, suppose the matrix were chunked by rows, so that a Map task is given the hash functions and a set of rows to work on. Design Map and Reduce functions to exploit MapReduce with data in this form.
3.4 Locality-Sensitive Hashing for Documents
Even though we can use minhashing to compress large documents into small signatures and preserve the expected similarity of any pair of documents, it still may be impossible to find the pairs with greatest similarity efficiently. The reason is that the number of pairs of documents may be too large, even if there are not too many documents.
Example 3.9: Suppose we have a million documents, and we use signatures
of length 250. Then we use 1000 bytes per document for the signatures, and
the entire data fits in a gigabyte – less than a typical main memory of a laptop.
However, there are  1,000,000  or half a trillion pairs of documents. If it takes a 2
microsecond to compute the similarity of two signatures, then it takes almost six days to compute all the similarities on that laptop. ✷
88 CHAPTER 3. FINDING SIMILAR ITEMS
If our goal is to compute the similarity of every pair, there is nothing we can do to reduce the work, although parallelism can reduce the elapsed time. However, often we want only the most similar pairs or all pairs that are above some lower bound in similarity. If so, then we need to focus our attention only on pairs that are likely to be similar, without investigating every pair. There is a general theory of how to provide such focus, called locality-sensitive hashing (LSH) or near-neighbor search. In this section we shall consider a specific form of LSH, designed for the particular problem we have been studying: documents, represented by shingle-sets, then minhashed to short signatures. In Section 3.6 we present the general theory of locality-sensitive hashing and a number of applications and related techniques.
3.4.1 LSH for Minhash Signatures
One general approach to LSH is to “hash” items several times, in such a way that similar items are more likely to be hashed to the same bucket than dissimilar items are. We then consider any pair that hashed to the same bucket for any of the hashings to be a candidate pair. We check only the candidate pairs for similarity. The hope is that most of the dissimilar pairs will never hash to the same bucket, and therefore will never be checked. Those dissimilar pairs that do hash to the same bucket are false positives; we hope these will be only a small fraction of all pairs. We also hope that most of the truly similar pairs will hash to the same bucket under at least one of the hash functions. Those that do not are false negatives; we hope these will be only a small fraction of the truly similar pairs.
If we have minhash signatures for the items, an effective way to choose the hashings is to divide the signature matrix into b bands consisting of r rows each. For each band, there is a hash function that takes vectors of r integers (the portion of one column within that band) and hashes them to some large number of buckets. We can use the same hash function for all the bands, but we use a separate bucket array for each band, so columns with the same vector in different bands will not hash to the same bucket.
Example 3.10 : Figure 3.6 shows part of a signature matrix of 12 rows divided into four bands of three rows each. The second and fourth of the explicitly shown columns each have the column vector [0, 2, 1] in the first band, so they will definitely hash to the same bucket in the hashing for the first band. Thus, regardless of what those columns look like in the other three bands, this pair of columns will be a candidate pair. It is possible that other columns, such as the first two shown explicitly, will also hash to the same bucket according to the hashing of the first band. However, since their column vectors are different, [1, 3, 0] and [0, 2, 1], and there are many buckets for each hashing, we expect the chances of an accidental collision to be very small. We shall normally assume that two vectors hash to the same bucket if and only if they are identical.
Two columns that do not agree in band 1 have three other chances to become a candidate pair; they might be identical in any one of these other bands.
3.4. LOCALITY-SENSITIVE HASHING FOR DOCUMENTS 89 band 1
band 2
band 3
band 4
Figure 3.6: Dividing a signature matrix into four bands of three rows per band
However, observe that the more similar two columns are, the more likely it is that they will be identical in some band. Thus, intuitively the banding strategy makes similar columns much more likely to be candidate pairs than dissimilar pairs. ✷
3.4.2 Analysis of the Banding Technique
Suppose we use b bands of r rows each, and suppose that a particular pair of documents have Jaccard similarity s. Recall from Section 3.3.3 that the prob- ability the minhash signatures for these documents agree in any one particular row of the signature matrix is s. We can calculate the probability that these documents (or rather their signatures) become a candidate pair as follows:
1. The probability that the signatures agree in all rows of one particular band is sr.
2. The probability that the signatures disagree in at least one row of a par- ticular band is 1 − sr.
3. The probability that the signatures disagree in at least one row of each of the bands is (1 − sr)b.
4. The probability that the signatures agree in all the rows of at least one band, and therefore become a candidate pair, is 1 − (1 − sr)b.
It may not be obvious, but regardless of the chosen constants b and r, this function has the form of an S-curve, as suggested in Fig. 3.7. The threshold, that is, the value of similarity s at which the probability of becoming a candidate is 1/2, is a function of b and r. The threshold is roughly where the rise is the steepest, and for large b and r there we find that pairs with similarity above the threshold are very likely to become candidates, while those below the threshold are unlikely to become candidates – exactly the situation we want.
 1 0 0 02
. . . 3 2 1 22 . . .
0131 1
   
90
CHAPTER 3.
FINDING SIMILAR ITEMS
     Probability of becoming a candidate
0
Jaccard similarity of documents
Figure 3.7: The S-curve
1
  An approximation to the threshold is (1/b)1/r. For example, if b = 16 and r = 4, then the threshold is approximately at s = 1/2, since the 4th root of 1/16 is 1/2.
Example 3.11 : Let us consider the case b = 20 and r = 5. That is, we suppose we have signatures of length 100, divided into twenty bands of five rows each. Figure 3.8 tabulates some of the values of the function 1 − (1 − s5)20. Notice that the threshold, the value of s at which the curve has risen halfway, is just slightly more than 0.5. Also notice that the curve is not exactly the ideal step function that jumps from 0 to 1 at the threshold, but the slope of the curve in the middle is significant. For example, it rises by more than 0.6 going from s = 0.4 to s = 0.6, so the slope in the middle is greater than 3.
s 1−(1−sr)b .2 .006
.3 .047
.4 .186
.5 .470 .6 .802 .7 .975 .8 .9996
Figure 3.8: Values of the S-curve for b = 20 and r = 5
For example, at s = 0.8, 1 − (0.8)5 is about 0.672. If you raise this number to the 20th power, you get about 0.00035. Subtracting this fraction from 1
 
3.4. LOCALITY-SENSITIVE HASHING FOR DOCUMENTS 91
yields 0.99965. That is, if we consider two documents with 80% similarity, then in any one band, they have only about a 33% chance of agreeing in all five rows and thus becoming a candidate pair. However, there are 20 bands and thus 20 chances to become a candidate. Only roughly one in 3000 pairs that are as high as 80% similar will fail to become a candidate pair and thus be a false negative. ✷
3.4.3 Combining the Techniques
We can now give an approach to finding the set of candidate pairs for similar documents and then discovering the truly similar documents among them. It must be emphasized that this approach can produce false negatives – pairs of similar documents that are not identified as such because they never become a candidate pair. There will also be false positives – candidate pairs that are evaluated, but are found not to be sufficiently similar.
1. Pick a value of k and construct from each document the set of k-shingles. Optionally, hash the k-shingles to shorter bucket numbers.
2. Sort the document-shingle pairs to order them by shingle.
3. Pick a length n for the minhash signatures. Feed the sorted list to the algorithm of Section 3.3.5 to compute the minhash signatures for all the documents.
4. Choose a threshold t that defines how similar documents have to be in order for them to be regarded as a desired “similar pair.” Pick a number of bands b and a number of rows r such that br = n, and the threshold t is approximately (1/b)1/r. If avoidance of false negatives is important, you may wish to select b and r to produce a threshold lower than t; if speed is important and you wish to limit false positives, select b and r to produce a higher threshold.
5. Construct candidate pairs by applying the LSH technique of Section 3.4.1.
6. Examine each candidate pair’s signatures and determine whether the frac-
tion of components in which they agree is at least t.
7. Optionally, if the signatures are sufficiently similar, go to the documents themselves and check that they are truly similar, rather than documents that, by luck, had similar signatures.
3.4.4 Exercises for Section 3.4
Exercise 3.4.1 : Evaluate the S-curve 1 − (1 − sr)b for s = 0.1, 0.2, . . . , 0.9, for the following values of r and b:
• r=3andb=10.
92
CHAPTER 3. FINDING SIMILAR ITEMS
• r=6andb=20.
• r=5andb=50.
! Exercise 3.4.2: For each of the (r,b) pairs in Exercise 3.4.1, compute the threshold, that is, the value of s for which the value of 1−(1−sr)b is exactly 1/2. How does this value compare with the estimate of (1/b)1/r that was suggested in Section 3.4.2?
! Exercise 3.4.3 : Use the techniques explained in Section 1.3.5 to approximate the S-curve 1 − (1 − sr)b when sr is very small.
! Exercise 3.4.4 : Suppose we wish to implement LSH by MapReduce. Specifi- cally, assume chunks of the signature matrix consist of columns, and elements are key-value pairs where the key is the column number and the value is the signature itself (i.e., a vector of values).
(a)
(b)
3.5
Show how to produce the buckets for all the bands as output of a single MapReduce process. Hint: Remember that a Map function can produce several key-value pairs from a single element.
Show how another MapReduce process can convert the output of (a) to a list of pairs that need to be compared. Specifically, for each column i, there should be a list of those columns j > i with which i needs to be compared.
Distance Measures
We now take a short detour to study the general notion of distance measures. The Jaccard similarity is a measure of how close sets are, although it is not really a distance measure. That is, the closer sets are, the higher the Jaccard similarity. Rather, 1 minus the Jaccard similarity is a distance measure, as we shall see; it is called the Jaccard distance.
However, Jaccard distance is not the only measure of closeness that makes sense. We shall examine in this section some other distance measures that have applications. Then, in Section 3.6 we see how some of these distance measures also have an LSH technique that allows us to focus on nearby points without comparing all points. Other applications of distance measures will appear when we study clustering in Chapter 7.
3.5.1 Definition of a Distance Measure
Suppose we have a set of points, called a space. A distance measure on this space is a function d(x, y) that takes two points in the space as arguments and produces a real number, and satisfies the following axioms:
1. d(x, y) ≥ 0 (no negative distances).
3.5. DISTANCE MEASURES 93
2. d(x,y) = 0 if and only if x = y (distances are positive, except for the
distance from a point to itself).
3. d(x, y) = d(y, x) (distance is symmetric).
4. d(x, y) ≤ d(x, z) + d(z, y) (the triangle inequality).
The triangle inequality is the most complex condition. It says, intuitively, that to travel from x to y, we cannot obtain any benefit if we are forced to travel via some particular third point z. The triangle-inequality axiom is what makes all distance measures behave as if distance describes the length of a shortest path from one point to another.
3.5.2 Euclidean Distances
The most familiar distance measure is the one we normally think of as “dis- tance.” An n-dimensional Euclidean space is one where points are vectors of n real numbers. The conventional distance measure in this space, which we shall refer to as the L2-norm, is defined:
  n d([x1,x2,...,xn], [y1,y2,...,yn])=  (xi −yi)2
i=1
That is, we square the distance in each dimension, sum the squares, and take the positive square root.
It is easy to verify the first three requirements for a distance measure are satisfied. The Euclidean distance between two points cannot be negative, be- cause the positive square root is intended. Since all squares of real numbers are nonnegative, any i such that xi ̸= yi forces the distance to be strictly positive. On the other hand, if xi = yi for all i, then the distance is clearly 0. Symmetry follows because (xi − yi)2 = (yi − xi)2. The triangle inequality requires a good deal of algebra to verify. However, it is well understood to be a property of Euclidean space: the sum of the lengths of any two sides of a triangle is no less than the length of the third side.
There are other distance measures that have been used for Euclidean spaces. For any constant r, we can define the Lr-norm to be the distance measure d defined by:
n d([x1,x2,...,xn], [y1,y2,...,yn])=( |xi −yi|r)1/r
i=1
The case r = 2 is the usual L2-norm just mentioned. Another common distance measure is the L1-norm, or Manhattan distance. There, the distance between two points is the sum of the magnitudes of the differences in each dimension. It is called “Manhattan distance” because it is the distance one would have to
 
94 CHAPTER 3. FINDING SIMILAR ITEMS
travel between points if one were constrained to travel along grid lines, as on the streets of a city such as Manhattan.
Another interesting distance measure is the L∞-norm, which is the limit as r approaches infinity of the Lr-norm. As r gets larger, only the dimension with the largest difference matters, so formally, the L∞-norm is defined as the maximum of |xi − yi| over all dimensions i.
max(|2 − 6|, |7 − 4|) = max(4, 3) = 4 3.5.3 Jaccard Distance
As mentioned at the beginning of the section, we define the Jaccard distance of sets by d(x, y) = 1 − SIM(x, y). That is, the Jaccard distance is 1 minus the ratio of the sizes of the intersection and union of sets x and y. We must verify that this function is a distance measure.
1. d(x,y) is nonnegative because the size of the intersection cannot exceed the size of the union.
2. d(x,y)=0ifx=y,becausex∪x=x∩x=x. However,ifx̸=y,then the size of x ∩ y is strictly less than the size of x ∪ y, so d(x,y) is strictly positive.
3. d(x, y) = d(y, x) because both union and intersection are symmetric; i.e., x ∪ y = y ∪ x and x ∩ y = y ∩ x.
4. For the triangle inequality, recall from Section 3.3.3 that SIM(x, y) is the probability a random minhash function maps x and y to the same value. Thus, the Jaccard distance d(x, y) is the probability that a random min- hash function does not send x and y to the same value. We can therefore translate the condition d(x, y) ≤ d(x, z) + d(z, y) to the statement that if h is a random minhash function, then the probability that h(x) ̸= h(y) is no greater than the sum of the probability that h(x) ̸= h(z) and the probability that h(z) ̸= h(y). However, this statement is true because whenever h(x) ̸= h(y), at least one of h(x) and h(y) must be different from h(z). They could not both be h(z), because then h(x) and h(y) would be the same.
.
Example 3.12 : Consider the two-dimensional Euclidean space (the custom- ary plane) and the points (2,7) and (6,4). The L -norm gives a distance
 √
|2−6|+|7−4|=4+3=7. TheL∞-normgivesadistanceof
2
of (2−6)2 +(7−4)2 = 42 +32 = 5. The L1-norm gives a distance of
  ✷
3.5. DISTANCE MEASURES 95 3.5.4 Cosine Distance
The cosine distance makes sense in spaces that have dimensions, including Eu- clidean spaces and discrete versions of Euclidean spaces, such as spaces where points are vectors with integer components or boolean (0 or 1) components. In such a space, points may be thought of as directions. We do not distinguish be- tween a vector and a multiple of that vector. Then the cosine distance between two points is the angle that the vectors to those points make. This angle will be in the range 0 to 180 degrees, regardless of how many dimensions the space has.
We can calculate the cosine distance by first computing the cosine of the angle, and then applying the arc-cosine function to translate to an angle in the 0-180 degree range. Given two vectors x and y, the cosine of the angle between them is the dot product x.y divided by the L2-norms of x and y (i.e., their Euclidean distances from the origin). Recall that the dot product of vectors [x1,x2,...,xn].[y1,y2,...,yn] is  ni=1 xiyi.
Example 3.13 : Let our two vectors be x = [1, 2, −1] and = [2, 1, 1]. The dot product x.y is 1×2+2×1+(−1)×1 = 3. The L -norm of both vectors is
√ 2√
6. For example, x has L2-norm 12 + 22 + (−1)2 = 6. Thus, the cosine of
   the angle between x and y is 3/(√6√6) or 1/2. The angle whose cosine is 1/2 is 60 degrees, so that is the cosine distance between x and y. ✷
  We must show that the cosine distance is indeed a distance measure. We have defined it so the values are in the range 0 to 180, so no negative distances are possible. Two vectors have angle 0 if and only if they are the same direction.4 Symmetry is obvious: the angle between x and y is the same as the angle between y and x. The triangle inequality is best argued by physical reasoning. Onewaytorotatefromxtoyistorotatetozandthencetoy. Thesumof those two rotations cannot be less than the rotation directly from x to y.
3.5.5 Edit Distance
This distance makes sense when points are strings. The distance between two strings x = x1x2 ···xn and y = y1y2 ···ym is the smallest number of insertions and deletions of single characters that will convert x to y.
Example 3.14 : The edit distance between the strings x = abcde and y = acfdeg is 3. To convert x to y:
1. Deleteb.
2. Insertfafterc.
4Notice that to satisfy the second axiom, we have to treat vectors that are multiples of one another, e.g. [1, 2] and [3, 6], as the same direction, which they are. If we regarded these as different vectors, we would give them distance 0 and thus violate the condition that only d(x, x) is 0.
 
96
CHAPTER 3. FINDING SIMILAR ITEMS
3. Insertgaftere.
No sequence of fewer than three insertions and/or deletions will convert x to y.
Thus, d(x, y) = 3. ✷
Another way to define and calculate the edit distance d(x, y) is to compute a longest common subsequence (LCS) of x and y. An LCS of x and y is a string that is constructed by deleting positions from x and y, and that is as long as any string that can be constructed that way. The edit distance d(x, y) can be calculated as the length of x plus the length of y minus twice the length of their LCS.
Example 3.15 : The strings x = abcde and y = acfdeg from Example 3.14 have a unique LCS, which is acde. We can be sure it is the longest possible, because it contains every symbol appearing in both x and y. Fortunately, these common symbols appear in the same order in both strings, so we are able to use them all in an LCS. Note that the length of x is 5, the length of y is 6, and the length of their LCS is 4. The edit distance is thus 5+6−2×4 = 3, which agrees with the direct calculation in Example 3.14.
For another example, consider x = aba and y = bab. Their edit distance is 2. For example, we can convert x to y by deleting the first a and then inserting b at the end. There are two LCS’s: ab and ba. Each can be obtained by deleting one symbol from each string. As must be the case for multiple LCS’s of the same pair of strings, both LCS’s have the same length. Therefore, we maycomputetheeditdistanceas3+3−2×2=2. ✷
Edit distance is a distance measure. Surely no edit distance can be negative, and only two identical strings have an edit distance of 0. To see that edit distance is symmetric, note that a sequence of insertions and deletions can be reversed, with each insertion becoming a deletion, and vice versa. The triangle inequality is also straightforward. One way to turn a string s into a string t is to turn s into some string u and then turn u into t. Thus, the number of edits made going from s to u, plus the number of edits made going from u to t cannot be less than the smallest number of edits that will turn s into t.
3.5.6 Hamming Distance
Given a space of vectors, we define the Hamming distance between two vectors to be the number of components in which they differ. It should be obvious that Hamming distance is a distance measure. Clearly the Hamming distance cannot be negative, and if it is zero, then the vectors are identical. The distance does not depend on which of two vectors we consider first. The triangle inequality should also be evident. If x and z differ in m components, and z and y differ in n components, then x and y cannot differ in more than m+n components. Most commonly, Hamming distance is used when the vectors are boolean; they consist of 0’s and 1’s only. However, in principle, the vectors can have components from any set.
3.5. DISTANCE MEASURES 97
   Non-Euclidean Spaces
Notice that several of the distance measures introduced in this section are not Euclidean spaces. A property of Euclidean spaces that we shall find important when we take up clustering in Chapter 7 is that the average of points in a Euclidean space always exists and is a point in the space. However, consider the space of sets for which we defined the Jaccard dis- tance. The notion of the “average” of two sets makes no sense. Likewise, the space of strings, where we can use the edit distance, does not let us take the “average” of strings.
Vector spaces, for which we suggested the cosine distance, may or may not be Euclidean. If the components of the vectors can be any real num- bers, then the space is Euclidean. However, if we restrict components to be integers, then the space is not Euclidean. Notice that, for instance, we cannot find an average of the vectors [1, 2] and [3, 1] in the space of vectors with two integer components, although if we treated them as members of the two-dimensional Euclidean space, then we could say that their average was [2.0, 1.5].
 Example 3.16 : The Hamming distance between the vectors 10101 and 11110 is 3. That is, these vectors differ in the second, fourth, and fifth components, while they agree in the first and third components. ✷
3.5.7 Exercises for Section 3.5
! Exercise 3.5.1 : On the space of nonnegative integers, which of the following functions are distance measures? If so, prove it; if not, prove that it fails to satisfy one or more of the axioms.
(a) max(x, y) = the larger of x and y.
(b) diff(x, y) = |x − y| (the absolute magnitude of the difference between x
and y).
(c) sum(x,y)=x+y.
Exercise 3.5.2 : Find the L1 and L2 distances between the points (5, 6, 7) and (8, 2, 4).
!! Exercise 3.5.3: Prove that if i and j are any positive integers, and i < j, then the Li norm between any two points is greater than the Lj norm between those same two points.
Exercise 3.5.4: Find the Jaccard distances between the following pairs of sets:
98 CHAPTER 3. FINDING SIMILAR ITEMS
(a) {1,2,3,4} and {2,3,4,5}.
(b) {1,2,3}and{4,5,6}.
Exercise 3.5.5: Compute the cosines of the angles between each of the fol-
lowing pairs of vectors.5
(a) (3,−1,2)and(−2,3,1).
(b) (1,2,3)and(2,4,6).
(c) (5,0,−4)and(−1,−6,2).
(d) (0,1,1,0,1,1) and (0,0,1,0,0,0).
! Exercise 3.5.6 : Prove that the cosine distance between any two vectors of 0’s and 1’s, of the same length, is at most 90 degrees.
Exercise 3.5.7 : Find the edit distances (using only insertions and deletions) between the following pairs of strings.
(a) abcdef and bdaefc. (b) abccdabc and acbdcab.
(c) abcdef and baedfc.
! Exercise 3.5.8 : There are a number of other notions of edit distance available. For instance, we can allow, in addition to insertions and deletions, the following operations:
i. Mutation, where one symbol is replaced by another symbol. Note that a mutation can always be performed by an insertion followed by a deletion, but if we allow mutations, then this change counts for only 1, not 2, when computing the edit distance.
ii. Transposition, where two adjacent symbols have their positions swapped. Like a mutation, we can simulate a transposition by one insertion followed by one deletion, but here we count only 1 for these two steps.
Repeat Exercise 3.5.7 if edit distance is defined to be the number of insertions, deletions, mutations, and transpositions needed to transform one string into another.
! Exercise 3.5.9: Prove that the edit distance discussed in Exercise 3.5.8 is indeed a distance measure.
Exercise 3.5.10: Find the Hamming distances between each pair of the fol- lowing vectors: 000000, 110011, 010101, and 011100.
5Note that what we are asking for is not precisely the cosine distance, but from the cosine of an angle, you can compute the angle itself, perhaps with the aid of a table or library function.
 
3.6. THE THEORY OF LOCALITY-SENSITIVE FUNCTIONS 99 3.6 The Theory of Locality-Sensitive Functions
The LSH technique developed in Section 3.4 is one example of a family of func- tions (the minhash functions) that can be combined (by the banding technique) to distinguish strongly between pairs at a low distance from pairs at a high dis- tance. The steepness of the S-curve in Fig. 3.7 reflects how effectively we can avoid false positives and false negatives among the candidate pairs.
Now, we shall explore other families of functions, besides the minhash func- tions, that can serve to produce candidate pairs efficiently. These functions can apply to the space of sets and the Jaccard distance, or to another space and/or another distance measure. There are three conditions that we need for a family of functions:
1. They must be more likely to make close pairs be candidate pairs than distant pairs. We make this notion precise in Section 3.6.1.
2. They must be statistically independent, in the sense that it is possible to estimate the probability that two or more functions will all give a certain response by the product rule for independent events.
3. They must be efficient, in two ways:
(a) They must be able to identify candidate pairs in time much less than the time it takes to look at all pairs. For example, minhash functions have this capability, since we can hash sets to minhash values in time proportional to the size of the data, rather than the square of the number of sets in the data. Since sets with common values are colocated in a bucket, we have implicitly produced the candidate pairs for a single minhash function in time much less than the number of pairs of sets.
(b) They must be combinable to build functions that are better at avoid- ing false positives and negatives, and the combined functions must also take time that is much less than the number of pairs. For ex- ample, the banding technique of Section 3.4.1 takes single minhash functions, which satisfy condition 3a but do not, by themselves have the S-curve behavior we want, and produces from a number of min- hash functions a combined function that has the S-curve shape.
Our first step is to define “locality-sensitive functions” generally. We then see how the idea can be applied in several applications. Finally, we discuss how to apply the theory to arbitrary data with either a cosine distance or a Euclidean distance measure.
3.6.1 Locality-Sensitive Functions
For the purposes of this section, we shall consider functions that take two items and render a decision about whether these items should be a candidate pair.
100 CHAPTER 3. FINDING SIMILAR ITEMS
In many cases, the function f will “hash” items, and the decision will be based on whether or not the result is equal. Because it is convenient to use the notation f(x) = f(y) to mean that f(x,y) is “yes; make x and y a candidate pair,” we shall use f(x) = f(y) as a shorthand with this meaning. We also use f(x) ̸= f(y) to mean “do not make x and y a candidate pair unless some other function concludes we should do so.”
A collection of functions of this form will be called a family of functions. For example, the family of minhash functions, each based on one of the possible permutations of rows of a characteristic matrix, form a family.
Let d1 < d2 be two distances according to some distance measure d. A family F of functions is said to be (d1, d2, p1, p2)-sensitive if for every f in F:
1. If d(x,y) ≤ d1, then the probability that f(x) = f(y) is at least p1. 2. If d(x,y) ≥ d2, then the probability that f(x) = f(y) is at most p2.
    Probabilty
of being
declared a
candidate p 2
p
1
d1 d2 Distance
  Figure 3.9: Behavior of a (d1, d2, p1, p2)-sensitive function
Figure 3.9 illustrates what we expect about the probability that a given function in a (d1, d2, p1, p2)-sensitive family will declare two items to be a can- didate pair. Notice that we say nothing about what happens when the distance between the items is strictly between d1 and d2, but we can make d1 and d2 as close as we wish. The penalty is that typically p1 and p2 are then close as well. As we shall see, it is possible to drive p1 and p2 apart while keeping d1 and d2 fixed.
3.6.2 Locality-Sensitive Families for Jaccard Distance
For the moment, we have only one way to find a family of locality-sensitive functions: use the family of minhash functions, and assume that the distance
3.6. THE THEORY OF LOCALITY-SENSITIVE FUNCTIONS 101 measure is the Jaccard distance. As before, we interpret a minhash function h
to make x and y a candidate pair if and only if h(x) = h(y).
• The family of minhash functions is a (d1,d2,1−d1,1−d2)-sensitive family
foranyd1 andd2,where0≤d1 <d2 ≤1.
The reason is that if d(x,y) ≤ d1, where d is the Jaccard distance, then SIM(x, y) = 1 − d(x, y) ≥ 1 − d1. But we know that the Jaccard similarity of x and y is equal to the probability that a minhash function will hash x and y to the same value. A similar argument applies to d2 or any distance.
Example 3.17 : We could let d1 = 0.3 and d2 = 0.6. Then we can assert that the family of minhash functions is a (0.3, 0.6, 0.7, 0.4)-sensitive family. That is, if the Jaccard distance between x and y is at most 0.3 (i.e., SIM(x,y) ≥ 0.7) then there is at least a 0.7 chance that a minhash function will send x and y to the same value, and if the Jaccard distance between x and y is at least 0.6 (i.e., SIM(x, y) ≤ 0.4), then there is at most a 0.4 chance that x and y will be sent to the same value. Note that we could make the same assertion with another choice of d1 and d2; only d1 < d2 is required. ✷
3.6.3 Amplifying a Locality-Sensitive Family
Suppose we are given a (d1, d2, p1, p2)-sensitive family F. We can construct a new family F′ by the AND-construction on F, which is defined as follows. Each memberofF′ consistsofrmembersofFforsomefixedr. Iff isinF′,andf is constructed from the set {f1,f2,...,fr} of members of F, we say f(x) = f(y) if and only if fi(x) = fi(y) for all i = 1, 2, . . . , r. Notice that this construction mirrors the effect of the r rows in a single band: the band makes x and y a candidate pair if every one of the r rows in the band say that x and y are equal (and therefore a candidate pair according to that row).
Since the members of F are independently chosen to make a member of F′, we can assert that F′ is a  d1,d2,(p1)r,(p2)r -sensitive family. That is, for any p, if p is the probability that a member of F will declare (x, y) to be a candidate pair, then the probability that a member of F′ will so declare is pr.
There is another construction, which we call the OR-construction, that turns a (d1, d2, p1, p2)-sensitive family F into a  d1, d2, 1 − (1 − p1)b, 1 − (1 − p2)b - sensitive family F′. Each member f of F′ is constructed from b members of F, say f1,f2,...,fb. We define f(x) = f(y) if and only if fi(x) = fi(y) for one or more values of i. The OR-construction mirrors the effect of combining several bands: x and y become a candidate pair if any band makes them a candidate pair.
If p is the probability that a member of F will declare (x, y) to be a candidate pair, then 1−p is the probability it will not so declare. (1−p)b is the probability that none of f1,f2,...,fb will declare (x,y) a candidate pair, and 1−(1−p)b is the probability that at least one fi will declare (x, y) a candidate pair, and therefore that f will declare (x, y) to be a candidate pair.
102 CHAPTER 3. FINDING SIMILAR ITEMS
Notice that the AND-construction lowers all probabilities, but if we choose F and r judiciously, we can make the small probability p2 get very close to 0, while the higher probability p1 stays significantly away from 0. Similarly, the OR- construction makes all probabilities rise, but by choosing F and b judiciously, we can make the larger probability approach 1 while the smaller probability remains bounded away from 1. We can cascade AND- and OR-constructions in any order to make the low probability close to 0 and the high probability close to 1. Of course the more constructions we use, and the higher the values of r and b that we pick, the larger the number of functions from the original family that we are forced to use. Thus, the better the final family of functions is, the longer it takes to apply the functions from this family.
Example 3.18 : Suppose we start with a family F. We use the AND-construc- tion with r = 4 to produce a family F1. We then apply the OR-construction to F1 with b = 4 to produce a third family F2. Note that the members of F2 each are built from 16 members of F, and the situation is analogous to starting with 16 minhash functions and treating them as four bands of four rows each.
p 1−(1−p4)4
0.2 0.0064
0.3   0.0320
0.4   0.0985
0.5   0.2275
0.6   0.4260
0.7   0.6666
0.8   0.8785
0.9   0.9860
Figure 3.10: Effect of the 4-way AND-construction followed by the 4-way OR- construction
The 4-way AND-function converts any probability p into p4. When we follow it by the 4-way OR-construction, that probability is further converted into 1 − (1 − p4)4. Some values of this transformation are indicated in Fig. 3.10. This function is an S-curve, staying low for a while, then rising steeply (although not too steeply; the slope never gets much higher than 2), and then leveling off at high values. Like any S-curve, it has a fixedpoint, the value of p that is left unchanged when we apply the function of the S-curve. In this case, the fixedpoint is the value of p for which p = 1−(1−p4)4. We can see that the fixedpoint is somewhere between 0.7 and 0.8. Below that value, probabilities are decreased, and above it they are increased. Thus, if we pick a high probability above the fixedpoint and a low probability below it, we shall have the desired effect that the low probability is decreased and the high probability is increased.
Suppose F is the minhash functions, regarded as a (0.2,0.6,0.8,0.4)-sens- itive family. Then F2, the family constructed by a 4-way AND followed by a
   
3.6. THE THEORY OF LOCALITY-SENSITIVE FUNCTIONS 103
4-way OR, is a (0.2, 0.6, 0.8785, 0.0985)-sensitive family, as we can read from the rows for 0.8 and 0.4 in Fig. 3.10. By replacing F by F2, we have reduced both the false-negative and false-positive rates, at the cost of making application of the functions take 16 times as long. ✷
p  1−(1−p)4 4 0.1 0.0140
    0.2 0.3 0.4 0.5 0.6 0.7 0.8
Figure 3.11: Effect of the 4-way construction
Example 3.19 : For the same
followed by a 4-way AND-construction. Figure 3.11 gives the transformation on probabilities implied by this construction. For instance, suppose that F is a (0.2, 0.6, 0.8, 0.4)-sensitive family. Then the constructed family is a
(0.2, 0.6, 0.9936, 0.5740)-sensitive
family. This choice is not necessarily the best. Although the higher probability has moved much closer to 1, the lower probability has also raised, increasing the number of false positives. ✷
Example 3.20 : We can cascade constructions as much as we like. For exam- ple, we could use the construction of Example 3.18 on the family of minhash functions and then use the construction of Example 3.19 on the resulting family. The constructed family would then have functions each built from 256 minhash functions. It would, for instance transform a (0.2, 0.8, 0.8, 0.2)-sensitive family into a (0.2, 0.8, 0.9991285, 0.0000004)-sensitive family. ✷
3.6.4 Exercises for Section 3.6
Exercise 3.6.1 : What is the effect on probability of starting with the family of minhash functions and applying:
(a) A 2-way AND construction followed by a 3-way OR construction. (b) A 3-way OR construction followed by a 2-way AND construction.
0.1215 0.3334 0.5740 0.7725 0.9015 0.9680 0.9936
OR-construction followed by the 4-way AND-
      cost, we can apply a 4-way OR-construction
104 (c)
(d)
CHAPTER 3. FINDING SIMILAR ITEMS
A 2-way AND construction followed by a 2-way OR construction, followed by a 2-way AND construction.
A 2-way OR construction followed by a 2-way AND construction, followed by a 2-way OR construction followed by a 2-way AND construction.
Exercise 3.6.2: Find the fixedpoints for each of the functions constructed in Exercise 3.6.1.
! Exercise 3.6.3 : Any function of probability p, such as that of Fig. 3.10, has a slope given by the derivative of the function. The maximum slope is where that derivative is a maximum. Find the value of p that gives a maximum slope for the S-curves given by Fig. 3.10 and Fig. 3.11. What are the values of these maximum slopes?
!! Exercise 3.6.4 : Generalize Exercise 3.6.3 to give, as a function of r and b, the point of maximum slope and the value of that slope, for families of functions defined from the minhash functions by:
(a) (b)
3.7
An r-way AND construction followed by a b-way OR construction. A b-way OR construction followed by an r-way AND construction.
LSH Families for Other Distance Measures
There is no guarantee that a distance measure has a locality-sensitive family of hash functions. So far, we have only seen such families for the Jaccard distance. In this section, we shall show how to construct locality-sensitive families for Hamming distance, the cosine distance and for the normal Euclidean distance.
3.7.1 LSH Families for Hamming Distance
It is quite simple to build a locality-sensitive family of functions for the Ham- ming distance. Suppose we have a space of d-dimensional vectors, and h(x, y) denotes the Hamming distance between vectors x and y. If we take any one position of the vectors, say the ith position, we can define the function fi(x) to be the ith bit of vector x. Then fi(x) = fi(y) if and only if vectors x and y agree in the ith position. Then the probability that fi(x) = fi(y) for a ran- domly chosen i is exactly 1 − h(x, y)/d; i.e., it is the fraction of positions in which x and y agree.
This situation is almost exactly like the one we encountered for minhashing. Thus, the family F consisting of the functions {f1, f2, . . . , fd} is a
(d1, d2, 1 − d1/d, 1 − d2/d)-sensitive
family of hash functions, for any d1 < d2. There are only two differences between this family and the family of minhash functions.
3.7. LSH FAMILIES FOR OTHER DISTANCE MEASURES 105
1. While Jaccard distance runs from 0 to 1, the Hamming distance on a vector space of dimension d runs from 0 to d. It is therefore necessary to scale the distances by dividing by d, to turn them into probabilities.
2. While there is essentially an unlimited supply of minhash functions, the size of the family F for Hamming distance is only d.
The first point is of no consequence; it only requires that we divide by d at appropriate times. The second point is more serious. If d is relatively small, then we are limited in the number of functions that can be composed using the AND and OR constructions, thereby limiting how steep we can make the S-curve be.
3.7.2 Random Hyperplanes and the Cosine Distance
Recall from Section 3.5.4 that the cosine distance between two vectors is the angle between the vectors. For instance, we see in Fig. 3.12 two vectors x and y that make an angle θ between them. Note that these vectors may be in a space of many dimensions, but they always define a plane, and the angle between them is measured in this plane. Figure 3.12 is a “top-view” of the plane containing x and y.
    θ
x
y
 Figure 3.12: Two vectors make an angle θ
Suppose we pick a hyperplane through the origin. This hyperplane intersects the plane of x and y in a line. Figure 3.12 suggests two possible hyperplanes, one whose intersection is the dashed line and the other’s intersection is the dotted line. To pick a random hyperplane, we actually pick the normal vector to the hyperplane, say v. The hyperplane is then the set of points whose dot product with v is 0.
106 CHAPTER 3. FINDING SIMILAR ITEMS
First, consider a vector v that is normal to the hyperplane whose projection is represented by the dashed line in Fig. 3.12; that is, x and y are on different sides of the hyperplane. Then the dot products v.x and v.y will have different signs. If we assume, for instance, that v is a vector whose projection onto the plane of x and y is above the dashed line in Fig. 3.12, then v.x is positive, while v.y is negative. The normal vector v instead might extend in the opposite direction, below the dashed line. In that case v.x is negative and v.y is positive, but the signs are still different.
On the other hand, the randomly chosen vector v could be normal to a hyperplane like the dotted line in Fig. 3.12. In that case, both v.x and v.y have the same sign. If the projection of v extends to the right, then both dot products are positive, while if v extends to the left, then both are negative.
What is the probability that the randomly chosen vector is normal to a hyperplane that looks like the dashed line rather than the dotted line? All angles for the line that is the intersection of the random hyperplane and the plane of x and y are equally likely. Thus, the hyperplane will look like the dashed line with probability θ/180 and will look like the dotted line otherwise.
Thus, each hash function f in our locality-sensitive family F is built from a randomly chosen vector vf. Given two vectors x and y, say f(x) = f(y) if and only if the dot products vf .x and vf .y have the same sign. Then F is a locality-sensitive family for the cosine distance. The parameters are essentially the same as for the Jaccard-distance family described in Section 3.6.2, except the scale of distances is 0–180 rather than 0–1. That is, F is a
(d1, d2, (180 − d1)/180, (180 − d2)/180)-sensitive
family of hash functions. From this basis, we can amplify the family as we wish,
just as for the minhash-based family.
3.7.3 Sketches
Instead of chosing a random vector from all possible vectors, it turns out to be sufficiently random if we restrict our choice to vectors whose components are +1 and −1. The dot product of any vector x with a vector v of +1’s and −1’s is formed by adding the components of x where v is +1 and then subtracting the other components of x – those where v is −1.
If we pick a collection of random vectors, say v1,v2,...,vn, then we can apply them to an arbitrary vector x by computing v1 .x, v2 .x, . . . , vn .x and then replacing any positive value by +1 and any negative value by −1. The result is called the sketch of x. You can handle 0’s arbitrarily, e.g., by chosing a result +1 or −1 at random. Since there is only a tiny probability of a zero dot product, the choice has essentially no effect.
Example 3.21 : Suppose our space consists of 4-dimensional vectors, and we pick three random vectors: v1 = [+1, −1, +1, +1], v2 = [−1, +1, −1, +1], and v3 = [+1, +1, −1, −1]. For the vector x = [3, 4, 5, 6], the sketch is [+1, +1, −1].
3.7. LSH FAMILIES FOR OTHER DISTANCE MEASURES 107
That is, v1.x = 3−4+5+6 = 10. Since the result is positive, the first component of the sketch is +1. Similarly, v2.x = 3 and v3.x = −4, so the second component of the sketch is +1 and the third component is −1.
Consider the vector y = [4, 3, 2, 1]. We can similarly compute its sketch to be [+1, −1, +1]. Since the sketches for x and y agree in 1/3 of the positions, we estimate that the angle between them is 120 degrees. That is, a randomly chosen hyperplane is twice as likely to look like the dashed line in Fig. 3.12 than like the dotted line.
The above conclusion turns out to be quite wrong. We can calculate the cosine of the angle between x and y to be x.y, which is
6 × 1 + 5 × 2 + 4 × 3 + 3 × 4 = 40
divided by the magnitudes of the two vectors. These magnitudes are
 62 +52 +42 +32 =9.274
and √12 + 22 + 32 + 42 = 5.477. Thus, the cosine of the angle between x and y is 0.7875, and this angle is about 38 degrees. However, if you look at all 16 different vectors v of length 4 that have +1 and −1 as components, you find that there are only four of these whose dot products with x and y have a different sign, namely v2, v3, and their complements [+1, −1, +1, −1] and [−1,−1,+1,+1]. Thus, had we picked all sixteen of these vectors to form a sketch, the estimate of the angle would have been 180/4 = 45 degrees. ✷
3.7.4 LSH Families for Euclidean Distance
Now, let us turn to the Euclidean distance (Section 3.5.2), and see if we can develop a locality-sensitive family of hash functions for this distance. We shall start with a 2-dimensional Euclidean space. Each hash function f in our family F will be associated with a randomly chosen line in this space. Pick a constant a and divide the line into segments of length a, as suggested by Fig. 3.13, where the “random” line has been oriented to be horizontal.
The segments of the line are the buckets into which function f hashes points. A point is hashed to the bucket in which its projection onto the line lies. If the distance d between two points is small compared with a, then there is a good chance the two points hash to the same bucket, and thus the hash function f will declare the two points equal. For example, if d = a/2, then there is at least a 50% chance the two points will fall in the same bucket. In fact, if the angle θ between the randomly chosen line and the line connecting the points is large, then there is an even greater chance that the two points will fall in the same bucket. For instance, if θ is 90 degrees, then the two points are certain to fall in the same bucket.
However, suppose d is larger than a. In order for there to be any chance of the two points falling in the same bucket, we need d cos θ ≤ a. The diagram of Fig. 3.13 suggests why this requirement holds. Note that even if d cos θ ≪ a it
  
108
CHAPTER 3. FINDING SIMILAR ITEMS
Points at distance d
θ
       Bucket width a
Figure 3.13: Two points at distance d ≫ a have a small chance of being hashed to the same bucket
is still not certain that the two points will fall in the same bucket. However, we can guarantee the following. If d ≥ 2a, then there is no more than a 1/3 chance the two points fall in the same bucket. The reason is that for cos θ to belessthan1/2,weneedtohaveθintherange60to90degrees. Ifθisinthe range 0 to 60 degrees, then cos θ is more than 1/2. But since θ is the smaller angle between two randomly chosen lines in the plane, θ is twice as likely to be between 0 and 60 as it is to be between 60 and 90.
We conclude that the family F just described forms a (a/2,2a,1/2,1/3)- sensitive family of hash functions. That is, for distances up to a/2 the proba- bility is at least 1/2 that two points at that distance will fall in the same bucket, while for distances at least 2a the probability points at that distance will fall in the same bucket is at most 1/3. We can amplify this family as we like, just as for the other examples of locality-sensitive hash functions we have discussed.
3.7.5 More LSH Families for Euclidean Spaces
There is something unsatisfying about the family of hash functions developed in Section 3.7.4. First, the technique was only described for two-dimensional Euclidean spaces. What happens if our data is points in a space with many dimensions? Second, for Jaccard and cosine distances, we were able to develop locality-sensitive families for any pair of distances d1 and d2 as long as d1 < d2. In Section 3.7.4 we appear to need the stronger condition d1 < 4d2.
However, we claim that there is a locality-sensitive family of hash func- tions for any d1 < d2 and for any number of dimensions. The family’s hash functions still derive from random lines through the space and a bucket size a that partitions the line. We still hash points by projecting them onto the line. Given that d1 < d2, we may not know what the probability p1 is that two
3.7. LSH FAMILIES FOR OTHER DISTANCE MEASURES 109
points at distance d1 hash to the same bucket, but we can be certain that it is greater than p2, the probability that two points at distance d2 hash to the same bucket. The reason is that this probability surely grows as the distance shrinks. Thus, even if we cannot calculate p1 and p2 easily, we know that there is a (d1, d2, p1, p2)-sensitive family of hash functions for any d1 < d2 and any given number of dimensions.
Using the amplification techniques of Section 3.6.3, we can then adjust the two probabilities to surround any particular value we like, and to be as far apart as we like. Of course, the further apart we want the probabilities to be, the larger the number of basic hash functions in F we must use.
3.7.6 Exercises for Section 3.7
Exercise 3.7.1 : Suppose we construct the basic family of six locality-sensitive functions for vectors of length six. For each pair of the vectors 000000, 110011, 010101, and 011100, which of the six functions makes them candidates?
Exercise 3.7.2: Let us compute sketches using the following four “random” vectors:
v1 = [+1,+1,+1,−1] v2 = [+1,+1,−1,+1] v3 = [+1,−1,+1,+1] v4 = [−1,+1,+1,+1]
Compute the sketches of the following vectors.
(a) [2,3,4,5]. (b) [−2,3,−4,5]. (c) [2,−3,4,−5].
For each pair, what is the estimated angle between them, according to the sketches? What are the true angles?
Exercise 3.7.3: Suppose we form sketches by using all sixteen of the vectors of length 4, whose components are each +1 or −1. Compute the sketches of the three vectors in Exercise 3.7.2. How do the estimates of the angles between each pair compare with the true angles?
Exercise 3.7.4: Suppose we form sketches using the four vectors from Exer- cise 3.7.2.
! (a) What are the constraints on a, b, c, and d that will cause the sketch of the vector [a, b, c, d] to be [+1, +1, +1, +1]?
!! (b) Consider two vectors [a, b, c, d] and [e, f, g, h]. What are the conditions on a, b, . . . , h that will make the sketches of these two vectors be the same?
110 CHAPTER 3. FINDING SIMILAR ITEMS
Exercise 3.7.5: Suppose we have points in a 3-dimensional Euclidean space: p1 = (1, 2, 3), p2 = (0, 2, 4), and p3 = (4, 3, 2). Consider the three hash functions defined by the three axes (to make our calculations very easy). Let buckets be of length a, with one bucket the interval [0, a) (i.e., the set of points x such that 0 ≤ x < a), the next [a, 2a), the previous one [−a, 0), and so on.
(a)
(b) (c) ! (d)
3.8
For each of the three lines, assign each of the points to buckets, assuming a = 1.
Repeat part (a), assuming a = 2.
What are the candidate pairs for the cases a = 1 and a = 2?
For each pair of points, for what values of a will that pair be a candidate pair?
Applications of Locality-Sensitive Hashing
In this section, we shall explore three examples of how LSH is used in practice. In each case, the techniques we have learned must be modified to meet certain constraints of the problem. The three subjects we cover are:
1. Entity Resolution: This term refers to matching data records that refer to the same real-world entity, e.g., the same person. The principal problem addressed here is that the similarity of records does not match exactly either the similar-sets or similar-vectors models of similarity on which the theory is built.
2. Matching Fingerprints: It is possible to represent fingerprints as sets. However, we shall explore a different family of locality-sensitive hash func- tions from the one we get by minhashing.
3. Matching Newspaper Articles: Here, we consider a different notion of shingling that focuses attention on the core article in an on-line news- paper’s Web page, ignoring all the extraneous material such as ads and newspaper-specific material.
3.8.1 Entity Resolution
It is common to have several data sets available, and to know that they refer to some of the same entities. For example, several different bibliographic sources provide information about many of the same books or papers. In the general case, we have records describing entities of some type, such as people or books. The records may all have the same format, or they may have different formats, with different kinds of information.
There are many reasons why information about an entity may vary, even if the field in question is supposed to be the same. For example, names may be
3.8. APPLICATIONS OF LOCALITY-SENSITIVE HASHING 111
expressed differently in different records because of misspellings, absence of a middle initial, use of a nickname, and many other reasons. For example, “Bob S. Jomes” and “Robert Jones Jr.” may or may not be the same person. If records come from different sources, the fields may differ as well. One source’s records may have an “age” field, while another does not. The second source might have a “date of birth” field, or it may have no information at all about when a person was born.
3.8.2 An Entity-Resolution Example
We shall examine a real example of how LSH was used to deal with an entity- resolution problem. Company A was engaged by Company B to solicit cus- tomers for B. Company B would pay A a yearly fee, as long as the customer maintained their subscription. They later quarreled and disagreed over how many customers A had provided to B. Each had about 1,000,000 records, some of which described the same people; those were the customers A had provided to B. The records had different data fields, but unfortunately none of those fields was “this is a customer that A had provided to B.” Thus, the problem was to match records from the two sets to see if a pair represented the same person.
Each record had fields for the name, address, and phone number of the person. However, the values in these fields could differ for many reasons. Not only were there the misspellings and other naming differences mentioned in Section 3.8.1, but there were other opportunities to disagree as well. A customer might give their home phone to A and their cell phone to B. Or they might move, and tell B but not A (because they no longer had need for a relationship with A). Area codes of phones sometimes change.
The strategy for identifying records involved scoring the differences in three fields: name, address, and phone. To create a score describing the likelihood that two records, one from A and the other from B, described the same per- son, 100 points was assigned to each of the three fields, so records with exact matches in all three fields got a score of 300. However, there were deductions for mismatches in each of the three fields. As a first approximation, edit-distance (Section 3.5.5) was used, but the penalty grew quadratically with the distance. Then, certain publicly available tables were used to reduce the penalty in ap- propriate situations. For example, “Bill” and “William” were treated as if they differed in only one letter, even though their edit-distance is 5.
However, it is not feasible to score all one trillion pairs of records. Thus, a simple LSH was used to focus on likely candidates. Three “hash functions” were used. The first sent records to the same bucket only if they had identical names; the second did the same but for identical addresses, and the third did the same for phone numbers. In practice, there was no hashing; rather the records were sorted by name, so records with identical names would appear consecutively and get scored for overall similarity of the name, address, and phone. Then the records were sorted by address, and those with the same
112 CHAPTER 3. FINDING SIMILAR ITEMS
   When Are Record Matches Good Enough?
While every case will be different, it may be of interest to know how the experiment of Section 3.8.3 turned out on the data of Section 3.8.2. For scores down to 185, the value of x was very close to 10; i.e., these scores indicated that the likelihood of the records representing the same person was essentially 1. Note that a score of 185 in this example represents a situation where one field is the same (as would have to be the case, or the records would never even be scored), one field was completely different, and the third field had a small discrepancy. Moreover, for scores as low as 115, the value of x was noticeably less than 45, meaning that some of these pairs did represent the same person. Note that a score of 115 represents a case where one field is the same, but there is only a slight similarity in the other two fields.
 address were scored. Finally, the records were sorted a third time by phone, and records with identical phones were scored.
This approach missed a record pair that truly represented the same person but none of the three fields matched exactly. Since the goal was to prove in a court of law that the persons were the same, it is unlikely that such a pair would have been accepted by a judge as sufficiently similar anyway.
3.8.3 Validating Record Matches
What remains is to determine how high a score indicates that two records truly represent the same individual. In the example at hand, there was an easy way to make that decision, and the technique can be applied in many similar situations. It was decided to look at the creation-dates for the records at hand, and to assume that 90 days was an absolute maximum delay between the time the service was bought at Company A and registered at B. Thus, a proposed match between two records that were chosen at random, subject only to the constraint that the date on the B-record was between 0 and 90 days after the date on the A-record, would have an average delay of 45 days.
It was found that of the pairs with a perfect 300 score, the average delay was 10 days. If you assume that 300-score pairs are surely correct matches, then you can look at the pool of pairs with any given score s, and compute the average delay of those pairs. Suppose that the average delay is x, and the fraction of true matches among those pairs with score s is f . Then x = 10f + 45(1 − f ), or x = 45 − 35f . Solving for f , we find that the fraction of the pairs with score s that are truly matches is (45 − x)/35.
The same trick can be used whenever:
1. There is a scoring system used to evaluate the likelihood that two records
3.8. APPLICATIONS OF LOCALITY-SENSITIVE HASHING 113 represent the same entity, and
2. There is some field, not used in the scoring, from which we can derive a measure that differs, on average, for true pairs and false pairs.
For instance, suppose there were a “height” field recorded by both companies A and B in our running example. We can compute the average difference in height for pairs of random records, and we can compute the average difference in height for records that have a perfect score (and thus surely represent the same entities). For a given score s, we can evaluate the average height difference of the pairs with that score and estimate the probability of the records representing the same entity. That is, if h0 is the average height difference for the perfect matches, h1 is the average height difference for random pairs, and h is the average height difference for pairs of score s, then the fraction of good pairs with score s is (h1 − h)/(h1 − h0).
3.8.4 Matching Fingerprints
When fingerprints are matched by computer, the usual representation is not an image, but a set of locations in which minutiae are located. A minutia, in the context of fingerprint descriptions, is a place where something unusual happens, such as two ridges merging or a ridge ending. If we place a grid over a fingerprint, we can represent the fingerprint by the set of grid squares in which minutiae are located.
Ideally, before overlaying the grid, fingerprints are normalized for size and orientation, so that if we took two images of the same finger, we would find minutiae lying in exactly the same grid squares. We shall not consider here the best ways to normalize images. Let us assume that some combination of techniques, including choice of grid size and placing a minutia in several adjacent grid squares if it lies close to the border of the squares enables us to assume that grid squares from two images have a significantly higher probability of agreeing in the presence or absence of a minutia than if they were from images of different fingers.
Thus, fingerprints can be represented by sets of grid squares – those where their minutiae are located – and compared like any sets, using the Jaccard sim- ilarity or distance. There are two versions of fingerprint comparison, however.
• The many-one problem is the one we typically expect. A fingerprint has been found on a gun, and we want to compare it with all the fingerprints in a large database, to see which one matches.
• The many-many version of the problem is to take the entire database, and see if there are any pairs that represent the same individual.
While the many-many version matches the model that we have been following for finding similar items, the same technology can be used to speed up the many-one problem.
114 CHAPTER 3. FINDING SIMILAR ITEMS 3.8.5 A LSH Family for Fingerprint Matching
We could minhash the sets that represent a fingerprint, and use the standard LSH technique from Section 3.4. However, since the sets are chosen from a relatively small set of grid points (perhaps 1000), the need to minhash them into more succinct signatures is not clear. We shall study here another form of locality-sensitive hashing that works well for data of the type we are discussing.
Suppose for an example that the probability of finding a minutia in a random grid square of a random fingerprint is 20%. Also, assume that if two fingerprints come from the same finger, and one has a minutia in a given grid square, then the probability that the other does too is 80%. We can define a locality-sensitive family of hash functions as follows. Each function f in this family F is defined by three grid squares. Function f says “yes” for two fingerprints if both have minutiae in all three grid squares, and otherwise f says “no.” Put another way, we may imagine that f sends to a single bucket all fingerprints that have minutiae in all three of f’s grid points, and sends each other fingerprint to a bucket of its own. In what follows, we shall refer to the first of these buckets as “the” bucket for f and ignore the buckets that are required to be singletons.
If we want to solve the many-one problem, we can use many functions from the family F and precompute their buckets of fingerprints to which they answer “yes.” Then, given a new fingerprint that we want to match, we determine which of these buckets it belongs to and compare it with all the fingerprints found in any of those buckets. To solve the many-many problem, we compute the buckets for each of the functions and compare all fingerprints in each of the buckets.
Let us consider how many functions we need to get a reasonable probability of catching a match, without having to compare the fingerprint on the gun with each of the millions of fingerprints in the database. First, the probability that two fingerprints from different fingers would be in the bucket for a function f in F is (0.2)6 = 0.000064. The reason is that they will both go into the bucket only if they each have a minutia in each of the three grid points associated with f, and the probability of each of those six independent events is 0.2.
Now, consider the probability that two fingerprints from the same finger wind up in the bucket for f. The probability that the first fingerprint has minutiae in each of the three squares belonging to f is (0.2)3 = 0.008. However, if it does, then the probability is (0.8)3 = 0.512 that the other fingerprint will as well. Thus, if the fingerprints are from the same finger, there is a 0.008 × 0.512 = 0.004096 probability that they will both be in the bucket of f . That is not much; it is about one in 200. However, if we use many functions from F, but not too many, then we can get a good probability of matching fingerprints from the same finger while not having too many false positives – fingerprints that must be considered but do not match.
Example 3.22: For a specific example, let us suppose that we use 1024 functions chosen randomly from F. Next, we shall construct a new fam- ily F1 by performing a 1024-way OR on F. Then the probability that F1
3.8. APPLICATIONS OF LOCALITY-SENSITIVE HASHING 115
will put fingerprints from the same finger together in at least one bucket is 1 − (1 − 0.004096)1024 = 0.985. On the other hand, the probability that two fingerprints from different fingers will be placed in the same bucket is (1 − (1 − 0.000064)1024 = 0.063. That is, we get about 1.5% false negatives and about 6.3% false positives. ✷
The result of Example 3.22 is not the best we can do. While it offers only a 1.5% chance that we shall fail to identify the fingerprint on the gun, it does force us to look at 6.3% of the entire database. Increasing the number of functions from F will increase the number of false positives, with only a small benefit of reducing the number of false negatives below 1.5%. On the other hand, we can also use the AND construction, and in so doing, we can greatly reduce the probability of a false positive, while making only a small increase in the false-negative rate. For instance, we could take 2048 functions from F in two groups of 1024. Construct the buckets for each of the functions. However, given a fingerprint P on the gun:
1. Find the buckets from the first group in which P belongs, and take the union of these buckets.
2. Do the same for the second group.
3. Take the intersection of the two unions.
4. Compare P only with those fingerprints in the intersection.
Note that we still have to take unions and intersections of large sets of finger- prints, but we compare only a small fraction of those. It is the comparison of fingerprints that takes the bulk of the time; in steps (1) and (2) fingerprints can be represented by their integer indices in the database.
If we use this scheme, the probability of detecting a matching fingerprint is (0.985)2 = 0.970; that is, we get about 3% false negatives. However, the probability of a false positive is (0.063)2 = 0.00397. That is, we only have to examine about 1/250th of the database.
3.8.6 Similar News Articles
Our last case study concerns the problem of organizing a large repository of on-line news articles by grouping together Web pages that were derived from the same basic text. It is common for organizations like The Associated Press to produce a news item and distribute it to many newspapers. Each newspaper puts the story in its on-line edition, but surrounds it by information that is special to that newspaper, such as the name and address of the newspaper, links to related articles, and links to ads. In addition, it is common for the newspaper to modify the article, perhaps by leaving off the last few paragraphs or even deleting text from the middle. As a result, the same news article can appear quite different at the Web sites of different newspapers.
116 CHAPTER 3. FINDING SIMILAR ITEMS
The problem looks very much like the one that was suggested in Section 3.4: find documents whose shingles have a high Jaccard similarity. Note that this problem is different from the problem of finding news articles that tell about the same events. The latter problem requires other techniques, typically examining the set of important words in the documents (a concept we discussed briefly in Section 1.3.1) and clustering them to group together different articles about the same topic.
However, an interesting variation on the theme of shingling was found to be more effective for data of the type described. The problem is that shingling as we described it in Section 3.2 treats all parts of a document equally. However, we wish to ignore parts of the document, such as ads or the headlines of other articles to which the newspaper added a link, that are not part of the news article. It turns out that there is a noticeable difference between text that appears in prose and text that appears in ads or headlines. Prose has a much greater frequency of stop words, the very frequent words such as “the” or “and.” The total number of words that are considered stop words varies with the application, but it is common to use a list of several hundred of the most frequent words.
Example 3.23 : A typical ad might say simply “Buy Sudzo.” On the other hand, a prose version of the same thought that might appear in an article is “I recommend that you buy Sudzo for your laundry.” In the latter sentence, it would be normal to treat “I,” “that,” “you,” “for,” and “your” as stop words. ✷
Suppose we define a shingle to be a stop word followed by the next two words. Then the ad “Buy Sudzo” from Example 3.23 has no shingles and would not be reflected in the representation of the Web page containing that ad. On the other hand, the sentence from Example 3.23 would be represented by five shingles: “I recommend that,” “that you buy,” “you buy Sudzo,” “for your laundry,” and “your laundry x,” where x is whatever word follows that sentence.
Suppose we have two Web pages, each of which consists of half news text and half ads or other material that has a low density of stop words. If the news text is the same but the surrounding material is different, then we would expect that a large fraction of the shingles of the two pages would be the same. They might have a Jaccard similarity of 75%. However, if the surrounding material is the same but the news content is different, then the number of common shingles would be small, perhaps 25%. If we were to use the conventional shingling, where shingles are (say) sequences of 10 consecutive characters, we would expect the two documents to share half their shingles (i.e., a Jaccard similarity of 1/3), regardless of whether it was the news or the surrounding material that they shared.
3.8. APPLICATIONS OF LOCALITY-SENSITIVE HASHING 117 3.8.7 Exercises for Section 3.8
Exercise 3.8.1: Suppose we are trying to perform entity resolution among bibliographic references, and we score pairs of references based on the similar- ities of their titles, list of authors, and place of publication. Suppose also that all references include a year of publication, and this year is equally likely to be any of the ten most recent years. Further, suppose that we discover that among the pairs of references with a perfect score, there is an average difference in the publication year of 0.1.6 Suppose that the pairs of references with a certain score s are found to have an average difference in their publication dates of 2. What is the fraction of pairs with score s that truly represent the same pub- lication? Note: Do not make the mistake of assuming the average difference in publication date between random pairs is 5 or 5.5. You need to calculate it exactly, and you have enough information to do so.
Exercise 3.8.2: Suppose we use the family F of functions described in Sec- tion 3.8.5, where there is a 20% chance of a minutia in an grid square, an 80% chance of a second copy of a fingerprint having a minutia in a grid square where the first copy does, and each function in F being formed from three grid squares. In Example 3.22, we constructed family F1 by using the OR construction on 1024 members of F. Suppose we instead used family F2 that is a 2048-way OR of members of F.
(a) Compute the rates of false positives and false negatives for F2.
(b) How do these rates compare with what we get if we organize the same 2048 functions into a 2-way AND of members of F1, as was discussed at the end of Section 3.8.5?
Exercise 3.8.3 : Suppose fingerprints have the same statistics outlined in Ex- ercise 3.8.2, but we use a base family of functions F′ defined like F, but using only two randomly chosen grid squares. Construct another set of functions F′1 from F′ by taking the n-way OR of functions from F′. What, as a function of n, are the false positive and false negative rates for F′1?
Exercise 3.8.4 : Suppose we use the functions F1 from Example 3.22, but we want to solve the many-many problem.
(a) If two fingerprints are from the same finger, what is the probability that they will not be compared (i.e., what is the false negative rate)?
(b) What fraction of the fingerprints from different fingers will be compared (i.e., what is the false positive rate)?
! Exercise 3.8.5: Assume we have the set of functions F as in Exercise 3.8.2, and we construct a new set of functions F3 by an n-way OR of functions in F. For what value of n is the sum of the false positive and false negative rates minimized?
6We might expect the average to be 0, but in practice, errors in publication year do occur.
 
118 CHAPTER 3. FINDING SIMILAR ITEMS 3.9 Methods for High Degrees of Similarity
LSH-based methods appear most effective when the degree of similarity we accept is relatively low. When we want to find sets that are almost identical, there are other methods that can be faster. Moreover, these methods are exact, in that they find every pair of items with the desired degree of similarity. There are no false negatives, as there can be with LSH.
3.9.1 Finding Identical Items
The extreme case is finding identical items, for example, Web pages that are identical, character-for-character. It is straightforward to compare two docu- ments and tell whether they are identical, but we still must avoid having to compare every pair of documents. Our first thought would be to hash docu- ments based on their first few characters, and compare only those documents that fell into the same bucket. That scheme should work well, unless all the documents begin with the same characters, such as an HTML header.
Our second thought would be to use a hash function that examines the entire document. That would work, and if we use enough buckets, it would be very rare that two documents went into the same bucket, yet were not identical. The downside of this approach is that we must examine every character of every document. If we limit our examination to a small number of characters, then we never have to examine a document that is unique and falls into a bucket of its own.
A better approach is to pick some fixed random positions for all documents, and make the hash function depend only on these. This way, we can avoid a problem where there is a common prefix for all or most documents, yet we need not examine entire documents unless they fall into a bucket with another document. One problem with selecting fixed positions is that if some documents are short, they may not have some of the selected positions. However, if we are looking for highly similar documents, we never need to compare two documents that differ significantly in their length. We exploit this idea in Section 3.9.3.
3.9.2 Representing Sets as Strings
Now, let us focus on the harder problem of finding, in a large collection of sets, all pairs that have a high Jaccard similarity, say at least 0.9. We can represent a set by sorting the elements of the universal set in some fixed order, and representing any set by listing its elements in this order. The list is essentially a string of “characters,” where the characters are the elements of the universal set. These strings are unusual, however, in that:
1. No character appears more than once in a string, and
2. If two characters appear in two different strings, then they appear in the same order in both strings.
3.9. METHODS FOR HIGH DEGREES OF SIMILARITY 119
Example 3.24 : Suppose the universal set consists of the 26 lower-case letters, and we use the normal alphabetical order. Then the set {d, a, b} is represented by the string abd. ✷
In what follows, we shall assume all strings represent sets in the manner just described. Thus, we shall talk about the Jaccard similarity of strings, when strictly speaking we mean the similarity of the sets that the strings represent. Also, we shall talk of the length of a string, as a surrogate for the number of elements in the set that the string represents.
Note that the documents discussed in Section 3.9.1 do not exactly match this model, even though we can see documents as strings. To fit the model, we would shingle the documents, assign an order to the shingles, and represent each document by its list of shingles in the selected order.
3.9.3 Length-Based Filtering
The simplest way to exploit the string representation of Section 3.9.2 is to sort the strings by length. Then, each string s is compared with those strings t that follow s in the list, but are not too long. Suppose the lower bound on Jaccard similarity between two strings is J. For any string x, denote its length by Lx. Note that Ls ≤ Lt. The intersection of the sets represented by s and t cannot have more than Ls members, while their union has at least Lt members. Thus, the Jaccard similarity of s and t, which we denote SIM(s,t), is at most Ls/Lt. That is, in order for s and t to require comparison, it must be that J ≤ Ls/Lt, or equivalently, Lt ≤ Ls/J.
Example 3.25 : Suppose that s is a string of length 9, and we are looking for strings with at least 0.9 Jaccard similarity. Then we have only to compare s with strings following it in the length-based sorted order that have length at most 9/0.9 = 10. That is, we compare s with those strings of length 9 that follow it in order, and all strings of length 10. We have no need to compare s with any other string.
Suppose the length of s were 8 instead. Then s would be compared with following strings of length up to 8/0.9 = 8.89. That is, a string of length 9 would be too long to have a Jaccard similarity of 0.9 with s, so we only have to compare s with the strings that have length 8 but follow it in the sorted order. ✷
3.9.4 Prefix Indexing
In addition to length, there are several other features of strings that can be exploited to limit the number of comparisons that must be made to identify all pairs of similar strings. The simplest of these options is to create an index for each symbol; recall a symbol of a string is any one of the elements of the universal set. For each string s, we select a prefix of s consisting of the first p
120 CHAPTER 3. FINDING SIMILAR ITEMS
   A Better Ordering for Symbols
Instead of using the obvious order for elements of the universal set, e.g., lexicographic order for shingles, we can order symbols rarest first. That is, determine how many times each element appears in the collection of sets, and order them by this count, lowest first. The advantage of doing so is that the symbols in prefixes will tend to be rare. Thus, they will cause that string to be placed in index buckets that have relatively few members. Then, when we need to examine a string for possible matches, we shall find few other strings that are candidates for comparison.
 symbols of s. How large p must be depends on Ls and J, the lower bound on Jaccard similarity. We add string s to the index for each of its first p symbols. In effect, the index for each symbol becomes a bucket of strings that must be
compared. We must be certain that any other string t such that SIM(s, t) ≥ J will have at least one symbol in its prefix that also appears in the prefix of s.
Suppose not; rather SIM(s,t) ≥ J, but t has none of the first p symbols of s. Then the highest Jaccard similarity that s and t can have occurs when t is a suffix of s, consisting of everything but the first p symbols of s. The Jaccard similarity of s and t would then be (Ls − p)/Ls. To be sure that we do not have to compare s with t, we must be certain that J > (Ls − p)/Ls. That is, p must be at least ⌊(1−J)Ls⌋+1. Of course we want p to be as small as possible, so we do not index string s in more buckets than we need to. Thus, we shall hereafter take p = ⌊(1 − J)Ls⌋ + 1 to be the length of the prefix that gets indexed.
Example3.26: Suppose J = 0.9. If Ls = 9, then p = ⌊0.1×9⌋+1 = ⌊0.9⌋ + 1 = 1. That is, we need to index s under only its first symbol. Any string t that does not have the first symbol of s in a position such that t is indexed by that symbol will have Jaccard similarity with s that is less than 0.9. Suppose s is bcdefghij. Then s is indexed under b only. Suppose t does not begin with b. There are two cases to consider.
1. If t begins with a, and SIM(s, t) ≥ 0.9, then it can only be that t is abcdefghij. But if that is the case, t will be indexed under both a and b. The reason is that Lt = 10, so t will be indexed under the symbols of its prefix of length ⌊0.1 × 10⌋ + 1 = 2.
2. If t begins with c or a later letter, then the maximum value of SIM(s, t) occurs when t is cdefghij. But then SIM(s, t) = 8/9 < 0.9.
In general, with J = 0.9, strings of length up to 9 are indexed by their first symbol, strings of lengths 10–19 are indexed under their first two symbols,
3.9. METHODS FOR HIGH DEGREES OF SIMILARITY 121 strings of length 20–29 are indexed under their first three symbols, and so on.
✷
We can use the indexing scheme in two ways, depending on whether we are trying to solve the many-many problem or a many-one problem; recall the distinction was introduced in Section 3.8.4. For the many-one problem, we create the index for the entire database. To query for matches to a new set S, we convert that set to a string s, which we call the probe string. Determine the length of the prefix that must be considered, that is, ⌊(1 − J)Ls⌋ + 1. For each symbol appearing in one of the prefix positions of s, we look in the index bucket for that symbol, and we compare s with all the strings appearing in that bucket.
If we want to solve the many-many problem, start with an empty database of strings and indexes. For each set S, we treat S as a new set for the many-one problem. We convert S to a string s, which we treat as a probe string in the many-one problem. However, after we examine an index bucket, we also add s to that bucket, so s will be compared with later strings that could be matches.
3.9.5 Using Position Information
Consider the strings s = acdefghijk and t = bcdefghijk, and assume J = 0.9. Since both strings are of length 10, they are indexed under their first two symbols. Thus, s is indexed under a and c, while t is indexed under b and c. Whichever is added last will find the other in the bucket for c, and they will be compared. However, since c is the second symbol of both, we know there will be two symbols, a and b in this case, that are in the union of the two sets but not in the intersection. Indeed, even though s and t are identical from c to the end, their intersection is 9 symbols and their union is 11; thus SIM(s, t) = 9/11, which is less than 0.9.
If we build our index based not only on the symbol, but on the position of the symbol within the string, we could avoid comparing s and t above. That is, let our index have a bucket for each pair (x, i), containing the strings that have symbol x in position i of their prefix. Given a string s, and assuming J is the minimum desired Jaccard similarity, we look at the prefix of s, that is, the positions 1 through ⌊(1 − J)Ls⌋ + 1. If the symbol in position i of the prefix is x, add s to the index bucket for (x, i).
Now consider s as a probe string. With what buckets must it be compared? We shall visit the symbols of the prefix of s from the left, and we shall take advantage of the fact that we only need to find a possible matching string t if none of the previous buckets we have examined for matches held t. That is, we only need to find a candidate match once. Thus, if we find that the ith symbol of s is x, then we need look in the bucket (x, j) for certain small values of j.
To compute the upper bound on j, suppose t is a string none of whose first j − 1 symbols matched anything in s, but the ith symbol of s is the same as the jth symbol of t. The highest value of SIM(s,t) occurs if s and t are identical
122
CHAPTER 3.
Symbols definitely appearing in
only one string
i
FINDING SIMILAR ITEMS
    s t
 j
Figure 3.14: Strings s and t begin with i − 1 and j − 1 unique symbols, respec- tively, and then agree beyond that
beyond their ith and jth symbols, respectively, as suggested by Fig. 3.14. If that is the case, the size of their intersection is Ls − i + 1, since that is the number of symbols of s that could possibly be in t. The size of their union is at least Ls + j − 1. That is, s surely contributes Ls symbols to the union, and there are also at least j − 1 symbols of t that are not in s. The ratio of the sizes of the intersection and union must be at least J, so we must have:
Ls − i + 1 ≥ J Ls + j − 1
If we isolate j in this inequality, we have j ≤  Ls(1 − J) − i + 1 + J /J.
Example 3.27 : Consider the string s = acdefghijk with J = 0.9 discussed at the beginning of this section. Suppose s is now a probe string. We already established that we need to consider the first two positions; that is, i can be 1 or 2. Suppose i = 1. Then j ≤ (10×0.1−1+1+0.9)/0.9. That is, we only have to compare the symbol a with strings in the bucket for (a, j) if j ≤ 2.11. Thus, j can be 1 or 2, but nothing higher.
Now suppose i = 2. Then we require j ≤ (10×0.1−2+1+0.9)/0.9, Or j ≤ 1. We conclude that we must look in the buckets for (a, 1), (a, 2), and (c, 1), but in no other bucket. In comparison, using the buckets of Section 3.9.4, we would look into the buckets for a and c, which is equivalent to looking to all buckets (a, j) and (c, j) for any j. ✷
3.9.6 Using Position and Length in Indexes
When we considered the upper limit on j in the previous section, we assumed that what follows positions i and j were as in Fig. 3.14, where what followed these positions in strings s and t matched exactly. We do not want to build an index that involves every symbol in the strings, because that makes the total work excessive. However, we can add to our index a summary of what follows the positions being indexed. Doing so expands the number of buckets, but not beyond reasonable bounds, and yet enables us to eliminate many candidate
 
3.9. METHODS FOR HIGH DEGREES OF SIMILARITY 123
matches without comparing entire strings. The idea is to use index buckets corresponding to a symbol, a position, and the suffix length, that is, the number of symbols following the position in question.
Example 3.28 : The string s = acdefghijk, with J = 0.9, would be indexed in the buckets for (a, 1, 9) and (c, 2, 8). That is, the first position of s has symbol a, and its suffix is of length 9. The second position has symbol c and its suffix is of length 8. ✷
Figure 3.14 assumes that the suffixes for position i of s and position j of t have the same length. If not, then we can either get a smaller upper bound on the size of the intersection of s and t (if t is shorter) or a larger lower bound on the size of the union (if t is longer). Suppose s has suffix length p and t has suffix length q.
Case 1: p ≥ q. Here, the maximum size of the intersection is Ls −i+1−(p−q)
Since Ls = i + p, we can write the above expression for the intersection size as q+1. TheminimumsizeoftheunionisLs+j−1,asitwaswhenwedidnot take suffix length into account. Thus, we require
q+1 ≥J Ls + j − 1
whenever p ≥ q.
Case 2: p < q. Here, the maximum size of the intersection is Ls − i + 1, as when suffix length was not considered. However, the minimum size of the union isnowLs+j−1+q−p. IfweagainusetherelationshipLs =i+p,wecan replace Ls − p by i and get the formula i + j − 1 + q for the size of the union. If the Jaccard similarity is at least J, then
Ls−i+1 ≥J i+j−1+q
whenever p < q.
Example 3.29 : Let us again consider the string s = acdefghijk, but to make the example show some details, let us choose J = 0.8 instead of 0.9. We know that Ls = 10. Since ⌊(1 − J)Ls⌋ + 1 = 3, we must consider prefix positions i=1,2,and3inwhatfollows. Asbefore,letpbethesuffixlengthofsandq the suffix length of t.
First, consider the case p ≥ q. The additional constraint we have on q and j is (q + 1)/(9 + j) ≥ 0.8. We can enumerate the pairs of values of j and q for each i between 1 and 3, as follows.
  
124 CHAPTER 3. FINDING SIMILAR ITEMS i=1: Here,p=9,soq≤9. Letusconsiderthepossiblevaluesofq:
q=9: Wemusthave10/(9+j)≥0.8. Thus,wecanhavej=1,j=2, orj=3. Notethatforj=4,10/13>0.8.
q=8: Wemusthave9/(9+j)≥0.8. Thus,wecanhavej=1orj=2. For j = 3, 9/12 > 0.8.
q = 7: We must have 8/(9 + j) ≥ 0.8. Only j = 1 satisfies this inequality. q = 6: There are no possible values of j, since 7/(9 + j) > 0.8 for every
positive integer j. The same holds for every smaller value of q.
i = 2: Here, p = 8, so we require q ≤ 8. Since the constraint (q+1)/(9+j) ≥ 0.8 does not depend on i,7 we can use the analysis from the above case, but exclude the case q = 9. Thus, the only possible values of j and q when i = 2 are
1. q=8;j=1. 2. q=8;j=2. 3. q=7;j=1.
i = 3: Now, p = 7 and the constraints are q ≤ 7 and (q+1)/(9+j) ≥ 0.8. The only option is q = 7 and j = 1.
Next, we must consider the case p < q. The additional constraint is 11 − i ≥ 0.8
 i+j+q−1 Again, consider each possible value of i.
i=1: Then p = 9, so we require q ≥ 10 and 10/(q+j) ≥ 0.8. The possible values of q and j are
1. q=10;j=1. 2. q=10;j=2. 3. q=11;j=1.
i = 2: Now, p = 8, so we require q ≥ 9 and 9/(q + j + 1) ≥ 0.8. Since j must be a positive integer, the only solution is q = 9 and j = 1, a possibility that we already knew about.
i=3: Here,p=7,sowerequireq≥8and8/(q+j+2)≥0.8. Thereareno solutions.
When we accumulate the possible combinations of i, j, and q, we see that the set of index buckets in which we must look forms a pyramid. Figure 3.15 shows the buckets in which we must search. That is, we must look in those buckets (x, j, q) such that the ith symbol of the string s is x, j is the position associated with the bucket and q the suffix length. ✷
7Note that i does influence the value of p, and through p, puts a limit on q.
 
3.9. METHODS FOR HIGH DEGREES OF SIMILARITY 125
q j=1 j=2 j=3 x
xx i=1xxx
xx x
x
i=2xx x
i=3 x
Figure 3.15: The buckets that must be examined to find possible matches for
the string s = acdefghijk with J = 0.8 are marked with an x 3.9.7 Exercises for Section 3.9
Exercise 3.9.1: Suppose our universal set is the lower-case letters, and the order of elements is taken to be the vowels, in alphabetic order, followed by the consonants in reverse alphabetic order. Represent the following sets as strings.
a {q,w,e,r,t,y}.
(b) {a,s,d,f,g,h,j,u,i}.
Exercise 3.9.2 : Suppose we filter candidate pairs based only on length, as in Section 3.9.3. If s is a string of length 20, with what strings is s compared when J, the lower bound on Jaccard similarity has the following values: (a) J = 0.85 (b) J = 0.95 (c) J = 0.98?
Exercise 3.9.3 : Suppose we have a string s of length 15, and we wish to index its prefix as in Section 3.9.4.
     7 8 9
10 11
          7 8 9
      7
   (a)
(b) ! (c)
How many positions are in the prefix if J = 0.85? How many positions are in the prefix if J = 0.95?
For what range of values of J will s be indexed under its first four symbols, but no more?
Exercise 3.9.4 : Suppose s is a string of length 12. With what symbol-position pairs will s be compared with if we use the indexing approach of Section 3.9.5, and (a) J = 0.75 (b) J = 0.95?
! Exercise 3.9.5 : Suppose we use position information in our index, as in Sec- tion 3.9.5. Strings s and t are both chosen at random from a universal set of 100 elements. Assume J = 0.9. What is the probability that s and t will be compared if
126 CHAPTER 3. FINDING SIMILAR ITEMS (a) s and t are both of length 9.
(b) s and t are both of length 10.
Exercise 3.9.6: Suppose we use indexes based on both position and suffix length, as in Section 3.9.6. If s is a string of length 20, with what symbol- position-length triples will s be compared with, if (a) J = 0.8 (b) J = 0.9?
3.10 Summary of Chapter 3
✦ Jaccard Similarity: The Jaccard similarity of sets is the ratio of the size of the intersection of the sets to the size of the union. This measure of similarity is suitable for many applications, including textual similarity of documents and similarity of buying habits of customers.
✦ Shingling: A k-shingle is any k characters that appear consecutively in a document. If we represent a document by its set of k-shingles, then the Jaccard similarity of the shingle sets measures the textual similarity of documents. Sometimes, it is useful to hash shingles to bit strings of shorter length, and use sets of hash values to represent documents.
✦ Minhashing: A minhash function on sets is based on a permutation of the universal set. Given any such permutation, the minhash value for a set is that element of the set that appears first in the permuted order.
✦ Minhash Signatures: We may represent sets by picking some list of per- mutations and computing for each set its minhash signature, which is the sequence of minhash values obtained by applying each permutation on the list to that set. Given two sets, the expected fraction of the permutations that will yield the same minhash value is exactly the Jaccard similarity of the sets.
✦ Efficient Minhashing: Since it is not really possible to generate random permutations, it is normal to simulate a permutation by picking a random hash function and taking the minhash value for a set to be the least hash value of any of the set’s members.
✦ Locality-Sensitive Hashing for Signatures: This technique allows us to avoid computing the similarity of every pair of sets or their minhash sig- natures. If we are given signatures for the sets, we may divide them into bands, and only measure the similarity of a pair of sets if they are identi- cal in at least one band. By choosing the size of bands appropriately, we can eliminate from consideration most of the pairs that do not meet our threshold of similarity.
✦ Distance Measures: A distance measure is a function on pairs of points in a space that satisfy certain axioms. The distance between two points is 0 if
3.10. SUMMARY OF CHAPTER 3 127
the points are the same, but greater than 0 if the points are different. The distance is symmetric; it does not matter in which order we consider the two points. A distance measure must satisfy the triangle inequality: the distance between two points is never more than the sum of the distances between those points and some third point.
✦ Euclidean Distance: The most common notion of distance is the Euclidean distance in an n-dimensional space. This distance, sometimes called the L2-norm, is the square root of the sum of the squares of the differences between the points in each dimension. Another distance suitable for Eu- clidean spaces, called Manhattan distance or the L1-norm is the sum of the magnitudes of the differences between the points in each dimension.
✦ Jaccard Distance: One minus the Jaccard similarity is a distance measure, called the Jaccard distance.
✦ Cosine Distance: The angle between vectors in a vector space is the cosine distance measure. We can compute the cosine of that angle by taking the dot product of the vectors and dividing by the lengths of the vectors.
✦ Edit Distance: This distance measure applies to a space of strings, and is the number of insertions and/or deletions needed to convert one string into the other. The edit distance can also be computed as the sum of the lengths of the strings minus twice the length of the longest common subsequence of the strings.
✦ Hamming Distance: This distance measure applies to a space of vectors. The Hamming distance between two vectors is the number of positions in which the vectors differ.
✦ Generalized Locality-Sensitive Hashing: We may start with any collection of functions, such as the minhash functions, that can render a decision as to whether or not a pair of items should be candidates for similarity checking. The only constraint on these functions is that they provide a lower bound on the probability of saying “yes” if the distance (according to some distance measure) is below a given limit, and an upper bound on the probability of saying “yes” if the distance is above another given limit. We can then increase the probability of saying “yes” for nearby items and at the same time decrease the probability of saying “yes” for distant items to as great an extent as we wish, by applying an AND construction and an OR construction.
✦ Random Hyperplanes and LSH for Cosine Distance: We can get a set of basis functions to start a generalized LSH for the cosine distance measure by identifying each function with a list of randomly chosen vectors. We apply a function to a given vector v by taking the dot product of v with each vector on the list. The result is a sketch consisting of the signs (+1 or −1) of the dot products. The fraction of positions in which the sketches of
128 CHAPTER 3. FINDING SIMILAR ITEMS two vectors agree, multiplied by 180, is an estimate of the angle between
the two vectors.
✦ LSH For Euclidean Distance: A set of basis functions to start LSH for Euclidean distance can be obtained by choosing random lines and project- ing points onto those lines. Each line is broken into fixed-length intervals, and the function answers “yes” to a pair of points that fall into the same interval.
✦ High-Similarity Detection by String Comparison: An alternative approach to finding similar items, when the threshold of Jaccard similarity is close to 1, avoids using minhashing and LSH. Rather, the universal set is ordered, and sets are represented by strings, consisting their elements in order. The simplest way to avoid comparing all pairs of sets or their strings is to note that highly similar sets will have strings of approximately the same length. If we sort the strings, we can compare each string with only a small number of the immediately following strings.
✦ Character Indexes: If we represent sets by strings, and the similarity threshold is close to 1, we can index all strings by their first few characters. The prefix whose characters must be indexed is approximately the length of the string times the maximum Jaccard distance (1 minus the minimum Jaccard similarity).
✦ Position Indexes: We can index strings not only on the characters in their prefixes, but on the position of that character within the prefix. We reduce the number of pairs of strings that must be compared, because if two strings share a character that is not in the first position in both strings, then we know that either there are some preceding characters that are in the union but not the intersection, or there is an earlier symbol that appears in both strings.
✦ Suffix Indexes: We can also index strings based not only on the characters in their prefixes and the positions of those characters, but on the length of the character’s suffix – the number of positions that follow it in the string. This structure further reduces the number of pairs that must be compared, because a common symbol with different suffix lengths implies additional characters that must be in the union but not in the intersection.
3.11 References for Chapter 3
The technique we called shingling is attributed to [10]. The use in the manner we discussed here is from [2]. Minhashing comes from [3]. The original works on locality-sensitive hashing were [9] and [7]. [1] is a useful summary of ideas in this field.
3.11. REFERENCES FOR CHAPTER 3 129
[4] introduces the idea of using random-hyperplanes to summarize items in a way that respects the cosine distance. [8] suggests that random hyperplanes plus LSH can be more accurate at detecting similar documents than minhashing plus LSH.
Techniques for summarizing points in a Euclidean space are covered in [6]. [11] presented the shingling technique based on stop words.
The length and prefix-based indexing schemes for high-similarity matching comes from [5]. The technique involving suffix length is from [12].
1. A. Andoni and P. Indyk, “Near-optimal hashing algorithms for approxi- mate nearest neighbor in high dimensions,” Comm. ACM 51:1, pp. 117– 122, 2008.
2. A.Z. Broder, “On the resemblance and containment of documents,” Proc. Compression and Complexity of Sequences, pp. 21–29, Positano Italy, 1997.
3. A.Z. Broder, M. Charikar, A.M. Frieze, and M. Mitzenmacher, “Min-wise independent permutations,” ACM Symposium on Theory of Computing, pp. 327–336, 1998.
4. M.S. Charikar, “Similarity estimation techniques from rounding algo- rithms,” ACM Symposium on Theory of Computing, pp. 380–388, 2002.
5. S. Chaudhuri, V. Ganti, and R. Kaushik, “A primitive operator for sim- ilarity joins in data cleaning,” Proc. Intl. Conf. on Data Engineering, 2006.
6. M. Datar, N. Immorlica, P. Indyk, and V.S. Mirrokni, “Locality-sensitive hashing scheme based on p-stable distributions,” Symposium on Compu- tational Geometry pp. 253–262, 2004.
7. A. Gionis, P. Indyk, and R. Motwani, “Similarity search in high dimen- sions via hashing,” Proc. Intl. Conf. on Very Large Databases, pp. 518– 529, 1999.
8. M. Henzinger, “Finding near-duplicate web pages: a large-scale evaluation of algorithms,” Proc. 29th SIGIR Conf., pp. 284–291, 2006.
9. P. Indyk and R. Motwani. “Approximate nearest neighbor: towards re- moving the curse of dimensionality,” ACM Symposium on Theory of Com- puting, pp. 604–613, 1998.
10. U. Manber, “Finding similar files in a large file system,” Proc. USENIX Conference, pp. 1–10, 1994.
11. M. Theobald, J. Siddharth, and A. Paepcke, “SpotSigs: robust and effi- cient near duplicate detection in large web collections,” 31st Annual ACM SIGIR Conference, July, 2008, Singapore.
130 CHAPTER 3. FINDING SIMILAR ITEMS
12. C. Xiao, W. Wang, X. Lin, and J.X. Yu, “Efficient similarity joins for near duplicate detection,” Proc. WWW Conference, pp. 131-140, 2008.
Chapter 4
Mining Data Streams
Most of the algorithms described in this book assume that we are mining a database. That is, all our data is available when and if we want it. In this chapter, we shall make another assumption: data arrives in a stream or streams, and if it is not processed immediately or stored, then it is lost forever. Moreover, we shall assume that the data arrives so rapidly that it is not feasible to store it all in active storage (i.e., in a conventional database), and then interact with it at the time of our choosing.
The algorithms for processing streams each involve summarization of the stream in some way. We shall start by considering how to make a useful sample of a stream and how to filter a stream to eliminate most of the “undesirable” elements. We then show how to estimate the number of different elements in a stream using much less storage than would be required if we listed all the elements we have seen.
Another approach to summarizing a stream is to look at only a fixed-length “window” consisting of the last n elements for some (typically large) n. We then query the window as if it were a relation in a database. If there are many streams and/or n is large, we may not be able to store the entire window for every stream, so we need to summarize even the windows. We address the fundamental problem of maintaining an approximate count on the number of 1’s in the window of a bit stream, while using much less space than would be needed to store the entire window itself. This technique generalizes to approximating various kinds of sums.
4.1 The Stream Data Model
Let us begin by discussing the elements of streams and stream processing. We explain the difference between streams and databases and the special problems that arise when dealing with streams. Some typical applications where the stream model applies will be examined.
131
132
CHAPTER 4.
Ad−hoc Queries
MINING DATA STREAMS
Output streams
Archival Storage
  Streams entering
1, 5, 2, 7, 4, 0, 3, 5 q, w, e, r, t, y, u, i, o
0, 1, 1, 0, 1, 0, 0, 0 .. .
time
                     4.1.1
Figure 4.1: A data-stream-management system
A Data-Stream-Management System
Standing Queries
Stream Processor
 Limited Working Storage
In analogy to a database-management system, we can view a stream processor as a kind of data-management system, the high-level organization of which is suggested in Fig. 4.1. Any number of streams can enter the system. Each stream can provide elements at its own schedule; they need not have the same data rates or data types, and the time between elements of one stream need not be uniform. The fact that the rate of arrival of stream elements is not under the control of the system distinguishes stream processing from the processing of data that goes on within a database-management system. The latter system controls the rate at which data is read from the disk, and therefore never has to worry about data getting lost as it attempts to execute queries.
Streams may be archived in a large archival store, but we assume it is not possible to answer queries from the archival store. It could be examined only under special circumstances using time-consuming retrieval processes. There is also a working store, into which summaries or parts of streams may be placed, and which can be used for answering queries. The working store might be disk, or it might be main memory, depending on how fast we need to process queries. But either way, it is of sufficiently limited capacity that it cannot store all the data from all the streams.
4.1. THE STREAM DATA MODEL 133 4.1.2 Examples of Stream Sources
Before proceeding, let us consider some of the ways in which stream data arises naturally.
Sensor Data
Imagine a temperature sensor bobbing about in the ocean, sending back to a base station a reading of the surface temperature each hour. The data produced by this sensor is a stream of real numbers. It is not a very interesting stream, since the data rate is so low. It would not stress modern technology, and the entire stream could be kept in main memory, essentially forever.
Now, give the sensor a GPS unit, and let it report surface height instead of temperature. The surface height varies quite rapidly compared with tempera- ture, so we might have the sensor send back a reading every tenth of a second. If it sends a 4-byte real number each time, then it produces 3.5 megabytes per day. It will still take some time to fill up main memory, let alone a single disk.
But one sensor might not be that interesting. To learn something about ocean behavior, we might want to deploy a million sensors, each sending back a stream, at the rate of ten per second. A million sensors isn’t very many; there would be one for every 150 square miles of ocean. Now we have 3.5 terabytes arriving every day, and we definitely need to think about what can be kept in working storage and what can only be archived.
Image Data
Satellites often send down to earth streams consisting of many terabytes of images per day. Surveillance cameras produce images with lower resolution than satellites, but there can be many of them, each producing a stream of images at intervals like one second. London is said to have six million such cameras, each producing a stream.
Internet and Web Traffic
A switching node in the middle of the Internet receives streams of IP packets from many inputs and routes them to its outputs. Normally, the job of the switch is to transmit data and not to retain it or query it. But there is a tendency to put more capability into the switch, e.g., the ability to detect denial-of-service attacks or the ability to reroute packets based on information about congestion in the network.
Web sites receive streams of various types. For example, Google receives sev- eral hundred million search queries per day. Yahoo! accepts billions of “clicks” per day on its various sites. Many interesting things can be learned from these streams. For example, an increase in queries like “sore throat” enables us to track the spread of viruses. A sudden increase in the click rate for a link could
134 CHAPTER 4. MINING DATA STREAMS indicate some news connected to that page, or it could mean that the link is
broken and needs to be repaired.
4.1.3 Stream Queries
There are two ways that queries get asked about streams. We show in Fig. 4.1 a place within the processor where standing queries are stored. These queries are, in a sense, permanently executing, and produce outputs at appropriate times.
Example 4.1: The stream produced by the ocean-surface-temperature sen- sor mentioned at the beginning of Section 4.1.2 might have a standing query to output an alert whenever the temperature exceeds 25 degrees centigrade. This query is easily answered, since it depends only on the most recent stream element.
Alternatively, we might have a standing query that, each time a new reading arrives, produces the average of the 24 most recent readings. That query also can be answered easily, if we store the 24 most recent stream elements. When a new stream element arrives, we can drop from the working store the 25th most recent element, since it will never again be needed (unless there is some other standing query that requires it).
Another query we might ask is the maximum temperature ever recorded by that sensor. We can answer this query by retaining a simple summary: the maximum of all stream elements ever seen. It is not necessary to record the entire stream. When a new stream element arrives, we compare it with the stored maximum, and set the maximum to whichever is larger. We can then answer the query by producing the current value of the maximum. Similarly, if we want the average temperature over all time, we have only to record two values: the number of readings ever sent in the stream and the sum of those readings. We can adjust these values easily each time a new reading arrives, and we can produce their quotient as the answer to the query. ✷
The other form of query is ad-hoc, a question asked once about the current state of a stream or streams. If we do not store all streams in their entirety, as normally we can not, then we cannot expect to answer arbitrary queries about streams. If we have some idea what kind of queries will be asked through the ad-hoc query interface, then we can prepare for them by storing appropriate parts or summaries of streams as in Example 4.1.
If we want the facility to ask a wide variety of ad-hoc queries, a common approach is to store a sliding window of each stream in the working store. A sliding window can be the most recent n elements of a stream, for some n, or it can be all the elements that arrived within the last t time units, e.g., one day. If we regard each stream element as a tuple, we can treat the window as a relation and query it with any SQL query. Of course the stream-management system must keep the window fresh, deleting the oldest elements as new ones come in.
4.1. THE STREAM DATA MODEL 135
Example 4.2 : Web sites often like to report the number of unique users over the past month. If we think of each login as a stream element, we can maintain a window that is all logins in the most recent month. We must associate the arrival time with each login, so we know when it no longer belongs to the window. If we think of the window as a relation Logins(name, time), then it is simple to get the number of unique users over the past month. The SQL query is:
SELECT COUNT(DISTINCT(name)) FROM Logins
WHERE time >= t;
Here, t is a constant that represents the time one month before the current time.
Note that we must be able to maintain the entire stream of logins for the past month in working storage. However, for even the largest sites, that data is not more than a few terabytes, and so surely can be stored on disk. ✷
4.1.4 Issues in Stream Processing
Before proceeding to discuss algorithms, let us consider the constraints under which we work when dealing with streams. First, streams often deliver elements very rapidly. We must process elements in real time, or we lose the opportunity to process them at all, without accessing the archival storage. Thus, it often is important that the stream-processing algorithm is executed in main memory, without access to secondary storage or with only rare accesses to secondary storage. Moreover, even when streams are “slow,” as in the sensor-data example of Section 4.1.2, there may be many such streams. Even if each stream by itself can be processed using a small amount of main memory, the requirements of all the streams together can easily exceed the amount of available main memory.
Thus, many problems about streaming data would be easy to solve if we had enough memory, but become rather hard and require the invention of new techniques in order to execute them at a realistic rate on a machine of realistic size. Here are two generalizations about stream algorithms worth bearing in mind as you read through this chapter:
• Often, it is much more efficient to get an approximate answer to our problem than an exact solution.
• As in Chapter 3, a variety of techniques related to hashing turn out to be useful. Generally, these techniques introduce useful randomness into the algorithm’s behavior, in order to produce an approximate answer that is very close to the true result.
136 CHAPTER 4. MINING DATA STREAMS 4.2 Sampling Data in a Stream
As our first example of managing streaming data, we shall look at extracting reliable samples from a stream. As with many stream algorithms, the “trick” involves using hashing in a somewhat unusual way.
4.2.1 A Motivating Example
The general problem we shall address is selecting a subset of a stream so that we can ask queries about the selected subset and have the answers be statistically representative of the stream as a whole. If we know what queries are to be asked, then there are a number of methods that might work, but we are looking for a technique that will allow ad-hoc queries on the sample. We shall look at a particular problem, from which the general idea will emerge.
Our running example is the following. A search engine receives a stream of queries,anditwouldliketostudythebehavioroftypicalusers.1 Weassumethe stream consists of tuples (user, query, time). Suppose that we want to answer queries such as “What fraction of the typical user’s queries were repeated over the past month?” Assume also that we wish to store only 1/10th of the stream elements.
The obvious approach would be to generate a random number, say an integer from 0 to 9, in response to each search query. Store the tuple if and only if the random number is 0. If we do so, each user has, on average, 1/10th of their queries stored. Statistical fluctuations will introduce some noise into the data, but if users issue many queries, the law of large numbers will assure us that most users will have a fraction quite close to 1/10th of their queries stored.
However, this scheme gives us the wrong answer to the query asking for the average number of duplicate queries for a user. Suppose a user has issued s search queries one time in the past month, d search queries twice, and no search queries more than twice. If we have a 1/10th sample, of queries, we shall see in the sample for that user an expected s/10 of the search queries issued once. Of the d search queries issued twice, only d/100 will appear twice in the sample; that fraction is d times the probability that both occurrences of the query will be in the 1/10th sample. Of the queries that appear twice in the full stream, 18d/100 will appear exactly once. To see why, note that 18/100 is the probability that one of the two occurrences will be in the 1/10th of the stream that is selected, while the other is in the 9/10th that is not selected.
The correct answer to the query about the fraction of repeated searches is d/(s+d). However, the answer we shall obtain from the sample is d/(10s+19d). To derive the latter formula, note that d/100 appear twice, while s/10+18d/100 appear once. Thus, the fraction appearing twice in the sample is d/100 divided
1While we shall refer to “users,” the search engine really receives IP addresses from which the search query was issued. We shall assume that these IP addresses identify unique users, which is approximately true, but not exactly true.
 
4.2. SAMPLING DATA IN A STREAM 137 by d/100 + s/10 + 18d/100. This ratio is d/(10s + 19d). For no positive values
of s and d is d/(s+d) = d/(10s+19d).
4.2.2 Obtaining a Representative Sample
The query of Section 4.2.1, like many queries about the statistics of typical users, cannot be answered by taking a sample of each user’s search queries. Thus, we must strive to pick 1/10th of the users, and take all their searches for the sample, while taking none of the searches from other users. If we can store a list of all users, and whether or not they are in the sample, then we could do the following. Each time a search query arrives in the stream, we look up the user to see whether or not they are in the sample. If so, we add this search query to the sample, and if not, then not. However, if we have no record of ever having seen this user before, then we generate a random integer between 0 and 9. If the number is 0, we add this user to our list with value “in,” and if the number is other than 0, we add the user with the value “out.”
That method works as long as we can afford to keep the list of all users and their in/out decision in main memory, because there isn’t time to go to disk for every search that arrives. By using a hash function, one can avoid keeping the list of users. That is, we hash each user name to one of ten buckets, 0 through 9. If the user hashes to bucket 0, then accept this search query for the sample, and if not, then not.
Note we do not actually store the user in the bucket; in fact, there is no data in the buckets at all. Effectively, we use the hash function as a random- number generator, with the important property that, when applied to the same user several times, we always get the same “random” number. That is, without storing the in/out decision for any user, we can reconstruct that decision any time a search query by that user arrives.
More generally, we can obtain a sample consisting of any rational fraction a/b of the users by hashing user names to b buckets, 0 through b − 1. Add the search query to the sample if the hash value is less than a.
4.2.3 The General Sampling Problem
The running example is typical of the following general problem. Our stream consists of tuples with n components. A subset of the components are the key components, on which the selection of the sample will be based. In our running example, there are three components – user, query, and time – of which only user is in the key. However, we could also take a sample of queries by making query be the key, or even take a sample of user-query pairs by making both those components form the key.
To take a sample of size a/b, we hash the key value for each tuple to b buckets, and accept the tuple for the sample if the hash value is less than a. If the key consists of more than one component, the hash function needs to combine the values for those components to make a single hash-value. The
138 CHAPTER 4. MINING DATA STREAMS
result will be a sample consisting of all tuples with certain key values. The selected key values will be approximately a/b of all the key values appearing in the stream.
4.2.4 Varying the Sample Size
Often, the sample will grow as more of the stream enters the system. In our running example, we retain all the search queries of the selected 1/10th of the users, forever. As time goes on, more searches for the same users will be accumulated, and new users that are selected for the sample will appear in the stream.
If we have a budget for how many tuples from the stream can be stored as the sample, then the fraction of key values must vary, lowering as time goes on. In order to assure that at all times, the sample consists of all tuples from a subset of the key values, we choose a hash function h from key values to a very large number of values 0, 1, . . . , B − 1. We maintain a threshold t, which initially can be the largest bucket number, B − 1. At all times, the sample consists of those tuples whose key K satisfies h(K) ≤ t. New tuples from the stream are added to the sample if and only if they satisfy the same condition.
If the number of stored tuples of the sample exceeds the allotted space, we lower t to t−1 and remove from the sample all those tuples whose key K hashes to t. For efficiency, we can lower t by more than 1, and remove the tuples with several of the highest hash values, whenever we need to throw some key values out of the sample. Further efficiency is obtained by maintaining an index on the hash value, so we can find all those tuples whose keys hash to a particular value quickly.
4.2.5 Exercises for Section 4.2
Exercise 4.2.1 : Suppose we have a stream of tuples with the schema Grades(university, courseID, studentID, grade)
Assume universities are unique, but a courseID is unique only within a uni- versity (i.e., different universities may have different courses with the same ID, e.g., “CS101”) and likewise, studentID’s are unique only within a university (different universities may assign the same ID to different students). Suppose we want to answer certain queries approximately from a 1/20th sample of the data. For each of the queries below, indicate how you would construct the sample. That is, tell what the key attributes should be.
(a) For each university, estimate the average number of students in a course. (b) Estimate the fraction of students who have a GPA of 3.5 or more.
(c) Estimate the fraction of courses where at least half the students got “A.”
4.3. FILTERING STREAMS 139 4.3 Filtering Streams
Another common process on streams is selection, or filtering. We want to accept those tuples in the stream that meet a criterion. Accepted tuples are passed to another process as a stream, while other tuples are dropped. If the selection criterion is a property of the tuple that can be calculated (e.g., the first component is less than 10), then the selection is easy to do. The problem becomes harder when the criterion involves lookup for membership in a set. It is especially hard, when that set is too large to store in main memory. In this section, we shall discuss the technique known as “Bloom filtering” as a way to eliminate most of the tuples that do not meet the criterion.
4.3.1 A Motivating Example
Again let us start with a running example that illustrates the problem and what we can do about it. Suppose we have a set S of one billion allowed email addresses – those that we will allow through because we believe them not to be spam. The stream consists of pairs: an email address and the email itself. Since the typical email address is 20 bytes or more, it is not reasonable to store S in main memory. Thus, we can either use disk accesses to determine whether or not to let through any given stream element, or we can devise a method that requires no more main memory than we have available, and yet will filter most of the undesired stream elements.
Suppose for argument’s sake that we have one gigabyte of available main memory. In the technique known as Bloom filtering, we use that main memory as a bit array. In this case, we have room for eight billion bits, since one byte equals eight bits. Devise a hash function h from email addresses to eight billion buckets. Hash each member of S to a bit, and set that bit to 1. All other bits of the array remain 0.
Since there are one billion members of S, approximately 1/8th of the bits will be 1. The exact fraction of bits set to 1 will be slightly less than 1/8th, because it is possible that two members of S hash to the same bit. We shall discuss the exact fraction of 1’s in Section 4.3.3. When a stream element arrives, we hash its email address. If the bit to which that email address hashes is 1, then we let the email through. But if the email address hashes to a 0, we are certain that the address is not in S, so we can drop this stream element.
Unfortunately, some spam email will get through. Approximately 1/8th of the stream elements whose email address is not in S will happen to hash to a bit whose value is 1 and will be let through. Nevertheless, since the majority of emails are spam (about 80% according to some reports), eliminating 7/8th of the spam is a significant benefit. Moreover, if we want to eliminate every spam, we need only check for membership in S those good and bad emails that get through the filter. Those checks will require the use of secondary memory to access S itself. There are also other options, as we shall see when we study the general Bloom-filtering technique. As a simple example, we could use a cascade
140 CHAPTER 4. MINING DATA STREAMS
of filters, each of which would eliminate 7/8th of the remaining spam.
4.3.2 The Bloom Filter
A Bloom filter consists of:
1. An array of n bits, initially all 0’s.
2. A collection of hash functions h1 , h2 , . . . , hk . Each hash function maps “key” values to n buckets, corresponding to the n bits of the bit-array.
3. A set S of m key values.
The purpose of the Bloom filter is to allow through all stream elements whose keys are in S, while rejecting most of the stream elements whose keys are not in S.
To initialize the bit array, begin with all bits 0. Take each key value in S and hash it using each of the k hash functions. Set to 1 each bit that is hi(K) for some hash function hi and some key value K in S.
To test a key K that arrives in the stream, check that all of h1(K), h2(K), . . . , hk(K)
are 1’s in the bit-array. If all are 1’s, then let the stream element through. If one or more of these bits are 0, then K could not be in S, so reject the stream element.
4.3.3 Analysis of Bloom Filtering
If a key value is in S, then the element will surely pass through the Bloom filter. However, if the key value is not in S, it might still pass. We need to understand how to calculate the probability of a false positive, as a function of n, the bit-array length, m the number of members of S, and k, the number of hash functions.
The model to use is throwing darts at targets. Suppose we have x targets and y darts. Any dart is equally likely to hit any target. After throwing the darts, how many targets can we expect to be hit at least once? The analysis is similar to the analysis in Section 3.4.2, and goes as follows:
• The probability that a given dart will not hit a given target is (x − 1)/x.
• The probability that none of the y darts will hit a given target is   x−1  y .
 xx.
We can write this expression as (1 − 1 )x( y ) x
  • Using the approximation (1 − ǫ)1/ǫ = 1/e for small ǫ (recall Section 1.3.5),
we conclude that the probability that none of the y darts hit a given target is e−y/x.
4.3. FILTERING STREAMS 141
Example 4.3 : Consider the running example of Section 4.3.1. We can use the above calculation to get the true expected number of 1’s in the bit array. Think of each bit as a target, and each member of S as a dart. Then the probability that a given bit will be 1 is the probability that the corresponding target will be hit by one or more darts. Since there are one billion members of S, we have y = 109 darts. As there are eight billion bits, there are x = 8 × 109 targets. Thus, the probability that a given target is not hit is e−y/x = e−1/8 and the probability that it is hit is 1 − e−1/8. That quantity is about 0.1175. In Section 4.3.1 we suggested that 1/8 = 0.125 is a good approximation, which it is, but now we have the exact calculation. ✷
We can apply the rule to the more general situation, where set S has m members, the array has n bits, and there are k hash functions. The number of targets is x = n, and the number of darts is y = km. Thus, the probability that a bit remains 0 is e−km/n. We want the fraction of 0 bits to be fairly large, or else the probability that a nonmember of S will hash at least once to a 0 becomes too small, and there are too many false positives. For example, we might choose k, the number of hash functions to be n/m or less. Then the probability of a 0 is at least e−1 or 37%. In general, the probability of a false positive is the probability of a 1 bit, which is 1 − e−km/n, raised to the kth power, i.e., (1 − e−km/n)k.
Example 4.4 : In Example 4.3 we found that the fraction of 1’s in the array of our running example is 0.1175, and this fraction is also the probability of a false positive. That is, a nonmember of S will pass through the filter if it hashes to a 1, and the probability of it doing so is 0.1175.
Suppose we used the same S and the same array, but used two different hash functions. This situation corresponds to throwing two billion darts at eight billion targets, and the probability that a bit remains 0 is e−1/4. In order to be a false positive, a nonmember of S must hash twice to bits that are 1, and this probability is (1 − e−1/4)2, or approximately 0.0493. Thus, adding a second hash function for our running example is an improvement, reducing the false-positive rate from 0.1175 to 0.0493. ✷
4.3.4 Exercises for Section 4.3
Exercise 4.3.1: For the situation of our running example (8 billion bits, 1 billion members of the set S), calculate the false-positive rate if we use three hash functions? What if we use four hash functions?
! Exercise 4.3.2: Suppose we have n bits of memory available, and our set S has m members. Instead of using k hash functions, we could divide the n bits into k arrays, and hash once to each array. As a function of n, m, and k, what is the probability of a false positive? How does it compare with using k hash functions into a single array?
142 CHAPTER 4. MINING DATA STREAMS
!! Exercise 4.3.3: As a function of n, the number of bits and m the number of members in the set S, what number of hash functions minimizes the false- positive rate?
4.4 Counting Distinct Elements in a Stream
In this section we look at a third simple kind of processing we might want to do on a stream. As with the previous examples – sampling and filtering – it is somewhat tricky to do what we want in a reasonable amount of main memory, so we use a variety of hashing and a randomized algorithm to get approximately what we want with little space needed per stream.
4.4.1 The Count-Distinct Problem
Suppose stream elements are chosen from some universal set. We would like to know how many different elements have appeared in the stream, counting either from the beginning of the stream or from some known time in the past.
Example 4.5 : As a useful example of this problem, consider a Web site gath- ering statistics on how many unique users it has seen in each given month. The universal set is the set of logins for that site, and a stream element is generated each time someone logs in. This measure is appropriate for a site like Amazon, where the typical user logs in with their unique login name.
A similar problem is a Web site like Google that does not require login to issue a search query, and may be able to identify users only by the IP address from which they send the query. There are about 4 billion IP addresses,2 sequences of four 8-bit bytes will serve as the universal set in this case. ✷
The obvious way to solve the problem is to keep in main memory a list of all the elements seen so far in the stream. Keep them in an efficient search structure such as a hash table or search tree, so one can quickly add new elements and check whether or not the element that just arrived on the stream was already seen. As long as the number of distinct elements is not too great, this structure can fit in main memory and there is little problem obtaining an exact answer to the question how many distinct elements appear in the stream.
However, if the number of distinct elements is too great, or if there are too many streams that need to be processed at once (e.g., Yahoo! wants to count the number of unique users viewing each of its pages in a month), then we cannot store the needed data in main memory. There are several options. We could use more machines, each machine handling only one or several of the streams. We could store most of the data structure in secondary memory and batch stream elements so whenever we brought a disk block to main memory there would be many tests and updates to be performed on the data in that block. Or we could use the strategy to be discussed in this section, where we
2At least that will be the case until IPv6 becomes the norm.
 
4.4. COUNTING DISTINCT ELEMENTS IN A STREAM 143 only estimate the number of distinct elements but use much less memory than
the number of distinct elements.
4.4.2 The Flajolet-Martin Algorithm
It is possible to estimate the number of distinct elements by hashing the ele- ments of the universal set to a bit-string that is sufficiently long. The length of the bit-string must be sufficient that there are more possible results of the hash function than there are elements of the universal set. For example, 64 bits is sufficient to hash URL’s. We shall pick many different hash functions and hash each element of the stream using these hash functions. The important property of a hash function is that when applied to the same element, it always produces the same result. Notice that this property was also essential for the sampling technique of Section 4.2.
The idea behind the Flajolet-Martin Algorithm is that the more different elements we see in the stream, the more different hash-values we shall see. As we see more different hash-values, it becomes more likely that one of these values will be “unusual.” The particular unusual property we shall exploit is that the value ends in many 0’s, although many other options exist.
Whenever we apply a hash function h to a stream element a, the bit string h(a) will end in some number of 0’s, possibly none. Call this number the tail length for a and h. Let R be the maximum tail length of any a seen so far in the stream. Then we shall use estimate 2R for the number of distinct elements seen in the stream.
This estimate makes intuitive sense. The probability that a given stream
element a has h(a) ending in at least r 0’s is 2−r. Suppose there are m distinct
elements in the stream. Then the probability that none of them has tail length
at least r is (1 − 2−r)m. This sort of expression should be familiar by now.   −r 2r m2−r
We can rewrite it as (1 − 2 ) . Assuming r is reasonably large, the inner expression is of the form (1 − ǫ)1/ǫ, which is approximately 1/e. Thus, the probability of not finding a stream element with as many as r 0’s at the end of its hash value is e−m2−r . We can conclude:
1. If m is much larger than 2r, then the probability that we shall find a tail of length at least r approaches 1.
2. If m is much less than 2r, then the probability of finding a tail length at least r approaches 0.
We conclude from these two points that the proposed estimate of m, which is 2R (recall R is the largest tail length for any stream element) is unlikely to be either much too high or much too low.
144 CHAPTER 4. MINING DATA STREAMS 4.4.3 Combining Estimates
Unfortunately, there is a trap regarding the strategy for combining the estimates of m, the number of distinct elements, that we obtain by using many different hash functions. Our first assumption would be that if we take the average of the values 2R that we get from each hash function, we shall get a value that approaches the true m, the more hash functions we use. However, that is not the case, and the reason has to do with the influence an overestimate has on the average.
Consider a value of r such that 2r is much larger than m. There is some probability p that we shall discover r to be the largest number of 0’s at the end of the hash value for any of the m stream elements. Then the probability of finding r + 1 to be the largest number of 0’s instead is at least p/2. However, if we do increase by 1 the number of 0’s at the end of a hash value, the value of 2R doubles. Consequently, the contribution from each possible large R to the expected value of 2R grows as R grows, and the expected value of 2R is actually infinite.3
Another way to combine estimates is to take the median of all estimates. The median is not affected by the occasional outsized value of 2R, so the worry described above for the average should not carry over to the median. Unfortu- nately, the median suffers from another defect: it is always a power of 2. Thus, no matter how many hash functions we use, should the correct value of m be between two powers of 2, say 400, then it will be impossible to obtain a close estimate.
There is a solution to the problem, however. We can combine the two methods. First, group the hash functions into small groups, and take their average. Then, take the median of the averages. It is true that an occasional outsized 2R will bias some of the groups and make them too large. However, taking the median of group averages will reduce the influence of this effect almost to nothing. Moreover, if the groups themselves are large enough, then the averages can be essentially any number, which enables us to approach the true value m as long as we use enough hash functions. In order to guarantee that any possible average can be obtained, groups should be of size at least a small multiple of log2 m.
4.4.4 Space Requirements
Observe that as we read the stream it is not necessary to store the elements seen. The only thing we need to keep in main memory is one integer per hash function; this integer records the largest tail length seen so far for that hash function and any stream element. If we are processing only one stream, we could use millions of hash functions, which is far more than we need to get a
3Technically, since the hash value is a bit-string of finite length, there is no contribution to 2R for R’s that are larger than the length of the hash value. However, this effect is not enough to avoid the conclusion that the expected value of 2R is much too large.
 
4.5. ESTIMATING MOMENTS 145
close estimate. Only if we are trying to process many streams at the same time would main memory constrain the number of hash functions we could associate with any one stream. In practice, the time it takes to compute hash values for each stream element would be the more significant limitation on the number of hash functions we use.
4.4.5 Exercises for Section 4.4
Exercise 4.4.1 : Suppose our stream consists of the integers 3, 1, 4, 1, 5, 9, 2, 6,5.Ourhashfunctionswillallbeoftheformh(x)=ax+b mod32forsome a and b. You should treat the result as a 5-bit binary integer. Determine the tail length for each stream element and the resulting estimate of the number of distinct elements if the hash function is:
(a) h(x)=2x+1 mod32. (b) h(x)=3x+7 mod32.
(c) h(x) = 4x mod 32.
! Exercise 4.4.2 : Do you see any problems with the choice of hash functions in Exercise 4.4.1? What advice could you give someone who was going to use a hashfunctionoftheformh(x)=ax+b mod2k?
4.5 Estimating Moments
In this section we consider a generalization of the problem of counting distinct elements in a stream. The problem, called computing “moments,” involves the distribution of frequencies of different elements in the stream. We shall define moments of all orders and concentrate on computing second moments, from which the general algorithm for all moments is a simple extension.
4.5.1 Definition of Moments
Suppose a stream consists of elements chosen from a universal set. Assume the universal set is ordered so we can speak of the ith element for any i. Let mi be the number of occurrences of the ith element for any i. Then the kth-order moment (or just kth moment) of the stream is the sum over all i of (mi)k.
Example 4.6 : The 0th moment is the sum of 1 for each mi that is greater than 0.4 That is, the 0th moment is a count of the number of distinct elements in the stream. We can use the method of Section 4.4 to estimate the 0th moment of a stream.
4Technically, since mi could be 0 for some elements in the universal set, we need to make explicit in the definition of “moment” that 00 is taken to be 0. For moments 1 and above, the contribution of mi’s that are 0 is surely 0.
 
146 CHAPTER 4. MINING DATA STREAMS
The 1st moment is the sum of the mi’s, which must be the length of the stream. Thus, first moments are especially easy to compute; just count the length of the stream seen so far.
The second moment is the sum of the squares of the mi’s. It is some- times called the surprise number, since it measures how uneven the distribu- tion of elements in the stream is. To see the distinction, suppose we have a stream of length 100, in which eleven different elements appear. The most even distribution of these eleven elements would have one appearing 10 times and the other ten appearing 9 times each. In this case, the surprise number is 102 + 10 × 92 = 910. At the other extreme, one of the eleven elements could appear 90 times and the other ten appear 1 time each. Then, the surprise numberwouldbe902+10×12 =8110. ✷
As in Section 4.4, there is no problem computing moments of any order if we can afford to keep in main memory a count for each element that appears in the stream. However, also as in that section, if we cannot afford to use that much memory, then we need to estimate the kth moment by keeping a limited number of values in main memory and computing an estimate from these values. For the case of distinct elements, each of these values were counts of the longest tail produced by a single hash function. We shall see another form of value that is useful for second and higher moments.
4.5.2 The Alon-Matias-Szegedy Algorithm for Second Moments
For now, let us assume that a stream has a particular length n. We shall show how to deal with growing streams in the next section. Suppose we do not have enough space to count all the mi’s for all the elements of the stream. We can still estimate the second moment of the stream using a limited amount of space; the more space we use, the more accurate the estimate will be. We compute some number of variables. For each variable X, we store:
1. A particular element of the universal set, which we refer to as X.element, and
2. An integer X.value, which is the value of the variable. To determine the value of a variable X, we choose a position in the stream between 1 and n, uniformly and at random. Set X.element to be the element found there, and initialize X.value to 1. As we read the stream, add 1 to X.value each time we encounter another occurrence of X.element.
Example 4.7: Suppose the stream is a,b,c,b,d,a,c,d,a,b,d,c,a,a,b. The length of the stream is n = 15. Since a appears 5 times, b appears 4 times, and c and d appear three times each, the second moment for the stream is 52 + 42 + 32 + 32 = 59. Suppose we keep three variables, X1, X2, and X3. Also,
4.5. ESTIMATING MOMENTS 147
assume that at “random” we pick the 3rd, 8th, and 13th positions to define these three variables.
When we reach position 3, we find element c, so we set X1.element = c and X1.value = 1. Position 4 holds b, so we do not change X1. Likewise, nothing happens at positions 5 or 6. At position 7, we see c again, so we set X1.value = 2.
At position 8 we find d, and so set X2.element = d and X2.value = 1. Positions 9 and 10 hold a and b, so they do not affect X1 or X2. Position 11 holds d so we set X2.value = 2, and position 12 holds c so we set X1.value = 3. At position 13, we find element a, and so set X3.element = a and X3.value = 1. Then, at position 14 we see another a and so set X3.value = 2. Position 15, with element b does not affect any of the variables, so we are done, with final values X1.value = 3 and X2.value = X3.value = 2. ✷
We can derive an estimate of the second moment from any variable X. This estimate is n(2X.value − 1).
Example 4.8 : Consider the three variables from Example 4.7. From X1 we derive the estimate n(2X1.value − 1) = 15 × (2 × 3 − 1) = 75. The other two variables, X2 and X3, each have value 2 at the end, so their estimates are 15 × (2 × 2 − 1) = 45. Recall that the true value of the second moment for this stream is 59. On the other hand, the average of the three estimates is 55, a fairly close approximation. ✷
4.5.3 Why the Alon-Matias-Szegedy Algorithm Works
We can prove that the expected value of any variable constructed as in Sec- tion 4.5.2 is the second moment of the stream from which it is constructed. Some notation will make the argument easier to follow. Let e(i) be the stream element that appears at position i in the stream, and let c(i) be the number of times element e(i) appears in the stream among positions i, i + 1, . . . , n.
Example 4.9 : Consider the stream of Example 4.7. e(6) = a, since the 6th position holds a. Also, c(6) = 4, since a appears at positions 9, 13, and 14, as well as at position 6. Note that a also appears at position 1, but that fact does not contribute to c(6). ✷
The expected value of n(2X.value − 1) is the average over all positions i between 1 and n of n(2c(i) − 1), that is
   1 n
E n(2X.value − 1) = n n(2c(i) − 1)
i=1
We can simplify the above by canceling factors 1/n and n, to get
n
E n(2X.value − 1)  =   2c(i) − 1  i=1
 
148 CHAPTER 4. MINING DATA STREAMS
However, to make sense of the formula, we need to change the order of summation by grouping all those positions that have the same element. For instance, concentrate on some element a that appears ma times in the stream. The term for the last position in which a appears must be 2×1−1 = 1. The term for the next-to-last position in which a appears is 2 × 2 − 1 = 3. The positions with a before that yield terms 5, 7, and so on, up to 2ma − 1, which is the term for the first position in which a appears. That is, the formula for the expected value of 2X.value − 1 can be written:
E n(2X.value−1) = 1+3+5+···+(2ma −1) a
Note that 1+3+5+· · ·+(2ma −1) = (ma)2. The proof is an easy induction on the number of terms in the sum. Thus, E n(2X.value − 1)  =  a(ma)2, which is the definition of the second moment.
4.5.4 Higher-Order Moments
We estimate kth moments, for k > 2, in essentially the same way as we estimate second moments. The only thing that changes is the way we derive an estimate from a variable. In Section 4.5.2 we used the formula n(2v − 1) to turn a value v, the count of the number of occurrences of some particular stream element a, into an estimate of the second moment. Then, in Section 4.5.3 we saw why this formula works: the terms 2v − 1, for v = 1,2,...,m sum to m2, where m is the number of times a appears in the stream.
Notice that 2v − 1 is the difference between v2 and (v − 1)2. Suppose we wanted the third moment rather than the second. Then all we have to do is replace 2v−1 by v3−(v−1)3 = 3v2−3v+1. Then  mv=1 3v2−3v+1 = m3, so we can use as our estimate of the third moment the formula n(3v2 − 3v + 1), where v = X.value is the value associated with some variable X. More generally, we can estimate kth moments for any k ≥ 2 by turning value v = X.value into n vk −(v−1)k .
4.5.5 Dealing With Infinite Streams
Technically, the estimate we used for second and higher moments assumes that n, the stream length, is a constant. In practice, n grows with time. That fact, by itself, doesn’t cause problems, since we store only the values of variables and multiply some function of that value by n when it is time to estimate the moment. If we count the number of stream elements seen and store this value, which only requires log n bits, then we have n available whenever we need it.
A more serious problem is that we must be careful how we select the positions for the variables. If we do this selection once and for all, then as the stream gets longer, we are biased in favor of early positions, and the estimate of the moment will be too large. On the other hand, if we wait too long to pick positions, then
4.5. ESTIMATING MOMENTS 149
early in the stream we do not have many variables and so will get an unreliable estimate.
The proper technique is to maintain as many variables as we can store at all times, and to throw some out as the stream grows. The discarded variables are replaced by new ones, in such a way that at all times, the probability of picking any one position for a variable is the same as that of picking any other position. Suppose we have space to store s variables. Then the first s positions of the stream are each picked as the position of one of the s variables.
Inductively, suppose we have seen n stream elements, and the probability of any particular position being the position of a variable is uniform, that is s/n. When the (n+1)st element arrives, pick that position with probability s/(n+1). If not picked, then the s variables keep their same positions. However, if the (n + 1)st position is picked, then throw out one of the current s variables, with equal probability. Replace the one discarded by a new variable whose element is the one at position n + 1 and whose value is 1.
Surely, the probability that position n + 1 is selected for a variable is what it should be: s/(n + 1). However, the probability of every other position also is s/(n + 1), as we can prove by induction on n. By the inductive hypothesis, before the arrival of the (n + 1)st stream element, this probability was s/n. With probability 1 − s/(n + 1) the (n + 1)st position will not be selected, and the probability of each of the first n positions remains s/n. However, with probability s/(n + 1), the (n + 1)st position is picked, and the probability for each of the first n positions is reduced by factor (s − 1)/s. Considering the two cases, the probability of selecting each of the first n positions is
 1− s   s +  s   s−1  s  n+1nn+1sn
This expression simplifies to
 1− s   s + s−1  s 
n+1 n+1 n   n   s = s
n+1 n n+1
Thus, we have shown by induction on the stream length n that all positions
have equal probability s/n of being chosen as the position of a variable. 4.5.6 Exercises for Section 4.5
Exercise 4.5.1 : Compute the surprise number (second moment) for the stream 3, 1, 4, 1, 3, 4, 2, 1, 2. What is the third moment of this stream?
         and then to
which in turn simplifies to
n+1nn+1n   1− s  + s−1   s 
      
150 CHAPTER 4. MINING DATA STREAMS
   A General Stream-Sampling Problem
Notice that the technique described in Section 4.5.5 actually solves a more general problem. It gives us a way to maintain a sample of s stream elements so that at all times, all stream elements are equally likely to be selected for the sample.
As an example of where this technique can be useful, recall that in Section 4.2 we arranged to select all the tuples of a stream having key value in a randomly selected subset. Suppose that, as time goes on, there are too many tuples associated with any one key. We can arrange to limit the number of tuples for any key K to a fixed constant s by using the technique of Section 4.5.5 whenever a new tuple for key K arrives.
 ! Exercise 4.5.2 : If a stream has n elements, of which m are distinct, what are the minimum and maximum possible surprise number, as a function of m and n?
Exercise 4.5.3: Suppose we are given the stream of Exercise 4.5.1, to which we apply the Alon-Matias-Szegedy Algorithm to estimate the surprise number. For each possible value of i, if Xi is a variable starting position i, what is the value of Xi.value?
Exercise 4.5.4 : Repeat Exercise 4.5.3 if the intent of the variables is to com- pute third moments. What is the value of each variable at the end? What estimate of the third moment do you get from each variable? How does the average of these estimates compare with the true value of the third moment?
Exercise 4.5.5: Prove by induction on m that 1+3+5+···+(2m−1) = m2. Exercise 4.5.6: If we wanted to compute fourth moments, how would we
convert X.value to an estimate of the fourth moment?
4.6 Counting Ones in a Window
We now turn our attention to counting problems for streams. Suppose we have a window of length N on a binary stream. We want at all times to be able to answer queries of the form “how many 1’s are there in the last k bits?” for any k ≤ N. As in previous sections, we focus on the situation where we cannot afford to store the entire window. After showing an approximate algorithm for the binary case, we discuss how this idea can be extended to summing numbers.
4.6. COUNTING ONES IN A WINDOW 151 4.6.1 The Cost of Exact Counts
To begin, suppose we want to be able to count exactly the number of 1’s in the last k bits for any k ≤ N. Then we claim it is necessary to store all N bits of the window, as any representation that used fewer than N bits could not work. In proof, suppose we have a representation that uses fewer than N bits to represent the N bits in the window. Since there are 2N sequences of N bits, but fewer than 2N representations, there must be two different bit strings w and x that have the same representation. Since w ̸= x, they must differ in at least one bit. Let the last k−1 bits of w and x agree, but let them differ on the kth bit from the right end.
Example 4.10 : If w = 0101 and x = 1010, then k = 1, since scanning from the right, they first disagree at position 1. If w = 1001 and x = 0101, then k = 3, because they first disagree at the third position from the right. ✷
Suppose the data representing the contents of the window is whatever se- quence of bits represents both w and x. Ask the query “how many 1’s are in the last k bits?” The query-answering algorithm will produce the same an- swer, whether the window contains w or x, because the algorithm can only see their representation. But the correct answers are surely different for these two bit-strings. Thus, we have proved that we must use at least N bits to answer queries about the last k bits for any k.
In fact, we need N bits, even if the only query we can ask is “how many 1’s are in the entire window of length N?” The argument is similar to that used above. Suppose we use fewer than N bits to represent the window, and therefore we can find w, x, and k as above. It might be that w and x have the same number of 1’s, as they did in both cases of Example 4.10. However, if we follow the current window by any N − k bits, we will have a situation where the true window contents resulting from w and x are identical except for the leftmost bit, and therefore, their counts of 1’s are unequal. However, since the representations of w and x are the same, the representation of the window must still be the same if we feed the same bit sequence to these representations. Thus, we can force the answer to the query “how many 1’s in the window?” to be incorrect for one of the two possible window contents.
4.6.2 The Datar-Gionis-Indyk-Motwani Algorithm
We shall present the simplest case of an algorithm called DGIM. This version of the algorithm uses O(log2 N) bits to represent a window of N bits, and allows us to estimate the number of 1’s in the window with an error of no more than 50%. Later, we shall discuss an improvement of the method that limits the error to any fraction ǫ > 0, and still uses only O(log2 N) bits (although with a constant factor that grows as ǫ shrinks).
To begin, each bit of the stream has a timestamp, the position in which it arrives. The first bit has timestamp 1, the second has timestamp 2, and so on.
152 CHAPTER 4. MINING DATA STREAMS
Since we only need to distinguish positions within the window of length N, we shall represent timestamps modulo N, so they can be represented by log2 N bits. If we also store the total number of bits ever seen in the stream (i.e., the most recent timestamp) modulo N, then we can determine from a timestamp modulo N where in the current window the bit with that timestamp is.
We divide the window into buckets,5 consisting of:
1. The timestamp of its right (most recent) end.
2. The number of 1’s in the bucket. This number must be a power of 2, and we refer to the number of 1’s as the size of the bucket.
To represent a bucket, we need log2 N bits to represent the timestamp (modulo N) of its right end. To represent the number of 1’s we only need log2 log2 N bits. The reason is that we know this number i is a power of 2, say 2j, so we can represent i by coding j in binary. Since j is at most log2 N, it requires log2 log2 N bits. Thus, O(log N ) bits suffice to represent a bucket.
There are six rules that must be followed when representing a stream by buckets.
• The right end of a bucket is always a position with a 1.
• Every position with a 1 is in some bucket.
• No position is in more than one bucket.
• There are one or two buckets of any given size, up to some maximum size.
• All sizes must be a power of 2.
• Buckets cannot decrease in size as we move to the left (back in time). . .10110110001 01110110010110
 . . . 101
 1011000 1
 1110 1
 100 1
  000
            At least one of size 8
Two of size 4
One of Two of size 2 size 1
Figure 4.2: A bit-stream divided into buckets following the DGIM rules
5Do not confuse these “buckets” with the “buckets” discussed in connection with hashing.
11
 
4.6. COUNTING ONES IN A WINDOW 153
Example 4.11 : Figure 4.2 shows a bit stream divided into buckets in a way that satisfies the DGIM rules. At the right (most recent) end we see two buckets of size 1. To its left we see one bucket of size 2. Note that this bucket covers four positions, but only two of them are 1. Proceeding left, we see two buckets of size 4, and we suggest that a bucket of size 8 exists further left.
Notice that it is OK for some 0’s to lie between buckets. Also, observe from Fig. 4.2 that the buckets do not overlap; there are one or two of each size up to the largest size, and sizes only increase moving left. ✷
In the next sections, we shall explain the following about the DGIM algo- rithm:
1. Why the number of buckets representing a window must be small.
2. How to estimate the number of 1’s in the last k bits for any k, with an error no greater than 50%.
3. How to maintain the DGIM conditions as new bits enter the stream.
4.6.3 Storage Requirements for the DGIM Algorithm
We observed that each bucket can be represented by O(logN) bits. If the window has length N, then there are no more than N 1’s, surely. Suppose the largest bucket is of size 2j. Then j cannot exceed log2 N, or else there are more 1’s in this bucket than there are 1’s in the entire window. Thus, there are at most two buckets of all sizes from log2 N down to 1, and no buckets of larger sizes.
We conclude that there are O(logN) buckets. Since each bucket can be represented in O(log N ) bits, the total space required for all the buckets repre- senting a window of size N is O(log2 N).
4.6.4 Query Answering in the DGIM Algorithm
Suppose we are asked how many 1’s there are in the last k bits of the window, for some 1 ≤ k ≤ N. Find the bucket b with the earliest timestamp that includes at least some of the k most recent bits. Estimate the number of 1’s to be the sum of the sizes of all the buckets to the right (more recent) than bucket b, plus half the size of b itself.
Example 4.12 : Suppose the stream is that of Fig. 4.2, and k = 10. Then the query asks for the number of 1’s in the ten rightmost bits, which happen to be 0110010110. Let the current timestamp (time of the rightmost bit) be t. Then the two buckets with one 1, having timestamps t − 1 and t − 2 are completely included in the answer. The bucket of size 2, with timestamp t − 4, is also completely included. However, the rightmost bucket of size 4, with timestamp t − 8 is only partly included. We know it is the last bucket to contribute to the answer, because the next bucket to its left has timestamp less than t − 9 and
154 CHAPTER 4. MINING DATA STREAMS
thus is completely out of the window. On the other hand, we know the buckets to its right are completely inside the range of the query because of the existence of a bucket to their left with timestamp t − 9 or greater.
Our estimate of the number of 1’s in the last ten positions is thus 6. This number is the two buckets of size 1, the bucket of size 2, and half the bucket of size 4 that is partially within range. Of course the correct answer is 5. ✷
Suppose the above estimate of the answer to a query involves a bucket b of size 2j that is partially within the range of the query. Let us consider how far from the correct answer c our estimate could be. There are two cases: the estimate could be larger or smaller than c.
Case 1: The estimate is less than c. In the worst case, all the 1’s of b are actually within the range of the query, so the estimate misses half bucket b, or 2j−1 1’s. But in this case, c is at least 2j; in fact it is at least 2j+1 − 1, since there is at least one bucket of each of the sizes 2j−1, 2j−2,...,1. We conclude that our estimate is at least 50% of c.
Case 2 : The estimate is greater than c. In the worst case, only the rightmost bit of bucket b is within range, and there is only one bucket of each of the sizes smaller than b. Then c = 1+2j−1 +2j−2 +···+1 = 2j and the estimate we give is 2j−1 + 2j−1 + 2j−2 + · · · + 1 = 2j + 2j−1 − 1. We see that the estimate is no more than 50% greater than c.
4.6.5 Maintaining the DGIM Conditions
Suppose we have a window of length N properly represented by buckets that satisfy the DGIM conditions. When a new bit comes in, we may need to modify the buckets, so they continue to represent the window and continue to satisfy the DGIM conditions. First, whenever a new bit enters:
• Check the leftmost (earliest) bucket. If its timestamp has now reached the current timestamp minus N, then this bucket no longer has any of its 1’s in the window. Therefore, drop it from the list of buckets.
Now, we must consider whether the new bit is 0 or 1. If it is 0, then no further change to the buckets is needed. If the new bit is a 1, however, we may need to make several changes. First:
• Create a new bucket with the current timestamp and size 1.
If there was only one bucket of size 1, then nothing more needs to be done. However, if there are now three buckets of size 1, that is one too many. We fix this problem by combining the leftmost (earliest) two buckets of size 1.
• To combine any two adjacent buckets of the same size, replace them by one bucket of twice the size. The timestamp of the new bucket is the timestamp of the rightmost (later in time) of the two buckets.
4.6. COUNTING ONES IN A WINDOW 155
Combining two buckets of size 1 may create a third bucket of size 2. If so, we combine the leftmost two buckets of size 2 into a bucket of size 4. That, in turn, may create a third bucket of size 4, and if so we combine the leftmost two into a bucket of size 8. This process may ripple through the bucket sizes, but there are at most log2 N different sizes, and the combination of two adjacent buckets of the same size only requires constant time. As a result, any new bit can be processed in O(log N ) time.
Example 4.13 : Suppose we start with the buckets of Fig. 4.2 and a 1 enters. First, the leftmost bucket evidently has not fallen out of the window, so we do not drop any buckets. We create a new bucket of size 1 with the current timestamp, say t. There are now three buckets of size 1, so we combine the leftmost two. They are replaced with a single bucket of size 2. Its timestamp is t − 2, the timestamp of the bucket on the right (i.e., the rightmost bucket that actually appears in Fig. 4.2.
 . . 101
 1011000 1
 1 1101
 100 1
  000
            At least one of size 8
Two of size 4
Two of One of size 2 size 1
Figure 4.3: Modified buckets after a new 1 arrives in the stream
There are now two buckets of size 2, but that is allowed by the DGIM rules. Thus, the final sequence of buckets after the addition of the 1 is as shown in Fig. 4.3. ✷
4.6.6 Reducing the Error
Instead of allowing either one or two of each size bucket, suppose we allow either r − 1 or r of each of the exponentially growing sizes 1, 2, 4, . . ., for some integer r > 2. In order to represent any possible number of 1’s, we must relax this condition for the buckets of size 1 and buckets of the largest size present; there may be any number, from 1 to r, of buckets of these sizes.
The rule for combining buckets is essentially the same as in Section 4.6.5. If we get r + 1 buckets of size 2j , combine the leftmost two into a bucket of size 2j+1. That may, in turn, cause there to be r + 1 buckets of size 2j+1, and if so we continue combining buckets of larger sizes.
The argument used in Section 4.6.4 can also be used here. However, because there are more buckets of smaller sizes, we can get a stronger bound on the error. We saw there that the largest relative error occurs when only one 1 from the leftmost bucket b is within the query range, and we therefore overestimate the true count. Suppose bucket b is of size 2j. Then the true count is at least
11
1
156 CHAPTER 4. MINING DATA STREAMS
   Bucket Sizes and Ripple-Carry Adders
There is a pattern to the distribution of bucket sizes as we execute the basic algorithm of Section 4.6.5. Think of two buckets of size 2j as a ”1” in position j and one bucket of size 2j as a ”0” in that position. Then as 1’s arrive in the stream, the bucket sizes after each 1 form consecutive binary integers. The occasional long sequences of bucket combinations are analogous to the occasional long rippling of carries as we go from an integer like 101111 to 110000.
 1+(r−1)(2j−1 +2j−2 +···+1) = 1+(r−1)(2j −1). The overestimate is 2j−1 − 1. Thus, the fractional error is
2j−1 − 1 1+(r−1)(2j −1)
No matter what j is, this fraction is upper bounded by 1/(r − 1). Thus, by picking r sufficiently large, we can limit the error to any desired ǫ > 0.
4.6.7 Extensions to the Counting of Ones
It is natural to ask whether we can extend the technique of this section to handle aggregations more general than counting 1’s in a binary stream. An obvious direction to look is to consider streams of integers and ask if we can estimate the sum of the last k integers for any 1 ≤ k ≤ N, where N, as usual, is the window size.
It is unlikely that we can use the DGIM approach to streams containing both positive and negative integers. We could have a stream containing both very large positive integers and very large negative integers, but with a sum in the window that is very close to 0. Any imprecision in estimating the values of these large integers would have a huge effect on the estimate of the sum, and so the fractional error could be unbounded.
For example, suppose we broke the stream into buckets as we have done, but represented the bucket by the sum of the integers therein, rather than the count of 1’s. If b is the bucket that is partially within the query range, it could be that b has, in its first half, very large negative integers and in its second half, equally large positive integers, with a sum of 0. If we estimate the contribution of b by half its sum, that contribution is essentially 0. But the actual contribution of that part of bucket b that is in the query range could be anything from 0 to the sum of all the positive integers. This difference could be far greater than the actual query answer, and so the estimate would be meaningless.
On the other hand, some other extensions involving integers do work. Sup- pose that the stream consists of only positive integers in the range 1 to 2m for
 
4.7. DECAYING WINDOWS 157
some m. We can treat each of the m bits of each integer as if it were a separate stream. We then use the DGIM method to count the 1’s in each bit. Suppose the count of the ith bit (assuming bits count from the low-order end, starting at 0) is ci. Then the sum of the integers is
m−1
  ci2i i=0
If we use the technique of Section 4.6.6 to estimate each ci with fractional error at most ǫ, then the estimate of the true sum has error at most ǫ. The worst case occurs when all the ci’s are overestimated or all are underestimated by the same fraction.
4.6.8 Exercises for Section 4.6
Exercise 4.6.1: Suppose the window is as shown in Fig. 4.2. Estimate the number of 1’s the the last k positions, for k = (a) 5 (b) 15. In each case, how far off the correct value is your estimate?
! Exercise 4.6.2 : There are several ways that the bit-stream 1001011011101 could be partitioned into buckets. Find all of them.
Exercise 4.6.3 : Describe what happens to the buckets if three more 1’s enter the window represented by Fig. 4.3. You may assume none of the 1’s shown leave the window.
4.7 Decaying Windows
We have assumed that a sliding window held a certain tail of the stream, either the most recent N elements for fixed N, or all the elements that arrived after some time in the past. Sometimes we do not want to make a sharp distinction between recent elements and those in the distant past, but want to weight the recent elements more heavily. In this section, we consider “exponentially decaying windows,” and an application where they are quite useful: finding the most common “recent” elements.
4.7.1 The Problem of Most-Common Elements
Suppose we have a stream whose elements are the movie tickets purchased all over the world, with the name of the movie as part of the element. We want to keep a summary of the stream that is the most popular movies “currently.” While the notion of “currently” is imprecise, intuitively, we want to discount the popularity of a movie like Star Wars–Episode 4, which sold many tickets, but most of these were sold decades ago. On the other hand, a movie that sold
158 CHAPTER 4. MINING DATA STREAMS
n tickets in each of the last 10 weeks is probably more popular than a movie that sold 2n tickets last week but nothing in previous weeks.
One solution would be to imagine a bit stream for each movie. The ith bit has value 1 if the ith ticket is for that movie, and 0 otherwise. Pick a window size N, which is the number of most recent tickets that would be considered in evaluating popularity. Then, use the method of Section 4.6 to estimate the number of tickets for each movie, and rank movies by their estimated counts. This technique might work for movies, because there are only thousands of movies, but it would fail if we were instead recording the popularity of items sold at Amazon, or the rate at which different Twitter-users tweet, because there are too many Amazon products and too many tweeters. Further, it only offers approximate answers.
4.7.2 Definition of the Decaying Window
An alternative approach is to redefine the question so that we are not asking for a count of 1’s in a window. Rather, let us compute a smooth aggregation of all the 1’s ever seen in the stream, with decaying weights, so the further back in the stream, the less weight is given. Formally, let a stream currently consist of the elements a1, a2, . . . , at, where a1 is the first element to arrive and at is the current element. Let c be a small constant, such as 10−6 or 10−9. Define the exponentially decaying window for this stream to be the sum
t−1  at−i(1−c)i i=0
The effect of this definition is to spread out the weights of the stream el- ements as far back in time as the stream goes. In contrast, a fixed window with the same sum of the weights, 1/c, would put equal weight 1 on each of the most recent 1/c elements to arrive and weight 0 on all previous elements. The distinction is suggested by Fig. 4.4.
Window of length 1/c
Figure 4.4: A decaying window and a fixed-length window of equal weight
It is much easier to adjust the sum in an exponentially decaying window than in a sliding window of fixed length. In the sliding window, we have to worry about the element that falls out of the window each time a new element arrives. That forces us to keep the exact elements along with the sum, or to use
       
4.7. DECAYING WINDOWS 159 an approximation scheme such as DGIM. However, when a new element at+1
arrives at the stream input, all we need to do is: 1. Multiply the current sum by 1 − c.
2. Add at+1.
The reason this method works is that each of the previous elements has now moved one position further from the current element, so its weight is multiplied by 1 − c. Further, the weight on the current element is (1 − c)0 = 1, so adding at+1 is the correct way to include the new element’s contribution.
4.7.3 Finding the Most Popular Elements
Let us return to the problem of finding the most popular movies in a stream of ticket sales.6 We shall use an exponentially decaying window with a constant c, which you might think of as 10−9. That is, we approximate a sliding window holding the last one billion ticket sales. For each movie, we imagine a separate stream with a 1 each time a ticket for that movie appears in the stream, and a 0 each time a ticket for some other movie arrives. The decaying sum of the 1’s measures the current popularity of the movie.
We imagine that the number of possible movies in the stream is huge, so we do not want to record values for the unpopular movies. Therefore, we establish a threshold, say 1/2, so that if the popularity score for a movie goes below this number, its score is dropped from the counting. For reasons that will become obvious, the threshold must be less than 1, although it can be any number less than 1. When a new ticket arrives on the stream, do the following:
1. For each movie whose score we are currently maintaining, multiply its score by (1 − c).
2. Suppose the new ticket is for movie M. If there is currently a score for M, add 1 to that score. If there is no score for M, create one and initialize it to 1.
3. If any score is below the threshold 1/2, drop that score.
It may not be obvious that the number of movies whose scores are main- tained at any time is limited. However, note that the sum of all scores is 1/c. There cannot be more than 2/c movies with score of 1/2 or more, or else the sum of the scores would exceed 1/c. Thus, 2/c is a limit on the number of movies being counted at any time. Of course in practice, the ticket sales would be concentrated on only a small number of movies at any time, so the number of actively counted movies would be much less than 2/c.
6This example should be taken with a grain of salt, because, as we pointed out, there aren’t enough different movies for this technique to be essential. Imagine, if you will, that the number of movies is extremely large, so counting ticket sales of each one separately is not feasible.
 
160 CHAPTER 4. MINING DATA STREAMS 4.8 Summary of Chapter 4
✦ The Stream Data Model: This model assumes data arrives at a processing engine at a rate that makes it infeasible to store everything in active storage. One strategy to dealing with streams is to maintain summaries of the streams, sufficient to answer the expected queries about the data. A second approach is to maintain a sliding window of the most recently arrived data.
✦ Sampling of Streams: To create a sample of a stream that is usable for a class of queries, we identify a set of key attributes for the stream. By hashing the key of any arriving stream element, we can use the hash value to decide consistently whether all or none of the elements with that key will become part of the sample.
✦ Bloom Filters: This technique allows us to filter streams so elements that belong to a particular set are allowed through, while most nonmembers are deleted. We use a large bit array, and several hash functions. Members of the selected set are hashed to buckets, which are bits in the array, and those bits are set to 1. To test a stream element for membership, we hash the element to a set of bits using each of the hash functions, and only accept the element if all these bits are 1.
✦ Counting Distinct Elements: To estimate the number of different elements appearing in a stream, we can hash elements to integers, interpreted as binary numbers. 2 raised to the power that is the longest sequence of 0’s seen in the hash value of any stream element is an estimate of the number of different elements. By using many hash functions and combining these estimates, first by taking averages within groups, and then taking the median of the averages, we get a reliable estimate.
✦ Moments of Streams: The kth moment of a stream is the sum of the kth powers of the counts of each element that appears at least once in the stream. The 0th moment is the number of distinct elements, and the 1st moment is the length of the stream.
✦ Estimating Second Moments: A good estimate for the second moment, or surprise number, is obtained by choosing a random position in the stream, taking twice the number of times this element appears in the stream from that position onward, subtracting 1, and multiplying by the length of the stream. Many random variables of this type can be combined like the estimates for counting the number of distinct elements, to produce a reliable estimate of the second moment.
✦ Estimating Higher Moments: The technique for second moments works for kth moments as well, as long as we replace the formula 2x − 1 (where x is the number of times the element appears at or after the selected position) by xk − (x − 1)k.
4.9. REFERENCES FOR CHAPTER 4 161
✦ Estimating the Number of 1’s in a Window: We can estimate the number of 1’s in a window of 0’s and 1’s by grouping the 1’s into buckets. Each bucket has a number of 1’s that is a power of 2; there are one or two buckets of each size, and sizes never decrease as we go back in time. If we record only the position and size of the buckets, we can represent the contents of a window of size N with O(log2 N) space.
✦ Answering Queries About Numbers of 1’s: If we want to know the approx- imate numbers of 1’s in the most recent k elements of a binary stream, we find the earliest bucket B that is at least partially within the last k positions of the window and estimate the number of 1’s to be the sum of the sizes of each of the more recent buckets plus half the size of B. This estimate can never be off by more that 50% of the true count of 1’s.
✦ Closer Approximations to the Number of 1’s: By changing the rule for how many buckets of a given size can exist in the representation of a binary window, so that either r or r − 1 of a given size may exist, we can assure that the approximation to the true number of 1’s is never off by more than 1/r.
✦ Exponentially Decaying Windows: Rather than fixing a window size, we can imagine that the window consists of all the elements that ever arrived in the stream, but with the element that arrived t time units ago weighted by e−ct for some time-constant c. Doing so allows us to maintain certain summaries of an exponentially decaying window easily. For instance, the weighted sum of elements can be recomputed, when a new element arrives, by multiplying the old sum by 1 − c and then adding the new element.
✦ Maintaining Frequent Elements in an Exponentially Decaying Window: We can imagine that each item is represented by a binary stream, where 0 means the item was not the element arriving at a given time, and 1 means that it was. We can find the elements whose sum of their binary stream is at least 1/2. When a new element arrives, multiply all recorded sums by 1 minus the time constant, add 1 to the count of the item that just arrived, and delete from the record any item whose sum has fallen below 1/2.
4.9 References for Chapter 4
Many ideas associated with stream management appear in the “chronicle data model” of [8]. An early survey of research in stream-management systems is [2]. Also, [6] is a recent book on the subject of stream management.
The sampling technique of Section 4.2 is from [7]. The Bloom Filter is generally attributed to [3], although essentially the same technique appeared as “superimposed codes” in [9].
162 CHAPTER 4. MINING DATA STREAMS
The algorithm for counting distinct elements is essentially that of [5], al- though the particular method we described appears in [1]. The latter is also the source for the algorithm for calculating the surprise number and higher moments. However, the technique for maintaining a uniformly chosen sample of positions in the stream is called “reservoir sampling” and comes from [10].
The technique for approximately counting 1’s in a window is from [4].
1. N. Alon, Y. Matias, and M. Szegedy, “The space complexity of approxi- mating frequency moments,” 28th ACM Symposium on Theory of Com- puting, pp. 20–29, 1996.
2. B. Babcock, S. Babu, M. Datar, R. Motwani, and J. Widom, “Models and issues in data stream systems,” Symposium on Principles of Database Systems, pp. 1–16, 2002.
3. B.H. Bloom, “Space/time trade-offs in hash coding with allowable errors,” Comm. ACM 13:7, pp. 422–426, 1970.
4. M. Datar, A. Gionis, P. Indyk, and R. Motwani, “Maintaining stream statistics over sliding windows,” SIAM J. Computing 31, pp. 1794–1813, 2002.
5. P. Flajolet and G.N. Martin, “Probabilistic counting for database applica- tions,” 24th Symposium on Foundations of Computer Science, pp. 76–82, 1983.
6. M. Garofalakis, J. Gehrke, and R. Rastogi (editors), Data Stream Man- agement, Springer, 2009.
7. P.B. Gibbons, “Distinct sampling for highly-accurate answers to distinct values queries and event reports,” Intl. Conf. on Very Large Databases, pp. 541–550, 2001.
8. H.V. Jagadish, I.S. Mumick, and A. Silberschatz, “View maintenance issues for the chronicle data model,” Proc. ACM Symp. on Principles of Database Systems, pp. 113–124, 1995.
9. W.H.KautzandR.C.Singleton,“Nonadaptivebinarysuperimposedcodes,” IEEE Transactions on Information Theory 10, pp. 363–377, 1964.
10. J. Vitter, “Random sampling with a reservoir,” ACM Transactions on Mathematical Software 11:1, pp. 37–57, 1985.
Chapter 5
Link Analysis
One of the biggest changes in our lives in the decade following the turn of the century was the availability of efficient and accurate Web search, through search engines such as Google. While Google was not the first search engine, it was the first able to defeat the spammers who had made search almost useless. Moreover, the innovation provided by Google was a nontrivial technological advance, called “PageRank.” We shall begin the chapter by explaining what PageRank is and how it is computed efficiently.
Yet the war between those who want to make the Web useful and those who would exploit it for their own purposes is never over. When PageRank was established as an essential technique for a search engine, spammers invented ways to manipulate the PageRank of a Web page, often called link spam.1 That development led to the response of TrustRank and other techniques for preventing spammers from attacking PageRank. We shall discuss TrustRank and other approaches to detecting link spam.
Finally, this chapter also covers some variations on PageRank. These tech- niques include topic-sensitive PageRank (which can also be adapted for combat- ing link spam) and the HITS, or “hubs and authorities” approach to evaluating pages on the Web.
5.1 PageRank
We begin with a portion of the history of search engines, in order to motivate the definition of PageRank,2 a tool for evaluating the importance of Web pages in a way that it is not easy to fool. We introduce the idea of “random surfers,” to explain why PageRank is effective. We then introduce the technique of “tax- ation” or recycling of random surfers, in order to avoid certain Web structures
1Link spammers sometimes try to make their unethicality less apparent by referring to what they do as “search-engine optimization.”
2The term PageRank comes from Larry Page, the inventor of the idea and a founder of Google.
163
 
164 CHAPTER 5. LINK ANALYSIS
that present problems for the simple version of PageRank.
5.1.1 Early Search Engines and Term Spam
There were many search engines before Google. Largely, they worked by crawl- ing the Web and listing the terms (words or other strings of characters other than white space) found in each page, in an inverted index. An inverted index is a data structure that makes it easy, given a term, to find (pointers to) all the places where that term occurs.
When a search query (list of terms) was issued, the pages with those terms were extracted from the inverted index and ranked in a way that reflected the use of the terms within the page. Thus, presence of a term in a header of the page made the page more relevant than would the presence of the term in ordinary text, and large numbers of occurrences of the term would add to the assumed relevance of the page for the search query.
As people began to use search engines to find their way around the Web, unethical people saw the opportunity to fool search engines into leading people to their page. Thus, if you were selling shirts on the Web, all you cared about was that people would see your page, regardless of what they were looking for. Thus, you could add a term like “movie” to your page, and do it thousands of times, so a search engine would think you were a terribly important page about movies. When a user issued a search query with the term “movie,” the search engine would list your page first. To prevent the thousands of occurrences of “movie” from appearing on your page, you could give it the same color as the background. And if simply adding “movie” to your page didn’t do the trick, then you could go to the search engine, give it the query “movie,” and see what page did come back as the first choice. Then, copy that page into your own, again using the background color to make it invisible.
Techniques for fooling search engines into believing your page is about some- thing it is not, are called term spam. The ability of term spammers to operate so easily rendered early search engines almost useless. To combat term spam, Google introduced two innovations:
1. PageRank was used to simulate where Web surfers, starting at a random page, would tend to congregate if they followed randomly chosen outlinks from the page at which they were currently located, and this process were allowed to iterate many times. Pages that would have a large number of surfers were considered more “important” than pages that would rarely be visited. Google prefers important pages to unimportant pages when deciding which pages to show first in response to a search query.
2. The content of a page was judged not only by the terms appearing on that page, but by the terms used in or near the links to that page. Note that while it is easy for a spammer to add false terms to a page they control, they cannot as easily get false terms added to the pages that link to their own page, if they do not control those pages.
5.1. PAGERANK 165
   Simplified PageRank Doesn’t Work
As we shall see, computing PageRank by simulating random surfers is a time-consuming process. One might think that simply counting the number of in-links for each page would be a good approximation to where random surfers would wind up. However, if that is all we did, then the hypothetical shirt-seller could simply create a “spam farm” of a million pages, each of which linked to his shirt page. Then, the shirt page looks very important indeed, and a search engine would be fooled.
 These two techniques together make it very hard for the hypothetical shirt vendor to fool Google. While the shirt-seller can still add “movie” to his page, the fact that Google believed what other pages say about him, over what he says about himself would negate the use of false terms. The obvious countermeasure is for the shirt seller to create many pages of his own, and link to his shirt- selling page with a link that says “movie.” But those pages would not be given much importance by PageRank, since other pages would not link to them. The shirt-seller could create many links among his own pages, but none of these pages would get much importance according to the PageRank algorithm, and therefore, he still would not be able to fool Google into thinking his page was about movies.
It is reasonable to ask why simulation of random surfers should allow us to approximate the intuitive notion of the “importance” of pages. There are two related motivations that inspired this approach.
• Users of the Web “vote with their feet.” They tend to place links to pages they think are good or useful pages to look at, rather than bad or useless pages.
• The behavior of a random surfer indicates which pages users of the Web are likely to visit. Users are more likely to visit useful pages than useless pages.
But regardless of the reason, the PageRank measure has been proved empirically to work, and so we shall study in detail how it is computed.
5.1.2 Definition of PageRank
PageRank is a function that assigns a real number to each page in the Web (or at least to that portion of the Web that has been crawled and its links discovered). The intent is that the higher the PageRank of a page, the more “important” it is. There is not one fixed algorithm for assignment of PageRank, and in fact variations on the basic idea can alter the relative PageRank of any two pages. We begin by defining the basic, idealized PageRank, and follow it
166 CHAPTER 5. LINK ANALYSIS
by modifications that are necessary for dealing with some real-world problems concerning the structure of the Web.
Think of the Web as a directed graph, where pages are the nodes, and there is an arc from page p1 to page p2 if there are one or more links from p1 to p2. Figure 5.1 is an example of a tiny version of the Web, where there are only four pages. Page A has links to each of the other three pages; page B has links to A and D only; page C has a link only to A, and page D has links to B and C only.
AB
CD
Figure 5.1: A hypothetical example of the Web
Suppose a random surfer starts at page A in Fig. 5.1. There are links to B, C, and D, so this surfer will next be at each of those pages with probability 1/3, and has zero probability of being at A. A random surfer at B has, at the next step, probability 1/2 of being at A, 1/2 of being at D, and 0 of being at B or C.
In general, we can define the transition matrix of the Web to describe what happens to random surfers after one step. This matrix M has n rows and columns, if there are n pages. The element mij in row i and column j has value 1/k if page j has k arcs out, and one of them is to page i. Otherwise, mij = 0.
                    Example 5.1 : The transition matrix for the
 0 1/2 1 M=1/3 0 0  1 / 3 0 0 1/3 1/2 0
Web of Fig. 5.1 is 0 
1/2 1 / 2 
0
In this matrix, the order of the pages is the natural one, A, B, C, and D. Thus, the first column expresses the fact, already discussed, that a surfer at A has a 1/3 probability of next being at each of the other pages. The second column expresses the fact that a surfer at B has a 1/2 probability of being next at A and the same of being at D. The third column says a surfer at C is certain to be at A next. The last column says a surfer at D has a 1/2 probability of being nextatBandthesameatC. ✷
5.1. PAGERANK 167
The probability distribution for the location of a random surfer can be described by a column vector whose jth component is the probability that the surfer is at page j. This probability is the (idealized) PageRank function.
Suppose we start a random surfer at any of the n pages of the Web with equal probability. Then the initial vector v0 will have 1/n for each component. If M is the transition matrix of the Web, then after one step, the distribution of the surfer will be Mv0, after two steps it will be M(Mv0) = M2v0, and so on. In general, multiplying the initial vector v0 by M a total of i times will give us the distribution of the surfer after i steps.
To see why multiplying a distribution vector v by M gives the distribution x = Mv at the next step, we reason as follows. The probability xi that a random surfer will be at node i at the next step, is  j mijvj. Here, mij is the probability that a surfer at node j will move to node i at the next step (often 0 because there is no link from j to i), and vj is the probability that the surfer was at node j at the previous step.
This sort of behavior is an example of the ancient theory of Markov processes. It is known that the distribution of the surfer approaches a limiting distribution v that satisfies v = Mv, provided two conditions are met:
1. The graph is strongly connected; that is, it is possible to get from any node to any other node.
2. There are no dead ends: nodes that have no arcs out.
Note that Fig. 5.1 satisfies both these conditions.
The limit is reached when multiplying the distribution by M another time
does not change the distribution. In other terms, the limiting v is an eigenvec- tor of M (an eigenvector of a matrix M is a vector v that satisfies v = λMv for some constant eigenvalue λ). In fact, because M is stochastic, meaning that its columns each add up to 1, v is the principal eigenvector (its associated eigen- value is the largest of all eigenvalues). Note also that, because M is stochastic, the eigenvalue associated with the principal eigenvector is 1.
The principal eigenvector of M tells us where the surfer is most likely to be after a long time. Recall that the intuition behind PageRank is that the more likely a surfer is to be at a page, the more important the page is. We can compute the principal eigenvector of M by starting with the initial vector v0 and multiplying by M some number of times, until the vector we get shows little change at each round. In practice, for the Web itself, 50–75 iterations are sufficient to converge to within the error limits of double-precision arithmetic.
Example 5.2: Suppose we apply the process described above to the matrix M from Example 5.1. Since there are four nodes, the initial vector v0 has four components, each 1/4. The sequence of approximations to the limit that we
168 CHAPTER 5. LINK ANALYSIS
   Solving Linear Equations
If you look at the 4-node “Web” of Example 5.2, you might think that the way to solve the equation v = Mv is by Gaussian elimination. Indeed, in that example, we argued what the limit would be essentially by doing so. However, in realistic examples, where there are tens or hundreds of billions of nodes, Gaussian elimination is not feasible. The reason is that Gaussian elimination takes time that is cubic in the number of equations. Thus, the only way to solve equations on this scale is to iterate as we have suggested. Even that iteration is quadratic at each round, but we can speed it up by taking advantage of the fact that the matrix M is very sparse; there are on average about ten links per page, i.e., ten nonzero entries per column.
Moreover, there is another difference between PageRank calculation and solving linear equations. The equation v = M v has an infinite number of solutions, since we can take any solution v, multiply its components by any fixed constant c, and get another solution to the same equation. When we include the constraint that the sum of the components is 1, as we have done, then we get a unique solution.
 get by multiplying at each step by M is:
1/4 9/24 15/48 11/32 3/9  1/4 , 5/24 , 11/48 , 7/32 ,..., 2/9  1/4 5/24 11/48  7/32 2/9
1/4 5/24 11/48
Notice that in this example, the probabilities for B, C, and D remain the same. It is easy to see that B and C must always have the same values at any iteration, because their rows in M are identical. To show that their values are also the same as the value for D, an inductive proof works, and we leave it as an exercise. Given that the last three values of the limiting vector must be the same, it is easy to discover the limit of the above sequence. The first row of M tells us that the probability of A must be 3/2 the other probabilities, so the limit has the probability of A equal to 3/9, or 1/3, while the probability for the other three nodes is 2/9.
This difference in probability is not great. But in the real Web, with billions of nodes of greatly varying importance, the true probability of being at a node like www.amazon.com is orders of magnitude greater than the probability of typical nodes. ✷
7/32 2/9
5.1. PAGERANK 169 5.1.3 Structure of the Web
It would be nice if the Web were strongly connected like Fig. 5.1. However, it is not, in practice. An early study of the Web found it to have the structure shown in Fig. 5.2. There was a large strongly connected component (SCC), but there were several other portions that were almost as large.
1. The in-component, consisting of pages that could reach the SCC by fol- lowing links, but were not reachable from the SCC.
2. The out-component, consisting of pages reachable from the SCC but un- able to reach the SCC.
3. Tendrils, which are of two types. Some tendrils consist of pages reachable from the in-component but not able to reach the in-component. The other tendrils can reach the out-component, but are not reachable from the out-component.
Tendrils Out
In Component
Strongly Connected Component
Tubes
Tendrils In
Out Component
                        Disconnected Components
  Figure 5.2: The “bowtie” picture of the Web
In addition, there were small numbers of pages found either in
170 CHAPTER 5. LINK ANALYSIS
(a) Tubes, which are pages reachable from the in-component and able to reach the out-component, but unable to reach the SCC or be reached from the SCC.
(b) Isolated components that are unreachable from the large components (the SCC, in- and out-components) and unable to reach those components.
Several of these structures violate the assumptions needed for the Markov- process iteration to converge to a limit. For example, when a random surfer enters the out-component, they can never leave. As a result, surfers starting in either the SCC or in-component are going to wind up in either the out- component or a tendril off the in-component. Thus, no page in the SCC or in- component winds up with any probability of a surfer being there. If we interpret this probability as measuring the importance of a page, then we conclude falsely that nothing in the SCC or in-component is of any importance.
As a result, PageRank is usually modified to prevent such anomalies. There are really two problems we need to avoid. First is the dead end, a page that has no links out. Surfers reaching such a page disappear, and the result is that in the limit no page that can reach a dead end can have any PageRank at all. The second problem is groups of pages that all have outlinks but they never link to any other pages. These structures are called spider traps.3 Both these problems are solved by a method called “taxation,” where we assume a random surfer has a finite probability of leaving the Web at any step, and new surfers are started at each page. We shall illustrate this process as we study each of the two problem cases.
5.1.4 Avoiding Dead Ends
Recall that a page with no link out is called a dead end. If we allow dead ends, the transition matrix of the Web is no longer stochastic, since some of the columns will sum to 0 rather than 1. A matrix whose column sums are at most 1 is called substochastic. If we compute Miv for increasing powers of a substochastic matrix M, then some or all of the components of the vector go to 0. That is, importance “drains out” of the Web, and we get no information about the relative importance of pages.
Example 5.3 : In Fig. 5.3 we have modified Fig. 5.1 by removing the arc from C to A. Thus, C becomes a dead end. In terms of random surfers, when a surfer reaches C they disappear at the next round. The matrix M that describes Fig. 5.3 is
 0 1/2 0 0  M=1/3 0 0 1/2  1 / 3 0 0 1 / 2 
1/3 1/2 0 0
3They are so called because the programs that crawl the Web, recording pages and links, are often referred to as “spiders.” Once a spider enters a spider trap, it can never leave.
 
5.1. PAGERANK
171
    AB
CD
Figure 5.3: C is now a dead end
              Note that it is substochastic, but not stochastic, because the sum of the third column, for C, is 0, not 1. Here is the sequence of vectors that result by starting with the vector with each component 1/4, and repeatedly multiplying the vector by M:
1/4 3/24 5/48 21/288 0  1/4 , 5/24 , 7/48 , 31/288 ,..., 0  1/4 5/24 7/48 31/288 0
1/4 5/24 7/48 31/288 0
As we see, the probability of a surfer being anywhere goes to 0, as the number
of steps increase. ✷
There are two approaches to dealing with dead ends.
1. We can drop the dead ends from the graph, and also drop their incoming arcs. Doing so may create more dead ends, which also have to be dropped, recursively. However, eventually we wind up with a strongly-connected component, none of whose nodes are dead ends. In terms of Fig. 5.2, recursive deletion of dead ends will remove parts of the out-component, tendrils, and tubes, but leave the SCC and the in-component, as well as parts of any small isolated components.4
2. We can modify the process by which random surfers are assumed to move about the Web. This method, which we refer to as “taxation,” also solves the problem of spider traps, so we shall defer it to Section 5.1.5.
If we use the first approach, recursive deletion of dead ends, then we solve the remaining graph G by whatever means are appropriate, including the taxation method if there might be spider traps in G. Then, we restore the graph, but keep
4You might suppose that the entire out-component and all the tendrils will be removed, but remember that they can have within them smaller strongly connected components, including spider traps, which cannot be deleted.
 
172 CHAPTER 5. LINK ANALYSIS
the PageRank values for the nodes of G. Nodes not in G, but with predecessors all in G can have their PageRank computed by summing, over all predecessors p, the PageRank of p divided by the number of successors of p in the full graph. Now there may be other nodes, not in G, that have the PageRank of all their predecessors computed. These may have their own PageRank computed by the same process. Eventually, all nodes outside G will have their PageRank computed; they can surely be computed in the order opposite to that in which they were deleted.
    AB
CD
                 E
Figure 5.4: A graph with two levels of dead ends
Example 5.4 : Figure 5.4 is a variation on Fig. 5.3, where we have introduced a successor E for C. But E is a dead end, and when we remove it, and the arc entering from C, we find that C is now a dead end. After removing C, no more nodes can be removed, since each of A, B, and D have arcs leaving. The resulting graph is shown in Fig. 5.5.
The matrix for the graph of Fig. 5.5 is
0 1/20 M =  1/2 0 1 
1/2 1/2 0
The rows and columns correspond to A, B, and D in that order. To get the PageRanks for this matrix, we start with a vector with all components equal to 1/3, and repeatedly multiply by M. The sequence of vectors we get is
1/3 1/6 3/12  5/24 2/9  1/3 , 3/6 , 5/12 , 11/24 ,..., 4/9  1/3 2/6 4/12 8/24 3/9
We now know that the PageRank of A is 2/9, the PageRank of B is 4/9, and the PageRank of D is 3/9. We still need to compute PageRanks for C
5.1. PAGERANK
173
    AB
D
         Figure 5.5: The reduced graph with no dead ends
and E, and we do so in the order opposite to that in which they were deleted. Since C was last to be deleted, we know all its predecessors have PageRanks computed. These predecessors are A and D. In Fig. 5.4, A has three successors, so it contributes 1/3 of its PageRank to C. Page D has two successors in Fig. 5.4, so it contributes half its PageRank to C. Thus, the PageRank of C is
1 × 2 + 1 × 3 = 13/54. 3929
Now we can compute the PageRank for E. That node has only one pre- decessor, C, and C has only one successor. Thus, the PageRank of E is the same as that of C. Note that the sums of the PageRanks exceed 1, and they no longer represent the distribution of a random surfer. Yet they do represent decent estimates of the relative importance of the pages. ✷
5.1.5 Spider Traps and Taxation
As we mentioned, a spider trap is a set of nodes with no dead ends but no arcs out. These structures can appear intentionally or unintentionally on the Web, and they cause the PageRank calculation to place all the PageRank within the spider traps.
Example 5.5 : Consider Fig. 5.6, which is Fig. 5.1 with the arc out of C changed to point to C itself. That change makes C a simple spider trap of one node. Note that in general spider traps can have many nodes, and as we shall
    see in Section 5.4, there are spider traps with construct intentionally.
The transition matrix for Fig. 5.6 is
 0 1/2 0 M=1/3 0 0  1 / 3 0 1 1/3 1/2 0
millions of nodes that spammers
0  1/2
1 / 2  0
If we perform the usual iteration to compute the PageRank of the nodes, we
174
CHAPTER 5. LINK ANALYSIS
AB
CD
Figure 5.6: A graph with a one-node spider trap
1/4  3/24  5/48  21/288 0  1/4 , 5/24 , 7/48 , 31/288 ,..., 0  1/4 11/24 29/48 205/288 1 1/4 5/24 7/48 31/288 0
                    get
As predicted, all the PageRank is at C, since once there a random surfer can never leave. ✷
To avoid the problem illustrated by Example 5.5, we modify the calculation of PageRank by allowing each random surfer a small probability of teleporting to a random page, rather than following an out-link from their current page. The iterative step, where we compute a new vector estimate of PageRanks v′ from the current PageRank estimate v and the transition matrix M is
v′ =βMv+(1−β)e/n
where β is a chosen constant, usually in the range 0.8 to 0.9, e is a vector of all 1’s with the appropriate number of components, and n is the number of nodes in the Web graph. The term βMv represents the case where, with probability β, the random surfer decides to follow an out-link from their present page. The term (1 − β)e/n is a vector each of whose components has value (1 − β)/n and represents the introduction, with probability 1 − β, of a new random surfer at a random page.
Note that if the graph has no dead ends, then the probability of introducing a new random surfer is exactly equal to the probability that the random surfer will decide not to follow a link from their current page. In this case, it is reasonable to visualize the surfer as deciding either to follow a link or teleport to a random page. However, if there are dead ends, then there is a third possibility, which is that the surfer goes nowhere. Since the term (1 − β)e/n does not depend on the sum of the components of the vector v, there will always be some fraction
5.1. PAGERANK 175 of a surfer operating on the Web. That is, when there are dead ends, the sum
of the components of v may be less than 1, but it will never reach 0.
Example 5.6 : Let us see how the new approach to computing PageRank fares on the graph of Fig. 5.6. We shall use β = 0.8 in this example. Thus, the equation for the iteration becomes
 0 2/5 0 0  1/20 v′ = 4/15 0 0 2/5 v+ 1/20   4 / 1 5 0 4 / 5 2 / 5   1 / 2 0 
4/15 2/5 0 0 1/20
Notice that we have incorporated the factor β into M by multiplying each of its elements by 4/5. The components of the vector (1 − β)e/n are each 1/20, since 1 − β = 1/5 and n = 4. Here are the first few iterations:
1/4  9/60  41/300  543/4500 15/148  1/4 , 13/60 , 53/300 , 707/4500 ,..., 19/148   1/4   25/60   153/300   2543/4500   95/148  1/4 13/60 53/300 707/4500 19/148
By being a spider trap, C has managed to get more than half of the PageRank for itself. However, the effect has been limited, and each of the nodes gets some of the PageRank. ✷
5.1.6 Using PageRank in a Search Engine
Having seen how to calculate the PageRank vector for the portion of the Web that a search engine has crawled, we should examine how this information is used. Each search engine has a secret formula that decides the order in which to show pages to the user in response to a search query consisting of one or more search terms (words). Google is said to use over 250 different properties of pages, from which a linear order of pages is decided.
First, in order to be considered for the ranking at all, a page has to have at least one of the search terms in the query. Normally, the weighting of properties is such that unless all the search terms are present, a page has very little chance of being in the top ten that are normally shown first to the user. Among the qualified pages, a score is computed for each, and an important component of this score is the PageRank of the page. Other components include the presence or absence of search terms in prominent places, such as headers or the links to the page itself.
5.1.7 Exercises for Section 5.1
Exercise 5.1.1: Compute the PageRank of each page in Fig. 5.7, assuming no taxation.
176 CHAPTER 5. LINK ANALYSIS
ab
c
Figure 5.7: An example graph for exercises
Exercise 5.1.2: Compute the PageRank of each page in Fig. 5.7, assuming
β = 0.8.
! Exercise 5.1.3: Suppose the Web consists of a clique (set of nodes with all possible arcs from one to another) of n nodes and a single additional node that is the successor of each of the n nodes in the clique. Figure 5.8 shows this graph for the case n = 4. Determine the PageRank of each page, as a function of n and β.
                                                      Figure 5.8: Example of graphs discussed in Exercise 5.1.3
!! Exercise 5.1.4 : Construct, for any integer n, a Web such that, depending on β, any of the n nodes can have the highest PageRank among those n. It is allowed for there to be other nodes in the Web besides these n.
! Exercise 5.1.5 : Show by induction on n that if the second, third, and fourth components of a vector v are equal, and M is the transition matrix of Exam- ple 5.1, then the second, third, and fourth components are also equal in Mnv for any n ≥ 0.
5.2. EFFICIENT COMPUTATION OF PAGERANK 177
.. .
Figure 5.9: A chain of dead ends
Exercise 5.1.6: Suppose we recursively eliminate dead ends from the graph, solve the remaining graph, and estimate the PageRank for the dead-end pages as described in Section 5.1.4. Suppose the graph is a chain of dead ends, headed by a node with a self-loop, as suggested in Fig. 5.9. What would be the Page- Rank assigned to each of the nodes?
Exercise 5.1.7 : Repeat Exercise 5.1.6 for the tree of dead ends suggested by Fig. 5.10. That is, there is a single node with a self-loop, which is also the root of a complete binary tree of n levels.
.. .
.. . .. .
.. .
Figure 5.10: A tree of dead ends
5.2 Efficient Computation of PageRank
To compute the PageRank for a large graph representing the Web, we have to perform a matrix–vector multiplication on the order of 50 times, until the vector is close to unchanged at one iteration. To a first approximation, the MapReduce method given in Section 2.3.1 is suitable. However, we must deal with two issues:
1. The transition matrix of the Web M is very sparse. Thus, representing it by all its elements is highly inefficient. Rather, we want to represent the matrix by its nonzero elements.
2. We may not be using MapReduce, or for efficiency reasons we may wish to use a combiner (see Section 2.2.4) with the Map tasks to reduce the amount of data that must be passed from Map tasks to Reduce tasks. In this case, the striping approach discussed in Section 2.3.1 is not sufficient to avoid heavy use of disk (thrashing).
We discuss the solution to these two problems in this section.
                                   
178 CHAPTER 5. LINK ANALYSIS 5.2.1 Representing Transition Matrices
The transition matrix is very sparse, since the average Web page has about 10 out-links. If, say, we are analyzing a graph of ten billion pages, then only one in a billion entries is not 0. The proper way to represent any sparse matrix is to list the locations of the nonzero entries and their values. If we use 4-byte integers for coordinates of an element and an 8-byte double-precision number for the value, then we need 16 bytes per nonzero entry. That is, the space needed is linear in the number of nonzero entries, rather than quadratic in the side of the matrix.
However, for a transition matrix of the Web, there is one further compression that we can do. If we list the nonzero entries by column, then we know what each nonzero entry is; it is 1 divided by the out-degree of the page. We can thus represent a column by one integer for the out-degree, and one integer per nonzero entry in that column, giving the row number where that entry is located. Thus, we need slightly more than 4 bytes per nonzero entry to represent a transition matrix.
Example 5.7 : Let us reprise the example Web graph from Fig. 5.1, whose
transition matrix is
 0 1/2 1 0  M=1/3 0 0 1/2  1 / 3 0 0 1 / 2 
1/3 1/2 0 0
Recall that the rows and columns represent nodes A, B, C, and D, in that
order. In Fig. 5.11 is a compact representation of this matrix.5
Figure 5.11: Represent a transition matrix by the out-degree of each node and the list of its successors
For instance, the entry for A has degree 3 and a list of three successors. From that row of Fig. 5.11 we can deduce that the column for A in matrix M has 0 in the row for A (since it is not on the list of destinations) and 1/3 in the rows for B, C, and D. We know that the value is 1/3 because the degree column in Fig. 5.11 tells us there are three links out of A. ✷
5Because M is not sparse, this representation is not very useful for M. However, the example illustrates the process of representing matrices in general, and the sparser the matrix is, the more this representation will save.
  Source
 Degree
 Destinations
  A B
C D
       3 2
1 2
    B, C, D A, D
A
B, C
       
5.2. EFFICIENT COMPUTATION OF PAGERANK 179 5.2.2 PageRank Iteration Using MapReduce
One iteration of the PageRank algorithm involves taking an estimated Page- Rank vector v and computing the next estimate v′ by
v′ =βMv+(1−β)e/n
Recall β is a constant slightly less than 1, e is a vector of all 1’s, and n is the number of nodes in the graph that transition matrix M represents.
If n is small enough that each Map task can store the full vector v in main memory and also have room in main memory for the result vector v′, then there is little more here than a matrix–vector multiplication. The additional steps are to multiply each component of M v by constant β and to add (1 − β)/n to each component.
However, it is likely, given the size of the Web today, that v is much too large to fit in main memory. As we discussed in Section 2.3.1, the method of striping, where we break M into vertical stripes (see Fig. 2.4) and break v into corresponding horizontal stripes, will allow us to execute the MapReduce process efficiently, with no more of v at any one Map task than can conveniently fit in main memory.
5.2.3 Use of Combiners to Consolidate the Result Vector
There are two reasons the method of Section 5.2.2 might not be adequate.
1. We might wish to add terms for vi′ , the ith component of the result vector v, at the Map tasks. This improvement is the same as using a combiner, since the Reduce function simply adds terms with a common key. Recall that for a MapReduce implementation of matrix–vector multiplication, the key is the value of i for which a term mijvj is intended.
2. We might not be using MapReduce at all, but rather executing the iter- ation step at a single machine or a collection of machines.
We shall assume that we are trying to implement a combiner in conjunction with a Map task; the second case uses essentially the same idea.
Suppose that we are using the stripe method to partition a matrix and vector that do not fit in main memory. Then a vertical stripe from the matrix M and a horizontal stripe from the vector v will contribute to all components of the result vector v′. Since that vector is the same length as v, it will not fit in main memory either. Moreover, as M is stored column-by-column for efficiency reasons, a column can affect any of the components of v′. As a result, it is unlikely that when we need to add a term to some component vi′, that component will already be in main memory. Thus, most terms will require that a page be brought into main memory to add it to the proper component. That situation, called thrashing, takes orders of magnitude too much time to be feasible.
180 CHAPTER 5. LINK ANALYSIS
An alternative strategy is based on partitioning the matrix into k2 blocks, while the vectors are still partitioned into k stripes. A picture, showing the division for k = 4, is in Fig. 5.12. Note that we have not shown the multiplica- tion of the matrix by β or the addition of (1 − β)e/n, because these steps are straightforward, regardless of the strategy we use.
=
Figure 5.12: Partitioning a matrix into square blocks
In this method, we use k2 Map tasks. Each task gets one square of the matrix M, say Mij, and one stripe of the vector v, which must be vj. Notice that each stripe of the vector is sent to k different Map tasks; vj is sent to the task handling Mij for each of the k possible values of i. Thus, v is transmitted over the network k times. However, each piece of the matrix is sent only once. Since the size of the matrix, properly encoded as described in Section 5.2.1, can be expected to be several times the size of the vector, the transmission cost is not too much greater than the minimum possible. And because we are doing considerable combining at the Map tasks, we save as data is passed from the Map tasks to the Reduce tasks.
The advantage of this approach is that we can keep both the jth stripe of v and the ith stripe of v′ in main memory as we process Mij. Note that all terms generated from Mij and vj contribute to vi′ and no other stripe of v′.
5.2.4 Representing Blocks of the Transition Matrix
Since we are representing transition matrices in the special way described in Section 5.2.1, we need to consider how the blocks of Fig. 5.12 are represented. Unfortunately, the space required for a column of blocks (a “stripe” as we called it earlier) is greater than the space needed for the stripe as a whole, but not too much greater.
For each block, we need data about all those columns that have at least one nonzero entry within the block. If k, the number of stripes in each dimension, is large, then most columns will have nothing in most blocks of its stripe. For a given block, we not only have to list those rows that have a nonzero entry for that column, but we must repeat the out-degree for the node represented by the column. Consequently, it is possible that the out-degree will be repeated as many times as the out-degree itself. That observation bounds from above the
  v’ 1
 v’ 2
 v’ 3
 v’ 4
M
11
M
12
M
13
M
14
M
21
M
22
M
23
M
24
M
31
M
32
M
33
M
34
M
41
M
42
M
43
M
44
 v
1
 v
2
 v
3
 v
4
5.2. EFFICIENT COMPUTATION OF PAGERANK 181 space needed to store the blocks of a stripe at twice the space needed to store
the stripe as a whole.
ABCD
 A B
C D
Figure 5.13: A four-node graph is divided into four 2-by-2 blocks
Example 5.8 : Let us suppose the matrix from Example 5.7 is partitioned into blocks, with k = 2. That is, the upper-left quadrant represents links from A or B to A or B, the upper-right quadrant represents links from C or D to A or B, and so on. It turns out that in this small example, the only entry that we can avoid is the entry for C in M22, because C has no arcs to either C or D. The tables representing each of the four blocks are shown in Fig. 5.14.
If we examine Fig. 5.14(a), we see the representation of the upper-left quad- rant. Notice that the degrees for A and B are the same as in Fig. 5.11, because we need to know the entire number of successors, not the number of successors within the relevant block. However, each successor of A or B is represented in Fig. 5.14(a) or Fig. 5.14(c), but not both. Notice also that in Fig. 5.14(d), there is no entry for C, because there are no successors of C within the lower half of the matrix (rows C and D). ✷
5.2.5 Other Efficient Approaches to PageRank Iteration
The algorithm discussed in Section 5.2.3 is not the only option. We shall discuss several other approaches that use fewer processors. These algorithms share with the algorithm of Section 5.2.3 the good property that the matrix M is read only once, although the vector v is read k times, where the parameter k is chosen so that 1/kth of the vectors v and v′ can be held in main memory. Recall that the algorithm of Section 5.2.3 uses k2 processors, assuming all Map tasks are executed in parallel at different processors.
We can assign all the blocks in one row of blocks to a single Map task, and thus reduce the number of Map tasks to k. For instance, in Fig. 5.12, M11, M12, M13, and M14 would be assigned to a single Map task. If we represent the blocks as in Fig. 5.14, we can read the blocks in a row of blocks one-at-a-time, so the matrix does not consume a significant amount of main-memory. At the same time that we read Mij, we must read the vector stripe vj. As a result, each of the k Map tasks reads the entire vector v, along with 1/kth of the matrix.
182
CHAPTER 5. LINK ANALYSIS
(a) Representation of M11 connecting A and B to A and B
(b) Representation of M12 connecting C and D to A and B
(c) Representation of M21 connecting A and B to C and D
(d) Representation of M22 connecting C and D to C and D Figure 5.14: Sparse representation of the blocks of a matrix
  Source
 Degree
 Destinations
  A B
   3 2
  B A
      Source
 Degree
 Destinations
  C D
   1 2
  A B
      Source
 Degree
 Destinations
  A B
   3 2
  C, D D
      Source
 Degree
 Destinations
  D
 2
 C
   The work reading M and v is thus the same as for the algorithm of Sec- tion 5.2.3, but the advantage of this approach is that each Map task can combine all the terms for the portion vi′ for which it is exclusively responsible. In other words, the Reduce tasks have nothing to do but to concatenate the pieces of v′ received from the k Map tasks.
We can extend this idea to an environment in which MapReduce is not used. Suppose we have a single processor, with M and v stored on its disk, using the same sparse representation for M that we have discussed. We can first simulate the first Map task, the one that uses blocks M11 through M1k and all of v to compute v1′ . Then we simulate the second Map task, reading M21 through M2k and all of v to compute v2′ , and so on. As for the previous algorithms, we thus read M once and v k times. We can make k as small as possible, subject to the constraint that there is enough main memory to store 1/kth of v and 1/kth of v′, along with as small a portion of M as we can read from disk (typically, one disk block).
5.3. TOPIC-SENSITIVE PAGERANK 183 5.2.6 Exercises for Section 5.2
Exercise 5.2.1 : Suppose we wish to store an n × n boolean matrix (0 and 1 elements only). We could represent it by the bits themselves, or we could represent the matrix by listing the positions of the 1’s as pairs of integers, each integer requiring ⌈log2 n⌉ bits. The former is suitable for dense matrices; the latter is suitable for sparse matrices. How sparse must the matrix be (i.e., what fraction of the elements should be 1’s) for the sparse representation to save space?
Exercise 5.2.2: Using the method of Section 5.2.1, represent the transition matrices of the following graphs:
(a) Figure 5.4. (b) Figure 5.7.
Exercise 5.2.3: Using the method of Section 5.2.4, represent the transition matrices of the graph of Fig. 5.3, assuming blocks have side 2.
Exercise 5.2.4: Consider a Web graph that is a chain, like Fig. 5.9, with n nodes. As a function of k, which you may assume divides n, describe the representation of the transition matrix for this graph, using the method of Section 5.2.4
5.3 Topic-Sensitive PageRank
There are several improvements we can make to PageRank. One, to be studied in this section, is that we can weight certain pages more heavily because of their topic. The mechanism for enforcing this weighting is to alter the way random surfers behave, having them prefer to land on a page that is known to cover the chosen topic. In the next section, we shall see how the topic-sensitive idea can also be applied to negate the effects of a new kind of spam, called “‘link spam,” that has developed to try to fool the PageRank algorithm.
5.3.1 Motivation for Topic-Sensitive Page Rank
Different people have different interests, and sometimes distinct interests are expressed using the same term in a query. The canonical example is the search query jaguar, which might refer to the animal, the automobile, a version of the MAC operating system, or even an ancient game console. If a search engine can deduce that the user is interested in automobiles, for example, then it can do a better job of returning relevant pages to the user.
Ideally, each user would have a private PageRank vector that gives the importance of each page to that user. It is not feasible to store a vector of length many billions for each of a billion users, so we need to do something
184 CHAPTER 5. LINK ANALYSIS
simpler. The topic-sensitive PageRank approach creates one vector for each of some small number of topics, biasing the PageRank to favor pages of that topic. We then endeavour to classify users according to the degree of their interest in each of the selected topics. While we surely lose some accuracy, the benefit is that we store only a short vector for each user, rather than an enormous vector for each user.
Example 5.9 : One useful topic set is the 16 top-level categories (sports, med- icine, etc.) of the Open Directory (DMOZ).6 We could create 16 PageRank vectors, one for each topic. If we could determine that the user is interested in one of these topics, perhaps by the content of the pages they have recently viewed, then we could use the PageRank vector for that topic when deciding on the ranking of pages. ✷
5.3.2 Biased Random Walks
Suppose we have identified some pages that represent a topic such as “sports.” To create a topic-sensitive PageRank for sports, we can arrange that the random surfers are introduced only to a random sports page, rather than to a random page of any kind. The consequence of this choice is that random surfers are likely to be at an identified sports page, or a page reachable along a short path from one of these known sports pages. Our intuition is that pages linked to by sports pages are themselves likely to be about sports. The pages they link to are also likely to be about sports, although the probability of being about sports surely decreases as the distance from an identified sports page increases.
The mathematical formulation for the iteration that yields topic-sensitive PageRank is similar to the equation we used for general PageRank. The only difference is how we add the new surfers. Suppose S is a set of integers consisting of the row/column numbers for the pages we have identified as belonging to a certain topic (called the teleport set). Let eS be a vector that has 1 in the components in S and 0 in other components. Then the topic-sensitive Page- Rank for S is the limit of the iteration
v′ =βMv+(1−β)eS/|S|
Here, as usual, M is the transition matrix of the Web, and |S| is the size of set
S.
Example 5.10 : Let us reconsider the original Web graph we used in Fig. 5.1, which we reproduce as Fig. 5.15. Suppose we use β = 0.8. Then the transition matrix for this graph, multiplied by β, is
 0 2/5 4/5 0  βM =  4/15 0 0 2/5  4/15 0 0 2/5
4/15 2/5 0 0
6This directory, found at www.dmoz.org, is a collection of human-classified Web pages.
 
5.3. TOPIC-SENSITIVE PAGERANK 185
AB
CD
Figure 5.15: Repeat of example Web graph
Suppose that our topic is represented by the teleport set S = {B, D}. Then the vector (1 − β)eS /|S| has 1/10 for its second and fourth components and 0 for the other two components. The reason is that 1 − β = 1/5, the size of S is 2, and eS has 1 in the components for B and D and 0 in the components for A and C. Thus, the equation that must be iterated is
 0 2/5 4/5 0   0  v′ = 4/15 0 0 2/5 v+ 1/10  4/15 0 0 2/5  0 
4/15 2/5 0 0 1/10
Here are the first few iterations of this equation. We have also started with the surfers only at the pages in the teleport set. Although the initial distribution has no effect on the limit, it may help the computation to converge faster.
                    0/2 2/10 42/150 62/250 54/210  1/2 , 3/10 , 41/150 , 71/250 ,..., 59/210   0/2   2/10   26/150   46/250   38/210 
1/2 3/10 41/150
Notice that because of the concentration of surfers at B and D, these nodes get a higher PageRank than they did in Example 5.2. In that example, A was the node of highest PageRank. ✷
5.3.3 Using Topic-Sensitive PageRank
In order to integrate topic-sensitive PageRank into a search engine, we must:
1. Decide on the topics for which we shall create specialized PageRank vec- tors.
2. Pick a teleport set for each of these topics, and use that set to compute the topic-sensitive PageRank vector for that topic.
71/250 59/210
186 3.
4.
CHAPTER 5. LINK ANALYSIS
Find a way of determining the topic or set of topics that are most relevant for a particular search query.
Use the PageRank vectors for that topic or topics in the ordering of the responses to the search query.
We have mentioned one way of selecting the topic set: use the top-level topics of the Open Directory. Other approaches are possible, but there is probably a need for human classification of at least some pages.
The third step is probably the trickiest, and several methods have been proposed. Some possibilities:
(a) Allow the user to select a topic from a menu.
(b) Infer the topic(s) by the words that appear in the Web pages recently searched by the user, or recent queries issued by the user. We need to discuss how one goes from a collection of words to a topic, and we shall do so in Section 5.3.4
(c) Infer the topic(s) by information about the user, e.g., their bookmarks or their stated interests on Facebook.
5.3.4 Inferring Topics from Words
The question of classifying documents by topic is a subject that has been studied for decades, and we shall not go into great detail here. Suffice it to say that topics are characterized by words that appear surprisingly often in documents on that topic. For example, neither fullback nor measles appear very often in documents on the Web. But fullback will appear far more often than average in pages about sports, and measles will appear far more often than average in pages about medicine.
If we examine the entire Web, or a large, random sample of the Web, we can get the background frequency of each word. Suppose we then go to a large sample of pages known to be about a certain topic, say the pages classified under sports by the Open Directory. Examine the frequencies of words in the sports sample, and identify the words that appear significantly more frequently in the sports sample than in the background. In making this judgment, we must be careful to avoid some extremely rare word that appears in the sports sample with relatively higher frequency. This word is probably a misspelling that happened to appear only in one or a few of the sports pages. Thus, we probably want to put a floor on the number of times a word appears, before it can be considered characteristic of a topic.
Once we have identified a large collection of words that appear much more frequently in the sports sample than in the background, and we do the same for all the topics on our list, we can examine other pages and classify them by topic. Here is a simple approach. Suppose that S1,S2,...,Sk are the sets of words that have been determined to be characteristic of each of the topics on
5.4. LINK SPAM 187
our list. Let P be the set of words that appear in a given page P. Compute the Jaccard similarity (recall Section 3.1.1) between P and each of the Si’s. Classify the page as that topic with the highest Jaccard similarity. Note that all Jaccard similarities may be very low, especially if the sizes of the sets Si are small. Thus, it is important to pick reasonably large sets Si to make sure that we cover all aspects of the topic represented by the set.
We can use this method, or a number of variants, to classify the pages the user has most recently retrieved. We could say the user is interested in the topic into which the largest number of these pages fall. Or we could blend the topic- sensitive PageRank vectors in proportion to the fraction of these pages that fall into each topic, thus constructing a single PageRank vector that reflects the user’s current blend of interests. We could also use the same procedure on the pages that the user currently has bookmarked, or combine the bookmarked pages with the recently viewed pages.
5.3.5 Exercises for Section 5.3
Exercise 5.3.1 : Compute the topic-sensitive PageRank for the graph of Fig. 5.15, assuming the teleport set is:
(a) (b)
5.4
A only. A and C.
Link Spam
When it became apparent that PageRank and other techniques used by Google made term spam ineffective, spammers turned to methods designed to fool the PageRank algorithm into overvaluing certain pages. The techniques for artificially increasing the PageRank of a page are collectively called link spam. In this section we shall first examine how spammers create link spam, and then see several methods for decreasing the effectiveness of these spamming techniques, including TrustRank and measurement of spam mass.
5.4.1 Architecture of a Spam Farm
A collection of pages whose purpose is to increase the PageRank of a certain page or pages is called a spam farm. Figure 5.16 shows the simplest form of spam farm. From the point of view of the spammer, the Web is divided into three parts:
1. Inaccessible pages: the pages that the spammer cannot affect. Most of the Web is in this part.
2. Accessible pages: those pages that, while they are not controlled by the spammer, can be affected by the spammer.
188 CHAPTER 5. LINK ANALYSIS 3. Own pages: the pages that the spammer owns and controls.
    Target Page
                                                  Inaccessible Pages
Accessible Pages
Own Pages
Figure 5.16: The Web from the point of view of the link spammer
The spam farm consists of the spammer’s own pages, organized in a special way as seen on the right, and some links from the accessible pages to the spammer’s pages. Without some links from the outside, the spam farm would be useless, since it would not even be crawled by a typical search engine.
Concerning the accessible pages, it might seem surprising that one can af- fect a page without owning it. However, today there are many sites, such as blogs or newspapers that invite others to post their comments on the site. In order to get as much PageRank flowing to his own pages from outside, the spammer posts many comments such as “I agree. Please see my article at www.mySpamFarm.com.”
In the spam farm, there is one page t, the target page, at which the spammer attempts to place as much PageRank as possible. There are a large number m of supporting pages, that accumulate the portion of the PageRank that is distributed equally to all pages (the fraction 1 − β of the PageRank that repre- sents surfers going to a random page). The supporting pages also prevent the PageRank of t from being lost, to the extent possible, since some will be taxed away at each round. Notice that t has a link to every supporting page, and every supporting page links only to t.
5.4. LINK SPAM 189 5.4.2 Analysis of a Spam Farm
Suppose that PageRank is computed using a taxation parameter β, typically around 0.85. That is, β is the fraction of a page’s PageRank that gets dis- tributed to its successors at the next round. Let there be n pages on the Web in total, and let some of them be a spam farm of the form suggested in Fig. 5.16, with a target page t and m supporting pages. Let x be the amount of PageRank contributed by the accessible pages. That is, x is the sum, over all accessible pages p with a link to t, of the PageRank of p times β, divided by the number of successors of p. Finally, let y be the unknown PageRank of t. We shall solve for y.
First, the PageRank of each supporting page is βy/m+(1−β)/n
The first term represents the contribution from t. The PageRank y of t is taxed, so only βy is distributed to t’s successors. That PageRank is divided equally among the m supporting pages. The second term is the supporting page’s share of the fraction 1 − β of the PageRank that is divided equally among all pages on the Web.
Now, let us compute the PageRank y of target page t. Its PageRank comes from three sources:
1. Contribution x from outside, as we have assumed.
2. β times the PageRank of every supporting page; that is,
β βy/m + (1 − β)/n 
3. (1 − β)/n, the share of the fraction 1 − β of the PageRank that belongs to
t. This amount is negligible and will be dropped to simplify the analysis. Thus, from (1) and (2) above, we can write
y = x + βm βy + 1 − β  = x + β2y + β(1 − β)m mnn
   We may solve the above equation for y, yielding y= x +cm
  1−β2 n where c = β(1 − β)/(1 − β2) = β/(1 + β).
Example 5.11: If we choose β = 0.85, then 1/(1 − β2) = 3.6, and c = β/(1 + β) = 0.46. That is, the structure has amplified the external PageRank contribution by 360%, and also obtained an amount of PageRank that is 46% of the fraction of the Web, m/n, that is in the spam farm. ✷
190 CHAPTER 5. LINK ANALYSIS 5.4.3 Combating Link Spam
It has become essential for search engines to detect and eliminate link spam, just as it was necessary in the previous decade to eliminate term spam. There are two approaches to link spam. One is to look for structures such as the spam farm in Fig. 5.16, where one page links to a very large number of pages, each of which links back to it. Search engines surely search for such structures and eliminate those pages from their index. That causes spammers to develop different structures that have essentially the same effect of capturing PageRank for a target page or pages. There is essentially no end to variations of Fig. 5.16, so this war between the spammers and the search engines will likely go on for a long time.
However, there is another approach to eliminating link spam that doesn’t rely on locating the spam farms. Rather, a search engine can modify its defini- tion of PageRank to lower the rank of link-spam pages automatically. We shall consider two different formulas:
1. TrustRank, a variation of topic-sensitive PageRank designed to lower the score of spam pages.
2. Spam mass, a calculation that identifies the pages that are likely to be spam and allows the search engine to eliminate those pages or to lower their PageRank strongly.
5.4.4 TrustRank
TrustRank is topic-sensitive PageRank, where the “topic” is a set of pages be- lieved to be trustworthy (not spam). The theory is that while a spam page might easily be made to link to a trustworthy page, it is unlikely that a trust- worthy page would link to a spam page. The borderline area is a site with blogs or other opportunities for spammers to create links, as was discussed in Section 5.4.1. These pages cannot be considered trustworthy, even if their own content is highly reliable, as would be the case for a reputable newspaper that allowed readers to post comments.
To implement TrustRank, we need to develop a suitable teleport set of trustworthy pages. Two approaches that have been tried are:
1. Let humans examine a set of pages and decide which of them are trust- worthy. For example, we might pick the pages of highest PageRank to examine, on the theory that, while link spam can raise a page’s rank from the bottom to the middle of the pack, it is essentially impossible to give a spam page a PageRank near the top of the list.
2. Pick a domain whose membership is controlled, on the assumption that it is hard for a spammer to get their pages into these domains. For example, we could pick the .edu domain, since university pages are unlikely to be spam farms. We could likewise pick .mil, or .gov. However, the problem
5.4. LINK SPAM 191
with these specific choices is that they are almost exclusively US sites. To get a good distribution of trustworthy Web pages, we should include the analogous sites from foreign countries, e.g., ac.il, or edu.sg.
It is likely that search engines today implement a strategy of the second type routinely, so that what we think of as PageRank really is a form of TrustRank.
5.4.5 Spam Mass
The idea behind spam mass is that we measure for each page the fraction of its PageRank that comes from spam. We do so by computing both the ordinary PageRank and the TrustRank based on some teleport set of trustworthy pages. Suppose page p has PageRank r and TrustRank t. Then the spam mass of p is (r − t)/r. A negative or small positive spam mass means that p is probably not a spam page, while a spam mass close to 1 suggests that the page probably is spam. It is possible to eliminate pages with a high spam mass from the index of Web pages used by a search engine, thus eliminating a great deal of the link spam without having to identify particular structures that spam farmers use.
Example 5.12 : Let us consider both the PageRank and topic-sensitive Page- Rank that were computed for the graph of Fig. 5.1 in Examples 5.2 and 5.10, respectively. In the latter case, the teleport set was nodes B and D, so let us assume those are the trusted pages. Figure 5.17 tabulates the PageRank, TrustRank, and spam mass for each of the four nodes.
Node
PageRank 3/9 2/9 2/9 2/9
TrustRank 54/210 59/210 38/210 59/210
Spam Mass 0.229
-0.264 0.186 -0.264
 A B
C D
Figure 5.17: Calculation of spam mass
In this simple example, the only conclusion is that the nodes B and D, which were a priori determined not to be spam, have negative spam mass and are therefore not spam. The other two nodes, A and C, each have a positive spam mass, since their PageRanks are higher than their TrustRanks. For instance, the spam mass of A is computed by taking the difference 3/9 − 54/210 = 8/105 and dividing 8/105 by the PageRank 3/9 to get 8/35 or about 0.229. However, their spam mass is still closer to 0 than to 1, so it is probable that they are not spam. ✷
5.4.6 Exercises for Section 5.4
Exercise 5.4.1 : In Section 5.4.2 we analyzed the spam farm of Fig. 5.16, where every supporting page links back to the target page. Repeat the analysis for a
192 CHAPTER 5. LINK ANALYSIS
spam farm in which:
(a) Each supporting page links to itself instead of to the target page. (b) Each supporting page links nowhere.
(c) Each supporting page links both to itself and to the target page.
Exercise 5.4.2 : For the original Web graph of Fig. 5.1, assuming only B is a trusted page:
(a) Compute the TrustRank of each page.
(b) Compute the spam mass of each page.
! Exercise 5.4.3: Suppose two spam farmers agree to link their spam farms. How would you link the pages in order to increase as much as possible the PageRank of each spam farm’s target page? Is there an advantage to linking spam farms?
5.5 Hubs and Authorities
An idea called “hubs and authorities’ was proposed shortly after PageRank was first implemented. The algorithm for computing hubs and authorities bears some resemblance to the computation of PageRank, since it also deals with the iterative computation of a fixedpoint involving repeated matrix–vector multi- plication. However, there are also significant differences between the two ideas, and neither can substitute for the other.
This hubs-and-authorities algorithm, sometimes called HITS (hyperlink- induced topic search), was originally intended not as a preprocessing step before handling search queries, as PageRank is, but as a step to be done along with the processing of a search query, to rank only the responses to that query. We shall, however, describe it as a technique for analyzing the entire Web, or the portion crawled by a search engine. There is reason to believe that something like this approach is, in fact, used by the Ask search engine.
5.5.1 The Intuition Behind HITS
While PageRank assumes a one-dimensional notion of importance for pages, HITS views important pages as having two flavors of importance.
1. Certain pages are valuable because they provide information about a topic. These pages are called authorities.
2. Other pages are valuable not because they provide information about any topic, but because they tell you where to go to find out about that topic. These pages are called hubs.
5.5. HUBS AND AUTHORITIES 193
Example 5.13 : A typical department at a university maintains a Web page listing all the courses offered by the department, with links to a page for each course, telling about the course – the instructor, the text, an outline of the course content, and so on. If you want to know about a certain course, you need the page for that course; the departmental course list will not do. The course page is an authority for that course. However, if you want to find out what courses the department is offering, it is not helpful to search for each courses’ page; you need the page with the course list first. This page is a hub for information about courses. ✷
Just as PageRank uses the recursive definition of importance that “a page is important if important pages link to it,” HITS uses a mutually recursive definition of two concepts: “a page is a good hub if it links to good authorities, and a page is a good authority if it is linked to by good hubs.”
5.5.2 Formalizing Hubbiness and Authority
To formalize the above intuition, we shall assign two scores to each Web page. One score represents the hubbiness of a page – that is, the degree to which it is a good hub, and the second score represents the degree to which the page is a good authority. Assuming that pages are enumerated, we represent these scores by vectors h and a. The ith component of h gives the hubbiness of the ith page, and the ith component of a gives the authority of the same page.
While importance is divided among the successors of a page, as expressed by the transition matrix of the Web, the normal way to describe the computation of hubbiness and authority is to add the authority of successors to estimate hubbiness and to add hubbiness of predecessors to estimate authority. If that is all we did, then the hubbiness and authority values would typically grow beyond bounds. Thus, we normally scale the values of the vectors h and a so that the largest component is 1. An alternative is to scale so that the sum of components is 1.
To describe the iterative computation of h and a formally, we use the link matrix of the Web, L. If we have n pages, then L is an n×n matrix, and Lij = 1 ifthereisalinkfrompageitopagej,andLij =0ifnot. Weshallalsohave need for LT, the transpose of L. That is, LTij = 1 if there is a link from page j to page i, and LTij = 0 otherwise. Notice that LT is similar to the matrix M that we used for PageRank, but where LT has 1, M has a fraction – 1 divided by the number of out-links from the page represented by that column.
Example 5.14 : For a running example, we shall use the Web of Fig. 5.4, which we reproduce here as Fig. 5.18. An important observation is that dead ends or spider traps do not prevent the HITS iteration from converging to a meaningful pair of vectors. Thus, we can work with Fig. 5.18 directly, with no “taxation” or alteration of the graph needed. The link matrix L and its transpose are shown in Fig. 5.19. ✷
194
CHAPTER 5. LINK ANALYSIS
AB
CD
                     E
Figure 5.18: Sample data used for HITS examples
01110 01000
10010 T 10010 L=0 0 0 0 1 L =1 0 0 1 0  0 1 1 0 0   1 1 0 0 0 
00000 00100
Figure 5.19: The link matrix for the Web of Fig. 5.18 and its transpose
The fact that the hubbiness of a page is proportional to the sum of the authority of its successors is expressed by the equation h = λLa, where λ is an unknown constant representing the scaling factor needed. Likewise, the fact that the authority of a page is proportional to the sum of the hubbinesses of its predecessors is expressed by a = μLTh, where μ is another scaling constant. These equations allow us to compute the hubbiness and authority indepen- dently, by substituting one equation in the other, as:
• h = λμLLTh. • a = λμLTLa.
However, since LLT and LTL are not as sparse as L and LT, we are usually better off computing h and a in a true mutual recursion. That is, start with h a vector of all 1’s.
1. Compute a = LTh and then scale so the largest component is 1. 2. Next, compute h = La and scale again.
5.5. HUBS AND AUTHORITIES 195
Now, we have a new h and can repeat steps (1) and (2) until at some iteration the changes to the two vectors are sufficiently small that we can stop and accept the current values as the limit.
1 1 1 2
1/2 3 1 3/2
1 1/2
1 2
 1   2  1 2  2/3 
1  1/2 
1 1 1/2 0 0
 1/6  h LTh a La h
1/2 3/10 29/10  1  5/3  1   6/5  12/29
5/3  1   1/10   1/29   3 / 2   9 / 1 0   2   2 0 / 2 9 
1/6 1/10 0 0
LTh a
Figure 5.20: First two iterations of the HITS algorithm
Example 5.15 : Let us perform the first two iterations of the HITS algorithm on the Web of Fig. 5.18. In Fig. 5.20 we see the succession of vectors computed. The first column is the initial h, all 1’s. In the second column, we have estimated the relative authority of pages by computing LTh, thus giving each page the sum of the hubbinesses of its predecessors. The third column gives us the first estimate of a. It is computed by scaling the second column; in this case we have divided each component by 2, since that is the largest value in the second column.
The fourth column is La. That is, we have estimated the hubbiness of each page by summing the estimate of the authorities of each of its successors. Then, the fifth column scales the fourth column. In this case, we divide by 3, since that is the largest value in the fourth column. Columns six through nine repeat the process outlined in our explanations for columns two through five, but with the better estimate of hubbiness given by the fifth column.
The limit of this process may not be obvious, but it can be computed by a simple program. The limits are:
 1   0.2087   0.3583   1 
h=0 a=1
 0.7165   0.7913  00
La
h
196 CHAPTER 5. LINK ANALYSIS
This result makes sense. First, we notice that the hubbiness of E is surely 0, since it leads nowhere. The hubbiness of C depends only on the authority of E and vice versa, so it should not surprise us that both are 0. A is the greatest hub, since it links to the three biggest authorities, B, C, and D. Also, B and C are the greatest authorities, since they are linked to by the two biggest hubs, A and D.
For Web-sized graphs, the only way of computing the solution to the hubs- and-authorities equations is iteratively. However, for this tiny example, we can compute the solution by solving equations. We shall use the equations h = λμLLTh. First, LLT is
31020 T 12000 LL =0 0 1 0 0  2 0 0 2 0 
00000
Let ν = 1/(λμ) and let the components of h for nodes A through E be a through
e, respectively. Then the equations for h can be written
νa=3a+b+2d νb=a+2b νc = c νd = 2a + 2d νe = 0
The equation for b tells us b = a/(ν − 2) and the equation for d tells us d = 2a/(ν − 2). If we substitute these expressions for b and d in the equation for a, we get νa = a 3+5/(ν−2) . From this equation, since a is a factor of both sides, we are left with a quadratic equation for ν which simplifies to ν2 − 5ν + 1 = 0. The positive root is ν = (5 + √21)/2 = 4.791. Now that we know ν is neither 0 or 1, the equations for c and e tell us immediately that c = e = 0.
Finally, if we recognize that a is the largest component of h and set a = 1, wegetb=0.3583andd=0.7165. Alongwithc=e=0,thesevaluesgiveus the limiting value of h. The value of a can be computed from h by multiplying by LT and scaling. ✷
5.5.3 Exercises for Section 5.5
Exercise 5.5.1 : Compute the hubbiness and authority of each of the nodes in our original Web graph of Fig. 5.1.
! Exercise 5.5.2 : Suppose our graph is a chain of n nodes, as was suggested by Fig. 5.9. Compute the hubs and authorities vectors, as a function of n.
5.6 Summary of Chapter 5
✦ Term Spam: Early search engines were unable to deliver relevant results because they were vulnerable to term spam – the introduction into Web pages of words that misrepresented what the page was about.
 
5.6. SUMMARY OF CHAPTER 5 197
✦ The Google Solution to Term Spam: Google was able to counteract term spam by two techniques. First was the PageRank algorithm for deter- mining the relative importance of pages on the Web. The second was a strategy of believing what other pages said about a given page, in or near their links to that page, rather than believing only what the page said about itself.
✦ PageRank: PageRank is an algorithm that assigns a real number, called its PageRank, to each page on the Web. The PageRank of a page is a measure of how important the page is, or how likely it is to be a good response to a search query. In its simplest form, PageRank is a solution to the recursive equation “a page is important if important pages link to it.”
✦ Transition Matrix of the Web: We represent links in the Web by a matrix whose ith row and ith column represent the ith page of the Web. If there are one or more links from page j to page i, then the entry in row i and column j is 1/k, where k is the number of pages to which page j links. Other entries of the transition matrix are 0.
✦ Computing PageRank on Strongly Connected Web Graphs: For strongly connected Web graphs (those where any node can reach any other node), PageRank is the principal eigenvector of the transition matrix. We can compute PageRank by starting with any nonzero vector and repeatedly multiplying the current vector by the transition matrix, to get a better estimate.7 After about 50 iterations, the estimate will be very close to the limit, which is the true PageRank.
✦ The Random Surfer Model: Calculation of PageRank can be thought of as simulating the behavior of many random surfers, who each start at a random page and at any step move, at random, to one of the pages to which their current page links. The limiting probability of a surfer being at a given page is the PageRank of that page. The intuition is that people tend to create links to the pages they think are useful, so random surfers will tend to be at a useful page.
✦ Dead Ends: A dead end is a Web page with no links out. The presence of dead ends will cause the PageRank of some or all of the pages to go to 0 in the iterative computation, including pages that are not dead ends. We can eliminate all dead ends before undertaking a PageRank calculation by recursively dropping nodes with no arcs out. Note that dropping one node can cause another, which linked only to it, to become a dead end, so the process must be recursive.
7Technically, the condition for this method to work is more restricted than simply “strongly connected.” However, the other necessary conditions will surely be met by any large strongly connected component of the Web that was not artificially constructed.
 
198 CHAPTER 5. LINK ANALYSIS
✦ Spider Traps: A spider trap is a set of nodes that, while they may link to each other, have no links out to other nodes. In an iterative calculation of PageRank, the presence of spider traps cause all the PageRank to be captured within that set of nodes.
✦ Taxation Schemes: To counter the effect of spider traps (and of dead ends, if we do not eliminate them), PageRank is normally computed in a way that modifies the simple iterative multiplication by the transition matrix. A parameter β is chosen, typically around 0.85. Given an estimate of the PageRank, the next estimate is computed by multiplying the estimate by β times the transition matrix, and then adding (1 − β)/n to the estimate for each page, where n is the total number of pages.
✦ Taxation and Random Surfers: The calculation of PageRank using taxa- tion parameter β can be thought of as giving each random surfer a prob- ability 1 − β of leaving the Web, and introducing an equivalent number of surfers randomly throughout the Web.
✦ Efficient Representation of Transition Matrices: Since a transition matrix is very sparse (almost all entries are 0), it saves both time and space to represent it by listing its nonzero entries. However, in addition to being sparse, the nonzero entries have a special property: they are all the same in any given column; the value of each nonzero entry is the inverse of the number of nonzero entries in that column. Thus, the preferred representation is column-by-column, where the representation of a column is the number of nonzero entries, followed by a list of the rows where those entries occur.
✦ Very Large-Scale Matrix–Vector Multiplication: For Web-sized graphs, it may not be feasible to store the entire PageRank estimate vector in the main memory of one machine. Thus, we can break the vector into k segments and break the transition matrix into k2 squares, called blocks, assigning each square to one machine. The vector segments are each sent to k machines, so there is a small additional cost in replicating the vector.
✦ Representing Blocks of a Transition Matrix : When we divide a transition matrix into square blocks, the columns are divided into k segments. To represent a segment of a column, nothing is needed if there are no nonzero entries in that segment. However, if there are one or more nonzero entries, then we need to represent the segment of the column by the total number of nonzero entries in the column (so we can tell what value the nonzero entries have) followed by a list of the rows with nonzero entries.
✦ Topic-Sensitive PageRank: If we know the queryer is interested in a cer- tain topic, then it makes sense to bias the PageRank in favor of pages on that topic. To compute this form of PageRank, we identify a set of pages known to be on that topic, and we use it as a “teleport set.” The
5.6. SUMMARY OF CHAPTER 5 199
PageRank calculation is modified so that only the pages in the teleport set are given a share of the tax, rather than distributing the tax among all pages on the Web.
✦ Creating Teleport Sets: For topic-sensitive PageRank to work, we need to identify pages that are very likely to be about a given topic. One approach is to start with the pages that the open directory (DMOZ) identifies with that topic. Another is to identify words known to be associated with the topic, and select for the teleport set those pages that have an unusually high number of occurrences of such words.
✦ Link Spam: To fool the PageRank algorithm, unscrupulous actors have created spam farms. These are collections of pages whose purpose is to concentrate high PageRank on a particular target page.
✦ Structure of a Spam Farm: Typically, a spam farm consists of a target page and very many supporting pages. The target page links to all the supporting pages, and the supporting pages link only to the target page. In addition, it is essential that some links from outside the spam farm be created. For example, the spammer might introduce links to their target page by writing comments in other people’s blogs or discussion groups.
✦ TrustRank: One way to ameliorate the effect of link spam is to compute a topic-sensitive PageRank called TrustRank, where the teleport set is a collection of trusted pages. For example, the home pages of universities could serve as the trusted set. This technique avoids sharing the tax in the PageRank calculation with the large numbers of supporting pages in spam farms and thus preferentially reduces their PageRank.
✦ Spam Mass: To identify spam farms, we can compute both the conven- tional PageRank and the TrustRank for all pages. Those pages that have much lower TrustRank than PageRank are likely to be part of a spam farm.
✦ Hubs and Authorities: While PageRank gives a one-dimensional view of the importance of pages, an algorithm called HITS tries to measure two different aspects of importance. Authorities are those pages that contain valuable information. Hubs are pages that, while they do not themselves contain the information, link to places where the information can be found.
✦ Recursive Formulation of the HITS Algorithm: Calculation of the hubs and authorities scores for pages depends on solving the recursive equa- tions: “a hub links to many authorities, and an authority is linked to by many hubs.” The solution to these equations is essentially an iter- ated matrix–vector multiplication, just like PageRank’s. However, the existence of dead ends or spider traps does not affect the solution to the
200 CHAPTER 5. LINK ANALYSIS HITS equations in the way they do for PageRank, so no taxation scheme
is necessary.
5.7 References for Chapter 5
The PageRank algorithm was first expressed in [1]. The experiments on the structure of the Web, which we used to justify the existence of dead ends and spider traps, were described in [2]. The block-stripe method for performing the PageRank iteration is taken from [5].
Topic-sensitive PageRank is taken from [6]. TrustRank is described in [4], and the idea of spam mass is taken from [3].
The HITS (hubs and authorities) idea was described in [7].
1. S. Brin and L. Page, “Anatomy of a large-scale hypertextual web search
engine,” Proc. 7th Intl. World-Wide-Web Conference, pp. 107–117, 1998.
2. A. Broder, R. Kumar, F. Maghoul, P. Raghavan, S. Rajagopalan, R. Stata, A. Tomkins, and J. Weiner, “Graph structure in the web,” Com- puter Networks 33:1–6, pp. 309–320, 2000.
3. Z. Gy ̈ongi, P. Berkhin, H. Garcia-Molina, and J. Pedersen, “Link spam detection based on mass estimation,” Proc. 32nd Intl. Conf. on Very Large Databases, pp. 439–450, 2006.
4. Z. Gy ̈ongi, H. Garcia-Molina, and J. Pedersen, “Combating link spam with trustrank,” Proc. 30th Intl. Conf. on Very Large Databases, pp. 576– 587, 2004.
5. T.H. Haveliwala, “Efficient computation of PageRank,” Stanford Univ. Dept. of Computer Science technical report, Sept., 1999. Available as
http://infolab.stanford.edu/~taherh/papers/efficient-pr.pdf
6. T.H. Haveliwala, “Topic-sensitive PageRank,” Proc. 11th Intl. World- Wide-Web Conference, pp. 517–526, 2002
7. J.M. Kleinberg, “Authoritative sources in a hyperlinked environment,” J. ACM 46:5, pp. 604–632, 1999.
Chapter 6
Frequent Itemsets
We turn in this chapter to one of the major families of techniques for character- izing data: the discovery of frequent itemsets. This problem is often viewed as the discovery of “association rules,” although the latter is a more complex char- acterization of data, whose discovery depends fundamentally on the discovery of frequent itemsets.
To begin, we introduce the “market-basket” model of data, which is essen- tially a many-many relationship between two kinds of elements, called “items” and “baskets,” but with some assumptions about the shape of the data. The frequent-itemsets problem is that of finding sets of items that appear in (are related to) many of the same baskets.
The problem of finding frequent itemsets differs from the similarity search discussed in Chapter 3. Here we are interested in the absolute number of baskets that contain a particular set of items. In Chapter 3 we wanted items that have a large fraction of their baskets in common, even if the absolute number of baskets is small.
The difference leads to a new class of algorithms for finding frequent item- sets. We begin with the A-Priori Algorithm, which works by eliminating most large sets as candidates by looking first at smaller sets and recognizing that a large set cannot be frequent unless all its subsets are. We then consider various improvements to the basic A-Priori idea, concentrating on very large data sets that stress the available main memory.
Next, we consider approximate algorithms that work faster but are not guaranteed to find all frequent itemsets. Also in this class of algorithms are those that exploit parallelism, including the parallelism we can obtain through a MapReduce formulation. Finally, we discuss briefly how to find frequent itemsets in a data stream.
201
202 CHAPTER 6. FREQUENT ITEMSETS 6.1 The Market-Basket Model
The market-basket model of data is used to describe a common form of many- many relationship between two kinds of objects. On the one hand, we have items, and on the other we have baskets, sometimes called “transactions.” Each basket consists of a set of items (an itemset), and usually we assume that the number of items in a basket is small – much smaller than the total number of items. The number of baskets is usually assumed to be very large, bigger than what can fit in main memory. The data is assumed to be represented in a file consisting of a sequence of baskets. In terms of the distributed file system described in Section 2.1, the baskets are the objects of the file, and each basket is of type “set of items.”
6.1.1 Definition of Frequent Itemsets
Intuitively, a set of items that appears in many baskets is said to be “frequent.” To be formal, we assume there is a number s, called the support threshold. If I is a set of items, the support for I is the number of baskets for which I is a subset. We say I is frequent if its support is s or more.
Example 6.1 : In Fig. 6.1 are sets of words. Each set is a basket, and the words are items. We took these sets by Googling cat dog and taking snippets from the highest-ranked pages. Do not be concerned if a word appears twice in a basket, as baskets are sets, and in principle items can appear only once. Also, ignore capitalization.
1. {Cat, and, dog, bites}
2. {Yahoo, news, claims, a, cat, mated, with, a, dog, and, produced, viable,
offspring}
3. {Cat, killer, likely, is, a, big, dog}
4. {Professional, free, advice, on, dog, training, puppy, training}
5. {Cat, and, kitten, training, and, behavior}
6. {Dog, &, Cat, provides, dog, training, in, Eugene, Oregon}
7. {“Dog, and, cat”, is, a, slang, term, used, by, police, officers, for, a, male– female, relationship}
8. {Shop, for, your, show, dog, grooming, and, pet, supplies}
Figure 6.1: Here are eight baskets, each consisting of items that are words
Since the empty set is a subset of any set, the support for ∅ is 8. However, we shall not generally concern ourselves with the empty set, since it tells us
6.1. THE MARKET-BASKET MODEL 203
nothing.
Among the singleton sets, obviously {cat} and {dog} are quite frequent.
“Dog” appears in all but basket (5), so its support is 7, while “cat” appears in all but (4) and (8), so its support is 6. The word “and” is also quite frequent; it appears in (1), (2), (5), (7), and (8), so its support is 5. The words “a” and “training” appear in three sets, while “for” and “is” appear in two each. No other word appears more than once.
Suppose that we set our threshold at s = 3. Then there are five frequent singleton itemsets: {dog}, {cat}, {and}, {a}, and {training}.
Now, let us look at the doubletons. A doubleton cannot be frequent unless both items in the set are frequent by themselves. Thus, there are only ten possible frequent doubletons. Fig. 6.2 is a table indicating which baskets contain which doubletons.
      training a
and cat
        dog cat and a
For example, we see from the table of Fig. 6.2 that doubleton {dog, training} appears only in baskets (4) and (6). Therefore, its support is 2, and it is not frequent. There are five frequent doubletons if s = 3; they are
{dog, a} {dog, and} {dog, cat} {cat, a} {cat, and}
Each appears exactly three times, except for {dog, cat}, which appears five times.
Next, let us see if there are frequent triples. In order to be a frequent triple, each pair of elements in the set must be a frequent doubleton. For example, {dog, a, and} cannot be a frequent itemset, because if it were, then surely {a, and} would be frequent, but it is not. The triple {dog, cat, and} might be frequent, because each of its doubleton subsets is frequent. Unfortunately, the three words appear together only in baskets (1) and (2), so there are in fact no frequent triples. The triple {dog, cat, a} might be frequent, since its doubletons are all frequent. In fact, all three words do appear in baskets (2), (3), and (7), so it is a frequent triple. No other triple of words is even a candidate for being a frequent triple, since for no other triple of words are its three doubleton subsets frequent. As there is only one frequent triple, there can be no frequent quadruples or larger sets. ✷
4, 6
5, 6
2, 3, 7
1, 2, 8
1, 2, 3, 6, 7
      2, 3, 7
1, 2, 5
     5
2, 7
    none
 Figure 6.2: Occurrences of doubletons
204 CHAPTER 6. FREQUENT ITEMSETS
   On-Line versus Brick-and-Mortar Retailing
We suggested in Section 3.1.3 that an on-line retailer would use similarity measures for items to find pairs of items that, while they might not be bought by many customers, had a significant fraction of their customers in common. An on-line retailer could then advertise one item of the pair to the few customers who had bought the other item of the pair. This methodology makes no sense for a bricks-and-mortar retailer, because un- less lots of people buy an item, it cannot be cost effective to advertise a sale on the item. Thus, the techniques of Chapter 3 are not often useful for brick-and-mortar retailers.
Conversely, the on-line retailer has little need for the analysis we dis- cuss in this chapter, since it is designed to search for itemsets that appear frequently. If the on-line retailer was limited to frequent itemsets, they would miss all the opportunities that are present in the “long tail” to select advertisements for each customer individually.
 6.1.2 Applications of Frequent Itemsets
The original application of the market-basket model was in the analysis of true market baskets. That is, supermarkets and chain stores record the contents of every market basket (physical shopping cart) brought to the register for checkout. Here the “items” are the different products that the store sells, and the “baskets” are the sets of items in a single market basket. A major chain might sell 100,000 different items and collect data about millions of market baskets.
By finding frequent itemsets, a retailer can learn what is commonly bought together. Especially important are pairs or larger sets of items that occur much more frequently than would be expected were the items bought independently. We shall discuss this aspect of the problem in Section 6.1.3, but for the moment let us simply consider the search for frequent itemsets. We will discover by this analysis that many people buy bread and milk together, but that is of little interest, since we already knew that these were popular items individually. We might discover that many people buy hot dogs and mustard together. That, again, should be no surprise to people who like hot dogs, but it offers the supermarket an opportunity to do some clever marketing. They can advertise a sale on hot dogs and raise the price of mustard. When people come to the store for the cheap hot dogs, they often will remember that they need mustard, and buy that too. Either they will not notice the price is high, or they reason that it is not worth the trouble to go somewhere else for cheaper mustard.
The famous example of this type is “diapers and beer.” One would hardly expect these two items to be related, but through data analysis one chain store discovered that people who buy diapers are unusually likely to buy beer. The
6.1. THE MARKET-BASKET MODEL 205
theory is that if you buy diapers, you probably have a baby at home, and if you have a baby, then you are unlikely to be drinking at a bar; hence you are more likely to bring beer home. The same sort of marketing ploy that we suggested for hot dogs and mustard could be used for diapers and beer.
However, applications of frequent-itemset analysis is not limited to market baskets. The same model can be used to mine many other kinds of data. Some examples are:
1. Related concepts: Let items be words, and let baskets be documents (e.g., Web pages, blogs, tweets). A basket/document contains those items/words that are present in the document. If we look for sets of words that appear together in many documents, the sets will be domi- nated by the most common words (stop words), as we saw in Example 6.1. There, even though the intent was to find snippets that talked about cats and dogs, the stop words “and” and “a” were prominent among the fre- quent itemsets. However, if we ignore all the most common words, then we would hope to find among the frequent pairs some pairs of words that represent a joint concept. For example, we would expect a pair like {Brad, Angelina} to appear with surprising frequency.
2. Plagiarism: Let the items be documents and the baskets be sentences. An item/document is “in” a basket/sentence if the sentence is in the document. This arrangement appears backwards, but it is exactly what we need, and we should remember that the relationship between items and baskets is an arbitrary many-many relationship. That is, “in” need not have its conventional meaning: “part of.” In this application, we look for pairs of items that appear together in several baskets. If we find such a pair, then we have two documents that share several sentences in common. In practice, even one or two sentences in common is a good indicator of plagiarism.
3. Biomarkers: Let the items be of two types – biomarkers such as genes or blood proteins, and diseases. Each basket is the set of data about a patient: their genome and blood-chemistry analysis, as well as their medical history of disease. A frequent itemset that consists of one disease and one or more biomarkers suggests a test for the disease.
6.1.3 Association Rules
While the subject of this chapter is extracting frequent sets of items from data, this information is often presented as a collection of if–then rules, called associ- ation rules. The form of an association rule is I → j, where I is a set of items and j is an item. The implication of this association rule is that if all of the items in I appear in some basket, then j is “likely” to appear in that basket as well.
206 CHAPTER 6. FREQUENT ITEMSETS
We formalize the notion of “likely” by defining the confidence of the rule I→jtobetheratioofthesupportforI∪{j}tothesupportforI. Thatis, the confidence of the rule is the fraction of the baskets with all of I that also contain j.
Example 6.2 : Consider the baskets of Fig. 6.1. The confidence of the rule {cat, dog} → and is 3/5. The words “cat” and “dog” appear in five baskets: (1), (2), (3), (6), and (7). Of these, “and” appears in (1), (2), and (7), or 3/5 of the baskets.
For another illustration, the confidence of {cat} → kitten is 1/6. The word “cat” appears in six baskets, (1), (2), (3), (5), (6), and (7). Of these, only (5) has the word “kitten.” ✷
Confidence alone can be useful, provided the support for the left side of the rule is fairly large. For example, we don’t need to know that people are unusually likely to buy mustard when they buy hot dogs, as long as we know that many people buy hot dogs, and many people buy both hot dogs and mustard. We can still use the sale-on-hot-dogs trick discussed in Section 6.1.2. However, there is often more value to an association rule if it reflects a true relationship, where the item or items on the left somehow affect the item on the right.
Thus, we define the interest of an association rule I → j to be the difference between its confidence and the fraction of baskets that contain j. That is, if I has no influence on j, then we would expect that the fraction of baskets including I that contain j would be exactly the same as the fraction of all baskets that contain j. Such a rule has interest 0. However, it is interesting, in both the informal and technical sense, if a rule has either high interest, meaning that the presence of I in a basket somehow causes the presence of j, or highly negative interest, meaning that the presence of I discourages the presence of j.
Example 6.3 : The story about beer and diapers is really a claim that the asso- ciation rule {diapers} → beer has high interest. That is, the fraction of diaper- buyers who buy beer is significantly greater than the fraction of all customers that buy beer. An example of a rule with negative interest is {coke} → pepsi. That is, people who buy Coke are unlikely to buy Pepsi as well, even though a good fraction of all people buy Pepsi – people typically prefer one or the other, but not both. Similarly, the rule {pepsi} → coke can be expected to have negative interest.
For some numerical calculations, let us return to the data of Fig. 6.1. The rule {dog} → cat has confidence 5/7, since “dog” appears in seven baskets, of which five have “cat.” However, “cat” appears in six out of the eight baskets, so we would expect that 75% of the seven baskets with “dog” would have “cat” as well. Thus, the interest of the rule is 5/7 − 3/4 = −0.036, which is essentially 0. The rule {cat} → kitten has interest 1/6 − 1/8 = 0.042. The justification is that one out of the six baskets with “cat” have “kitten” as well, while “kitten”
6.1. THE MARKET-BASKET MODEL 207 appears in only one of the eight baskets. This interest, while positive, is close
to 0 and therefore indicates the association rule is not very “interesting.” ✷ 6.1.4 Finding Association Rules with High Confidence
Identifying useful association rules is not much harder than finding frequent itemsets. We shall take up the problem of finding frequent itemsets in the balance of this chapter, but for the moment, assume it is possible to find those frequent itemsets whose support is at or above a support threshold s.
If we are looking for association rules I → j that apply to a reasonable fraction of the baskets, then the support of I must be reasonably high. In practice, such as for marketing in brick-and-mortar stores, “reasonably high” is often around 1% of the baskets. We also want the confidence of the rule to be reasonably high, perhaps 50%, or else the rule has little practical effect. As a result, the set I ∪ {j} will also have fairly high support.
Suppose we have found all itemsets that meet a threshold of support, and that we have the exact support calculated for each of these itemsets. We can find within them all the association rules that have both high support and high confidence. That is, if J is a set of n items that is found to be frequent, there are only n possible association rules involving this set of items, namely J − {j} → j for each j in J. If J is frequent, J − {j} must be at least as frequent. Thus, it too is a frequent itemset, and we have already computed the support of both J and J − {j}. Their ratio is the confidence of the rule J − {j} → j.
It must be assumed that there are not too many frequent itemsets and thus not too many candidates for high-support, high-confidence association rules. The reason is that each one found must be acted upon. If we give the store manager a million association rules that meet our thresholds for support and confidence, they cannot even read them, let alone act on them. Likewise, if we produce a million candidates for biomarkers, we cannot afford to run the ex- periments needed to check them out. Thus, it is normal to adjust the support threshold so that we do not get too many frequent itemsets. This assump- tion leads, in later sections, to important consequences about the efficiency of algorithms for finding frequent itemsets.
6.1.5 Exercises for Section 6.1
Exercise 6.1.1 : Suppose there are 100 items, numbered 1 to 100, and also 100 baskets, also numbered 1 to 100. Item i is in basket b if and only if i divides b with no remainder. Thus, item 1 is in all the baskets, item 2 is in all fifty of the even-numbered baskets, and so on. Basket 12 consists of items {1, 2, 3, 4, 6, 12}, since these are all the integers that divide 12. Answer the following questions:
(a) If the support threshold is 5, which items are frequent?
! (b) If the support threshold is 5, which pairs of items are frequent?
208 CHAPTER 6. FREQUENT ITEMSETS ! (c) What is the sum of the sizes of all the baskets?
! Exercise 6.1.2: For the item-basket data of Exercise 6.1.1, which basket is the largest?
Exercise 6.1.3 : Suppose there are 100 items, numbered 1 to 100, and also 100 baskets, also numbered 1 to 100. Item i is in basket b if and only if b divides i with no remainder. For example, basket 12 consists of items
{12, 24, 36, 48, 60, 72, 84, 96} Repeat Exercise 6.1.1 for this data.
! Exercise 6.1.4: This question involves data from which nothing interesting can be learned about frequent itemsets, because there are no sets of items that are correlated. Suppose the items are numbered 1 to 10, and each basket is constructed by including item i with probability 1/i, each decision being made independently of all other decisions. That is, all the baskets contain item 1, half contain item 2, a third contain item 3, and so on. Assume the number of baskets is sufficiently large that the baskets collectively behave as one would expect statistically. Let the support threshold be 1% of the baskets. Find the frequent itemsets.
Exercise 6.1.5 : For the data of Exercise 6.1.1, what is the confidence of the following association rules?
(a) {5,7}→2. (b) {2,3,4} → 5.
Exercise 6.1.6 : For the data of Exercise 6.1.3, what is the confidence of the following association rules?
(a) {24, 60} → 8. (b) {2,3,4} → 5.
!! Exercise 6.1.7: Describe all the association rules that have 100% confidence for the market-basket data of:
(a) Exercise 6.1.1.
(b) Exercise 6.1.3.
! Exercise 6.1.8 : Prove that in the data of Exercise 6.1.4 there are no interest- ing association rules; i.e., the interest of every association rule is 0.
6.2. MARKET BASKETS AND THE A-PRIORI ALGORITHM 209 6.2 Market Baskets and the A-Priori Algorithm
We shall now begin a discussion of how to find frequent itemsets or information derived from them, such as association rules with high support and confidence. The original improvement on the obvious algorithms, known as “A-Priori,” from which many variants have been developed, will be covered here. The next two sections will discuss certain further improvements. Before discussing the A- priori Algorithm itself, we begin the section with an outline of the assumptions about how data is stored and manipulated when searching for frequent itemsets.
6.2.1 Representation of Market-Basket Data
As we mentioned, we assume that market-basket data is stored in a file basket- by-basket. Possibly, the data is in a distributed file system as in Section 2.1, and the baskets are the objects the file contains. Or the data may be stored in a conventional file, with a character code to represent the baskets and their items.
Example 6.4 : We could imagine that such a file begins: {23,456,1001}{3,18,92,145}{...
Here, the character { begins a basket and the character } ends it. The items in a basket are represented by integers and are separated by commas. Thus, the first basket contains items 23, 456, and 1001; the second basket contains items 3, 18, 92, and 145. ✷
It may be that one machine receives the entire file. Or we could be using MapReduce or a similar tool to divide the work among many processors, in which case each processor receives only a part of the file. It turns out that combining the work of parallel processors to get the exact collection of itemsets that meet a global support threshold is hard, and we shall address this question only in Section 6.4.4.
We also assume that the size of the file of baskets is sufficiently large that it
does not fit in main memory. Thus, a major cost of any algorithm is the time
it takes to read the baskets from disk. Once a disk block full of baskets is in
main memory, we can expand it, generating all the subsets of size k. Since one
of the assumptions of our model is that the average size of a basket is small,
generating all the pairs in main memory should take time that is much less
than the time it took to read the basket from disk. For example, if there are 20
items in a basket, then there are  20  = 190 pairs of items in the basket, and 2
these can be generated easily in a pair of nested for-loops.
As the size of the subsets we want to generate gets larger, the time required
grows larger; in fact takes approximately time nk/k! to generate all the subsets of size k for a basket with n items. Eventually, this time dominates the time needed to transfer the data from disk. However:
210 1.
2.
CHAPTER 6. FREQUENT ITEMSETS
Often, we need only small frequent itemsets, so k never grows beyond 2 or 3.
And when we do need the itemsets for a large size k, it is usually possible to eliminate many of the items in each basket as not able to participate in a frequent itemset, so the value of n drops as k increases.
The conclusion we would like to draw is that the work of examining each of the baskets can usually be assumed proportional to the size of the file. We can thus measure the running time of a frequent-itemset algorithm by the number of times that each disk block of the data file is read.
Moreover, all the algorithms we discuss have the property that they read the basket file sequentially. Thus, algorithms can be characterized by the number of passes through the basket file that they make, and their running time is proportional to the product of the number of passes they make through the basket file times the size of that file. Since we cannot control the amount of data, only the number of passes taken by the algorithm matters, and it is that aspect of the algorithm that we shall focus upon when measuring the running time of a frequent-itemset algorithm.
6.2.2 Use of Main Memory for Itemset Counting
There is a second data-related issue that we must examine, however. All frequent-itemset algorithms require us to maintain many different counts as we make a pass through the data. For example, we might need to count the number of times that each pair of items occurs in baskets. If we do not have enough main memory to store each of the counts, then adding 1 to a random count will most likely require us to load a page from disk. In that case, the algorithm will thrash and run many orders of magnitude slower than if we were certain to find each count in main memory. The conclusion is that we cannot count anything that doesn’t fit in main memory. Thus, each algorithm has a limit on how many items it can deal with.
Example 6.5: Suppose a certain algorithm has to count all pairs of items, and there are n items. We thus need space to store  n2  integers, or about n2/2 integers. If integers take 4 bytes, we require 2n2 bytes. If our machine has 2 gigabytes, or 231 bytes of main memory, then we require n ≤ 215, or approximately n < 33,000. ✷
It is not trivial to store the  n2  counts in a way that makes it easy to find the count for a pair {i,j}. First, we have not assumed anything about how items are represented. They might, for instance, be strings like “bread.” It is more space-efficient to represent items by consecutive integers from 1 to n, where n is the number of distinct items. Unless items are already represented this way, we need a hash table that translates items as they appear in the file to integers. That is, each time we see an item in the file, we hash it. If it is
6.2. MARKET BASKETS AND THE A-PRIORI ALGORITHM 211
already in the hash table, we can obtain its integer code from its entry in the table. If the item is not there, we assign it the next available number (from a count of the number of distinct items seen so far) and enter the item and its code into the table.
The Triangular-Matrix Method
Even after coding items as integers, we still have the problem that we must count a pair {i,j} in only one place. For example, we could order the pair so that i < j, and only use the entry a[i,j] in a two-dimensional array a. That strategy would make half the array useless. A more space-efficient way is to use a one-dimensional triangular array. We store in a[k] the count for the pair {i, j}, with 1 ≤ i < j ≤ n, where
k=(i−1) n− i +j−i 2
The result of this layout is that the pairs are stored in lexicographic order, that is first {1,2}, {1,3},...,{1,n}, then {2,3}, {2,4},...,{2,n}, and so on, down to {n−2,n−1}, {n−2,n}, and finally {n−1,n}.
The Triples Method
There is another approach to storing counts that may be more appropriate, depending on the fraction of the possible pairs of items that actually appear in some basket. We can store counts as triples [i, j, c], meaning that the count of pair {i, j}, with i < j, is c. A data structure, such as a hash table with i and j as the search key, is used so we can tell if there is a triple for a given i and j and, if so, to find it quickly. We call this approach the triples method of storing counts.
Unlike the triangular matrix, the triples method does not require us to store anything if the count for a pair is 0. On the other hand, the triples method requires us to store three integers, rather than one, for every pair that does appear in some basket. In addition, there is the space needed for the hash table or other data structure used to support efficient retrieval. The conclusion is that the triangular matrix will be better if at least 1/3 of the  n2  possible pairs actually appear in some basket, while if significantly fewer than 1/3 of the possible pairs occur, we should consider using the triples method.
Example 6.6 : Suppose there are 100,000 items, and 10,000,000 baskets of 10
 items each. Then the triangular-matrix method requires  100000  = 5 × 109 12
(approximately) integer counts. On the other hand, the total number of pairs
among all the baskets is 107 10  = 4.5 × 108. Even in the extreme case that 28
every pair of items appeared only once, there could be only 4.5 × 10 pairs with
 1Here, and throughout the chapter, we shall use the approximation that  n2  = n2/2 for large n.
212 CHAPTER 6. FREQUENT ITEMSETS
nonzero counts. If we used the triples method to store counts, we would need only three times that number of integers, or 1.35 × 109 integers. Thus, in this case the triples method will surely take much less space than the triangular matrix.
However, even if there were ten or a hundred times as many baskets, it would be normal for there to be a sufficiently uneven distribution of items that we might still be better off using the triples method. That is, some pairs would have very high counts, and the number of different pairs that occurred in one or more baskets would be much less than the theoretical maximum number of such pairs. ✷
6.2.3 Monotonicity of Itemsets
Much of the effectiveness of the algorithms we shall discuss is driven by a single observation, called monotonicity for itemsets:
• If a set I of items is frequent, then so is every subset of I.
The reason is simple. Let J ⊆ I. Then every basket that contains all the items in I surely contains all the items in J. Thus, the count for J must be at least as great as the count for I, and if the count for I is at least s, then the count for J is at least s. Since J may be contained in some baskets that are missing one or more elements of I − J, it is entirely possible that the count for J is strictly greater than the count for I.
In addition to making the A-Priori Algorithm work, monotonicity offers us a way to compact the information about frequent itemsets. If we are given a support threshold s, then we say an itemset is maximal if no superset is frequent. If we list only the maximal itemsets, then we know that all subsets of a maximal itemset are frequent, and no set that is not a subset of some maximal itemset can be frequent.
Example 6.7 : Let us reconsider the data of Example 6.1 with support thresh- old s = 3. We found that there were five frequent singletons, those with words “cat,” “dog,” “a,” “and,” and “training.” Each of these is contained in a frequent doubleton, except for “training,” so one maximal frequent itemset is {training}. There are also five frequent doubletons with s = 3, namely
{dog, a} {dog, and} {dog, cat} {cat, a} {cat, and}
We also found one frequent triple, {dog, cat, a}, and there are no larger frequent itemsets. Thus, this triple is maximal, but the three frequent doubletons it contains are not maximal. The other frequent doubletons, {dog, and} and {cat, and}, are maximal. Notice that we can deduce from the frequent doubletons that singletons like {dog} are frequent. ✷
6.2. MARKET BASKETS AND THE A-PRIORI ALGORITHM 213 6.2.4 Tyranny of Counting Pairs
As you may have noticed, we have focused on the matter of counting pairs in the discussion so far. There is a good reason to do so: in practice the most main memory is required for determining the frequent pairs. The number of items, while possibly very large, is rarely so large we cannot count all the singleton sets in main memory at the same time.
What about larger sets – triples, quadruples, and so on? Recall that in order for frequent-itemset analysis to make sense, the result has to be a small number of sets, or we cannot even read them all, let alone consider their significance. Thus, in practice the support threshold is set high enough that it is only a rare set that is frequent. Monotonicity tells us that if there is a frequent triple, then there are three frequent pairs contained within it. And of course there may be frequent pairs contained in no frequent triple as well. Thus, we expect to find more frequent pairs than frequent triples, more frequent triples than frequent quadruples, and so on.
That argument would not be enough were it impossible to avoid counting all the triples, since there are many more triples than pairs. It is the job of the A-Priori Algorithm and related algorithms to avoid counting many triples or larger sets, and they are, as we shall see, effective in doing so. Thus, in what follows, we concentrate on algorithms for computing frequent pairs.
6.2.5 The A-Priori Algorithm
For the moment, let us concentrate on finding the frequent pairs only. If we have enough main memory to count all pairs, using either of the methods discussed in Section 6.2.2 (triangular matrix or triples), then it is a simple matter to read the file of baskets in a single pass. For each basket, we use a double loop to generate all the pairs. Each time we generate a pair, we add 1 to its count. At the end, we examine all pairs to see which have counts that are equal to or greater than the support threshold s; these are the frequent pairs.
However, this simple approach fails if there are too many pairs of items to count them all in main memory. The A-Priori Algorithm is designed to reduce the number of pairs that must be counted, at the expense of performing two passes over data, rather than one pass.
The First Pass of A-Priori
In the first pass, we create two tables. The first table, if necessary, translates item names into integers from 1 to n, as described in Section 6.2.2. The other table is an array of counts; the ith array element counts the occurrences of the item numbered i. Initially, the counts for all the items are 0.
As we read baskets, we look at each item in the basket and translate its name into an integer. Next, we use that integer to index into the array of counts, and we add 1 to the integer found there.
214 CHAPTER 6. FREQUENT ITEMSETS Between the Passes of A-Priori
After the first pass, we examine the counts of the items to determine which of them are frequent as singletons. It might appear surprising that many singletons are not frequent. But remember that we set the threshold s sufficiently high that we do not get too many frequent sets; a typical s would be 1% of the baskets. If we think about our own visits to a supermarket, we surely buy certain things more than 1% of the time: perhaps milk, bread, Coke or Pepsi, and so on. We can even believe that 1% of the customers buy diapers, even though we may not do so. However, many of the items on the shelves are surely not bought by 1% of the customers: Creamy Caesar Salad Dressing for example.
For the second pass of A-Priori, we create a new numbering from 1 to m for just the frequent items. This table is an array indexed 1 to n, and the entry for i is either 0, if item i is not frequent, or a unique integer in the range 1 to m if item i is frequent. We shall refer to this table as the frequent-items table.
The Second Pass of A-Priori
During the second pass, we count all the pairs that consist of two frequent items. Recall from Section 6.2.3 that a pair cannot be frequent unless both its members are frequent. Thus, we miss no frequent pairs. The space required on the second pass is 2m2 bytes, rather than 2n2 bytes, if we use the triangular- matrix method for counting. Notice that the renumbering of just the frequent items is necessary if we are to use a triangular matrix of the right size. The complete set of main-memory structures used in the first and second passes is shown in Fig. 6.3.
Also notice that the benefit of eliminating infrequent items is amplified; if only half the items are frequent we need one quarter of the space to count. Likewise, if we use the triples method, we need to count only those pairs of two frequent items that occur in at least one basket.
The mechanics of the second pass are as follows.
1. For each basket, look in the frequent-items table to see which of its items are frequent.
2. In a double loop, generate all pairs of frequent items in that basket.
3. For each such pair, add one to its count in the data structure used to store counts.
Finally, at the end of the second pass, examine the structure of counts to determine which pairs are frequent.
6.2.6 A-Priori for All Frequent Itemsets
The same technique used for finding frequent pairs without counting all pairs lets us find larger frequent itemsets without an exhaustive count of all sets. In
6.2. MARKET BASKETS AND THE A-PRIORI ALGORITHM 215
  Item names to integers
 1 2
n
Item counts
Unused
  Item names to integers
 1 2
n
Fre− quent
items
 Data structure for counts
of pairs
  Pass 1
Pass 2
Figure 6.3: Schematic of main-memory use during the two passes of the A-Priori Algorithm
the A-Priori Algorithm, one pass is taken for each set-size k. If no frequent itemsets of a certain size are found, then monotonicity tells us there can be no larger frequent itemsets, so we can stop.
The pattern of moving from one size k to the next size k + 1 can be sum- marized as follows. For each size k, there are two sets of itemsets:
1. Ck is the set of candidate itemsets of size k – the itemsets that we must count in order to determine whether they are in fact frequent.
2. Lk is the set of truly frequent itemsets of size k.
The pattern of moving from one set to the next and one size to the next is
suggested by Fig. 6.4.
C1 L1 C2 L2 C3 L3
      Filter
Filter
Filter
.. .
Construct
Construct
Construct
            All items
Frequent items
Pairs of frequent items
Frequent pairs
Figure 6.4: The A-Priori Algorithm alternates between constructing candidate sets and filtering to find those that are truly frequent
216 CHAPTER 6. FREQUENT ITEMSETS
We start with C1, which is all singleton itemsets, i.e., the items themselves. That is, before we examine the data, any item could be frequent as far as we know. The first filter step is to count all items, and those whose counts are at least the support threshold s form the set L1 of frequent items.
The set C2 of candidate pairs is the set of pairs both of whose items are in L1; that is, they are frequent items. Note that we do not construct C2 explicitly. Rather we use the definition of C2, and we test membership in C2 by testing whether both of its members are in L1. The second pass of the A-Priori Algorithm counts all the candidate pairs and determines which appear at least s times. These pairs form L2, the frequent pairs.
We can follow this pattern as far as we wish. The set C3 of candidate triples is constructed (implicitly) as the set of triples, any two of which is a pair in L2. Our assumption about the sparsity of frequent itemsets, outlined in Section 6.2.4 implies that there will not be too many frequent pairs, so they can be listed in a main-memory table. Likewise, there will not be too many candidate triples, so these can all be counted by a generalization of the triples method. That is, while triples are used to count pairs, we would use quadruples, consisting of the three item codes and the associated count, when we want to count triples. Similarly, we can count sets of size k using tuples with k + 1 components, the last of which is the count, and the first k of which are the item codes, in sorted order.
To find L3 we make a third pass through the basket file. For each basket, we need only look at those items that are in L1. From these items, we can examine each pair and determine whether or not that pair is in L2. Any item of the basket that does not appear in at least two frequent pairs, both of which consist of items in the basket, cannot be part of a frequent triple that the basket contains. Thus, we have a fairly limited search for triples that are both contained in the basket and are candidates in C3. Any such triples found have 1 added to their count.
Example 6.8 : Suppose our basket consists of items 1 through 10. Of these, 1 through 5 have been found to be frequent items, and the following pairs have been found frequent: {1, 2}, {2, 3}, {3, 4}, and {4, 5}. At first, we eliminate the nonfrequent items, leaving only 1 through 5. However, 1 and 5 appear in only one frequent pair in the itemset, and therefore cannot contribute to a frequent triple contained in the basket. Thus, we must consider adding to the count of triples that are contained in {2, 3, 4}. There is only one such triple, of course. However, we shall not find it in C3, because {2,4} evidently is not frequent. ✷
The construction of the collections of larger frequent itemsets and candidates proceeds in essentially the same manner, until at some pass we find no new frequent itemsets and stop. That is:
1. Define Ck to be all those itemsets of size k, every k − 1 of which is an itemset in Lk−1.
6.2. MARKET BASKETS AND THE A-PRIORI ALGORITHM 217
2. Find Lk by making a pass through the baskets and counting all and only the itemsets of size k that are in Ck. Those itemsets that have count at least s are in Lk.
6.2.7 Exercises for Section 6.2
Exercise 6.2.1 : If we use a triangular matrix to count pairs, and n, the num- ber of items, is 20, what pair’s count is in a[100]?
! Exercise 6.2.2: In our description of the triangular-matrix method in Sec- tion 6.2.2, the formula for k involves dividing an arbitrary integer i by 2. Yet we need to have k always be an integer. Prove that k will, in fact, be an integer.
! Exercise 6.2.3 : Let there be I items in a market-basket data set of B baskets. Suppose that every basket contains exactly K items. As a function of I, B, and K:
(a) How much space does the triangular-matrix method take to store the counts of all pairs of items, assuming four bytes per array element?
(b) What is the largest possible number of pairs with a nonzero count?
(c) Under what circumstances can we be certain that the triples method will use less space than the triangular array?
!! Exercise 6.2.4 : How would you count all itemsets of size 3 by a generalization of the triangular-matrix method? That is, arrange that in a one-dimensional array there is exactly one element for each set of three items.
! Exercise 6.2.5: Suppose the support threshold is 5. Find the maximal fre- quent itemsets for the data of:
(a) Exercise 6.1.1.
(b) Exercise 6.1.3.
Exercise 6.2.6: Apply the A-Priori Algorithm with support threshold 5 to the data of:
(a) Exercise 6.1.1.
(b) Exercise 6.1.3.
! Exercise 6.2.7: Suppose we have market baskets that satisfy the following assumptions:
1. The support threshold is 10,000.
2. There are one million items, represented by the integers 0, 1, . . . , 999999.
218 CHAPTER 6. FREQUENT ITEMSETS
3. There are N frequent items, that is, items that occur 10,000 times or
more.
4. There are one million pairs that occur 10,000 times or more.
5. There are 2M pairs that occur exactly once. Of these pairs, M consist of two frequent items; the other M each have at least one nonfrequent item.
6. No other pairs occur at all.
7. Integers are always represented by 4 bytes.
Suppose we run the A-Priori Algorithm and can choose on the second pass between the triangular-matrix method for counting candidate pairs and a hash table of item-item-count triples. Neglect in the first case the space needed to translate between original item numbers and numbers for the frequent items, and in the second case neglect the space needed for the hash table. As a function of N and M, what is the minimum number of bytes of main memory needed to execute the A-Priori Algorithm on this data?
6.3 Handling Larger Datasets in Main Memory
The A-Priori Algorithm is fine as long as the step with the greatest requirement for main memory – typically the counting of the candidate pairs C2 – has enough memory that it can be accomplished without thrashing (repeated moving of data between disk and main memory). Several algorithms have been proposed to cut down on the size of candidate set C2. Here, we consider the PCY Algorithm, which takes advantage of the fact that in the first pass of A-Priori there is typically lots of main memory not needed for the counting of single items. Then we look at the Multistage Algorithm, which uses the PCY trick and also inserts extra passes to further reduce the size of C2.
6.3.1 The Algorithm of Park, Chen, and Yu
This algorithm, which we call PCY after its authors, exploits the observation that there may be much unused space in main memory on the first pass. If there are a million items and gigabytes of main memory, we do not need more than 10% of the main memory for the two tables suggested in Fig. 6.3 – a translation table from item names to small integers and an array to count those integers. The PCY Algorithm uses that space for an array of integers that generalizes the idea of a Bloom filter (see Section 4.3). The idea is shown schematically in Fig. 6.5.
Think of this array as a hash table, whose buckets hold integers rather than sets of keys (as in an ordinary hash table) or bits (as in a Bloom filter). Pairs of items are hashed to buckets of this hash table. As we examine a basket during the first pass, we not only add 1 to the count for each item in the basket, but
6.3. HANDLING LARGER DATASETS IN MAIN MEMORY 219
  Item names to integers
 1 2
n
Item counts
 Hash table for bucket counts
  Item names to integers
 1 2
n
Fre− quent
items
 Bitmap
 Data structure for counts
of pairs
  Pass 1
Pass 2
Figure 6.5: Organization of main memory for the first two passes of the PCY Algorithm
we generate all the pairs, using a double loop. We hash each pair, and we add 1 to the bucket into which that pair hashes. Note that the pair itself doesn’t go into the bucket; the pair only affects the single integer in the bucket.
At the end of the first pass, each bucket has a count, which is the sum of the counts of all the pairs that hash to that bucket. If the count of a bucket is at least as great as the support threshold s, it is called a frequent bucket. We can say nothing about the pairs that hash to a frequent bucket; they could all be frequent pairs from the information available to us. But if the count of the bucket is less than s (an infrequent bucket), we know no pair that hashes to this bucket can be frequent, even if the pair consists of two frequent items. That fact gives us an advantage on the second pass. We can define the set of candidate pairs C2 to be those pairs {i, j} such that:
1. i and j are frequent items.
2. {i, j} hashes to a frequent bucket.
It is the second condition that distinguishes PCY from A-Priori.
Example 6.9 : Depending on the data and the amount of available main mem- ory, there may or may not be a benefit to using the hash table on pass 1. In the worst case, all buckets are frequent, and the PCY Algorithm counts exactly the same pairs as A-Priori does on the second pass. However, sometimes, we can expect most of the buckets to be infrequent. In that case, PCY reduces the memory requirements of the second pass.
220 CHAPTER 6. FREQUENT ITEMSETS
Suppose we have a gigabyte of main memory available for the hash table
on the first pass. Suppose also that the data file has a billion baskets, each
with ten items. A bucket is an integer, typically 4 bytes, so we can maintain a
quarter of a billion buckets. The number of pairs in all the baskets is 109 ×  10  10 2
or 4.5 × 10 pairs; this number is also the sum of the counts in the buckets. Thus, the average count is 4.5 × 1010/2.5 × 108, or 180. If the support threshold s is around 180 or less, we might expect few buckets to be infrequent. However, if s is much larger, say 1000, then it must be that the great majority of the buckets are infrequent. The greatest possible number of frequent buckets is 4.5 × 1010/1000, or 45 million out of the 250 million buckets. ✷
Between the passes of PCY, the hash table is summarized as a bitmap, with one bit for each bucket. The bit is 1 if the bucket is frequent and 0 if not. Thus integers of 32 bits are replaced by single bits, and the bitmap shown in the second pass in Fig. 6.5 takes up only 1/32 of the space that would otherwise be available to store counts. However, if most buckets are infrequent, we expect that the number of pairs being counted on the second pass will be much smaller than the total number of pairs of frequent items. Thus, PCY can handle some data sets without thrashing during the second pass, while A-Priori would run out of main memory and thrash.
There is another subtlety regarding the second pass of PCY that affects the amount of space needed. While we were able to use the triangular-matrix method on the second pass of A-Priori if we wished, because the frequent items could be renumbered from 1 to some m, we cannot do so for PCY. The reason is that the pairs of frequent items that PCY lets us avoid counting are placed randomly within the triangular matrix; they are the pairs that happen to hash to an infrequent bucket on the first pass. There is no known way of compacting the matrix to avoid leaving space for the uncounted pairs.
Consequently, we are forced to use the triples method in PCY. That re- striction may not matter if the fraction of pairs of frequent items that actually appear in buckets were small; we would then want to use triples for A-Priori anyway. However, if most pairs of frequent items appear together in at least one bucket, then we are forced in PCY to use triples, while A-Priori can use a triangular matrix. Thus, unless PCY lets us avoid counting at least 2/3 of the pairs of frequent items, we cannot gain by using PCY instead of A-Priori.
While the discovery of frequent pairs by PCY differs significantly from A- Priori, the later stages, where we find frequent triples and larger sets if desired, are essentially the same as A-Priori. This statement holds as well for each of the improvements to A-Priori that we cover in this section. As a result, we shall cover only the construction of the frequent pairs from here on.
6.3.2 The Multistage Algorithm
The Multistage Algorithm improves upon PCY by using several successive hash tables to reduce further the number of candidate pairs. The tradeoff is that
6.3. HANDLING LARGER DATASETS IN MAIN MEMORY 221 Multistage takes more than two passes to find the frequent pairs. An outline of
the Multistage Algorithm is shown in Fig. 6.6.
  Item names to integers
 1 2
n
Item counts
 Hash table for bucket counts
  Item names to integers
 1 2
n
Fre− quent
items
 Bitmap 1
 Second hash table for bucket counts
  Item names to integers
 1 2
n
Fre− quent
items
 Bitmap 1
 Bitmap 2
 Data structure
for counts of pairs
   Pass 1
Pass 2 Pass 3
Figure 6.6: The Multistage Algorithm uses additional hash tables to reduce the number of candidate pairs
The first pass of Multistage is the same as the first pass of PCY. After that pass, the frequent buckets are identified and summarized by a bitmap, again the same as in PCY. But the second pass of Multistage does not count the candidate pairs. Rather, it uses the available main memory for another hash table, using another hash function. Since the bitmap from the first hash table takes up 1/32 of the available main memory, the second hash table has almost as many buckets as the first.
On the second pass of Multistage, we again go through the file of baskets. There is no need to count the items again, since we have those counts from the first pass. However, we must retain the information about which items are frequent, since we need it on both the second and third passes. During the second pass, we hash certain pairs of items to buckets of the second hash table. A pair is hashed only if it meets the two criteria for being counted in the second pass of PCY; that is, we hash {i,j} if and only if i and j are both frequent, and the pair hashed to a frequent bucket on the first pass. As a result, the sum of the counts in the second hash table should be significantly less than the sum for the first pass. The result is that, even though the second hash table has only 31/32 of the number of buckets that the first table has, we expect there to be many fewer frequent buckets in the second hash table than in the first.
After the second pass, the second hash table is also summarized as a bitmap, and that bitmap is stored in main memory. The two bitmaps together take up
222 CHAPTER 6. FREQUENT ITEMSETS
   A Subtle Error in Multistage
Occasionally, an implementation tries to eliminate the second requirement for {i, j} to be a candidate – that it hashes to a frequent bucket on the first pass. The (false) reasoning is that if it didn’t hash to a frequent bucket on the first pass, it wouldn’t have been hashed at all on the second pass, and thus would not contribute to the count of its bucket on the second pass. While it is true that the pair is not counted on the second pass, that doesn’t mean it wouldn’t have hashed to a frequent bucket had it been hashed. Thus, it is entirely possible that {i,j} consists of two frequent items and hashes to a frequent bucket on the second pass, yet it did not hash to a frequent bucket on the first pass. Therefore, all three conditions must be checked on the counting pass of Multistage.
 slightly less than 1/16th of the available main memory, so there is still plenty of space to count the candidate pairs on the third pass. A pair {i, j} is in C2 if and only if:
1. i and j are both frequent items.
2. {i, j} hashed to a frequent bucket in the first hash table.
3. {i, j} hashed to a frequent bucket in the second hash table.
The third condition is the distinction between Multistage and PCY.
It might be obvious that it is possible to insert any number of passes between the first and last in the multistage Algorithm. There is a limiting factor that each pass must store the bitmaps from each of the previous passes. Eventually, there is not enough space left in main memory to do the counts. No matter how many passes we use, the truly frequent pairs will always hash to a frequent
bucket, so there is no way to avoid counting them.
6.3.3 The Multihash Algorithm
Sometimes, we can get most of the benefit of the extra passes of the Multistage Algorithm in a single pass. This variation of PCY is called the Multihash Algorithm. Instead of using two different hash tables on two successive passes, use two hash functions and two separate hash tables that share main memory on the first pass, as suggested by Fig. 6.7.
The danger of using two hash tables on one pass is that each hash table has half as many buckets as the one large hash table of PCY. As long as the average count of a bucket for PCY is much lower than the support threshold, we can operate two half-sized hash tables and still expect most of the buckets of both hash tables to be infrequent. Thus, in this situation we might well choose the multihash approach.
6.3. HANDLING LARGER DATASETS IN MAIN MEMORY 223
  Item names to integers
 1 2
n
Item counts
 Hash Table 1
 Hash Table 2
  Item names to integers
 1 2
n
Fre− quent
items
Bitmap 1
 Bitmap 2
 Data structure for counts
of pairs
   Pass 1
Pass 2
Figure 6.7: The Multihash Algorithm uses several hash tables in one pass
Example 6.10 : Suppose that if we run PCY, the average bucket will have a count s/10, where s is the support threshold. Then if we used the Multihash approach with two half-sized hash tables, the average count would be s/5. As a result, at most 1/5th of the buckets in either hash table could be frequent, and a random infrequent pair has at most probability (1/5)2 = 0.04 of being in a frequent bucket in both hash tables.
By the same reasoning, the upper bound on the infrequent pair being in a frequent bucket in the one PCY hash table is at most 1/10. That is, we might expect to have to count 2.5 times as many infrequent pairs in PCY as in the version of Multihash suggested above. We would therefore expect Multihash to have a smaller memory requirement for the second pass than would PCY.
But these upper bounds do not tell the complete story. There may be many fewer frequent buckets than the maximum for either algorithm, since the presence of some very frequent pairs will skew the distribution of bucket counts. However, this analysis is suggestive of the possibility that for some data and support thresholds, we can do better by running several hash functions in main memory at once. ✷
For the second pass of Multihash, each hash table is converted to a bitmap, as usual. Note that the two bitmaps for the two hash functions in Fig. 6.7 occupy exactly as much space as a single bitmap would for the second pass of the PCY Algorithm. The conditions for a pair {i,j} to be in C2, and thus to require a count on the second pass, are the same as for the third pass of Multistage: i and j must both be frequent, and the pair must have hashed to a frequent bucket according to both hash tables.
224 CHAPTER 6. FREQUENT ITEMSETS
Just as Multistage is not limited to two hash tables, we can divide the available main memory into as many hash tables as we like on the first pass of Multihash. The risk is that should we use too many hash tables, the average count for a bucket will exceed the support threshold. At that point, there may be very few infrequent buckets in any of the hash tables. Even though a pair must hash to a frequent bucket in every hash table to be counted, we may find that the probability an infrequent pair will be a candidate rises, rather than falls, if we add another hash table.
6.3.4 Exercises for Section 6.3
Exercise 6.3.1 : Here is a collection of twelve baskets. Each contains three of the six items 1 through 6.
{1,2,3} {2,3,4} {3,4,5} {4,5,6} {1,3,5} {2,4,6} {1,3,4} {2,4,5} {3,5,6} {1,2,4} {2,3,5} {3,4,6}
Suppose the support threshold is 4. On the first pass of the PCY Algorithm we use a hash table with 11 buckets, and the set {i, j} is hashed to bucket i × j mod 11.
(a) By any method, compute the support for each item and each pair of items. (b) Which pairs hash to which buckets?
(c) Which buckets are frequent?
(d) Which pairs are counted on the second pass of the PCY Algorithm?
Exercise 6.3.2: Suppose we run the Multistage Algorithm on the data of Exercise 6.3.1, with the same support threshold of 4. The first pass is the same as in that exercise, and for the second pass, we hash pairs to nine buckets, using the hash function that hashes {i, j} to bucket i + j mod 9. Determine the counts of the buckets on the second pass. Does the second pass reduce the set of candidate pairs? Note that all items are frequent, so the only reason a pair would not be hashed on the second pass is if it hashed to an infrequent bucket on the first pass.
Exercise 6.3.3: Suppose we run the Multihash Algorithm on the data of Exercise 6.3.1. We shall use two hash tables with five buckets each. For one, the set {i,j}, is hashed to bucket 2i+3j+4 mod 5, and for the other, the set is hashed to i + 4j mod 5. Since these hash functions are not symmetric in i and j, order the items so that i < j when evaluating each hash function. Determine the counts of each of the 10 buckets. How large does the support threshold have to be for the Multistage Algorithm to eliminate more pairs than the PCY Algorithm would, using the hash table and function described in Exercise 6.3.1?
6.3. HANDLING LARGER DATASETS IN MAIN MEMORY 225 ! Exercise 6.3.4: Suppose we perform the PCY Algorithm to find frequent
pairs, with market-basket data meeting the following specifications:
1. The support threshold is 10,000.
2. There are one million items, represented by the integers 0, 1, . . . , 999999.
3. There are 250,000 frequent items, that is, items that occur 10,000 times or more.
4. There are one million pairs that occur 10,000 times or more.
5. There are P pairs that occur exactly once and consist of two frequent items.
6. No other pairs occur at all.
7. Integers are always represented by 4 bytes.
8. When we hash pairs, they distribute among buckets randomly, but as evenly as possible; i.e., you may assume that each bucket gets exactly its fair share of the P pairs that occur once.
Suppose there are S bytes of main memory. In order to run the PCY Algorithm successfully, the number of buckets must be sufficiently large that most buckets are not frequent. In addition, on the second pass, there must be enough room to count all the candidate pairs. As a function of S, what is the largest value of P for which we can successfully run the PCY Algorithm on this data?
! Exercise 6.3.5 : Under the assumptions given in Exercise 6.3.4, will the Mul- tihash Algorithm reduce the main-memory requirements for the second pass? As a function of S and P, what is the optimum number of hash tables to use on the first pass?
! Exercise 6.3.6 : Suppose we perform the 3-pass Multistage Algorithm to find frequent pairs, with market-basket data meeting the following specifications:
1. The support threshold is 10,000.
2. There are one million items, represented by the integers 0, 1, . . . , 999999. All items are frequent; that is, they occur at least 10,000 times.
3. There are one million pairs that occur 10,000 times or more.
4. There are P pairs that occur exactly once.
5. No other pairs occur at all.
6. Integers are always represented by 4 bytes.
226 7.
8.
CHAPTER 6. FREQUENT ITEMSETS
When we hash pairs, they distribute among buckets randomly, but as evenly as possible; i.e., you may assume that each bucket gets exactly its fair share of the P pairs that occur once.
The hash functions used on the first two passes are completely indepen- dent.
Suppose there are S bytes of main memory. As a function of S and P, what is the expected number of candidate pairs on the third pass of the Multistage Algorithm?
6.4 Limited-Pass Algorithms
The algorithms for frequent itemsets discussed so far use one pass for each size of itemset we investigate. If main memory is too small to hold the data and the space needed to count frequent itemsets of one size, there does not seem to be any way to avoid k passes to compute the exact collection of frequent itemsets. However, there are many applications where it is not essential to discover every frequent itemset. For instance, if we are looking for items purchased together at a supermarket, we are not going to run a sale based on every frequent itemset we find, so it is quite sufficient to find most but not all of the frequent itemsets.
In this section we explore some algorithms that have been proposed to find all or most frequent itemsets using at most two passes. We begin with the obvious approach of using a sample of the data rather than the entire dataset. An algorithm called SON uses two passes, gets the exact answer, and lends itself to implementation by MapReduce or another parallel computing regime. Finally, Toivonen’s Algorithm uses two passes on average, gets an exact answer, but may, rarely, not terminate in any given amount of time.
6.4.1 The Simple, Randomized Algorithm
Instead of using the entire file of baskets, we could pick a random subset of the baskets and pretend it is the entire dataset. We must adjust the support threshold to reflect the smaller number of baskets. For instance, if the support threshold for the full dataset is s, and we choose a sample of 1% of the baskets, then we should examine the sample for itemsets that appear in at least s/100 of the baskets.
The safest way to pick the sample is to read the entire dataset, and for each basket, select that basket for the sample with some fixed probability p. Suppose there are m baskets in the entire file. At the end, we shall have a sample whose size is very close to pm baskets. However, if we have reason to believe that the baskets appear in random order in the file already, then we do not even have to read the entire file. We can select the first pm baskets for our sample. Or, if the file is part of a distributed file system, we can pick some chunks at random to serve as the sample.
6.4. LIMITED-PASS ALGORITHMS 227
   Why Not Just Pick the First Part of the File?
The risk in selecting a sample from one portion of a large file is that the data is not uniformly distributed in the file. For example, suppose the file were a list of true market-basket contents at a department store, organized by date of sale. If you took only the first baskets in the file, you would have old data. For example, there would be no iPods in these baskets, even though iPods might have become a popular item later.
As another example, consider a file of medical tests performed at different hospitals. If each chunk comes from a different hospital, then picking chunks at random will give us a sample drawn from only a small subset of the hospitals. If hospitals perform different tests or perform them in different ways, the data may be highly biased.
 Having selected our sample of the baskets, we use part of main memory to store these baskets. The balance of the main memory is used to execute one of the algorithms we have discussed, such as A-Priori, PCY, Multistage, or Multihash. However, the algorithm must run passes over the main-memory sample for each itemset size, until we find a size with no frequent items. There are no disk accesses needed to read the sample, since it resides in main memory. As frequent itemsets of each size are discovered, they can be written out to disk; this operation and the initial reading of the sample from disk are the only disk I/O’s the algorithm does.
Of course the algorithm will fail if whichever method from Section 6.2 or 6.3 we choose cannot be run in the amount of main memory left after storing the sample. If we need more main memory, then an option is to read the sample from disk for each pass. Since the sample is much smaller than the full dataset, we still avoid most of the disk I/O’s that the algorithms discussed previously would use.
6.4.2 Avoiding Errors in Sampling Algorithms
We should be mindful of the problem with the simple algorithm of Section 6.4.1: it cannot be relied upon either to produce all the itemsets that are frequent in the whole dataset, nor will it produce only itemsets that are frequent in the whole. An itemset that is frequent in the whole but not in the sample is a false negative, while an itemset that is frequent in the sample but not the whole is a false positive.
If the sample is large enough, there are unlikely to be serious errors. That is, an itemset whose support is much larger than the threshold will almost surely be identified from a random sample, and an itemset whose support is much less than the threshold is unlikely to appear frequent in the sample. However, an itemset whose support in the whole is very close to the threshold is as likely to
228 CHAPTER 6. FREQUENT ITEMSETS
be frequent in the sample as not.
We can eliminate false positives by making a pass through the full dataset
and counting all the itemsets that were identified as frequent in the sample. Retain as frequent only those itemsets that were frequent in the sample and also frequent in the whole. Note that this improvement will eliminate all false positives, but a false negative is not counted and therefore remains undiscovered.
To accomplish this task in a single pass, we need to be able to count all frequent itemsets of all sizes at once, within main memory. If we were able to run the simple algorithm successfully with the main memory available, then there is a good chance we shall be able to count all the frequent itemsets at once, because:
(a) The frequent singletons and pairs are likely to dominate the collection of all frequent itemsets, and we had to count them all in one pass already.
(b) We now have all of main memory available, since we do not have to store the sample in main memory.
We cannot eliminate false negatives completely, but we can reduce their number if the amount of main memory allows it. We have assumed that if s is the support threshold, and the sample is fraction p of the entire dataset, then we use ps as the support threshold for the sample. However, we can use something smaller than that as the threshold for the sample, such a 0.9ps. Having a lower threshold means that more itemsets of each size will have to be counted, so the main-memory requirement rises. On the other hand, if there is enough main memory, then we shall identify as having support at least 0.9ps in the sample almost all those itemsets that have support at least s is the whole. If we then make a complete pass to eliminate those itemsets that were identified as frequent in the sample but are not frequent in the whole, we have no false positives and hopefully have none or very few false negatives.
6.4.3 The Algorithm of Savasere, Omiecinski, and Navathe
Our next improvement avoids both false negatives and false positives, at the cost of making two full passes. It is called the SON Algorithm after the authors. The idea is to divide the input file into chunks (which may be “chunks” in the sense of a distributed file system, or simply a piece of the file). Treat each chunk as a sample, and run the algorithm of Section 6.4.1 on that chunk. We use ps as the threshold, if each chunk is fraction p of the whole file, and s is the support threshold. Store on disk all the frequent itemsets found for each chunk.
Once all the chunks have been processed in that way, take the union of all the itemsets that have been found frequent for one or more chunks. These are the candidate itemsets. Notice that if an itemset is not frequent in any chunk, then its support is less than ps in each chunk. Since the number of
6.4. LIMITED-PASS ALGORITHMS 229
chunks is 1/p, we conclude that the total support for that itemset is less than (1/p)ps = s. Thus, every itemset that is frequent in the whole is frequent in at least one chunk, and we can be sure that all the truly frequent itemsets are among the candidates; i.e., there are no false negatives.
We have made a total of one pass through the data as we read each chunk and processed it. In a second pass, we count all the candidate itemsets and select those that have support at least s as the frequent itemsets.
6.4.4 The SON Algorithm and MapReduce
The SON algorithm lends itself well to a parallel-computing environment. Each of the chunks can be processed in parallel, and the frequent itemsets from each chunk combined to form the candidates. We can distribute the candidates to many processors, have each processor count the support for each candidate in a subset of the baskets, and finally sum those supports to get the support for each candidate itemset in the whole dataset. This process does not have to be implemented in MapReduce, but there is a natural way of expressing each of the two passes as a MapReduce operation. We shall summarize this MapReduce-MapReduce sequence below.
First Map Function: Take the assigned subset of the baskets and find the itemsets frequent in the subset using the algorithm of Section 6.4.1. As de- scribed there, lower the support threshold from s to ps if each Map task gets fraction p of the total input file. The output is a set of key-value pairs (F, 1), where F is a frequent itemset from the sample. The value is always 1 and is irrelevant.
First Reduce Function: Each Reduce task is assigned a set of keys, which are itemsets. The value is ignored, and the Reduce task simply produces those keys (itemsets) that appear one or more times. Thus, the output of the first Reduce function is the candidate itemsets.
Second Map Function: The Map tasks for the second Map function take all the output from the first Reduce Function (the candidate itemsets) and a portion of the input data file. Each Map task counts the number of occurrences of each of the candidate itemsets among the baskets in the portion of the dataset that it was assigned. The output is a set of key-value pairs (C, v), where C is one of the candidate sets and v is the support for that itemset among the baskets that were input to this Map task.
Second Reduce Function: The Reduce tasks take the itemsets they are given as keys and sum the associated values. The result is the total support for each of the itemsets that the Reduce task was assigned to handle. Those itemsets whose sum of values is at least s are frequent in the whole dataset, so
230 CHAPTER 6. FREQUENT ITEMSETS
the Reduce task outputs these itemsets with their counts. Itemsets that do not have total support at least s are not transmitted to the output of the Reduce task.2
6.4.5 Toivonen’s Algorithm
This algorithm uses randomness in a different way from the simple sampling algorithm of Section 6.4.1. Toivonen’s Algorithm, given sufficient main mem- ory, will use one pass over a small sample and one full pass over the data. It will give neither false negatives nor positives, but there is a small yet nonzero probability that it will fail to produce any answer at all. In that case it needs to be repeated until it gives an answer. However, the average number of passes needed before it produces all and only the frequent itemsets is a small constant.
Toivonen’s algorithm begins by selecting a small sample of the input dataset, and finding from it the candidate frequent itemsets. The process is exactly that of Section 6.4.1, except that it is essential the threshold be set to something less than its proportional value. That is, if the support threshold for the whole dataset is s, and the sample size is fraction p, then when looking for frequent itemsets in the sample, use a threshold such as 0.9ps or 0.8ps. The smaller we make the threshold, the more main memory we need for computing all itemsets that are frequent in the sample, but the more likely we are to avoid the situation where the algorithm fails to provide an answer.
Having constructed the collection of frequent itemsets for the sample, we next construct the negative border. This is the collection of itemsets that are not frequent in the sample, but all of their immediate subsets (subsets constructed by deleting exactly one item) are frequent in the sample.
Example 6.11 : Suppose the items are {A, B, C, D, E} and we have found the following itemsets to be frequent in the sample: {A}, {B}, {C}, {D}, {B,C}, {C,D}. Note that ∅ is also frequent, as long as there are at least as many baskets as the support threshold, although technically the algorithms we have described omit this obvious fact. First, {E} is in the negative border, because it is not frequent in the sample, but its only immediate subset, ∅, is frequent.
The sets {A,B}, {A,C}, {A,D} and {B,D} are in the negative border. None of these sets are frequent, and each has two immediate subsets, both of which are frequent. For instance, {A, B} has immediate subsets {A} and {B}. Of the other six doubletons, none are in the negative border. The sets {B,C} and {C,D} are not in the negative border, because they are frequent. The remaining four pairs are each E together with another item, and those are not in the negative border because they have an immediate subset {E} that is not frequent.
None of the triples or larger sets are in the negative border. For instance, {B,C,D} is not in the negative border because it has an immediate subset
2Strictly speaking, the Reduce function has to produce a value for each key. It can produce 1 as the value for itemsets found frequent and 0 for those not frequent.
 
6.4. LIMITED-PASS ALGORITHMS 231 {B,D} that is not frequent. Thus, the negative border consists of five sets:
{E}, {A, B}, {A, C}, {A, D} and {B, D}. ✷
To complete Toivonen’s algorithm, we make a pass through the entire data- set, counting all the itemsets that are frequent in the sample or are in the negative border. There are two possible outcomes.
1. No member of the negative border is frequent in the whole dataset. In this case, the correct set of frequent itemsets is exactly those itemsets from the sample that were found to be frequent in the whole.
2. Some member of the negative border is frequent in the whole. Then we cannot be sure that there are not some even larger sets, in neither the negative border nor the collection of frequent itemsets for the sample, that are also frequent in the whole. Thus, we can give no answer at this time and must repeat the algorithm with a new random sample.
6.4.6 Why Toivonen’s Algorithm Works
Clearly Toivonen’s algorithm never produces a false positive, since it only re- ports as frequent those itemsets that have been counted and found to be frequent in the whole. To argue that it never produces a false negative, we must show that when no member of the negative border is frequent in the whole, then there can be no itemset whatsoever that is:
1. Frequent in the whole, but
2. In neither the negative border nor the collection of frequent itemsets for the sample.
Suppose the contrary. That is, there is a set S that is frequent in the whole, but not in the negative border and not frequent in the sample. Also, this round of Toivonen’s Algorithm produced an answer, which would certainly not include S among the frequent itemsets. By monotonicity, all subsets of S are also frequent in the whole. Let T be a subset of S that is of the smallest possible size among all subsets of S that are not frequent in the sample.
We claim that T must be in the negative border. Surely T meets one of the conditions for being in the negative border: it is not frequent in the sample. It also meets the other condition for being in the negative border: each of its immediate subsets is frequent in the sample. For if some immediate subset of T were not frequent in the sample, then there would be a subset of S that is smaller than T and not frequent in the sample, contradicting our selection of T as a subset of S that was not frequent in the sample, yet as small as any such set.
Now we see that T is both in the negative border and frequent in the whole dataset. Consequently, this round of Toivonen’s algorithm did not produce an answer.
232 CHAPTER 6. FREQUENT ITEMSETS 6.4.7 Exercises for Section 6.4
Exercise 6.4.1 : Suppose there are eight items, A, B, . . . , H, and the following are the maximal frequent itemsets: {A, B}, {B, C}, {A, C}, {A, D}, {E}, and {F}. Find the negative border.
Exercise 6.4.2: Apply Toivonen’s Algorithm to the data of Exercise 6.3.1, with a support threshold of 4. Take as the sample the first row of baskets: {1, 2, 3}, {2, 3, 4}, {3, 4, 5}, and {4, 5, 6}, i.e., one-third of the file. Our scaled- down support theshold will be 1.
(a) What are the itemsets frequent in the sample?
(b) What is the negative border?
(c) What is the outcome of the pass through the full dataset? Are any of the itemsets in the negative border frequent in the whole?
!! Exercise 6.4.3 : Suppose item i appears exactly s times in a file of n baskets, where s is the support threshold. If we take a sample of n/100 baskets, and lower the support threshold for the sample to s/100, what is the probability that i will be found to be frequent? You may assume that both s and n are divisible by 100.
6.5 Counting Frequent Items in a Stream
Suppose that instead of a file of baskets we have a stream of baskets, from which we want to mine the frequent itemsets. Recall from Chapter 4 that the difference between a stream and a data file is that stream elements are only available when they arrive, and typically the arrival rate is so great that we cannot store the entire stream in a way that allows easy querying. Further, it is common that streams evolve over time, so the itemsets that are frequent in today’s stream may not be frequent tomorrow.
A clear distinction between streams and files, when frequent itemsets are considered, is that there is no end to a stream, so eventually an itemset is going to exceed the support threshold, as long as it appears repeatedly in the stream. As a result, for streams, we must think of the support threshold s as a fraction of the baskets in which an itemset must appear in order to be considered frequent. Even with this adjustment, we still have several options regarding the portion of the stream over which that fraction is measured.
In this section, we shall discuss several ways that we might extract frequent itemsets from a stream. First, we consider ways to use the sampling tech- niques of the previous section. Then, we consider the decaying-window model from Section 4.7, and extend the method described in Section 4.7.3 for finding “popular” items.
6.5. COUNTING FREQUENT ITEMS IN A STREAM 233 6.5.1 Sampling Methods for Streams
In what follows, we shall assume that stream elements are baskets of items. Perhaps the simplest approach to maintaining a current estimate of the frequent itemsets in a stream is to collect some number of baskets and store it as a file. Run one of the frequent-itemset algorithms discussed in this chapter, meanwhile ignoring the stream elements that arrive, or storing them as another file to be analyzed later. When the frequent-itemsets algorithm finishes, we have an estimate of the frequent itemsets in the stream. We then have several options.
1. We can use this collection of frequent itemsets for whatever application is at hand, but start running another iteration of the chosen frequent-itemset algorithm immediately. This algorithm can either:
(a) Use the file that was collected while the first iteration of the algo- rithm was running. At the same time, collect yet another file to be used at another iteration of the algorithm, when this current itera- tion finishes.
(b) Start collecting another file of baskets now, and run the algorithm when an adequate number of baskets has been collected.
2. We can continue to count the numbers of occurrences of each of these frequent itemsets, along with the total number of baskets seen in the stream, since the counting started. If any itemset is discovered to occur in a fraction of the baskets that is significantly below the threshold fraction s, then this set can be dropped from the collection of frequent itemsets. When computing the fraction, it is important to include the occurrences from the original file of baskets, from which the frequent itemsets were derived. If not, we run the risk that we shall encounter a short period in which a truly frequent itemset does not appear sufficiently frequently and throw it out. We should also allow some way for new frequent itemsets to be added to the current collection. Possibilities include:
(a) Periodically gather a new segment of the baskets in the stream and use it as the data file for another iteration of the chosen frequent- itemsets algorithm. The new collection of frequent items is formed from the result of this iteration and the frequent itemsets from the previous collection that have survived the possibility of having been deleted for becoming infrequent.
(b) Add some random itemsets to the current collection, and count their fraction of occurrences for a while, until one has a good idea of whether or not they are currently frequent. Rather than choosing new itemsets completely at random, one might focus on sets with items that appear in many itemsets already known to be frequent. For example, a good choice is to pick new itemsets from the negative border (Section 6.4.5) of the current set of frequent itemsets.
234 CHAPTER 6. FREQUENT ITEMSETS 6.5.2 Frequent Itemsets in Decaying Windows
Recall from Section 4.7 that a decaying window on a stream is formed by picking a small constant c and giving the ith element prior to the most recent element the weight (1 − c)i, or approximately e−ci. Section 4.7.3 actually presented a method for computing the frequent items, provided the support threshold is defined in a somewhat different way. That is, we considered, for each item, a stream that had 1 if the item appeared at a certain stream element and 0 if not. We defined the “score” for that item to be the sum of the weights where its stream element was 1. We were constrained to record all items whose score was at least 1/2. We can not use a score threshold above 1, because we do not initiate a count for an item until the item appears in the stream, and the first time it appears, its score is only 1 (since 1, or (1 − c)0, is the weight of the current item).
If we wish to adapt this method to streams of baskets, there are two modifi- cations we must make. The first is simple. Stream elements are baskets rather than individual items, so many items may appear at a given stream element. Treat each of those items as if they were the “current” item and add 1 to their score after multiplying all current scores by 1 − c, as described in Section 4.7.3. If some items in a basket have no current score, initialize the scores of those items to 1.
The second modification is trickier. We want to find all frequent itemsets, not just singleton itemsets. If we were to initialize a count for an itemset whenever we saw it, we would have too many counts. For example, one basket of 20 items has over a million subsets, and all of these would have to be initiated for one basket. On the other hand, as we mentioned, if we use a requirement above 1 for initiating the scoring of an itemset, then we would never get any itemsets started, and the method would not work.
A way of dealing with this problem is to start scoring certain itemsets as soon as we see one instance, but be conservative about which itemsets we start. We may borrow from the A-Priori trick, and only start an itemset I if all its immediate proper subsets are already being scored. The consequence of this restriction is that if I is truly frequent, eventually we shall begin to count it, but we never start an itemset unless it would at least be a candidate in the sense used in the A-Priori Algorithm.
Example 6.12: Suppose I is a large itemset, but it appears in the stream periodically, once every 1/2c baskets. Then its score, and that of its subsets, never falls below e−1/2, which is greater than 1/2. Thus, once a score is created for some subset of I, that subset will continue to be scored forever. The first time I appears, only its singleton subsets will have scores created for them. However, the next time I appears, each of its doubleton subsets will commence scoring, since each of the immediate subsets of those doubletons is already being scored. Likewise, the kth time I appears, its subsets of size k − 1 are all being scored, so we initiate scores for each of its subsets of size k. Eventually, we reach the size |I|, at which time we start scoring I itself. ✷
6.5. COUNTING FREQUENT ITEMS IN A STREAM 235 6.5.3 Hybrid Methods
The approach of Section 6.5.2 offers certain advantages. It requires a limited amount of work each time a stream element arrives, and it always provides an up-to-date picture of what is frequent in the decaying window. Its big disadvantage is that it requires us to maintain scores for each itemset with a score of at least 1/2. We can limit the number of itemsets being scored by increasing the value of the parameter c. But the larger c is, the smaller the decaying window is. Thus, we could be forced to accept information that tracks the local fluctuations in frequency too closely, rather than integrating over a long period.
We can combine the ideas from Sections 6.5.1 and 6.5.2. For example, we could run a standard algorithm for frequent itemsets on a sample of the stream, with a conventional threshold for support. The itemsets that are found frequent by this algorithm will be treated as if they all arrived at the current time. That is, they each get a score equal to a fixed fraction of their count.
More precisely, suppose the initial sample has b baskets, c is the decay constant for the decaying window, and the minimum score we wish to accept for a frequent itemset in the decaying window is s. Then the support threshold for the initial run of the frequent-itemset algorithm is bcs. If an itemset I is found to have support t in the sample, then it is initially given a score of t/(bc).
Example 6.13 : Suppose c = 10−6 and the minimum score we wish to accept in the decaying window is 10. Suppose also we take a sample of 108 baskets from the stream. Then when analyzing that sample, we use a support threshold of 108 × 10−6 × 10 = 1000.
Consider an itemset I that has support 2000 in the sample. Then the initial score we use for I is 2000/(108 × 10−6) = 20. After this initiation step, each time a basket arrives in the stream, the current score will be multiplied by 1 − c = 0.999999. If I is a subset of the current basket, then add 1 to the score. If the score for I goes below 10, then it is considered to be no longer frequent, so it is dropped from the collection of frequent itemsets. ✷
We do not, sadly, have a reasonable way of initiating the scoring of new itemsets. If we have no score for itemset I, and 10 is the minimum score we want to maintain, there is no way that a single basket can jump its score from 0 to anything more than 1. The best strategy for adding new sets is to run a new frequent-itemsets calculation on a sample from the stream, and add to the collection of itemsets being scored any that meet the threshold for that sample but were not previously being scored.
6.5.4 Exercises for Section 6.5
!! Exercise 6.5.1 : Suppose we are counting frequent itemsets in a decaying win- dow with a decay constant c. Suppose also that with probability p, a given
236 CHAPTER 6. FREQUENT ITEMSETS
stream element (basket) contains both items i and j. Additionally, with prob- ability p the basket contains i but not j, and with probability p it contains j but not i. As a function of c and p, what is the fraction of time we shall be scoring the pair {i, j}?
6.6 Summary of Chapter 6
✦ Market-Basket Data: This model of data assumes there are two kinds of entities: items and baskets. There is a many–many relationship between items and baskets. Typically, baskets are related to small sets of items, while items may be related to many baskets.
✦ Frequent Itemsets: The support for a set of items is the number of baskets containing all those items. Itemsets with support that is at least some threshold are called frequent itemsets.
✦ Association Rules: These are implications that if a basket contains a certain set of items I, then it is likely to contain another particular item j as well. The probability that j is also in a basket containing I is called the confidence of the rule. The interest of the rule is the amount by which the confidence deviates from the fraction of all baskets that contain j.
✦ The Pair-Counting Bottleneck: To find frequent itemsets, we need to examine all baskets and count the number of occurrences of sets of a certain size. For typical data, with a goal of producing a small number of itemsets that are the most frequent of all, the part that often takes the most main memory is the counting of pairs of items. Thus, methods for finding frequent itemsets typically concentrate on how to minimize the main memory needed to count pairs.
✦ Triangular Matrices: While one could use a two-dimensional array to count pairs, doing so wastes half the space, because there is no need to count pair {i, j} in both the i-j and j-i array elements. By arranging the pairs (i,j) for which i < j in lexicographic order, we can store only the needed counts in a one-dimensional array with no wasted space, and yet be able to access the count for any pair efficiently.
✦ Storage of Pair Counts as Triples: If fewer than 1/3 of the possible pairs actually occur in baskets, then it is more space-efficient to store counts of pairs as triples (i, j, c), where c is the count of the pair {i, j}, and i < j. An index structure such as a hash table allows us to find the triple for (i, j) efficiently.
✦ Monotonicity of Frequent Itemsets: An important property of itemsets is that if a set of items is frequent, then so are all its subsets. We exploit this property to eliminate the need to count certain itemsets by using its contrapositive: if an itemset is not frequent, then neither are its supersets.
6.6. SUMMARY OF CHAPTER 6 237
✦ The A-Priori Algorithm for Pairs: We can find all frequent pairs by making two passes over the baskets. On the first pass, we count the items themselves, and then determine which items are frequent. On the second pass, we count only the pairs of items both of which are found frequent on the first pass. Monotonicity justifies our ignoring other pairs.
✦ Finding Larger Frequent Itemsets: A-Priori and many other algorithms allow us to find frequent itemsets larger than pairs, if we make one pass over the baskets for each size itemset, up to some limit. To find the frequent itemsets of size k, monotonicity lets us restrict our attention to only those itemsets such that all their subsets of size k − 1 have already been found frequent.
✦ The PCY Algorithm: This algorithm improves on A-Priori by creating a hash table on the first pass, using all main-memory space that is not needed to count the items. Pairs of items are hashed, and the hash-table buckets are used as integer counts of the number of times a pair has hashed to that bucket. Then, on the second pass, we only have to count pairs of frequent items that hashed to a frequent bucket (one whose count is at least the support threshold) on the first pass.
✦ The Multistage Algorithm: We can insert additional passes between the first and second pass of the PCY Algorithm to hash pairs to other, in- dependent hash tables. At each intermediate pass, we only have to hash pairs of frequent items that have hashed to frequent buckets on all previ- ous passes.
✦ The Multihash Algorithm: We can modify the first pass of the PCY Algo- rithm to divide available main memory into several hash tables. On the second pass, we only have to count a pair of frequent items if they hashed to frequent buckets in all hash tables.
✦ Randomized Algorithms: Instead of making passes through all the data, we may choose a random sample of the baskets, small enough that it is possible to store both the sample and the needed counts of itemsets in main memory. The support threshold must be scaled down in proportion. We can then find the frequent itemsets for the sample, and hope that it is a good representation of the data as whole. While this method uses at most one pass through the whole dataset, it is subject to false positives (itemsets that are frequent in the sample but not the whole) and false negatives (itemsets that are frequent in the whole but not the sample).
✦ The SON Algorithm: An improvement on the simple randomized algo- rithm is to divide the entire file of baskets into segments small enough that all frequent itemsets for the segment can be found in main memory. Candidate itemsets are those found frequent for at least one segment. A
238
✦
✦
6.7
CHAPTER 6. FREQUENT ITEMSETS
second pass allows us to count all the candidates and find the exact col- lection of frequent itemsets. This algorithm is especially appropriate in a MapReduce setting.
Toivonen’s Algorithm: This algorithm starts by finding frequent itemsets in a sample, but with the threshold lowered so there is little chance of missing an itemset that is frequent in the whole. Next, we examine the entire file of baskets, counting not only the itemsets that are frequent in the sample, but also, the negative border – itemsets that have not been found frequent, but all their immediate subsets are. If no member of the negative border is found frequent in the whole, then the answer is exact. But if a member of the negative border is found frequent, then the whole process has to repeat with another sample.
Frequent Itemsets in Streams: If we use a decaying window with constant c, then we can start counting an item whenever we see it in a basket. We start counting an itemset if we see it contained within the current basket, and all its immediate proper subsets already are being counted. As the window is decaying, we multiply all counts by 1 − c and eliminate those that are less than 1/2.
References for Chapter 6
The market-basket data model, including association rules and the A-Priori Algorithm, are from [1] and [2].
The PCY Algorithm is from [4]. The Multistage and Multihash Algorithms are found in [3].
The SON Algorithm is from [5]. Toivonen’s Algorithm appears in [6].
1. R. Agrawal, T. Imielinski, and A. Swami, “Mining associations between sets of items in massive databases,” Proc. ACM SIGMOD Intl. Conf. on Management of Data, pp. 207–216, 1993.
2. R. Agrawal and R. Srikant, “Fast algorithms for mining association rules,” Intl. Conf. on Very Large Databases, pp. 487–499, 1994.
3. M. Fang, N. Shivakumar, H. Garcia-Molina, R. Motwani, and J.D. Ull- man, “Computing iceberg queries efficiently,” Intl. Conf. on Very Large Databases, pp. 299-310, 1998.
4. J.S. Park, M.-S. Chen, and P.S. Yu, “An effective hash-based algorithm for mining association rules,” Proc. ACM SIGMOD Intl. Conf. on Man- agement of Data, pp. 175–186, 1995.
5. A. Savasere, E. Omiecinski, and S.B. Navathe, “An efficient algorithm for mining association rules in large databases,” Intl. Conf. on Very Large Databases, pp. 432–444, 1995.
6.7. REFERENCES FOR CHAPTER 6 239
6. H. Toivonen, “Sampling large databases for association rules,” Intl. Conf. on Very Large Databases, pp. 134–145, 1996.
240 CHAPTER 6. FREQUENT ITEMSETS
Chapter 7
Clustering
Clustering is the process of examining a collection of “points,” and grouping the points into “clusters” according to some distance measure. The goal is that points in the same cluster have a small distance from one another, while points in different clusters are at a large distance from one another. A suggestion of what clusters might look like was seen in Fig. 1.1. However, there the intent was that there were three clusters around three different road intersections, but two of the clusters blended into one another because they were not sufficiently separated.
Our goal in this chapter is to offer methods for discovering clusters in data. We are particularly interested in situations where the data is very large, and/or where the space either is high-dimensional, or the space is not Euclidean at all. We shall therefore discuss several algorithms that assume the data does not fit in main memory. However, we begin with the basics: the two general approaches to clustering and the methods for dealing with clusters in a non- Euclidean space.
7.1 Introduction to Clustering Techniques
We begin by reviewing the notions of distance measures and spaces. The two major approaches to clustering – hierarchical and point-assignment – are de- fined. We then turn to a discussion of the “curse of dimensionality,” which makes clustering in high-dimensional spaces difficult, but also, as we shall see, enables some simplifications if used correctly in a clustering algorithm.
7.1.1 Points, Spaces, and Distances
A dataset suitable for clustering is a collection of points, which are objects belonging to some space. In its most general sense, a space is just a universal set of points, from which the points in the dataset are drawn. However, we should be mindful of the common case of a Euclidean space (see Section 3.5.2),
241
242 CHAPTER 7. CLUSTERING
which has a number of important properties useful for clustering. In particular, a Euclidean space’s points are vectors of real numbers. The length of the vector is the number of dimensions of the space. The components of the vector are commonly called coordinates of the represented points.
All spaces for which we can perform a clustering have a distance measure, giving a distance between any two points in the space. We introduced distances in Section 3.5. The common Euclidean distance (square root of the sums of the squares of the differences between the coordinates of the points in each dimen- sion) serves for all Euclidean spaces, although we also mentioned some other options for distance measures in Euclidean spaces, including the Manhattan distance (sum of the magnitudes of the differences in each dimension) and the L∞-distance (maximum magnitude of the difference in any dimension).
 Beagles
           Height
Chihuahuas
  Dachshunds
              Weight
  Figure 7.1: Heights and weights of dogs taken from three varieties
Example 7.1 : Classical applications of clustering often involve low-dimen- sional Euclidean spaces. For example, Fig. 7.1 shows height and weight mea- surements of dogs of several varieties. Without knowing which dog is of which variety, we can see just by looking at the diagram that the dogs fall into three clusters, and those clusters happen to correspond to three varieties. With small amounts of data, any clustering algorithm will establish the correct clusters, and simply plotting the points and “eyeballing” the plot will suffice as well. ✷
However, modern clustering problems are not so simple. They may involve Euclidean spaces of very high dimension or spaces that are not Euclidean at all. For example, it is challenging to cluster documents by their topic, based on the occurrence of common, unusual words in the documents. It is challenging to cluster moviegoers by the type or types of movies they like.
We also considered in Section 3.5 distance measures for non-Euclidean spa- ces. These include the Jaccard distance, cosine distance, Hamming distance,
7.1. INTRODUCTION TO CLUSTERING TECHNIQUES 243 and edit distance. Recall that the requirements for a function on pairs of points
to be a distance measure are that
1. Distances are always nonnegative, and only the distance between a point and itself is 0.
2. Distance is symmetric; it doesn’t matter in which order you consider the points when computing their distance.
3. Distance measures obey the triangle inequality; the distance from x to y to z is never less than the distance going from x to z directly.
7.1.2 Clustering Strategies
We can divide (cluster!) clustering algorithms into two groups that follow two fundamentally different strategies.
1. Hierarchical or agglomerative algorithms start with each point in its own cluster. Clusters are combined based on their “closeness,” using one of many possible definitions of “close.” Combination stops when further combination leads to clusters that are undesirable for one of several rea- sons. For example, we may stop when we have a predetermined number of clusters, or we may use a measure of compactness for clusters, and refuse to construct a cluster by combining two smaller clusters if the resulting cluster has points that are spread out over too large a region.
2. The other class of algorithms involve point assignment. Points are con- sidered in some order, and each one is assigned to the cluster into which it best fits. This process is normally preceded by a short phase in which initial clusters are estimated. Variations allow occasional combining or splitting of clusters, or may allow points to be unassigned if they are outliers (points too far from any of the current clusters).
Algorithms for clustering can also be distinguished by:
(a) Whether the algorithm assumes a Euclidean space, or whether the al- gorithm works for an arbitrary distance measure. We shall see that a key distinction is that in a Euclidean space it is possible to summarize a collection of points by their centroid – the average of the points. In a non-Euclidean space, there is no notion of a centroid, and we are forced to develop another way to summarize clusters.
(b) Whether the algorithm assumes that the data is small enough to fit in main memory, or whether data must reside in secondary memory, pri- marily. Algorithms for large amounts of data often must take shortcuts, since it is infeasible to look at all pairs of points, for example. It is also necessary to summarize clusters in main memory, since we cannot hold all the points of all the clusters in main memory at the same time.
244 CHAPTER 7. CLUSTERING 7.1.3 The Curse of Dimensionality
High-dimensional Euclidean spaces have a number of unintuitive properties that are sometimes referred to as the “curse of dimensionality.” Non-Euclidean spaces usually share these anomalies as well. One manifestation of the “curse” is that in high dimensions, almost all pairs of points are equally far away from one another. Another manifestation is that almost any two vectors are almost orthogonal. We shall explore each of these in turn.
The Distribution of Distances in a High-Dimensional Space
Let us consider a d-dimensional Euclidean space. Suppose we choose n random points in the unit cube, i.e., points [x1, x2, . . . , xd], where each xi is in the range 0 to 1. If d = 1, we are placing random points on a line of length 1. We expect that some pairs of points will be very close, e.g., consecutive points on the line. We also expect that some points will be very far away – those at or near opposite ends of the line. The average distance between a pair of points is 1/3.1
Suppose that d is very large. The Euclidean distance between two random points [x1,x2,...,xd] and [y1,y2,...,yd] is
  d
  (xi −yi)2
i=1
Here, each xi and yi is a random variable chosen uniformly in the range 0 to 1. Since d is large, we can expect that for some i, |xi − yi| will be close to 1. That puts a lower bound of 1 on the distance between almost any two random points. In fact, a more careful argument can put a stronger lower bound on the distance between all but a vanishingly small fraction of the pairs of points. However, the maximum distance between two points is √d, and one can argue that all but a vanishingly small fraction of the pairs do not have a distance close to this upper limit. In fact, almost all points will have a distance close to the average distance.
If there are essentially no pairs of points that are close, it is hard to build clusters at all. There is little justification for grouping one pair of points and not another. Of course, the data may not be random, and there may be useful clusters, even in very high-dimensional spaces. However, the argument about random data suggests that it will be hard to find these clusters among so many pairs that are all at approximately the same distance.
Angles Between Vectors
Suppose again that we have three random points A, B, and C in a d-dimensional space, where d is large. Here, we do not assume points are in the unit cube;
1You can prove this fact by evaluating a double integral, but we shall not do the math here, as it is not central to the discussion.
   
7.2. HIERARCHICAL CLUSTERING 245
they can be anywhere in the space. What is angle ABC? We may assume that A is the point [x1,x2,...,xd] and C is the point [y1,y2,...,yd], while B is the origin. Recall from Section 3.5.4 that the cosine of the angle ABC is the dot product of A and C divided by the product of the lengths of the vectors A and C. That is, the cosine is
 di=1 xiyi   di=1 x2i   di=1 yi2
As d grows, the denominator grows linearly in d, but the numerator is a sum of random values, which are as likely to be positive as negative. Thus, the expected value of the numerator is 0, and as d grows, its standard deviation grows only as √d. Thus, for large d, the cosine of the angle between any two vectors is almost certain to be close to 0, which means the angle is close to 90 degrees.
An important consequence of random vectors being orthogonal is that if we have three random points A, B, and C, and we know the distance from A to B is d1, while the distance from B to C is d2, we can assume the distance from A to C is approximately  d21 + d2. That rule does not hold, even approximately, if the number of dimensions is small. As an extreme case, if d = 1, then the distance from A to C would necessarily be d1 + d2 if A and C were on opposite sides of B, or |d1 − d2| if they were on the same side.
7.1.4 Exercises for Section 7.1
! Exercise 7.1.1: Prove that if you choose two points uniformly and indepen- dently on a line of length 1, then the expected distance between the points is 1/3.
!! Exercise 7.1.2: If you choose two points uniformly in the unit square, what is their expected Euclidean distance?
! Exercise 7.1.3 : Suppose we have a d-dimensional Euclidean space. Consider vectors whose componen√ts are only +1 or −1 in each dimension. Note that each vector has length d, so the product of their lengths (denominator in the formula for the cosine of the angle between them) is d. If we chose each component independently, and a component is as likely to be +1 as −1, what is the distribution of the value of the numerator of the formula (i.e., the sum of the products of the corresponding components from each vector)? What can you say about the expected value of the cosine of the angle between the vectors, as d grows large?
7.2 Hierarchical Clustering
We begin by considering hierarchical clustering in a Euclidean space. This algorithm can only be used for relatively small datasets, but even so, there
      
246 CHAPTER 7. CLUSTERING
are some efficiencies we can make by careful implementation. When the space is non-Euclidean, there are additional problems associated with hierarchical clustering. We therefore consider “clustroids” and the way we can represent a cluster when there is no centroid or average point in a cluster.
7.2.1 Hierarchical Clustering in a Euclidean Space
Any hierarchical clustering algorithm works as follows. We begin with every point in its own cluster. As time goes on, larger clusters will be constructed by combining two smaller clusters, and we have to decide in advance:
1. How will clusters be represented?
2. How will we choose which two clusters to merge? 3. When will we stop combining clusters?
Once we have answers to these questions, the algorithm can be described suc- cinctly as:
WHILE it is not time to stop DO
pick the best two clusters to merge; combine those two clusters into one cluster;
END;
To begin, we shall assume the space is Euclidean. That allows us to represent a cluster by its centroid or average of the points in the cluster. Note that in a cluster of one point, that point is the centroid, so we can initialize the clusters straightforwardly. We can then use the merging rule that the distance between any two clusters is the Euclidean distance between their centroids, and we should pick the two clusters at the shortest distance. Other ways to define intercluster distance are possible, and we can also pick the best pair of clusters on a basis other than their distance. We shall discuss some options in Section 7.2.3.
Example 7.2 : Let us see how the basic hierarchical clustering would work on the data of Fig. 7.2. These points live in a 2-dimensional Euclidean space, and each point is named by its (x, y) coordinates. Initially, each point is in a cluster by itself and is the centroid of that cluster. Among all the pairs of points, there are two pairs that are closest: (10,5) and (11,4) or (11,4) and (12,3). Each is at distance √2. Let us break ties arbitrarily and decide to combine (11,4) with (12,3). The result is shown in Fig. 7.3, including the centroid of the new cluster, which is at (11.5, 3.5).
You might think that (10,5) gets combined with the new cluster next, since it is so close to (11,4). But our distance rule requires us to compare only cluster centroids, and the distance from (10,5) to the centroid of the new cluster is 1.5√2, which is slightly greater than 2. Thus, now the two closest clusters are
  
7.2. HIERARCHICAL CLUSTERING
247
(4,10)
(4,8)
(3,4)
(2,2)
Figure 7.2: Twelve points to be clustered hierarchically
(7,10)
(6,8)
     (10,5)
(9,3)
(12,6)
(11,4)
(12,3)
       (5,2)
those of the points (4,8) and (4,10). We combine them into one cluster with centroid (4,9).
At this point, the two closest centroids are (10,5) and (11.5, 3.5), so we combine these two clusters. The result is a cluster of three points (10,5), (11,4), and (12,3). The centroid of this cluster is (11,4), which happens to be one of the points of the cluster, but that situation is coincidental. The state of the clusters is shown in Fig. 7.4. √
Now, there are several pairs of centroids that are at distance 5, and these are the closest centroids. We show in Fig. 7.5 the result of picking three of these:
1. (6,8) is combined with the cluster of two elements having centroid (4,9). 2. (2,2) is combined with (3,4).
3. (9,3) is combined with the cluster of three elements having centroid (11,4).
We can proceed to combine clusters further. We shall discuss alternative stop- ping rules next. ✷
There are several approaches we might use to stopping the clustering pro- cess.
1. We could be told, or have a belief, about how many clusters there are in the data. For example, if we are told that the data about dogs is taken from Chihuahuas, Dachshunds, and Beagles, then we know to stop when there are three clusters left.
2. We could stop combining when at some point the best combination of existing clusters produces a cluster that is inadequate. We shall discuss
 
248
CHAPTER 7.
(12,6)
(11,4) (11.5,3.5)
(12,3)
CLUSTERING
(4,10)
(4,8)
(3,4)
(2,2)
Figure 7.3: Combining the first two points into a cluster
various tests for the adequacy of a cluster in Section 7.2.3, but for an example, we could insist that any cluster have an average distance between the centroid and its points no greater than some limit. This approach is only sensible if we have a reason to believe that no cluster extends over too much of the space.
We could continue clustering until there is only one cluster. However, it is meaningless to return a single cluster consisting of all the points. Rather, we return the tree representing the way in which all the points were combined. This form of answer makes good sense in some applica- tions, such as one in which the points are genomes of different species, and the distance measure reflects the difference in the genome.2 Then, the tree represents the evolution of these species, that is, the likely order in which two species branched from a common ancestor.
(7,10)
(6,8)
     (10,5)
(9,3)
        3.
(5,2)
Example 7.3 : If we complete the clustering of the data of Fig. 7.2, the tree describing how clusters were grouped is the tree shown in Fig. 7.6. ✷
7.2.2 Efficiency of Hierarchical Clustering
The basic algorithm for hierarchical clustering is not very efficient. At each step, we must compute the distances between each pair of clusters, in order to find the best merger. The initial step takes O(n2) time, but subsequent steps take time proportional to (n − 1)2, (n − 2)2, . . .. The sum of squares up to n is O(n3), so this algorithm is cubic. Thus, it cannot be run except for fairly small numbers of points.
2This space would not be Euclidean, of course, but the principles regarding hierarchical clustering carry over, with some modifications, to non-Euclidean clustering.
 
7.2. HIERARCHICAL CLUSTERING
249
 (4,10)
(4,9) (4,8)
(3,4)
(7,10)
     (6,8)
  (10,5)
(9,3)
(12,6)
(11,4)
(12,3)
       (2,2)
(5,2)
Figure 7.4: Clustering after two additional steps
However, there is a somewhat more efficient implementation of which we should be aware.
1. We start, as we must, by computing the distances between all pairs of points, and this step is O(n2).
2. Form the pairs and their distances into a priority queue, so we can always find the smallest distance in one step. This operation is also O(n2).
3. When we decide to merge two clusters C and D, we remove all entries in the priority queue involving one of these two clusters; that requires work O(n log n) since there are at most 2n deletions to be performed, and priority-queue deletion can be performed in O(log n) time.
4. We then compute all the distances between the new cluster and the re- maining clusters. This work is also O(nlogn), as there are at most n entries to be inserted into the priority queue, and insertion into a priority queue can also be done in O(log n) time.
Since the last two steps are executed at most n times, and the first two steps are executed only once, the overall running time of this algorithm is O(n2 log n). That is better than O(n3), but it still puts a strong limit on how large n can be before it becomes infeasible to use this clustering approach.
7.2.3 Alternative Rules for Controlling Hierarchical Clustering
We have seen one rule for picking the best clusters to merge: find the pair with the smallest distance between their centroids. Some other options are:
250
CHAPTER 7.
(12,6)
(11,4)
(12,3)
CLUSTERING
 (4,10)
(4,9) (4,8)
(3,4) (2.5, 3)
(2,2)
(7,10)
  (4.7, 8.7)
(6,8)
(5,2)
(10,5)
          (10.5, 3.8)
(9,3)
     1.
2.
Figure 7.5: Three more steps of the hierarchical clustering
Take the distance between two clusters to be the minimum of the distances between any two points, one chosen from each cluster. For example, in Fig. 7.3 we would next chose to cluste√r the point (10,5) with the cluster of two points, since (10,5) has distance 2, and no other pair of unclustered points is that close. Note that in Example 7.2, we did make this combi- nation eventually, but not until we had combined another pair of points. In general, it is possible that this rule will result in an entirely different clustering from that obtained using the distance-of-centroids rule.
Take the distance between two clusters to be the average distance of all pairs of points, one from each cluster.
  (2,2) (3,4) (5,2) (4,8) (4,10) (6,8) (7,10) (11,4) (12,3) (10,5) (9,3) (12,6)
Figure 7.6: Tree showing the complete grouping of the points of Fig. 7.2
7.2. HIERARCHICAL CLUSTERING 251
3. The radius of a cluster is the maximum distance between all the points and the centroid. Combine the two clusters whose resulting cluster has the lowest radius. A slight modification is to combine the clusters whose result has the lowest average distance between a point and the centroid. Another modification is to use the sum of the squares of the distances between the points and the centroid. In some algorithms, we shall find these variant definitions of “radius” referred to as “the radius.”
4. The diameter of a cluster is the maximum distance between any two points of the cluster. Note that the radius and diameter of a cluster are not related directly, as they are in a circle, but there is a tendency for them to be proportional. We may choose to merge those clusters whose resulting cluster has the smallest diameter. Variants of this rule, analogous to the rule for radius, are possible.
Example 7.4 : Let us consider the cluster consisting of the five points at the right of Fig. 7.2. The centroid of these five points is (10.8, 4.2). There is a tie for the two furthest points from the centroid: (9,3) and (12,6), both at distance √4.68 = 2.16. Thus, the radius is 2.16. For the diameter, we find the two points in the cluster having the greatest distance. These are again (9,3) and (12,6). Their distance is √18 = 4.24, so that is the diameter. Notice that the diameter is not exactly twice the radius, although it is close in this case. The reason is that the centroid is not on the line between (9,3) and (12,6). ✷
We also have options in determining when to stop the merging process. We already mentioned “stop when we have k clusters” for some predetermined k. Here are some other options.
1. Stop if the diameter of the cluster that results from the best merger ex- ceeds a threshold. We can also base this rule on the radius, or on any of the variants of the radius mentioned above.
2. Stop if the density of the cluster that results from the best merger is below some threshold. The density can be defined in many different ways. Roughly, it should be the number of cluster points per unit volume of the cluster. That ratio can be estimated by the number of points divided by some power of the diameter or radius of the cluster. The correct power could be the number of dimensions of the space. Sometimes, 1 or 2 is chosen as the power, regardless of the number of dimensions.
3. Stop when there is evidence that the next pair of clusters to be combined yields a bad cluster. For example, we could track the average diameter of all the current clusters. As long as we are combining points that truly belong in a cluster, this average will rise gradually. However, if we combine two clusters that really don’t deserve to be combined, then the average diameter will take a sudden jump.
  
252 CHAPTER 7. CLUSTERING
Example 7.5 : Let us reconsider Fig. 7.2. It has three natural clusters. We computed the diameter of the largest – the five points at the right – in Ex- ample 7.4; it is 4.24. The diameter of the 3-node cluster at the lower left is 3, the distance between (2,2) and (5,2). The diameter of the 4-node cluster at the upper left is √13 = 3.61. The average diameter, 3.62, was reached starting from 0 after nine mergers, so the rise is evidently slow: about 0.4 per merger.
If we are forced to merge two of these natural clusters, the best we can do is merge the two at the left. The diameter of this cluster is √89 = 9.43; that is the distance between the two points (2,2) and (7,10). Now, the average of the diameters is (9.43 + 4.24)/2 = 6.84. This average has jumped almost as much in one step as in all nine previous steps. That comparison indicates that the last merger was inadvisable, and we should roll it back and stop. ✷
7.2.4 Hierarchical Clustering in Non-Euclidean Spaces
When the space is non-Euclidean, we need to use some distance measure that is computed from points, such as Jaccard, cosine, or edit distance. That is, we cannot base distances on “location” of points. The algorithm of Section 7.2.1 requires distances between points to be computed, but presumably we have a way to compute those distances. A problem arises when we need to represent a cluster, because we cannot replace a collection of points by their centroid.
Example 7.6 : The problem arises for any of the non-Euclidean distances we have discussed, but to be concrete, suppose we are using edit distance, and we decide to merge the strings abcd and aecdb. These have edit distance 3 and might well be merged. However, there is no string that represents their average, or that could be thought of as lying naturally between them. We could take one of the strings that we might pass through when transforming one string to the other by single insertions or deletions, such as aebcd, but there are many such options. Moreover, when clusters are formed from more than two strings, the notion of “on the path between” stops making sense. ✷
Given that we cannot combine points in a cluster when the space is non- Euclidean, our only choice is to pick one of the points of the cluster itself to represent the cluster. Ideally, this point is close to all the points of the cluster, so it in some sense lies in the “center.” We call the representative point the clustroid. We can select the clustroid in various ways, each designed to, in some sense, minimize the distances between the clustroid and the other points in the cluster. Common choices include selecting as the clustroid the point that minimizes:
1. The sum of the distances to the other points in the cluster.
2. The maximum distance to another point in the cluster.
3. The sum of the squares of the distances to the other points in the cluster.
  
7.2. HIERARCHICAL CLUSTERING 253
Example 7.7: Suppose we are using edit distance, and a cluster consists of the four points abcd, aecdb, abecb, and ecdab. Their distances are found in the following table:
ecdab abecb aecdb
                   abcd
                   aecdb
                   abecb
If we apply the three criteria for being the centroid to each of the four points of the cluster, we find:
Point Sum Max Sum-Sq abcd 11 5 43 aecdb 7 3 17 abecb 9 4 29 ecdab 11 5 45
We can see from these measurements that whichever of the three criteria we choose, aecdb will be selected as the clustroid. In general, different criteria could yield different clustroids. ✷
The options for measuring the distance between clusters that were outlined in Section 7.2.3 can be applied in a non-Euclidean setting, provided we use the clustroid in place of the centroid. For example, we can merge the two clusters whose clustroids are closest. We could also use the average or minimum distance between all pairs of points from the clusters.
Other suggested criteria involved measuring the density of a cluster, based on the radius or diameter. Both these notions make sense in the non-Euclidean environment. The diameter is still the maximum distance between any two points in the cluster. The radius can be defined using the clustroid in place of the centroid. Moreover, it makes sense to use the same sort of evaluation for the radius as we used to select the clustroid in the first place. For example, if we take the clustroid to be the point with the smallest sum of squares of distances to the other nodes, then define the radius to be that sum of squares (or its square root).
Finally, Section 7.2.3 also discussed criteria for stopping the merging of clusters. None of these criteria made direct use of the centroid, except through the notion of radius, and we have already observed that “radius” makes good sense in non-Euclidean spaces. Thus, there is no substantial change in the options for stopping criteria when we move from Euclidean to non-Euclidean spaces.
7.2.5 Exercises for Section 7.2
Exercise 7.2.1 : Perform a hierarchical clustering of the one-dimensional set of points 1, 4, 9, 16, 25, 36, 49, 64, 81, assuming clusters are represented by
            5
3
3
     2
2
    4
       
254 CHAPTER 7. CLUSTERING their centroid (average), and at each step the clusters with the closest centroids
are merged.
Exercise 7.2.2: How would the clustering of Example 7.2 change if we used for the distance between two clusters:
(a) The minimum of the distances between any two points, one from each cluster.
(b) The average of the distances between pairs of points, one from each of the two clusters.
Exercise 7.2.3: Repeat the clustering of Example 7.2 if we choose to merge the two clusters whose resulting cluster has:
(a) The smallest radius.
(b) The smallest diameter.
Exercise 7.2.4 : Compute the density of each of the three clusters in Fig. 7.2, if “density” is defined to be the number of points divided by
(a) The square of the radius.
(b) The diameter (not squared).
What are the densities, according to (a) and (b), of the clusters that result from the merger of any two of these three clusters. Does the difference in densities suggest the clusters should or should not be merged?
Exercise 7.2.5: We can select clustroids for clusters, even if the space is Euclidean. Consider the three natural clusters in Fig. 7.2, and compute the clustroids of each, assuming the criterion for selecting the clustroid is the point with the minimum sum of distances to the other point in the cluster.
! Exercise 7.2.6 : Consider the space of strings with edit distance as the distance measure. Give an example of a set of strings such that if we choose the clustroid by minimizing the sum of the distances to the other points we get one point as the clustroid, but if we choose the clustroid by minimizing the maximum distance to the other points, another point becomes the clustroid.
7.3 K-means Algorithms
In this section we begin the study of point-assignment algorithms. The best known family of clustering algorithms of this type is called k-means. They assume a Euclidean space, and they also assume the number of clusters, k, is known in advance. It is, however, possible to deduce k by trial and error. After an introduction to the family of k-means algorithms, we shall focus on a particular algorithm, called BFR after its authors, that enables us to execute k-means on data that is too large to fit in main memory.
7.3. K-MEANS ALGORITHMS 255 7.3.1 K-Means Basics
A k-means algorithm is outlined in Fig. 7.7. There are several ways to select the initial k points that represent the clusters, and we shall discuss them in Section 7.3.2. The heart of the algorithm is the for-loop, in which we consider each point other than the k selected points and assign it to the closest cluster, where “closest” means closest to the centroid of the cluster. Note that the centroid of a cluster can migrate as points are assigned to it. However, since only points near the cluster are likely to be assigned, the centroid tends not to move too much.
Initially choose k points that are likely to be in different clusters;
Make these points the centroids of their clusters; FOR each remaining point p DO
find the centroid to which p is closest;
Add p to the cluster of that centroid;
Adjust the centroid of that cluster to account for p;
END;
Figure 7.7: Outline of k-means algorithms
An optional step at the end is to fix the centroids of the clusters and to reassign each point, including the k initial points, to the k clusters. Usually, a point p will be assigned to the same cluster in which it was placed on the first pass. However, there are cases where the centroid of p’s original cluster moved quite far from p after p was placed there, and p is assigned to a different cluster on the second pass. In fact, even some of the original k points could wind up being reassigned. As these examples are unusual, we shall not dwell on the subject.
7.3.2 Initializing Clusters for K-Means
We want to pick points that have a good chance of lying in different clusters. There are two approaches.
1. Pick points that are as far away from one another as possible.
2. Cluster a sample of the data, perhaps hierarchically, so there are k clus- ters. Pick a point from each cluster, perhaps that point closest to the centroid of the cluster.
The second approach requires little elaboration. For the first approach, there are variations. One good choice is:
Pick the first point at random;
256 CHAPTER 7. CLUSTERING
WHILE there are fewer than k points DO
Add the point whose minimum distance from the selected
points is as large as possible; END;
Example 7.8 : Let us consider the twelve points of Fig. 7.2, which we repro- duce here as Fig. 7.8. In the worst case, our initial choice of a point is near the center, say (6,8). The furthest point from (6,8) is (12,3), so that point is chosen next.
(4,10)
(4,8)
(7,10)
(6,8)
        (3,4)
(10,5)
(9,3)
(12,6)
(11,4)
(12,3)
    (2,2)
(5,2)
Figure 7.8: Repeat of Fig. 7.2
Among the remaining ten points, the one whose minimum distance to either (6,8) or (12,3) is a maximum is (2,2). That point has distance √52 = 7.21 from (6,8) and distance √101 = 10.05 to (12,3); thus its “score” is 7.21. You can check easily that any other point is less than distance 7.21 from at least one of (6,8) and (12,3). Our selection of three starting points is thus (6,8), (12,3), and (2,2). Notice that these three belong to different clusters.
Had we started with a different point, say (10,5), we would get a different set of three initial points. In this case, the starting points would be (10,5), (2,2), and (4,10). Again, these points belong to the three different clusters. ✷
7.3.3 Picking the Right Value of k
We may not know the correct value of k to use in a k-means clustering. How- ever, if we can measure the quality of the clustering for various values of k, we can usually guess what the right value of k is. Recall the discussion in Sec- tion 7.2.3, especially Example 7.5, where we observed that if we take a measure of appropriateness for clusters, such as average radius or diameter, that value will grow slowly, as long as the number of clusters we assume remains at or above the true number of clusters. However, as soon as we try to form fewer
  
7.3. K-MEANS ALGORITHMS 257 clusters than there really are, the measure will rise precipitously. The idea is
expressed by the diagram of Fig. 7.9.
Average Diameter
Correct value of k
   Number of Clusters
Figure 7.9: Average diameter or another measure of diffuseness rises quickly as soon as the number of clusters falls below the true number present in the data
If we have no idea what the correct value of k is, we can find a good value in a number of clustering operations that grows only logarithmically with the true number. Begin by running the k-means algorithm for k = 1,2,4,8,... . Eventually, you will find two values v and 2v between which there is very little decrease in the average diameter, or whatever measure of cluster cohesion you are using. We may conclude that the value of k that is justified by the data lies between v/2 and v. If you use a binary search (discussed below) in that range, you can find the best value for k in another log2 v clustering operations, for a total of 2 log2 v clusterings. Since the true value of k is at least v/2, we have used a number of clusterings that is logarithmic in k.
Since the notion of “not much change” is imprecise, we cannot say exactly how much change is too much. However, the binary search can be conducted as follows, assuming the notion of “not much change” is made precise by some formula. We know that there is too much change between v/2 and v, or else we would not have gone on to run a clustering for 2v clusters. Suppose at some point we have narrowed the range of k to between x and y. Let z = (x + y)/2. Run a clustering with z as the target number of clusters. If there is not too much change between z and y, then the true value of k lies between x and z. So recursively narrow that range to find the correct value of k. On the other hand, if there is too much change between z and y, then use binary search in the range between z and y instead.
7.3.4 The Algorithm of Bradley, Fayyad, and Reina
This algorithm, which we shall refer to as BFR after its authors, is a variant of k-means that is designed to cluster data in a high-dimensional Euclidean space. It makes a very strong assumption about the shape of clusters: they must be normally distributed about a centroid. The mean and standard deviation for a cluster may differ for different dimensions, but the dimensions must be
  
258 CHAPTER 7. CLUSTERING independent. For instance, in two dimensions a cluster may be cigar-shaped,
but the cigar must not be rotated off of the axes. Figure 7.10 makes the point.
OK OK Not OK
Figure 7.10: The clusters in data for which the BFR algorithm may be used can have standard deviations that differ along different axes, but the axes of the cluster must align with the axes of the space
The BFR Algorithm begins by selecting k points, using one of the methods discussed in Section 7.3.2. Then, the points of the data file are read in chunks. These might be chunks from a distributed file system or a conventional file might be partitioned into chunks of the appropriate size. Each chunk must consist of few enough points that they can be processed in main memory. Also stored in main memory are summaries of the k clusters and some other data, so the entire memory is not available to store a chunk. The main-memory data other than the chunk from the input consists of three types of objects:
1. The Discard Set: These are simple summaries of the clusters themselves. We shall address the form of cluster summarization shortly. Note that the cluster summaries are not “discarded”; they are in fact essential. However, the points that the summary represents are discarded and have no representation in main memory other than through this summary.
2. The Compressed Set: These are summaries, similar to the cluster sum- maries, but for sets of points that have been found close to one another, but not close to any cluster. The points represented by the compressed set are also discarded, in the sense that they do not appear explicitly in main memory. We call the represented sets of points miniclusters.
3. The Retained Set: Certain points can neither be assigned to a cluster nor are they sufficiently close to any other points that we can represent them by a compressed set. These points are held in main memory exactly as they appear in the input file.
The picture in Fig. 7.11 suggests how the points processed so far are represented. The discard and compressed sets are represented by 2d + 1 values, if the
data is d-dimensional. These numbers are: (a) The number of points represented, N.
   
7.3. K-MEANS ALGORITHMS
Compressed sets
A cluster. Its points are in the discard set.
259
 Points in the retained set
                   Figure 7.11: Points in the discard, compressed, and retained sets
(b) The sum of the components of all the points in each dimension. This data is a vector SUM of length d, and the component in the ith dimension is SUMi .
(c) The sum of the squares of the components of all the points in each di- mension. This data is a vector SUMSQ of length d, and its component in the ith dimension is SUMSQi.
Our real goal is to represent a set of points by their count, their centroid and the standard deviation in each dimension. However, these 2d + 1 values give us those statistics. N is the count. The centroid’s coordinate in the ith dimension is the SUMi/N, that is the sum in that dimension divided by the number of points. The variance in the ith dimension is SUMSQi/N − (SUMi/N)2. We can compute the standard deviation in each dimension, since it is the square root of the variance.
Example 7.9 : Suppose a cluster consists of the points (5, 1), (6, −2), and (7,0). Then N = 3, SUM = [18,−1], and SUMSQ = [110,5]. The centroid is SUM/N , or [6, −1/3]. The variance in the first dimension is 110/3 − (18/3)2 = 0.667, so the standard deviation is √0.667 = 0.816. In the second dimension, the variance is 5/3 − (−1/3)2 = 1.56, so the standard deviation is 1.25. ✷
7.3.5 Processing Data in the BFR Algorithm
We shall now outline what happens when we process a chunk of points.
Its centroid
 
260 CHAPTER 7. CLUSTERING
   Benefits of the N, SUM, SUMSQ Representation
There is a significant advantage to representing sets of points as it is done in the BFR Algorithm, rather than by storing N, the centroid, and the standard deviation in each dimension. Consider what we need to do when we add a new point to a cluster. N is increased by 1, of course. But we can also add the vector representing the location of the point to SUM to get the new SUM, and we can add the squares of the components of the vector to SUMSQ to get the new SUMSQ. Had we used the centroid in place of SUM, then we could not adjust the centroid to account for the new point without doing some calculation involving N , and the recomputation of the standard deviations would be far more complex as well. Similarly, if we want to combine two sets, we just add corresponding values of N, SUM, and SUMSQ, while if we used the centroid and standard deviations as a representation, the calculation would be far more complex.
 1. First, all points that are sufficiently close to the centroid of a cluster are added to that cluster. As described in the box on benefits, it is simple to add the information about the point to the N, SUM, and SUMSQ that represent the cluster. We then discard the point. The question of what “sufficiently close” means will be addressed shortly.
2. For the points that are not sufficiently close to any centroid, we clus- ter them, along with the points in the retained set. Any main-memory clustering algorithm can be used, such as the hierarchical methods dis- cussed in Section 7.2. We must use some criterion for deciding when it is reasonable to combine two points into a cluster or two clusters into one. Section 7.2.3 covered the ways we might make this decision. Clusters of more than one point are summarized and added to the compressed set. Singleton clusters become the retained set of points.
3. We now have miniclusters derived from our attempt to cluster new points and the old retained set, and we have the miniclusters from the old com- pressed set. Although none of these miniclusters can be merged with one of the k clusters, they might merge with one another. The criterion for merger may again be chosen according to the discussion in Section 7.2.3. Note that the form of representation for compressed sets (N, SUM, and SUMSQ) makes it easy to compute statistics such as the variance for the combination of two miniclusters that we consider merging.
4. Points that are assigned to a cluster or a minicluster, i.e., those that are not in the retained set, are written out, with their assignment, to secondary memory.
7.3. K-MEANS ALGORITHMS 261
Finally, if this is the last chunk of input data, we need to do something with the compressed and retained sets. We can treat them as outliers, and never cluster them at all. Or, we can assign each point in the retained set to the cluster of the nearest centroid. We can combine each minicluster with the cluster whose centroid is closest to the centroid of the minicluster.
An important decision that must be examined is how we decide whether a new point p is close enough to one of the k clusters that it makes sense to add p to the cluster. Two approaches have been suggested.
(a) Add p to a cluster if it not only has the centroid closest to p, but it is very unlikely that, after all the points have been processed, some other cluster centroid will be found to be nearer to p. This decision is a complex statistical calculation. It must assume that points are ordered randomly, and that we know how many points will be processed in the future. Its advantage is that if we find one centroid to be significantly closer to p than any other, we can add p to that cluster and be done with it, even if p is very far from all centroids.
(b) We can measure the probability that, if p belongs to a cluster, it would be found as far as it is from the centroid of that cluster. This calculation makes use of the fact that we believe each cluster to consist of normally distributed points with the axes of the distribution aligned with the axes of the space. It allows us to make the calculation through the Mahalanobis distance of the point, which we shall describe next.
The Mahalanobis distance is essentially the distance between a point and the centroid of a cluster, normalized by the standard deviation of the cluster in each dimension. Since the BFR Algorithm assumes the axes of the cluster align with the axes of the space, the computation of Mahalanobis distance is especially simple. Let p = [p1,p2,...,pd] be a point and c = [c1,c2,...,cd] the centroid of a cluster. Let σi be the standard deviation of points in the cluster in the ith dimension. Then the Mahalanobis distance between p and c is
         d p i − c i 2 i=1  σi  
That is, we normalize the difference between p and c in the ith dimension by dividing by the standard deviation of the cluster in that dimension. The rest of the formula combines the normalized distances in each dimension in the normal way for a Euclidean space.
To assign point p to a cluster, we compute the Mahalanobis distance between p and each of the cluster centroids. We choose that cluster whose centroid has the least Mahalanobis distance, and we add p to that cluster provided the Mahalanobis distance is less than a threshold. For instance, suppose we pick four as the threshold. If data is normally distributed, then the probability of
  
262 CHAPTER 7. CLUSTERING
a value as far as four standard deviations from the mean is less than one in a million. Thus, if the points in the cluster are really normally distributed, then the probability that we will fail to include a point that truly belongs is less than 10−6. And such a point is likely to be assigned to that cluster eventually anyway, as long as it does not wind up closer to some other centroid as centroids migrate in response to points added to their cluster.
7.3.6 Exercises for Section 7.3
Exercise 7.3.1: For the points of Fig. 7.8, if we select three starting points using the method of Section 7.3.2, and the first point we choose is (3,4), which other points are selected.
!! Exercise 7.3.2 : Prove that no matter what point we start with in Fig. 7.8, if we select three starting points by the method of Section 7.3.2 we obtain points in each of the three clusters. Hint: You could solve this exhaustively by begining with each of the twelve points in turn. However, a more generally applicable solution is to consider the diameters of the three clusters and also consider the minimum intercluster distance, that is, the minimum distance between two points chosen from two different clusters. Can you prove a general theorem based on these two parameters of a set of points?
! Exercise 7.3.3: Give an example of a dataset and a selection of k initial centroids such that when the points are reassigned to their nearest centroid at the end, at least one of the initial k points is reassigned to a different cluster.
Exercise 7.3.4 : For the three clusters of Fig. 7.8:
(a) Compute the representation of the cluster as in the BFR Algorithm. That
is, compute N, SUM, and SUMSQ.
(b) Compute the variance and standard deviation of each cluster in each of
the two dimensions.
Exercise 7.3.5: Suppose a cluster of three-dimensional points has standard deviations of 2, 3, and 5, in the three dimensions, in that order. Compute the Mahalanobis distance between the origin (0, 0, 0) and the point (1, −3, 4).
7.4 The CURE Algorithm
We now turn to another large-scale-clustering algorithm in the point-assignment class. This algorithm, called CURE (Clustering Using REpresentatives), as- sumes a Euclidean space. However, it does not assume anything about the shape of clusters; they need not be normally distributed, and can even have strange bends, S-shapes, or even rings. Instead of representing clusters by their centroid, it uses a collection of representative points, as the name implies.
7.4. THE CURE ALGORITHM 263
 Figure 7.12: Two clusters, one surrounding the other
Example 7.10 : Figure 7.12 is an illustration of two clusters. The inner clus- ter is an ordinary circle, while the second is a ring around the circle. This arrangement is not completely pathological. A creature from another galaxy might look at our solar system and observe that the objects cluster into an inner circle (the planets) and an outer ring (the Kuyper belt), with little in between. ✷
7.4.1 Initialization in CURE
We begin the CURE algorithm by:
1. Take a small sample of the data and cluster it in main memory. In prin- ciple, any clustering method could be used, but as CURE is designed to handle oddly shaped clusters, it is often advisable to use a hierarchical method in which clusters are merged when they have a close pair of points. This issue is discussed in more detail in Example 7.11 below.
2. Select a small set of points from each cluster to be representative points. These points should be chosen to be as far from one another as possible, using the method described in Section 7.3.2.
3. Move each of the representative points a fixed fraction of the distance between its location and the centroid of its cluster. Perhaps 20% is a good fraction to choose. Note that this step requires a Euclidean space, since otherwise, there might not be any notion of a line between two points.
Example 7.11 : We could use a hierarchical clustering algorithm on a sample of the data from Fig. 7.12. If we took as the distance between clusters the shortest distance between any pair of points, one from each cluster, then we would correctly find the two clusters. That is, pieces of the ring would stick
264 CHAPTER 7. CLUSTERING
together, and pieces of the inner circle would stick together, but pieces of ring would always be far away from the pieces of the circle. Note that if we used the rule that the distance between clusters was the distance between their centroids, then we might not get the intuitively correct result. The reason is that the centroids of both clusters are in the center of the diagram.
 Figure 7.13: Select representative points from each cluster, as far from one another as possible
For the second step, we pick the representative points. If the sample from which the clusters are constructed is large enough, we can count on a cluster’s sample points at greatest distance from one another lying on the boundary of the cluster. Figure 7.13 suggests what our initial selection of sample points might look like.
Finally, we move the representative points a fixed fraction of the distance from their true location toward the centroid of the cluster. Note that in Fig. 7.13 both clusters have their centroid in the same place: the center of the inner circle. Thus, the representative points from the circle move inside the cluster, as was intended. Points on the outer edge of the ring also move into their cluster, but points on the ring’s inner edge move outside the cluster. The final locations of the representative points from Fig. 7.13 are suggested by Fig. 7.14. ✷
7.4.2 Completion of the CURE Algorithm
The next phase of CURE is to merge two clusters if they have a pair of rep- resentative points, one from each cluster, that are sufficiently close. The user may pick the distance that defines “close.” This merging step can repeat, until there are no more sufficiently close clusters.
Example 7.12 : The situation of Fig. 7.14 serves as a useful illustration. There is some argument that the ring and circle should really be merged, because their centroids are the same. For instance, if the gap between the ring and circle were
7.4. THE CURE ALGORITHM 265
 Figure 7.14: Moving the representative points 20% of the distance to the clus- ter’s centroid
much smaller, it might well be argued that combining the points of the ring and circle into a single cluster reflected the true state of affairs. For instance, the rings of Saturn have narrow gaps between them, but it is reasonable to visualize the rings as a single object, rather than several concentric objects. In the case of Fig. 7.14 the choice of
1. The fraction of the distance to the centroid that we move the representa- tive points and
2. The choice of how far apart representative points of two clusters need to be to avoid merger
together determine whether we regard Fig. 7.12 as one cluster or two. ✷
The last step of CURE is point assignment. Each point p is brought from secondary storage and compared with the representative points. We assign p to the cluster of the representative point that is closest to p.
Example 7.13: In our running example, points within the ring will surely be closer to one of the ring’s representative points than to any representative point of the circle. Likewise, points within the circle will surely be closest to a representative point of the circle. An outlier – a point not within the ring or the circle – will be assigned to the ring if it is outside the ring. If the outlier is between the ring and the circle, it will be assigned to one or the other, somewhat favoring the ring because its representative points have been moved toward the circle. ✷
7.4.3 Exercises for Section 7.4
Exercise 7.4.1 : Consider two clusters that are a circle and a surrounding ring, as in the running example of this section. Suppose:
266 CHAPTER 7. CLUSTERING
i. The radius of the circle is c.
ii. The inner and outer circles forming the ring have radii i and o, respec- tively.
iii. All representative points for the two clusters are on the boundaries of the clusters.
iv. Representative points are moved 20% of the distance from their initial position toward the centroid of their cluster.
v. Clusters are merged if, after repositioning, there are representative points from the two clusters at distance d or less.
In terms of d, c, i, and o, under what circumstances will the ring and circle be merged into a single cluster?
7.5 Clustering in Non-Euclidean Spaces
We shall next consider an algorithm that handles non-main-memory data, but does not require a Euclidean space. The algorithm, which we shall refer to as GRGPF for its authors (V. Ganti, R. Ramakrishnan, J. Gehrke, A. Powell, and J. French), takes ideas from both hierarchical and point-assignment approaches. Like CURE, it represents clusters by sample points in main memory. However, it also tries to organize the clusters hierarchically, in a tree, so a new point can be assigned to the appropriate cluster by passing it down the tree. Leaves of the tree hold summaries of some clusters, and interior nodes hold subsets of the information describing the clusters reachable through that node. An attempt is made to group clusters by their distance from one another, so the clusters at a leaf are close, and the clusters reachable from one interior node are relatively close as well.
7.5.1 Representing Clusters in the GRGPF Algorithm
As we assign points to clusters, the clusters can grow large. Most of the points in a cluster are stored on disk, and are not used in guiding the assignment of points, although they can be retrieved. The representation of a cluster in main memory consists of several features. Before listing these features, if p is any point in a cluster, let ROWSUM(p) be the sum of the squares of the distances from p to each of the other points in the cluster. Note that, although we are not in a Euclidean space, there is some distance measure d that applies to points, or else it is not possible to cluster points at all. The following features form the representation of a cluster.
1. N, the number of points in the cluster.
7.5. CLUSTERING IN NON-EUCLIDEAN SPACES 267
2. The clustroid of the cluster, which is defined specifically to be the point in the cluster that minimizes the sum of the squares of the distances to the other points; that is, the clustroid is the point in the cluster with the smallest ROWSUM.
3. The rowsum of the clustroid of the cluster.
4. For some chosen constant k, the k points of the cluster that are closest to the clustroid, and their rowsums. These points are part of the represen- tation in case the addition of points to the cluster causes the clustroid to change. The assumption is made that the new clustroid would be one of these k points near the old clustroid.
5. The k points of the cluster that are furthest from the clustroid and their rowsums. These points are part of the representation so that we can consider whether two clusters are close enough to merge. The assumption is made that if two clusters are close, then a pair of points distant from their respective clustroids would be close.
7.5.2 Initializing the Cluster Tree
The clusters are organized into a tree, and the nodes of the tree may be very large, perhaps disk blocks or pages, as would be the case for a B-tree or R-tree, which the cluster-representing tree resembles. Each leaf of the tree holds as many cluster representations as can fit. Note that a cluster representation has a size that does not depend on the number of points in the cluster.
An interior node of the cluster tree holds a sample of the clustroids of the clusters represented by each of its subtrees, along with pointers to the roots of those subtrees. The samples are of fixed size, so the number of children that an interior node may have is independent of its level. Notice that as we go up the tree, the probability that a given cluster’s clustroid is part of the sample diminishes.
We initialize the cluster tree by taking a main-memory sample of the dataset and clustering it hierarchically. The result of this clustering is a tree T , but T is not exactly the tree used by the GRGPF Algorithm. Rather, we select from T certain of its nodes that represent clusters of approximately some desired size n. These are the initial clusters for the GRGPF Algorithm, and we place their representations at the leaf of the cluster-representing tree. We then group clus- ters with a common ancestor in T into interior nodes of the cluster-representing tree, so in some sense, clusters descended from one interior node are as close as possible. In some cases, rebalancing of the cluster-representing tree will be necessary. This process is similar to the reorganization of a B-tree, and we shall not examine this issue in detail.
268 CHAPTER 7. CLUSTERING 7.5.3 Adding Points in the GRGPF Algorithm
We now read points from secondary storage and insert each one into the nearest cluster. We start at the root, and look at the samples of clustroids for each of the children of the root. Whichever child has the clustroid closest to the new point p is the node we examine next. When we reach any node in the tree, we look at the sample clustroids for its children and go next to the child with the clustroid closest to p. Note that some of the sample clustroids at a node may have been seen at a higher level, but each level provides more detail about the clusters lying below, so we see many new sample clustroids each time we go a level down the tree.
Finally, we reach a leaf. This leaf has the cluster features for each cluster represented by that leaf, and we pick the cluster whose clustroid is closest to p. We adjust the representation of this cluster to account for the new node p. In particular, we:
1. Add1toN.
2. Add the square of the distance between p and each of the nodes q men- tioned in the representation to ROWSUM(q). These points q include the clustroid, the k nearest points, and the k furthest points.
We also estimate the rowsum of p, in case p needs to be part of the represen- tation (e.g., it turns out to be one of the k points closest to the clustroid). Note we cannot compute ROWSUM(p) exactly, without going to disk and retrieving all the points of the cluster. The estimate we use is
ROWSUM(p) = ROWSUM(c) + N d2 (p, c)
where d(p, c) is the distance between p and the clustroid c. Note that N and ROWSUM(c) in this formula are the values of these features before they were adjusted to account for the addition of p.
We might well wonder why this estimate works. In Section 7.1.3 we discussed the “curse of dimensionality,” in particular the observation that in a high- dimensional Euclidean space, almost all angles are right angles. Of course the assumption of the GRGPF Algorithm is that the space might not be Euclidean, but typically a non-Euclidean space also suffers from the curse of dimensionality, in that it behaves in many ways like a high-dimensional Euclidean space. If we assume that the angle between p, c, and another point q in the cluster is a right angle, then the Pythagorean theorem tell us that
d2(p, q) = d2(p, c) + d2(c, q)
If we sum over all q other than c, and then add d2(p,c) to ROWSUM(p) to account for the fact that the clustroid is one of the points in the cluster, we derive ROWSUM(p) = ROWSUM(c) + N d2 (p, c).
Now, we must see if the new point p is one of the k closest or furthest points from the clustroid, and if so, p and its rowsum become a cluster feature,
7.5. CLUSTERING IN NON-EUCLIDEAN SPACES 269
replacing one of the other features – whichever is no longer one of the k closest or furthest. We also need to consider whether the rowsum for one of the k closest points q is now less than ROWSUM(c). That situation could happen if p were closer to one of these points than to the current clustroid. If so, we swap the roles of c and q. Eventually, it is possible that the true clustroid will no longer be one of the original k closest points. We have no way of knowing, since we do not see the other points of the cluster in main memory. However, they are all stored on disk, and can be brought into main memory periodically for a recomputation of the cluster features.
7.5.4 Splitting and Merging Clusters
The GRGPF Algorithm assumes that there is a limit on the radius that a cluster may have. The particular definition used for the radius is  ROWSUM(c)/N, where c is the clustroid of the cluster and N the number of points in the cluster. That is, the radius is the square root of the average square of the distance from the clustroid of the points in the cluster. If a cluster’s radius grows too large, it is split into two. The points of that cluster are brought into main memory, and divided into two clusters to minimize the rowsums. The cluster features for both clusters are computed.
As a result, the leaf of the split cluster now has one more cluster to represent. We should manage the cluster tree like a B-tree, so usually, there will be room in a leaf to add one more cluster. However, if not, then the leaf must be split into two leaves. To implement the split, we must add another pointer and more sample clustroids at the parent node. Again, there may be extra space, but if not, then this node too must be split, and we do so to minimize the squares of the distances between the sample clustroids assigned to different nodes. As in a B-tree, this splitting can ripple all the way up to the root, which can then be split if needed.
The worst thing that can happen is that the cluster-representing tree is now too large to fit in main memory. There is only one thing to do: we make it smaller by raising the limit on how large the radius of a cluster can be, and we consider merging pairs of clusters. It is normally sufficient to consider clusters that are “nearby,” in the sense that their representatives are at the same leaf or at leaves with a common parent. However, in principle, we can consider merging any two clusters C1 and C2 into one cluster C.
To merge clusters, we assume that the clustroid of C will be one of the points that are as far as possible from the clustroid of C1 or the clustroid of C2. Suppose we want to compute the rowsum in C for the point p, which is one of the k points in C1 that are as far as possible from the centroid of C1. We use the curse-of-dimensionality argument that says all angles are approximately right angles, to justify the following formula.
ROWSUMC (p) = ROWSUMC1 (p) + NC2  d2(p, c1) + d2(c1, c2)  + ROWSUMC2 (c2) In the above, we subscript N and ROWSUM by the cluster to which that feature
 
270 CHAPTER 7. CLUSTERING
refers. We use c1 and c2 for the clustroids of C1 and C2, respectively.
In detail, we compute the sum of the squares of the distances from p to all the nodes in the combined cluster C by beginning with ROWSUMC1 (p) to get the terms for the points in the same cluster as p. For the NC2 points q in C2, we consider the path from p to the clustroid of C1, then to the clustroid of C2, and finally to q. We assume there is a right angle between the legs from p to c1 and c1 to c2, and another right angle between the shortest path from p to c2 and the leg from c2 to q. We then use the Pythagorean theorem to justify computing the square of the length of the path to each q as the sum of the
squares of the three legs.
We must then finish computing the features for the merged cluster. We
need to consider all the points in the merged cluster for which we know the rowsum. These are, the centroids of the two clusters, the k points closest to the clustroids for each cluster, and the k points furthest from the clustroids for each cluster, with the exception of the point that was chosen as the new clustroid. We can compute the distances from the new clustroid for each of these 4k + 1 points. We select the k with the smallest distances as the “close” points and the k with the largest distances as the “far” points. We can then compute the rowsums for the chosen points, using the same formulas above that we used to compute the rowsums for the candidate clustroids.
7.5.5 Exercises for Section 7.5
Exercise 7.5.1: Using the cluster representation of Section 7.5.1, represent the twelve points of Fig. 7.8 as a single cluster. Use parameter k = 2 as the number of close and distant points to be included in the representation. Hint: Since the distance is Euclidean, we can get the square of the distance between two points by taking the sum of the squares of the differences along the x- and y-axes.
Exercise 7.5.2 : Compute the radius, in the sense used by the GRGPF Algo- rithm (square root of the average square of the distance from the clustroid) for the cluster that is the five points in the lower right of Fig. 7.8. Note that (11,4) is the clustroid.
7.6 Clustering for Streams and Parallelism
In this section, we shall consider briefly how one might cluster a stream. The model we have in mind is one where there is a sliding window (recall Sec- tion 4.1.3) of N points, and we can ask for the centroids or clustroids of the best clusters formed from the last m of these points, for any m ≤ N. We also study a similar approach to clustering a large, fixed set of points using MapRe- duce on a computing cluster (no pun intended). This section provides only a rough outline to suggest the possibilities, which depend on our assumptions about how clusters evolve in a stream.
7.6. CLUSTERING FOR STREAMS AND PARALLELISM 271 7.6.1 The Stream-Computing Model
We assume that each stream element is a point in some space. The sliding window consists of the most recent N points. Our goal is to precluster subsets of the points in the stream, so that we may quickly answer queries of the form “what are the clusters of the last m points?” for any m ≤ N. There are many variants of this query, depending on what we assume about what constitutes a cluster. For instance, we may use a k-means approach, where we are really asking that the last m points be partitioned into exactly k clusters. Or, we may allow the number of clusters to vary, but use one of the criteria in Section 7.2.3 or 7.2.4 to determine when to stop merging clusters into larger clusters.
We make no restriction regarding the space in which the points of the stream live. It may be a Euclidean space, in which case the answer to the query is the centroids of the selected clusters. The space may be non-Euclidean, in which case the answer is the clustroids of the selected clusters, where any of the definitions for “clustroid” may be used (see Section 7.2.4).
The problem is considerably easier if we assume that all stream elements are chosen with statistics that do not vary along the stream. Then, a sample of the stream is good enough to estimate the clusters, and we can in effect ignore the stream after a while. However, the stream model normally assumes that the statistics of the stream elements varies with time. For example, the centroids of the clusters may migrate slowly as time goes on, or clusters may expand, contract, divide, or merge.
7.6.2 A Stream-Clustering Algorithm
In this section, we shall present a greatly simplified version of an algorithm referred to as BDMO (for the authors, B. Babcock, M. Datar, R. Motwani, and L. O’Callaghan). The true version of the algorithm involves much more complex structures, which are designed to provide performance guarantees in the worst case.
The BDMO Algorithm builds on the methodology for counting ones in a stream that was described in Section 4.6. Here are the key similarities and differences:
• Like that algorithm, the points of the stream are partitioned into, and summarized by, buckets whose sizes are a power of two. Here, the size of a bucket is the number of points it represents, rather than the number of stream elements that are 1.
• As before, the sizes of buckets obey the restriction that there are one or two of each size, up to some limit. However, we do not assume that the sequence of allowable bucket sizes starts with 1. Rather, they are required only to form a sequence where each size is twice the previous size, e.g., 3,6,12,24,... .
272 CHAPTER 7. CLUSTERING
• Bucket sizes are again restrained to be nondecreasing as we go back in time. As in Section 4.6, we can conclude that there will be O(logN) buckets.
• The contents of a bucket consists of: 1. The size of the bucket.
7.6.3
2. The timestamp of the bucket, that is, the most recent point that contributes to the bucket. As in Section 4.6, timestamps can be recorded modulo N.
3. A collection of records that represent the clusters into which the points of that bucket have been partitioned. These records contain:
(a) The number of points in the cluster.
(b) The centroid or clustroid of the cluster.
(c) Any other parameters necessary to enable us to merge clusters and maintain approximations to the full set of parameters for the merged cluster. We shall give some examples when we discuss the merger process in Section 7.6.4.
Initializing Buckets
Our smallest bucket size will be p, a power of 2. Thus, every p stream elements, we create a new bucket, with the most recent p points. The timestamp for this bucket is the timestamp of the most recent point in the bucket. We may leave each point in a cluster by itself, or we may perform a clustering of these points according to whatever clustering strategy we have chosen. For instance, if we choose a k-means algorithm, then (assuming k < p) we cluster the points into k clusters by some algorithm.
Whatever method we use to cluster initially, we assume it is possible to compute the centroids or clustroids for the clusters and count the points in each cluster. This information becomes part of the record for each cluster. We also compute whatever other parameters for the clusters will be needed in the merging process.
7.6.4 Merging Buckets
Following the strategy from Section 4.6, whenever we create a new bucket, we need to review the sequence of buckets. First, if some bucket has a timestamp that is more than N time units prior to the current time, then nothing of that bucket is in the window, and we may drop it from the list. Second, we may have created three buckets of size p, in which case we must merge the oldest two of the three. The merger may create two buckets of size 2p, in which case we may have to merge buckets of increasing sizes, recursively, just as in Section 4.6.
To merge two consecutive buckets, we need to do several things:
7.6. CLUSTERING FOR STREAMS AND PARALLELISM 273
1. The size of the bucket is twice the sizes of the two buckets being merged.
2. The timestamp for the merged bucket is the timestamp of the more recent of the two consecutive buckets.
3. We must consider whether to merge clusters, and if so, we need to compute the parameters of the merged clusters. We shall elaborate on this part of the algorithm by considering several examples of criteria for merging and ways to estimate the needed parameters.
Example 7.14: Perhaps the simplest case is where we are using a k-means approach in a Euclidean space. We represent clusters by the count of their points and their centroids. Each bucket has exactly k clusters, so we can pick p = k, or we can pick p larger than k and cluster the p points into k clusters when we create a bucket initially as in Section 7.6.3. We must find the best matching between the k clusters of the first bucket and the k clusters of the second. Here, “best” means the matching that minimizes the sum of the distances between the centroids of the matched clusters.
Note that we do not consider merging two clusters from the same bucket, because our assumption is that clusters do not evolve too much between con- secutive buckets. Thus, we would expect to find in each of two adjacent buckets a representation of each of the k “true” clusters that exist in the stream.
When we decide to merge two clusters, one from each bucket, the number of points in the merged cluster is surely the sum of the numbers of points in the two clusters. The centroid of the merged cluster is the weighted average of the centroids of the two clusters, where the weighting is by the numbers of points in the clusters. That is, if the two clusters have n1 and n2 points, respectively, and have centroids c1 and c2 (the latter are d-dimensional vectors for some d), then the combined cluster has n = n1 + n2 points and has centroid
c = n1c1 + n2c2 n1 + n2
✷
Example 7.15: The method of Example 7.14 suffices when the clusters are changing very slowly. Suppose we might expect the cluster centroids to mi- grate sufficiently quickly that when matching the centroids from two consecu- tive buckets, we might be faced with an ambiguous situation, where it is not clear which of two clusters best matches a given cluster from the other bucket. One way to protect against such a situation is to create more than k clusters in each bucket, even if we know that, when we query (see Section 7.6.5), we shall have to merge into exactly k clusters. For example, we might choose p to be much larger than k, and, when we merge, only merge clusters when the result is sufficiently coherent according to one of the criteria outlined in Section 7.2.3. Or, we could use a hierarchical strategy, and make the best merges, so as to maintain p > k clusters in each bucket.
 
274 CHAPTER 7. CLUSTERING
Suppose, to be specific, that we want to put a limit on the sum of the distances between all the points of a cluster and its centroid. Then in addition to the count of points and the centroid of a cluster, we can include an estimate of this sum in the record for a cluster. When we initialize a bucket, we can compute the sum exactly. But as we merge clusters, this parameter becomes an estimate only. Suppose we merge two clusters, and want to compute the sum of distances for the merged cluster. Use the notation for centroids and counts in Example 7.14, and in addition, let s1 and s2 be the sums for the two clusters. Then we may estimate the radius of the merged cluster to be
n1|c1 −c|+n2|c2 −c|+s1 +s2
That is, we estimate the distance between any point x and the new centroid c to be the distance of that point to its old centroid (these distances sum to s1 + s2, the last two terms in the above expression) plus the distance from the old centroid to the new (these distances sum to the first two terms of the above expression). Note that this estimate is an upper bound, by the triangle inequality.
An alternative is to replace the sum of distances by the sum of the squares of the distances from the points to the centroid. If these sums for the two clusters are t1 and t2, respectively, then we can produce an estimate for the same sum in the new cluster as
n1|c1 −c|2 +n2|c2 −c|2 +t1 +t2
This estimate is close to correct if the space is high-dimensional, by the “curse
of dimensionality.” ✷
Example 7.16 : Our third example will assume a non-Euclidean space and no constraint on the number of clusters. We shall borrow several of the techniques from the GRGPF Algorithm of Section 7.5. Specifically, we represent clusters by their clustroid and rowsum (sum of the squares of the distances from each node of the cluster to its clustroid). We include in the record for a cluster information about a set of points at maximum distance from the clustroid, including their distances from the clustroid and their rowsums. Recall that their purpose is to suggest a clustroid when this cluster is merged with another.
When we merge buckets, we may choose one of many ways to decide which clusters to merge. For example, we may consider pairs of clusters in order of the distance between their clustroids. We may also choose to merge clusters when we consider them, provided the sum of their rowsums is below a certain limit. Alternatively, we may perform the merge if the sum of rowsums divided by the number of points in the clusters is below a limit. Any of the other strategies discussed for deciding when to merge clusters may be used as well, provided we arrange to maintain the data (e.g., cluster diameter) necessary to make decisions.
We then must pick a new clustroid, from among the points most distant from the clustroids of the two merged clusters. We can compute rowsums for
7.6. CLUSTERING FOR STREAMS AND PARALLELISM 275
each of these candidate clustroids using the formulas given in Section 7.5.4. We also follow the strategy given in that section to pick a subset of the distant points from each cluster to be the set of distant points for the merged cluster, and to compute the new rowsum and distance-to-clustroid for each. ✷
7.6.5 Answering Queries
Recall that we assume a query is a request for the clusters of the most recent m points in the stream, where m ≤ N. Because of the strategy we have adopted of combining buckets as we go back in time, we may not be able to find a set of buckets that covers exactly the last m points. However, if we choose the smallest set of buckets that cover the last m points, we shall include in these buckets no more than the last 2m points. We shall produce, as answer to the query, the centroids or clustroids of all the points in the selected buckets. In order for the result to be a good approximation to the clusters for exactly the last m points, we must assume that the points between 2m and m + 1 will not have radically different statistics from the most recent m points. However, if the statistics vary too rapidly, recall from Section 4.6.6 that a more complex bucketing scheme can guarantee that we can find buckets to cover at most the last m(1 + ǫ) points, for any ǫ > 0.
Having selected the desired buckets, we pool all their clusters. We then use some methodology for deciding which clusters to merge. Examples 7.14 and 7.16 are illustrations of two approaches to this merger. For instance, if we are required to produce exactly k clusters, then we can merge the clusters with the closest centroids until we are left with only k clusters, as in Example 7.14. Or we can make a decision whether or not to merge clusters in various ways, as we sampled in Example 7.16.
7.6.6 Clustering in a Parallel Environment
Now, let us briefly consider the use of parallelism available in a computing cluster.3 Weassumewearegivenaverylargecollectionofpoints,andwewish to exploit parallelism to compute the centroids of their clusters. The simplest approach is to use a MapReduce strategy, but in most cases we are constrained to use a single Reduce task.
Begin by creating many Map tasks. Each task is assigned a subset of the points. The Map function’s job is to cluster the points it is given. Its output is a set of key-value pairs with a fixed key 1, and a value that is the description of one cluster. This description can be any of the possibilities suggested in Section 7.6.2, such as the centroid, count, and diameter of the cluster.
Since all key-value pairs have the same key, there can be only one Reduce task. This task gets descriptions of the clusters produced by each of the Map
3Do not forget that the term “cluster” has two completely different meanings in this section.
 
276 CHAPTER 7. CLUSTERING
tasks, and must merge them appropriately. We may use the discussion in Sec- tion 7.6.4 as representative of the various strategies we might use to produce the final clustering, which is the output of the Reduce task.
7.6.7 Exercises for Section 7.6
Exercise 7.6.1: Execute the BDMO Algorithm with p = 3 on the following 1-dimensional, Euclidean data:
1,45,80,24,56,71,17,40,66,32,48,96,9,41,75,11,58,93,28,39,77
The clustering algorithms is k-means with k = 3. Only the centroid of a cluster,
along with its count, is needed to represent a cluster.
Exercise 7.6.2: Using your clusters from Exercise 7.6.1, produce the best centroids in response to a query asking for a clustering of the last 10 points.
7.7 Summary of Chapter 7
✦ Clustering: Clusters are often a useful summary of data that is in the form of points in some space. To cluster points, we need a distance measure on that space. Ideally, points in the same cluster have small distances be- tween them, while points in different clusters have large distances between them.
✦ Clustering Algorithms: Clustering algorithms generally have one of two forms. Hierarchical clustering algorithms begin with all points in a cluster of their own, and nearby clusters are merged iteratively. Point-assignment clustering algorithms consider points in turn and assign them to the clus- ter in which they best fit.
✦ The Curse of Dimensionality: Points in high-dimensional Euclidean spa- ces, as well as points in non-Euclidean spaces often behave unintuitively. Two unexpected properties of these spaces are that random points are almost always at about the same distance, and random vectors are almost always orthogonal.
✦ Centroids and Clustroids: In a Euclidean space, the members of a cluster can be averaged, and this average is called the centroid. In non-Euclidean spaces, there is no guarantee that points have an “average,” so we are forced to use one of the members of the cluster as a representative or typical element of the cluster. That representative is called the clustroid.
✦ Choosing the Clustroid: There are many ways we can define a typical point of a cluster in a non-Euclidean space. For example, we could choose the point with the smallest sum of distances to the other points, the smallest sum of the squares of those distances, or the smallest maximum distance to any other point in the cluster.
7.7. SUMMARY OF CHAPTER 7 277
✦ Radius and Diameter: Whether or not the space is Euclidean, we can de- fine the radius of a cluster to be the maximum distance from the centroid or clustroid to any point in that cluster. We can define the diameter of the cluster to be the maximum distance between any two points in the cluster. Alternative definitions, especially of the radius, are also known, for example, average distance from the centroid to the other points.
✦ Hierarchical Clustering: This family of algorithms has many variations, which differ primarily in two areas. First, we may chose in various ways which two clusters to merge next. Second, we may decide when to stop the merge process in various ways.
✦ Picking Clusters to Merge: One strategy for deciding on the best pair of clusters to merge in a hierarchical clustering is to pick the clusters with the closest centroids or clustroids. Another approach is to pick the pair of clusters with the closest points, one from each cluster. A third approach is to use the average distance between points from the two clusters.
✦ Stopping the Merger Process: A hierarchical clustering can proceed until there are a fixed number of clusters left. Alternatively, we could merge until it is impossible to find a pair of clusters whose merger is sufficiently compact, e.g., the merged cluster has a radius or diameter below some threshold. Another approach involves merging as long as the resulting cluster has a sufficiently high “density,” which can be defined in various ways, but is the number of points divided by some measure of the size of the cluster, e.g., the radius.
✦ K-Means Algorithms: This family of algorithms is of the point-assignment type and assumes a Euclidean space. It is assumed that there are exactly k clusters for some known k. After picking k initial cluster centroids, the points are considered one at a time and assigned to the closest centroid. The centroid of a cluster can migrate during point assignment, and an optional last step is to reassign all the points, while holding the centroids fixed at their final values obtained during the first pass.
✦ Initializing K-Means Algorithms: One way to find k initial centroids is to pick a random point, and then choose k − 1 additional points, each as far away as possible from the previously chosen points. An alternative is to start with a small sample of points and use a hierarchical clustering to merge them into k clusters.
✦ Picking K in a K-Means Algorithm: If the number of clusters is unknown, we can use a binary-search technique, trying a k-means clustering with different values of k. We search for the largest value of k for which a decrease below k clusters results in a radically higher average diameter of the clusters. This search can be carried out in a number of clustering operations that is logarithmic in the true value of k.
278 CHAPTER 7. CLUSTERING
✦ The BFR Algorithm: This algorithm is a version of k-means designed to handle data that is too large to fit in main memory. It assumes clusters are normally distributed about the axes.
✦ Representing Clusters in BFR: Points are read from disk one chunk at a time. Clusters are represented in main memory by the count of the num- ber of points, the vector sum of all the points, and the vector formed by summing the squares of the components of the points in each dimension. Other collection of points, too far from a cluster centroid to be included in a cluster, are represented as “miniclusters” in the same way as the k clusters, while still other points, which are not near any other point will be represented as themselves and called “retained” points.
✦ Processing Points in BFR: Most of the points in a main-memory load will be assigned to a nearby cluster and the parameters for that cluster will be adjusted to account for the new points. Unassigned points can be formed into new miniclusters, and these miniclusters can be merged with previously discovered miniclusters or retained points. After the last memory load, the miniclusters and retained points can be merged to their nearest cluster or kept as outliers.
✦ The CURE Algorithm: This algorithm is of the point-assignment type. It is designed for a Euclidean space, but clusters can have any shape. It handles data that is too large to fit in main memory.
✦ Representing Clusters in CURE: The algorithm begins by clustering a small sample of points. It then selects representative points for each cluster, by picking points in the cluster that are as far away from each other as possible. The goal is to find representative points on the fringes of the cluster. However, the representative points are then moved a fraction of the way toward the centroid of the cluster, so they lie somewhat in the interior of the cluster.
✦ Processing Points in CURE: After creating representative points for each cluster, the entire set of points can be read from disk and assigned to a cluster. We assign a given point to the cluster of the representative point that is closest to the given point.
✦ The GRGPF Algorithm: This algorithm is of the point-assignment type. It handles data that is too big to fit in main memory, and it does not assume a Euclidean space.
✦ Representing Clusters in GRGPF: A cluster is represented by the count of points in the cluster, the clustroid, a set of points nearest the clustroid and a set of points furthest from the clustroid. The nearby points allow us to change the clustroid if the cluster evolves, and the distant points allow for merging clusters efficiently in appropriate circumstances. For each of these points, we also record the rowsum, that is the square root
7.7. SUMMARY OF CHAPTER 7 279 of the sum of the squares of the distances from that point to all the other
points of the cluster.
✦ Tree Organization of Clusters in GRGPF : Cluster representations are or- ganized into a tree structure like a B-tree, where nodes of the tree are typically disk blocks and contain information about many clusters. The leaves hold the representation of as many clusters as possible, while inte- rior nodes hold a sample of the clustroids of the clusters at their descen- dant leaves. We organize the tree so that the clusters whose representa- tives are in any subtree are as close as possible.
✦ Processing Points in GRGPF : After initializing clusters from a sample of points, we insert each point into the cluster with the nearest clustroid. Because of the tree structure, we can start at the root and choose to visit the child with the sample clustroid nearest to the given point. Following this rule down one path in the tree leads us to a leaf, where we insert the point into the cluster with the nearest clustroid on that leaf.
✦ Clustering Streams: A generalization of the DGIM Algorithm (for count- ing 1’s in the sliding window of a stream) can be used to cluster points that are part of a slowly evolving stream. The BDMO Algorithm uses buckets similar to those of DGIM, with allowable bucket sizes forming a sequence where each size is twice the previous size.
✦ Representation of Buckets in BDMO: The size of a bucket is the number of points it represents. The bucket itself holds only a representation of the clusters of these points, not the points themselves. A cluster representa- tion includes a count of the number of points, the centroid or clustroid, and other information that is needed for merging clusters according to some selected strategy.
✦ Merging Buckets in BDMO: When buckets must be merged, we find the best matching of clusters, one from each of the buckets, and merge them in pairs. If the stream evolves slowly, then we expect consecutive buckets to have almost the same cluster centroids, so this matching makes sense.
✦ Answering Queries in BDMO: A query is a length of a suffix of the sliding window. We take all the clusters in all the buckets that are at least partially within that suffix and merge them using some strategy. The resulting clusters are the answer to the query.
✦ Clustering Using MapReduce: We can divide the data into chunks and cluster each chunk in parallel, using a Map task. The clusters from each Map task can be further clustered in a single Reduce task.
280 CHAPTER 7. CLUSTERING 7.8 References for Chapter 7
The ancestral study of clustering for large-scale data is the BIRCH Algorithm of [6]. The BFR Algorithm is from [2]. The CURE Algorithm is found in [5].
The paper on the GRGPF Algorithm is [3]. The necessary background regarding B-trees and R-trees can be found in [4]. The study of clustering on streams is taken from [1].
1. B. Babcock, M. Datar, R. Motwani, and L. O’Callaghan, “Maintaining variance and k-medians over data stream windows,” Proc. ACM Symp. on Principles of Database Systems, pp. 234–243, 2003.
2. P.S. Bradley, U.M. Fayyad, and C. Reina, “Scaling clustering algorithms to large databases,” Proc. Knowledge Discovery and Data Mining, pp. 9– 15, 1998.
3. V. Ganti, R. Ramakrishnan, J. Gehrke, A.L. Powell, and J.C. French:, “Clustering large datasets in arbitrary metric spaces,” Proc. Intl. Conf. on Data Engineering, pp. 502–511, 1999.
4. H. Garcia-Molina, J.D. Ullman, and J. Widom, Database Systems: The Complete Book Second Edition, Prentice-Hall, Upper Saddle River, NJ, 2009.
5. S. Guha, R. Rastogi, and K. Shim, “CURE: An efficient clustering algo- rithm for large databases,” Proc. ACM SIGMOD Intl. Conf. on Manage- ment of Data, pp. 73–84, 1998.
6. T. Zhang, R. Ramakrishnan, and M. Livny, “BIRCH: an efficient data clustering method for very large databases,” Proc. ACM SIGMOD Intl. Conf. on Management of Data, pp. 103–114, 1996.
Chapter 8
Advertising on the Web
One of the big surprises of the 21st century has been the ability of all sorts of interesting Web applications to support themselves through advertising, rather than subscription. While radio and television have managed to use advertising as their primary revenue source, most media – newspapers and magazines, for example – have had to use a hybrid approach, combining revenue from advertising and subscriptions.
By far the most lucrative venue for on-line advertising has been search, and much of the effectiveness of search advertising comes from the “adwords” model of matching search queries to advertisements. We shall therefore devote much of this chapter to algorithms for optimizing the way this assignment is done. The algorithms used are of an unusual type; they are greedy and they are “on- line” in a particular technical sense to be discussed. We shall therefore digress to discuss these two algorithmic issues – greediness and on-line algorithms – in general, before tackling the adwords problem.
A second interesting on-line advertising problem involves selecting items to advertise at an on-line store. This problem involves “collaborative filtering,” where we try to find customers with similar behavior in order to suggest they buy things that similar customers have bought. This subject will be treated in Section 9.3.
8.1 Issues in On-Line Advertising
In this section, we summarize the technical problems that are presented by the opportunities for on-line advertising. We begin by surveying the types of ads found on the Web.
8.1.1 Advertising Opportunities
The Web offers many ways for an advertiser to show their ads to potential customers. Here are the principal venues.
281
282 CHAPTER 8. ADVERTISING ON THE WEB
1. Some sites, such as eBay, Craig’s List or auto trading sites allow adver-
tisers to post their ads directly, either for free, for a fee, or a commission.
2. Display ads are placed on many Web sites. Advertisers pay for the display at a fixed rate per impression (one display of the ad with the download of the page by some user). Normally, a second download of the page, even by the same user, will result in the display of a different ad and is a second impression.
3. On-line stores such as Amazon show ads in many contexts. The ads are not paid for by the manufacturers of the product advertised, but are selected by the store to maximize the probability that the customer will be interested in the product. We consider this kind of advertising in Chapter 9.
4. Search ads are placed among the results of a search query. Advertisers bid for the right to have their ad shown in response to certain queries, but they pay only if the ad is clicked on. The particular ads to be shown are selected by a complex process, to be discussed in this chapter, involving the search terms that the advertiser has bid for, the amount of their bid, the observed probability that the ad will be clicked on, and the total budget that the advertiser has offered for the service.
8.1.2 Direct Placement of Ads
When advertisers can place ads directly, such as a free ad on Craig’s List or the “buy it now” feature at eBay, there are several problems that the site must deal with. Ads are displayed in response to query terms, e.g., “apartment Palo Alto.” The Web site can use an inverted index of words, just as a search engine does (see Section 5.1.1) and return those ads that contain all the words in the query. Alternatively, one can ask the advertiser to specify parameters of the ad, which are stored in a database. For instance, an ad for a used car could specify the manufacturer, model, color, and year from pull-down menus, so only clearly understood terms can be used. Queryers can use the same menus of terms in their queries.
Ranking ads is a bit more problematic, since there is nothing like the links on the Web to tell us which ads are more “important.” One strategy used is “most-recent first.” That strategy, while equitable, is subject to abuse, where advertisers post small variations of their ads at frequent intervals. The tech- nology for discovering ads that are too similar has already been covered, in Section 3.4.
An alternative approach is to try to measure the attractiveness of an ad. Each time it is displayed, record whether or not the queryer clicked on it. Presumably, attractive ads will be clicked on more frequently than those that are not. However, there are several factors that must be considered in evaluating ads:
8.1. ISSUES IN ON-LINE ADVERTISING 283
1. The position of the ad in a list has great influence on whether or not it is clicked. The first on the list has by far the highest probability, and the probability drops off exponentially as the position increases.
2. The ad may have attractiveness that depends on the query terms. For example, an ad for a used convertible would be more attractive if the search query includes the term “convertible,” even though it might be a valid response to queries that look for that make of car, without specifying whether or not a convertible is wanted.
3. All ads deserve the opportunity to be shown until their click probability can be approximated closely. If we start all ads out with a click probability of 0, we shall never show them and thus never learn whether or not they are attractive ads.
8.1.3 Issues for Display Ads
This form of advertising on the Web most resembles advertising in traditional media. An ad for a Chevrolet run in the pages of the New York Times is a display ad, and its effectiveness is limited. It may be seen by many people, but most of them are not interested in buying a car, just bought a car, don’t drive, or have another good reason to ignore the ad. Yet the cost of printing the ad was still borne by the newspaper and hence by the advertiser. An impression of a similar ad on the Yahoo! home page is going to be relatively ineffective for essentially the same reason. The fee for placing such an ad is typically a fraction of a cent per impression.
The response of traditional media to this lack of focus was to create newspa- pers or magazines for special interests. If you are a manufacturer of golf clubs, running your ad in Golf Digest would give you an order-of-magnitude increase in the probability that the person seeing your ad would be interested in it. This phenomenon explains the existence of many specialized, low-circulation maga- zines. They are able to charge much more per impression for an ad than is a general-purpose outlet such as a daily newspaper. The same phenomenon ap- pears on the Web. An ad for golf clubs on sports.yahoo.com/golf has much more value per impression than does the same ad on the Yahoo! home page or an ad for Chevrolets on the Yahoo! golf page.
However, the Web offers an opportunity to tailor display ads in a way that hardcopy media cannot: it is possible to use information about the user to determine which ad they should be shown, regardless of what page they are looking at. If it is known that Sally likes golf, then it makes sense to show her an ad for golf clubs, regardless of what page she is looking at. We could determine Sally’s love for golf in various ways:
1. She may belong to a golf-related group on Facebook.
2. She may mention “golf” frequently in emails posted on her gmail account.
284 CHAPTER 8. ADVERTISING ON THE WEB
3. She may spend a lot of time on the Yahoo! golf page.
4. She may issue search queries with golf-related terms frequently. 5. She may bookmark the Web sites of one or more golf courses.
Each of these methods, and many others like these, raise enormous privacy issues. It is not the purpose of this book to try to resolve those issues, which in practice probably have no solution that will satisfy all concerns. On the one hand, people like the free services that have recently become advertising- supported, and these services depend on advertising being much more effective than conventional ads. There is a general agreement that, if there must be ads, it is better to see things you might actually use than to have what pages you view cluttered with irrelevancies. On the other hand, there is great potential for misuse if the information leaves the realm of the machines that execute advertising algorithms and get into the hands of real people.
8.2 On-Line Algorithms
Before addressing the question of matching advertisements to search queries, we shall digress slightly by examining the general class to which such algorithms belong. This class is referred to as “on-line,” and they generally involve an ap- proach called “greedy.” We also give, in the next section, a preliminary example of an on-line greedy algorithm for a simpler problem: maximal matching.
8.2.1 On-Line and Off-Line Algorithms
Typical algorithms work as follows. All the data needed by the algorithm is presented initially. The algorithm can access the data in any order. At the end, the algorithm produces its answer. Such an algorithm is called off-line.
However, there are times when we cannot see all the data before our al- gorithm must make some decisions. Chapter 4 covered stream mining, where we could store only a limited amount of the stream, and had to answer queries about the entire stream when called upon to do so. There is an extreme form of stream processing, where we must respond with an output after each stream ele- ment arrives. We thus must decide about each stream element knowing nothing at all of the future. Algorithms of this class are called on-line algorithms.1
As the case in point, selecting ads to show with search queries would be relatively simple if we could do it off-line. We would see a month’s worth of search queries, and look at the bids advertisers made on search terms, as well as their advertising budgets for the month, and we could then assign ads to
1Unfortunately, we are faced with another case of dual meanings, like the coincidence involving the term “cluster” that we noted in Section 7.6.6, where we needed to interpret properly phrases such as “algorithms for computing clusters on computer clusters.” Here, the term “on-line” refers to the nature of the algorithm, and should not be confused with “on-line” meaning “on the Internet” in phrases such as “on-line algorithms for on-line advertising.”
 
8.2. ON-LINE ALGORITHMS 285
the queries in a way that maximized both the revenue to the search engine and the number of impressions that each advertiser got. The problem with off-line algorithms is that most queryers don’t want to wait a month to get their search results.
Thus, we must use an on-line algorithm to assign ads to search queries. That is, when a search query arrives, we must select the ads to show with that query immediately. We can use information about the past, e.g., we do not have to show an ad if the advertiser’s budget has already been spent, and we can examine the click-through rate (fraction of the time the ad is clicked on when it is displayed) that an ad has obtained so far. However, we cannot use anything about future search queries. For instance, we cannot know whether there will be lots of queries arriving later and using search terms on which this advertiser has made higher bids.
Example 8.1 : Let us take a very simple example of why knowing the future could help. A manufacturer A of replica antique furniture has bid 10 cents on the search term “chesterfield”.2 A more conventional manufacturer B has bid 20 cents on both the terms “chesterfield” and “sofa.” Both have monthly budgets of $100, and there are no other bidders on either of these terms. It is the beginning of the month, and a search query “chesterfield” has just arrived. We are allowed to display only one ad with the query.
The obvious thing to do is to display B’s ad, because they bid more. How- ever, suppose there will be lots of search queries this month for “sofa,” but very few for “chesterfield.” Then A will never spend its $100 budget, while B will spend its full budget even if we give the query to A. Specifically, if there will be at least 500 more queries for either “sofa” or “chesterfield,” then there is no harm, and potentially a benefit, in giving the query to A. It will still be possible for B to spend its entire budget, while we are increasing the amount of A’s budget that will be spent. Note that this argument makes sense both from the point of view of the search engine, which wants to maximize total revenue, and from the point of view of both A and B, who presumably want to get all the impressions that their budgets allow.
If we could know the future, then we would know how many more “sofa” queries and how many more “chesterfield” queries were going to arrive this month. If that number is below 500, then we want to give the query to B to maximize revenue, but if it is 500 or more, then we want to give it to A. Since we don’t know the future, an on-line algorithm cannot always do as well as an off-line algorithm. ✷
8.2.2 Greedy Algorithms
Many on-line algorithms are of the greedy algorithm type. These algorithms make their decision in response to each input element by maximizing some function of the input element and the past.
2A chesterfield is a type of sofa. See, for example, www.chesterfields.info.
 
286 CHAPTER 8. ADVERTISING ON THE WEB
Example 8.2: The obvious greedy algorithm for the situation described in Example 8.1 is to assign a query to the highest bidder who still has budget left. For the data of that example, what will happen is that the first 500 “sofa” or “chesterfield” queries will be assigned to B. At that time, B runs out of budget and is assigned no more queries. After that, the next 1000 “chesterfield” queries are assigned to A, and “sofa” queries get no ad and therefore earn the search engine no money.
The worst thing that can happen is that 500 “chesterfield” queries arrive, followed by 500 “sofa” queries. An off-line algorithm could optimally assign the first 500 to A, earning $50, and the next 500 to B, earning $100, or a total of $150. However, the greedy algorithm will assign the first 500 to B, earning $100, and then has no ad for the next 500, earning nothing. ✷
8.2.3 The Competitive Ratio
As we see from Example 8.2, an on-line algorithm need not give as good a result as the best off-line algorithm for the same problem. The most we can expect is that there will be some constant c less than 1, such that on any input, the result of a particular on-line algorithm is at least c times the result of the optimum off-line algorithm. The constant c, if it exists, is called the competitive ratio for the on-line algorithm.
Example 8.3 : The greedy algorithm, on the particular data of Example 8.2, gives a result that is 2/3 as good as that of the optimum algorithm: $100 versus $150. That proves that the competitive ratio is no greater than 2/3. But it could be less. The competitive ratio for an algorithm may depend on what kind of data is allowed to be input to the algorithm. Even if we restrict inputs to the situation described in Example 8.2, but with the bids allowed to vary, then we can show the greedy algorithm has a competitive ratio no greater than 1/2. Just raise the bid by A to ǫ less than 20 cents. As ǫ approaches 0, the greedy algorithm still produces only $100, but the return from the optimum algorithm approaches $200. We can show that it is impossible to do worse than half the optimum in this simple case, so the competitive ratio is indeed 1/2. However, we’ll leave this sort of proof for later sections. ✷
8.2.4 Exercises for Section 8.2
! Exercise 8.2.1: A popular example of the design of an on-line algorithm to minimize the competitive ratio is the ski-buying problem.3 Suppose you can buy skis for $100, or you can rent skis for $10 per day. You decide to take up skiing, but you don’t know if you will like it. You may try skiing for any number of days and then give it up. The merit of an algorithm is the cost per day of skis, and we must try to minimize this cost.
3Thanks to Anna Karlin for this example.
 
8.3. THE MATCHING PROBLEM 287
One on-line algorithm for making the rent/buy decision is “buy skis im- mediately.” If you try skiing once, fall down and give it up, then this on-line algorithm costs you $100 per day, while the optimum off-line algorithm would have you rent skis for $10 for the one day you used them. Thus, the competitive ratio of the algorithm “buy skis immediately” is at most 1/10th, and that is in fact the exact competitive ratio, since using the skis one day is the worst possible outcome for this algorithm. On the other hand, the on-line algorithm “always rent skis” has an arbitrarily small competitive ratio. If you turn out to really like skiing and go regularly, then after n days, you will have paid $10n or $10/day, while the optimum off-line algorithm would have bought skis at once, and paid only $100, or $100/n per day.
Your question: design an on-line algorithm for the ski-buying problem that has the best possible competitive ratio. What is that competitive ratio? Hint: Since you could, at any time, have a fall and decide to give up skiing, the only thing the on-line algorithm can use in making its decision is how many times previously you have gone skiing.
8.3 The Matching Problem
We shall now take up a problem that is a simplified version of the problem of matching ads to search queries. This problem, called “maximal matching,” is an abstract problem involving bipartite graphs (graphs with two sets of nodes – left and right – with all edges connecting a node in the left set to a node in the right set. Figure 8.1 is an example of a bipartite graph. Nodes 1, 2, 3, and 4 form the left set, while nodes a, b, c, and d form the right set.
8.3.1 Matches and Perfect Matches
Suppose we are given a bipartite graph. A matching is a subset of the edges such that no node is an end of two or more edges. A matching is said to be perfect if every node appears in the matching. Note that a matching can only be perfect if the left and right sets are of the same size. A matching that is as large as any other matching for the graph in question is said to be maximal.
Example 8.4: The set of edges {(1,a), (2,b), (3,d)} is a matching for the bipartite graph of Fig. 8.1. Each member of the set is an edge of the bipartite graph, and no node appears more than once. The set of edges
{(1,c), (2,b), (3,d), (4,a)}
is a perfect matching, represented by heavy lines in Fig. 8.2. Every node appears exactly once. It is, in fact, the sole perfect matching for this graph, although some bipartite graphs have more than one perfect matching. The matching of Fig. 8.2 is also maximal, since every perfect matching is maximal. ✷
288
CHAPTER 8.
ADVERTISING ON THE WEB
a
b
c
d
   8.3.2
Figure 8.1: A bipartite graph
The Greedy Algorithm for Maximal Matching
1
2
3
4
Off-line algorithms for finding a maximal matching have been studied for dec- ades, and one can get very close to O(n2) for an n-node graph. On-line algo- rithms for the problem have also been studied, and it is this class of algorithms we shall consider here. In particular, the greedy algorithm for maximal match- ing works as follows. We consider the edges in whatever order they are given. When we consider (x, y), add this edge to the matching if neither x nor y are ends of any edge selected for the matching so far. Otherwise, skip (x, y).
Example 8.5 : Let us consider a greedy match for the graph of Fig. 8.1. Sup- pose we order the nodes lexicographically, that is, by order of their left node, breaking ties by the right node. Then we consider the edges in the order (1, a), (1, c), (2, b), (3, b), (3, d), (4, a). The first edge, (1, a), surely becomes part of the matching. The second edge, (1,c), cannot be chosen, because node 1 already appears in the matching. The third edge, (2,b), is selected, because neither node 2 nor node b appears in the matching so far. Edge (3,b) is rejected for the match because b is already matched, but then (3, d) is added to the match because neither 3 nor d has been matched so far. Finally, (4,a) is rejected because a appears in the match. Thus, the matching produced by the greedy algorithm for this ordering of the edges is {(1, a), (2, b), (3, d)}. As we saw, this matching is not maximal. ✷
Example 8.6 : A greedy match can be even worse than that of Example 8.5. On the graph of Fig. 8.1, any ordering that begins with the two edges (1,a) and (3, b), in either order, will match those two pairs but then will be unable to match nodes 2 or 4. Thus, the size of the resulting match is only 2. ✷
8.3. THE MATCHING PROBLEM
289
   8.3.3
Figure 8.2: The only perfect matching for the graph of Fig. 8.1
Competitive Ratio for Greedy Matching
1
2
3
4
a
b
c
d
We can show a competitive ratio of 1/2 for the greedy matching algorithm of Section 8.3.2. First, the ratio cannot be more than 1/2. We already saw that for the graph of Fig. 8.1, there is a perfect matching of size 4. However, if the edges are presented in any of the orders discussed in Example 8.6, the size of the match is only 2, or half the optimum. Since the competitive ratio for an algorithm is the minimum over all possible inputs of the ratio of what that algorithm achieves to the optimum result, we see that 1/2 is an upper bound on the competitive ratio.
Suppose Mo is a maximal matching, and Mg is the matching that the greedy algorithm produces. Let L be the set of left nodes that are matched in Mo but not in Mg. Let R be the set of right nodes that are connected by edges to any node in L. We claim that every node in R is matched in Mg. Suppose not; in particular, suppose node r in R is not matched in Mg. Then the greedy algorithm will eventually consider some edge (l,r), where l is in L. At that time, neither end of this edge is matched, because we have supposed that neither l nor r is ever matched by the greedy algorithm. That observation contradicts the definition of how the greedy algorithm works; that is, the greedy algorithm would indeed match (l,r). We conclude that every node in R is matched in Mg.
Now, we know several things about the sizes of sets and matchings.
1. |Mo| ≤ |Mg|+|L|, since among the nodes on the left, only nodes in L can
be matched in Mo but not Mg.
2. |L| ≤ |R|, because in Mo, all the nodes in L were matched.
290 CHAPTER 8. ADVERTISING ON THE WEB 3. |R| ≤ |Mg|, because every node in R is matched in Mg.
Now, (2) and (3) give us |L| ≤ |Mg|. That, together with (1), gives us |Mo| ≤ 2|Mg|, or |Mg| ≥ 1 |Mo|. The latter inequality says that the competitive
2
ratio is at least 1/2. Since we already observed that the competitive ratio is no more than 1/2, we now conclude the ratio is exactly 1/2.
8.3.4 Exercises for Section 8.3
Exercise 8.3.1 : Define the graph Gn to have the 2n nodes a0,a1,...,an−1,b0,b1,...,bn−1
and the following edges. Each node ai, for i = 0,1,...,n − 1, is connected to the nodes bj and bk, where
j=2i modnandk=(2i+1) modn
For instance, the graph G4 has the following edges: (a0, b0), (a0, b1), (a1, b2),
(a1, b3), (a2, b0), (a2, b1), (a3, b2), and (a3, b3).
(a) Find a perfect matching for G4.
(b) Find a perfect matching for G5.
!! (c) Prove that for every n, Gn has a perfect matching.
! Exercise 8.3.2: How many perfect matchings do the graphs G4 and G5 of Exercise 8.3.1 have?
! Exercise 8.3.3 : Whether or not the greedy algorithm gives us a perfect match- ing for the graph of Fig. 8.1 depends on the order in which we consider the edges. Of the 6! possible orders of the six edges, how many give us a perfect match- ing? Give a simple test for distinguishing those orders that do give the perfect matching from those that do not.
8.4 The Adwords Problem
We now consider the fundamental problem of search advertising, which we term the “adwords problem,” because it was first encountered in the Google Adwords system. We then discuss a greedy algorithm called “Balance” that offers a good competitive ratio. We analyze this algorithm for a simplified case of the adwords problem.
 
8.4. THE ADWORDS PROBLEM 291 8.4.1 History of Search Advertising
Around the year 2000, a company called Overture (later bought by Yahoo!) introduced a new kind of search. Advertisers bid on keywords (words in a search query), and when a user searched for that keyword, the links to all the advertisers who bid on that keyword are displayed in the order highest-bid-first. If the advertiser’s link was clicked on, they paid what they had bid.
That sort of search was very useful for the case where the search queryer really was looking for advertisements, but it was rather useless for the queryer who was just looking for information. Recall our discussion in Section 5.1.1 about the point that unless a search engine can provide reliable responses to queries that are for general information, no one will want to use the search engine when they are looking to buy something.
Several years later, Google adapted the idea in a system called Adwords. By that time, the reliability of Google was well established, so people were willing to trust the ads they were shown. Google kept the list of responses based on PageRank and other objective criteria separate from the list of ads, so the same system was useful for the queryer who just wanted information as well as the queryer looking to buy something.
The Adwords system went beyond the earlier system in several ways that made the selection of ads more complex.
1. Google would show only a limited number of ads with each query. Thus, while Overture simply ordered all ads for a given keyword, Google had to decide which ads to show, as well as the order in which to show them.
2. Users of the Adwords system specified a budget: the amount they were willing to pay for all clicks on their ads in a month. These constraints make the problem of assigning ads to search queries more complex, as we hinted at in Example 8.1.
3. Google did not simply order ads by the amount of the bid, but by the amount they expected to receive for display of each ad. That is, the click- through rate was observed for each ad, based on the history of displays of that ad. The value of an ad was taken to be the product of the bid and the click-through rate.
8.4.2 Definition of the Adwords Problem
Of course, the decision regarding which ads to show must be made on-line. Thus, we are only going to consider on-line algorithms for solving the adwords problem, which is as follows.
• Given:
1. A set of bids by advertisers for search queries.
2. A click-through rate for each advertiser-query pair.
292 CHAPTER 8. ADVERTISING ON THE WEB 3. A budget for each advertiser. We shall assume budgets are for a
month, although any unit of time could be used.
4. A limit on the number of ads to be displayed with each search query.
• Respond to each search query with a set of advertisers such that:
1. The size of the set is no larger than the limit on the number of ads
per query.
2. Each advertiser has bid on the search query.
3. Each advertiser has enough budget left to pay for the ad if it is clicked upon.
The revenue of a selection of ads is the total value of the ads selected, where the value of an ad is the product of the bid and the click-through rate for the ad and query. The merit of an on-line algorithm is the total revenue obtained over a month (the time unit over which budgets are assumed to apply). We shall try to measure the competitive ratio for algorithms, that is, the minimum total revenue for that algorithm, on any sequence of search queries, divided by the revenue of the optimum off-line algorithm for the same sequence of search queries.
8.4.3 The Greedy Approach to the Adwords Problem
Since only an on-line algorithm is suitable for the adwords problem, we should first examine the performance of the obvious greedy algorithm. We shall make a number of simplifications to the environment; our purpose is to show even- tually that there is a better algorithm than the obvious greedy algorithm. The simplifications:
(a) There is one ad shown for each query.
(b) All advertisers have the same budget.
(c) All click-through rates are the same.
(d) All bids are either 0 or 1. Alternatively, we may assume that the value of each ad (product of bid and click-through rate) is the same.
The greedy algorithm picks, for each search query, any advertiser who has bid 1 for that query. The competitive ratio for this algorithm is 1/2, as the following example shows.
Example 8.7: Suppose there are two advertisers A and B, and only two possible queries, x and y. Advertiser A bids only on x, while B bids on both x and y. The budget for each advertiser is 2. Notice the similarity to the situation in Example 8.1; the only differences are the fact that the bids by each advertiser are the same and the budgets are smaller here.
8.4. THE ADWORDS PROBLEM 293
   Adwords Aspects not in Our Model
There are several ways in which the real AdWords system differs from the simplified model of this section.
Matching Bids and Search Queries: In our simplified model, advertis- ers bid on sets of words, and an advertiser’s bid is eligible to be shown for search queries that have exactly the same set of words as the advertiser’s bid. In reality, Google, Yahoo!, and Microsoft all offer advertisers a feature known as broad matching, where an ad is eligible to be shown for search queries that are inexact matches of the bid keywords. Examples include queries that include a subset or superset of keywords, and also queries that use words with very similar meanings to the words the advertiser bid on. For such broad matches, search engines charge the advertiser based on complicated formulas taking into account how closely related the search query is to the advertiser’s bid. These formulas vary across search engines and are not made public.
Charging Advertisers for Clicks: In our simplified model, when a user clicks on an advertiser’s ad, the advertiser is charged the amount they bid. This policy is known as a first-price auction. In reality, search engines use a more complicated system known as a second-price auction, where each advertiser pays approximately the bid of the advertiser who placed immediately behind them in the auction. For example, the first- place advertiser for a search might pay the bid of the advertiser in second place, plus one cent. It has been shown that second-price auctions are less susceptible to being gamed by advertisers than first-price auctions and lead to higher revenues for the search engine.
 Let the sequence of queries be xxyy. The greedy algorithm is able to allocate the first two x’s to B, whereupon there is no one with an unexpended budget to pay for the two y’s. The revenue for the greedy algorithm in this case is thus 2. However, the optimum off-line algorithm will allocate the x’s to A and the y’s to B, achieving a revenue of 4. The competitive ratio for the greedy algorithm is thus no more than 1/2. We can argue that on any sequence of queries the ratio of the revenues for the greedy and optimal algorithms is at least 1/2, using essentially the same idea as in Section 8.3.3. ✷
8.4.4 The Balance Algorithm
There is a simple improvement to the greedy algorithm that gives a competitive ratio of 3/4 for the simple case of Section 8.4.3. This algorithm, called the Balance Algorithm, assigns a query to the advertiser who bids on the query and has the largest remaining budget. Ties may be broken arbitrarily.
294 CHAPTER 8. ADVERTISING ON THE WEB
Example 8.8 : Consider the same situation as in Example 8.7. The Balance Algorithm can assign the first query x to either A or B, because they both bid on x and their remaining budgets are the same. However, the second x must be assigned to the other of A and B, because they then have the larger remaining budget. The first y is assigned to B, since it has budget remaining and is the only bidder on y. The last y cannot be assigned, since B is out of budget, and A did not bid. Thus, the total revenue for the Balance Algorithm on this data is 3. In comparison, the total revenue for the optimum off-line algorithm is 4, since it can assign the x’s to A and the y’s to B. Our conclusion is that, for the simplified adwords problem of Section 8.4.3, the competitive ratio of the Balance Algorithm is no more than 3/4. We shall see next that with only two advertisers, 3/4 is exactly the competitive ratio, although as the number of advertisers grows, the competitive ratio lowers to 0.63 (actually 1 − 1/e) but no lower. ✷
8.4.5 A Lower Bound on Competitive Ratio for Balance
In this section we shall prove that in the simple case of the Balance Algorithm that we are considering, the competitive ratio is 3/4. Given Example 8.8, we have only to prove that the total revenue obtained by the Balance Algorithm is at least 3/4 of the revenue for the optimum off-line algorithm. Thus, consider a situation in which there are two advertisers, A1 and A2, each with a budget of B. We shall assume that each query is assigned to an advertiser by the optimum algorithm. If not, we can delete those queries without affecting the revenue of the optimum algorithm and possibly reducing the revenue of Balance. Thus, the lowest possible competitive ratio is achieved when the query sequence consists only of ads assigned by the optimum algorithm.
We shall also assume that both advertisers’ budgets are consumed by the optimum algorithm. If not, we can reduce the budgets, and again argue that the revenue of the optimum algorithm is not reduced while that of Balance can only shrink. That change may force us to use different budgets for the two advertisers, but we shall continue to assume the budgets are both B. We leave as an exercise the extension of the proof to the case where the budgets of the two advertisers are different.
Figure 8.3 suggests how the 2B queries are assigned to advertisers by the two algorithms. In (a) we see that B queries are assigned to each of A1 and A2 by the optimum algorithm. Now, consider how these same queries are assigned by Balance. First, observe that Balance must exhaust the budget of at least one of the advertisers, say A2. If not, then there would be some query assigned to neither advertiser, even though both had budget. We know at least one of the advertisers bids on each query, because that query is assigned in the optimum algorithm. That situation contradicts how Balance is defined to operate; it always assigns a query if it can.
Thus, we see in Fig. 8.3(b) that A2 is assigned B queries. These queries could have been assigned to either A1 or A2 by the optimum algorithm. We
8.4. THE ADWORDS PROBLEM
295
    A1 A2 (a) Optimum
B
      x
y
B
         Figure 8.3: Illustration of the assignments of queries to advertisers in the opti- mum and Balance algorithms
also see in Fig. 8.3(b) that we use y as the number of queries assigned to A1 and x as B − y. It is our goal to show y ≥ x. That inequality will show the revenue of Balance is at least 3B/2, or 3/4th the revenue of the optimum algorithm.
We note that x is also the number of unassigned queries for the Balance Algorithm, and that all the unassigned queries must have been assigned to A2 by the optimum algorithm. The reason is that A1 never runs out of budget, so any query assigned by the optimum algorithm to A1 is surely bid on by A1. Since A1 always has budget during the running of the Balance Algorithm, that algorithm will surely assign this query, either to A1 or to A2.
There are two cases, depending on whether more of the queries that are assigned to A1 by the optimum algorithm are assigned to A1 or A2 by Balance.
1. Suppose at least half of these queries are assigned by Balance to A1. Then y ≥ B/2, so surely y ≥ x.
2. Suppose more than half of these queries are assigned by Balance to A2. Consider the last of these queries q that is assigned to A2 by the Balance Algorithm. At that time, A2 must have had at least as great a budget
x
   A1 A2 Not used
(b) Balance
296
CHAPTER 8. ADVERTISING ON THE WEB
available as A1, or else Balance would have assigned query q to A1, just as the optimum algorithm did. Since more than half of the B queries that the optimum algorithm assigns to A1 are assigned to A2 by Balance, we know that when q was assigned, the remaining budget of A2 was less than B/2. Therefore, at that time, the remaining budget of A1 was also less than B/2. Since budgets only decrease, we know that x < B/2. It follows that y > x, since x + y = B.
We conclude that y ≥ x in either case, so the competitive ratio of the Balance Algorithm is 3/4.
8.4.6 The Balance Algorithm with Many Bidders
When there are many advertisers, the competitive ratio for the Balance Algo- rithm can be under 3/4, but not too far below that fraction. The worst case for Balance is as follows.
1. ThereareN advertisers,A1,A2,...,AN.
2. Each advertiser has a budget B = N!.
3. There are N queries q1,q2,...,qN.
4. Advertiser Ai bids on queries q1 , q2 , . . . , qi and no other queries.
5. The query sequence consists of N rounds. The ith round consists of B occurrences of query qi and nothing else.
The optimum off-line algorithm assigns the B queries qi in the ith round to Ai for all i. Thus, all queries are assigned to a bidder, and the total revenue of the optimum algorithm is NB.
However, the Balance Algorithm assigns each of the queries in round 1 to the N advertisers equally, because all bid on q1, and the Balance Algorithm prefers the bidder with the greatest remaining budget. Thus, each advertiser gets B/N of the queries q1. Now consider the queries q2 in round 2. All but A1 bid on these queries, so they are divided equally among A2 through AN , with each of these N − 1 bidders getting B/(N − 1) queries. The pattern, suggested by Fig. 8.4, repeats for each round i = 3, 4, . . ., with Ai through AN getting B/(N − i + 1) queries.
However, eventually, the budgets of the higher-numbered advertisers will be exhausted. That will happen at the lowest round j such that
   that is,
B 1 + 1 +···+ 1  ≥B N N−1 N−j+1
1+1+···+1≥1 N N−1 N−j+1
   
8.4. THE ADWORDS PROBLEM
297
     .. .
            A1 A2 A3
Figure 8.4: Apportioning queries to N advertisers in the worst case
Euler showed that as k gets large,  ki=1 1/i approaches loge k. Using this observation, we can approximate the above sum as loge N − loge(N − j).
We are thus looking for the j such that loge N − loge(N − j) = 1, approxi- mately. If we replace loge N − loge(N − j) by the equivalent loge N/(N − j)  and exponentiate both sides of the equation loge N/(N − j)  = 1, we get N/(N − j) = e. Solving this equation for j, we get
j = N  1 − 1   e
as the approximate value of j for which all advertisers are either out of budget
or do not bid on any of the remaining queries. Thus, the approximate revenue
An−1 An
B / ( n −2) B / ( n −1) B/n
 obtained by the Balance Algorithm is BN (1 − 1 ), that is, the queries of the e1
 first j rounds. Therefore, the competitive ratio is 1 − e , or approximately 0.63. 8.4.7 The Generalized Balance Algorithm
 The Balance Algorithm works well when all bids are 0 or 1. However, in practice, bids can be arbitrary, and with arbitrary bids and budgets Balance fails to weight the sizes of the bids properly. The following example illustrates the point.
Example 8.9: Suppose there are two advertisers A1 and A2, and one query q. The bids on q and budgets are:
Bidder Bid Budget A1 1 110 A2 10 100
If there are 10 occurrences of q, the optimum off-line algorithm will assign them all to A2 and gain revenue 100. However, because A1’s budget is larger, Balance will assign all ten queries to A1 for a revenue of 10. In fact, one can extend this idea easily to show that for situations like this one, there is no competitive ratio higher than 0 that holds for the Balance Algorithm. ✷
             
298 CHAPTER 8. ADVERTISING ON THE WEB
In order to make Balance work in more general situations, we need to make two modifications. First, we need to bias the choice of ad in favor of higher bids. Second, we need to be less absolute about the remaining budget. Rather, we consider the fraction of the budgets remaining, so we are biased toward using some of each advertiser’s budget. The latter change will make the Balance Algorithm more “risk averse”; it will not leave too much of any advertiser’s budget unused. It can be shown (see the chapter references) that the following generalization of the Balance Algorithm has a competitive ratio of 1 − 1/e = 0.63.
• Suppose that a query q arrives, advertiser Ai has bid xi for this query (note that xi could be 0). Also, suppose that fraction fi of the budget of Ai is currently unspent. Let Ψi = xi(1 − e−fi ). Then assign q to the advertiser Ai such that Ψi is a maximum. Break ties arbitrarily.
Example 8.10 : Consider how the generalized Balance Algorithm would work on the data of Example 8.9. For the first occurrence of query q,
Ψ1 =1×(1−e−1)
since A1 has bid 1, and fraction 1 of A1’s budget remains. That is,
Ψ1 =1−1/e=0.63
On the other hand, Ψ2 = 10 × (1 − e−1) = 6.3. Thus, the first q is awarded to A2.
The same thing happens for each of the q’s. That is, Ψ1 stays at 0.63, while Ψ2 decreases. However, it never goes below 0.63. Even for the 10th q, when 90% of A2’s budget has already been used, Ψ2 = 10 × (1 − e−1/10). Recall (Section 1.3.5) the Taylor expansion for ex = 1 + x + x2/2! + x3/3! + · · · . Thus,
e−1/10=1−1+1− 1 +··· 10 200 6000
or approximately, e−1/10 = 0.905. Thus, Ψ2 = 10 × 0.095 = 0.95. ✷
We leave unproved the assertion that the competitive ratio for this algorithm is 1 − 1/e. We also leave unproved an additional surprising fact: no on-line algorithm for the adwords problem as described in this section can have a competitive ratio above 1 − 1/e.
8.4.8 Final Observations About the Adwords Problem
The Balance Algorithm, as described, does not take into account the possibility that the click-through rate differs for different ads. It is simple to multiply the bid by the click-through rate when computing the Ψi’s, and doing so will maximize the expected revenue. We can even incorporate information about
   
8.5. ADWORDS IMPLEMENTATION 299
the click-through rate for each ad on each query for which a nonzero amount has been bid. When faced with the problem of allocating a particular query q, we incorporate a factor that is the click-through rate for that ad on query q, when computing each of the Ψ’s.
Another issue we must consider in practice is the historical frequency of queries. If, for example, we know that advertiser Ai has a budget sufficiently small that there are sure to be enough queries coming later in the month to satisfy Ai’s demand, then there is no point in boosting Ψi if some of Ai’s budget has already been expended. That is, maintain Ψi = xi(1 − e−1) as long as we can expect that there will be enough queries remaining in the month to give Ai its full budget of ads. This change can cause Balance to perform worse if the sequence of queries is governed by an adversary who can control the sequence of queries. Such an adversary can cause the queries Ai bid on suddenly to disappear. However, search engines get so many queries, and their generation is so random, that it is not necessary in practice to imagine significant deviation from the norm.
8.4.9 Exercises for Section 8.4
Exercise 8.4.1: Using the simplifying assumptions of Example 8.7, suppose that there are three advertisers, A, B, and C. There are three queries, x, y, and z. Each advertiser has a budget of 2. Advertiser A bids only on x; B bids on x and y, while C bids on x, y, and z. Note that on the query sequence xxyyzz, the optimum off-line algorithm would yield a revenue of 6, since all queries can be assigned.
! (a) Show that the greedy algorithm will assign at least 4 of these 6 queries.
!! (b) Find another sequence of queries such that the greedy algorithm can as- sign as few as half the queries that the optimum off-line algorithm assigns on that sequence.
!! Exercise 8.4.2: Extend the proof of Section 8.4.5 to the case where the two advertisers have unequal budgets.
! Exercise 8.4.3 : Show how to modify Example 8.9 by changing the bids and/or budgets to make the competitive ratio come out as close to 0 as you like.
8.5 Adwords Implementation
While we should now have an idea of how ads are selected to go with the answer to a search query, we have not addressed the problem of finding the bids that have been made on a given query. As long as bids are for the exact set of words in a query, the solution is relatively easy. However, there are a number of extensions to the query/bid matching process that are not as simple. We shall explain the details in this section.
300 CHAPTER 8. ADVERTISING ON THE WEB 8.5.1 Matching Bids and Search Queries
As we have described the adwords problem, and as it normally appears in practice, advertisers bid on sets of words. If a search query occurs having exactly that set of words in some order, then the bid is said to match the query, and it becomes a candidate for selection. We can avoid having to deal with word order by storing all sets of words representing a bid in lexicographic (alphabetic) order. The list of words in sorted order forms the hash-key for the bid, and these bids may be stored in a hash table used as an index, as discussed in Section 1.3.2.
Search queries also have their words sorted prior to lookup. When we hash the sorted list, we find in the hash table all the bids for exactly that set of words. They can be retrieved quickly, since we have only to look at the contents of that bucket.
Moreover, there is a good chance that we can keep the entire hash table in main memory. If there are a million advertisers, each bidding on 100 queries, and the record of the bid requires 100 bytes, then we require ten gigabytes of main memory, which is well within the limits of what is feasible for a single machine. If more space is required, we can split the buckets of the hash table among as many machines as we need. Search queries can be hashed and sent to the appropriate machine.
In practice, search queries may be arriving far too rapidly for a single ma- chine, or group of machines that collaborate on a single query at a time, to handle them all. In that case, the stream of queries is split into as many pieces as necessary, and each piece is handled by one group of machines. In fact, an- swering the search query, independent of ads, will require a group of machines working in parallel anyway, in order that the entire processing of a query can be done in main memory.
8.5.2 More Complex Matching Problems
However, the potential for matching bids to objects is not limited to the case where the objects are search queries and the match criterion is same-set-of- words. For example, Google also matches adwords bids to emails. There, the match criterion is not based on the equality of sets. Rather, a bid on a set of words S matches an email if all the words in S appear anywhere in the email.
This matching problem is much harder. We can still maintain a hash-table index for the bids, but the number of subsets of words in a hundred-word email is much too large to look up all the sets, or even all the small sets of (say) three or fewer words. There are a number of other potential applications of this sort of matching that, at the time of this writing, are not implemented but could be. They all involve standing queries – queries that users post to a site, expecting the site to notify them whenever something matching the query becomes available at the site. For example:
1. Twitter allows one to follow all the “tweets” of a given person. However,
8.5. ADWORDS IMPLEMENTATION 301
it is feasible to allow users to specify a set of words, such as
ipod free music
and see all the tweets where all these words appear, not necessarily in order, and not necessarily adjacent.
2. On-line news sites often allow users to select from among certain key- words or phrases, e.g., “healthcare” or “Barack Obama,” and receive alerts whenever a new news article contains that word or consecutive sequence of words. This problem is simpler than the email/adwords prob- lem for several reasons. Matching single words or consecutive sequences of words, even in a long article, is not as time-consuming as matching small sets of words. Further, the sets of terms that one can search for is limited, so there aren’t too many “bids.” Even if many people want alerts about the same term, only one index entry, with the list of all those people associated, is required. However, a more advanced system could allow users to specify alerts for sets of words in a news article, just as the Adwords system allows anyone to bid on a set of words in an email.
8.5.3 A Matching Algorithm for Documents and Bids
We shall offer an algorithm that will match many “bids” against many “docu- ments.” As before, a bid is a (typically small) set of words. A document is a larger set of words, such as an email, tweet, or news article. We assume there may be hundreds of documents per second arriving, although if there are that many, the document stream may be split among many machines or groups of machines. We assume there are many bids, perhaps on the order of a hundred million or a billion. As always, we want to do as much in main memory as we can.
We shall, as before, represent a bid by its words listed in some order. There are two new elements in the representation. First, we shall include a status with each list of words. The status is an integer indicating how many of the first words on the list have been matched by the current document. When a bid is stored in the index, its status is always 0.
Second, while the order of words could be lexicographic, we can lower the amount of work by ordering words rarest-first. However, since the number of different words that can appear in emails is essentially unlimited, it is not feasible to order all words in this manner. As a compromise, we might identify the n most common words on the Web or in a sample of the stream of documents we are processing. Here, n might be a hundred thousand or a million. These n words are sorted by frequency, and they occupy the end of the list, with the most frequent words at the very end. All words not among the n most frequent can be assumed equally infrequent and ordered lexicographically. Then, the words of any document can be ordered. If a word does not appear on the list of n frequent words, place it at the front of the order, lexicographically. Those
302 CHAPTER 8. ADVERTISING ON THE WEB
words in the document that do appear on the list of most frequent words appear after the infrequent words, in the reverse order of frequency (i.e., with the most frequent words of the documents ordered last).
Example 8.11 : Suppose our document is ’Twas brillig, and the slithy toves
“The” is the most frequent word in English, and “and” is only slightly less frequent. Let us suppose that “twas” makes the list of frequent words, although its frequency is surely lower than that of “the” or “and.” The other words do not make the list of frequent words.
Then the end of the list consists of “twas,” “and,” and “the,” in that order, since that is the inverse order of frequency. The other three words are placed at the front of the list in lexicographic order. Thus,
brillig slithy toves twas and the
is the sequence of words in the document, properly ordered. ✷
The bids are stored in a hash-table, whose hash key is the first word of the bid, in the order explained above. The record for the bid will also include information about what to do when the bid is matched. The status is 0 and need not be stored explicitly. There is another hash table, whose job is to contain copies of those bids that have been partially matched. These bids have a status that is at least 1, but less than the number of words in the set. If the status is i, then the hash-key for this hash table is the (i + 1)st word. The arrangement of hash tables is suggested by Fig. 8.5. To process a document, do the following.
1. Sort the words of the document in the order discussed above. Eliminate duplicate words.
2. For each word w, in the sorted order:
(i) Using w as the hash-key for the table of partially matched bids, find
those bids having w as key.
(ii) Foreachsuchbidb,ifwisthelastwordofb,movebtothetableof
matched bids.
(iii) If w is not the last word of b, add 1 to b’s status, and rehash b using the word whose position is one more than the new status, as the hash-key.
(iv) Using w as the hash key for the table of all bids, find those bids for which w is their first word in the sorted order.
(v) For each such bid b, if there is only one word on its list, copy it to the table of matched bids.
8.6. SUMMARY OF CHAPTER 8
303
 (d) Look up bids with
hash−key w
Word w
(a) Look up bids with
hash−key w
Hash−table index of bids
(f) Copy with status 1 if more than one word
(e) Copy to output if w
is the only word
     Hash on second word
    Partially matched bids
(b) Move to output if w
is the last word
     (c) Rehash w with one higher status if w
is not last
Figure 8.5: Managing large numbers of bids and large numbers of documents
(vi) If b consists of more than one word, add it, with status 1, to the table of partially matched bids, using the second word of b as the hash-key.
3. Produce the list of matched bids as the output.
The benefit of the rarest-first order should now be visible. A bid is only copied to the second hash table if its rarest word appears in the document. In comparison, if lexicographic order was used, more bids would be copied to the second hash table. By minimizing the size of that table, we not only reduce the amount of work in steps 2(i)–2(iii), but we make it more likely that this entire table can be kept in main memory.
8.6 Summary of Chapter 8
✦ Targeted Advertising: The big advantage that Web-based advertising has over advertising in conventional media such as newspapers is that Web advertising can be selected according to the interests of each individual user. This advantage has enabled many Web services to be supported entirely by advertising revenue.
✦ On- and Off-Line Algorithms: Conventional algorithms that are allowed to see all their data before producing an answer are called off-line. An on-
List of matched bids
304 CHAPTER 8. ADVERTISING ON THE WEB
line algorithm is required to make a response to each element in a stream immediately, with knowledge of only the past, not the future elements in the stream.
✦ Greedy Algorithms: Many on-line algorithms are greedy, in the sense that they select their action at every step by minimizing some objective func- tion.
✦ Competitive Ratio: We can measure the quality of an on-line algorithm by minimizing, over all possible inputs, the value of the result of the on- line algorithm compared with the value of the result of the best possible off-line algorithm.
✦ Bipartite Matching: This problem involves two sets of nodes and a set of edges between members of the two sets. The goal is to find a maximal matching – as large a set of edges as possible that includes no node more than once.
✦ On-Line Solution to the Matching Problem: One greedy algorithm for finding a match in a bipartite graph (or any graph, for that matter) is to order the edges in some way, and for each edge in turn, add it to the match if neither of its ends are yet part of an edge previously selected for the match. This algorithm can be proved to have a competitive ratio of 1/2; that is, it never fails to match at least half as many nodes as the best off-line algorithm matches.
✦ Search Ad Management: A search engine receives bids from advertisers on certain search queries. Some ads are displayed with each search query, and the search engine is paid the amount of the bid only if the queryer clicks on the ad. Each advertiser can give a budget, the total amount they are willing to pay for clicks in a month.
✦ The Adwords Problem: The data for the adwords problem is a set of bids by advertisers on certain search queries, together with a total budget for each advertiser and information about the historical click-through rate for each ad for each query. Another part of the data is the stream of search queries received by the search engine. The objective is to select on-line a fixed-size set of ads in response to each query that will maximize the revenue to the search engine.
✦ Simplified Adwords Problem: To see some of the nuances of ad selection, we considered a simplified version in which all bids are either 0 or 1, only one ad is shown with each query, and all advertisers have the same budget. Under this model the obvious greedy algorithm of giving the ad placement to anyone who has bid on the query and has budget remaining can be shown to have a competitive ratio of 1/2.
8.7. REFERENCES FOR CHAPTER 8 305
✦ The Balance Algorithm: This algorithm improves on the simple greedy algorithm. A query’s ad is given to the advertiser who has bid on the query and has the largest remaining budget. Ties can be broken arbitrarily.
✦ Competitive Ratio of the Balance Algorithm: For the simplified adwords model, the competitive ratio of the Balance Algorithm is 3/4 for the case of two advertisers and 1−1/e, or about 63% for any number of advertisers.
✦ The Balance Algorithm for the Generalized Adwords Problem: When bid- ders can make differing bids, have different budgets, and have different click-through rates for different queries, the Balance Algorithm awards an ad to the advertiser with the highest value of the function Ψ = x(1 − e−f ). Here, x is the product of the bid and the click-through rate for that ad- vertiser and query, and f is the fraction of the advertiser’s budget that remains unspent.
✦ Implementing an Adwords Algorithm: The simplest version of the imple- mentation serves in situations where the bids are on exactly the set of words in the search query. We can represent a query by the list of its words, in sorted order. Bids are stored in a hash table or similar struc- ture, with a hash key equal to the sorted list of words. A search query can then be matched against bids by a straightforward lookup in the table.
✦ Matching Word Sets Against Documents: A harder version of the ad- words-implementation problem allows bids, which are still small sets of words as in a search query, to be matched against larger documents, such as emails or tweets. A bid set matches the document if all the words appear in the document, in any order and not necessarily adjacent.
✦ Hash Storage of Word Sets: A useful data structure stores the words of each bid set in the order rarest-first. Documents have their words sorted in the same order. Word sets are stored in a hash table with the first word, in the rarest-first order, as the key.
✦ Processing Documents for Bid Matches: We process the words of the document rarest-first. Word sets whose first word is the current word are copied to a temporary hash table, with the second word as the key. Sets already in the temporary hash table are examined to see if the word that is their key matches the current word, and, if so, they are rehashed using their next word. Sets whose last word is matched are copied to the output.
8.7 References for Chapter 8
[1] is an investigation of the way ad position influences the click-through rate. The Balance Algorithm was developed in [2] and its application to the ad-
words problem is from [3].
306 CHAPTER 8. ADVERTISING ON THE WEB
1. N. Craswell, O. Zoeter, M. Taylor, and W. Ramsey, “An experimental comparison of click-position bias models,” Proc. Intl. Conf. on Web Search and Web Data Mining pp. 87–94, 2008.
2. B. Kalyanasundaram and K.R. Pruhs, “An optimal deterministic algo- rithm for b-matching,” Theoretical Computer Science 233:1–2, pp. 319– 325, 2000.
3. A Mehta, A. Saberi, U. Vazirani, and V. Vazirani, “Adwords and general- ized on-line matching,” IEEE Symp. on Foundations of Computer Science, pp. 264–273, 2005.
Chapter 9
Recommendation Systems
There is an extensive class of Web applications that involve predicting user responses to options. Such a facility is called a recommendation system. We shall begin this chapter with a survey of the most important examples of these systems. However, to bring the problem into focus, two good examples of recommendation systems are:
1. Offering news articles to on-line newspaper readers, based on a prediction of reader interests.
2. Offering customers of an on-line retailer suggestions about what they might like to buy, based on their past history of purchases and/or product searches.
Recommendation systems use a number of different technologies. We can classify these systems into two broad groups.
• Content-based systems examine properties of the items recommended. For instance, if a Netflix user has watched many cowboy movies, then recom- mend a movie classified in the database as having the “cowboy” genre.
• Collaborative filtering systems recommend items based on similarity mea- sures between users and/or items. The items recommended to a user are those preferred by similar users. This sort of recommendation system can use the groundwork laid in Chapter 3 on similarity search and Chapter 7 on clustering. However, these technologies by themselves are not suffi- cient, and there are some new algorithms that have proven effective for recommendation systems.
9.1 A Model for Recommendation Systems
In this section we introduce a model for recommendation systems, based on a utility matrix of preferences. We introduce the concept of a “long-tail,”
307
308 CHAPTER 9. RECOMMENDATION SYSTEMS
which explains the advantage of on-line vendors over conventional, brick-and- mortar vendors. We then briefly survey the sorts of applications in which recommendation systems have proved useful.
9.1.1 The Utility Matrix
In a recommendation-system application there are two classes of entities, which we shall refer to as users and items. Users have preferences for certain items, and these preferences must be teased out of the data. The data itself is repre- sented as a utility matrix, giving for each user-item pair, a value that represents what is known about the degree of preference of that user for that item. Values come from an ordered set, e.g., integers 1–5 representing the number of stars that the user gave as a rating for that item. We assume that the matrix is sparse, meaning that most entries are “unknown.” An unknown rating implies that we have no explicit information about the user’s preference for the item.
Example 9.1 : In Fig. 9.1 we see an example utility matrix, representing users’ ratings of movies on a 1–5 scale, with 5 the highest rating. Blanks represent the situation where the user has not rated the movie. The movie names are HP1, HP2, and HP3 for Harry Potter I, II, and III, TW for Twilight, and SW1, SW2, and SW3 for Star Wars episodes 1, 2, and 3. The users are represented by capital letters A through D.
HP1 HP2 HP3 TW SW1 SW2 SW3 A4 51 B554
C 245 D33
Figure 9.1: A utility matrix representing ratings of movies on a 1–5 scale
Notice that most user-movie pairs have blanks, meaning the user has not rated the movie. In practice, the matrix would be even sparser, with the typical user rating only a tiny fraction of all available movies. ✷
The goal of a recommendation system is to predict the blanks in the utility matrix. For example, would user A like SW2? There is little evidence from the tiny matrix in Fig. 9.1. We might design our recommendation system to take into account properties of movies, such as their producer, director, stars, or even the similarity of their names. If so, we might then note the similarity between SW1 and SW2, and then conclude that since A did not like SW1, they were unlikely to enjoy SW2 either. Alternatively, with much more data, we might observe that the people who rated both SW1 and SW2 tended to give them similar ratings. Thus, we could conclude that A would also give SW2 a low rating, similar to A’s rating of SW1.
      
9.1. A MODEL FOR RECOMMENDATION SYSTEMS 309
We should also be aware of a slightly different goal that makes sense in many applications. It is not necessary to predict every blank entry in a utility matrix. Rather, it is only necessary to discover some entries in each row that are likely to be high. In most applications, the recommendation system does not offer users a ranking of all items, but rather suggests a few that the user should value highly. It may not even be necessary to find all items with the highest expected ratings, but only to find a large subset of those with the highest ratings.
9.1.2 The Long Tail
Before discussing the principal applications of recommendation systems, let us ponder the long tail phenomenon that makes recommendation systems neces- sary. Physical delivery systems are characterized by a scarcity of resources. Brick-and-mortar stores have limited shelf space, and can show the customer only a small fraction of all the choices that exist. On the other hand, on-line stores can make anything that exists available to the customer. Thus, a physical bookstore may have several thousand books on its shelves, but Amazon offers millions of books. A physical newspaper can print several dozen articles per day, while on-line news services offer thousands per day.
Recommendation in the physical world is fairly simple. First, it is not possible to tailor the store to each individual customer. Thus, the choice of what is made available is governed only by the aggregate numbers. Typically, a bookstore will display only the books that are most popular, and a newspaper will print only the articles it believes the most people will be interested in. In the first case, sales figures govern the choices, in the second case, editorial judgement serves.
The distinction between the physical and on-line worlds has been called the long tail phenomenon, and it is suggested in Fig. 9.2. The vertical axis represents popularity (the number of times an item is chosen). The items are ordered on the horizontal axis according to their popularity. Physical institu- tions provide only the most popular items to the left of the vertical line, while the corresponding on-line institutions provide the entire range of items: the tail as well as the popular items.
The long-tail phenomenon forces on-line institutions to recommend items to individual users. It is not possible to present all available items to the user, the way physical institutions can. Neither can we expect users to have heard of each of the items they might like.
9.1.3 Applications of Recommendation Systems
We have mentioned several important applications of recommendation systems, but here we shall consolidate the list in a single place.
1. Product Recommendations: Perhaps the most important use of recom- mendation systems is at on-line retailers. We have noted how Amazon or similar on-line vendors strive to present each returning user with some
310 CHAPTER 9. RECOMMENDATION SYSTEMS
    The Long Tail
Figure 9.2: The long tail: physical institutions can only provide what is popular, while on-line institutions can make everything available
suggestions of products that they might like to buy. These suggestions are not random, but are based on the purchasing decisions made by similar customers or on other techniques we shall discuss in this chapter.
2. Movie Recommendations: Netflix offers its customers recommendations of movies they might like. These recommendations are based on ratings provided by users, much like the ratings suggested in the example utility matrix of Fig. 9.1. The importance of predicting ratings accurately is so high, that Netflix offered a prize of one million dollars for the first algorithm that could beat its own recommendation system by 10%.1 The prize was finally won in 2009, by a team of researchers called “Bellkor’s Pragmatic Chaos,” after over three years of competition.
3. News Articles: News services have attempted to identify articles of in- terest to readers, based on the articles that they have read in the past. The similarity might be based on the similarity of important words in the documents, or on the articles that are read by people with similar reading tastes. The same principles apply to recommending blogs from among the millions of blogs available, videos on YouTube, or other sites where content is provided regularly.
1To be exact, the algorithm had to have a root-mean-square error (RMSE) that was 10% less than the RMSE of the Netflix algorithm on a test set taken from actual ratings of Netflix users. To develop an algorithm, contestants were given a training set of data, also taken from actual Netflix data.
 
9.1. A MODEL FOR RECOMMENDATION SYSTEMS 311
   Into Thin Air and Touching the Void
An extreme example of how the long tail, together with a well designed recommendation system can influence events is the story told by Chris An- derson about a book called Touching the Void. This mountain-climbing book was not a big seller in its day, but many years after it was pub- lished, another book on the same topic, called Into Thin Air was pub- lished. Amazon’s recommendation system noticed a few people who bought both books, and started recommending Touching the Void to peo- ple who bought, or were considering, Into Thin Air. Had there been no on-line bookseller, Touching the Void might never have been seen by poten- tial buyers, but in the on-line world, Touching the Void eventually became very popular in its own right, in fact, more so than Into Thin Air.
 9.1.4 Populating the Utility Matrix
Without a utility matrix, it is almost impossible to recommend items. However, acquiring data from which to build a utility matrix is often difficult. There are two general approaches to discovering the value users place on items.
1. We can ask users to rate items. Movie ratings are generally obtained this way, and some on-line stores try to obtain ratings from their purchasers. Sites providing content, such as some news sites or YouTube also ask users to rate items. This approach is limited in its effectiveness, since generally users are unwilling to provide responses, and the information from those who do may be biased by the very fact that it comes from people willing to provide ratings.
2. We can make inferences from users’ behavior. Most obviously, if a user buys a product at Amazon, watches a movie on YouTube, or reads a news article, then the user can be said to “like” this item. Note that this sort of rating system really has only one value: 1 means that the user likes the item. Often, we find a utility matrix with this kind of data shown with 0’s rather than blanks where the user has not purchased or viewed the item. However, in this case 0 is not a lower rating than 1; it is no rating at all. More generally, one can infer interest from behavior other than purchasing. For example, if an Amazon customer views information about an item, we can infer that they are interested in the item, even if they don’t buy it.
312 CHAPTER 9. RECOMMENDATION SYSTEMS 9.2 Content-Based Recommendations
As we mentioned at the beginning of the chapter, there are two basic architec- tures for a recommendation system:
1. Content-Based systems focus on properties of items. Similarity of items is determined by measuring the similarity in their properties.
2. Collaborative-Filtering systems focus on the relationship between users and items. Similarity of items is determined by the similarity of the ratings of those items by the users who have rated both items.
In this section, we focus on content-based recommendation systems. The next section will cover collaborative filtering.
9.2.1 Item Profiles
In a content-based system, we must construct for each item a profile, which is a record or collection of records representing important characteristics of that item. In simple cases, the profile consists of some characteristics of the item that are easily discovered. For example, consider the features of a movie that might be relevant to a recommendation system.
1. The set of actors of the movie. Some viewers prefer movies with their favorite actors.
2. The director. Some viewers have a preference for the work of certain directors.
3. The year in which the movie was made. Some viewers prefer old movies; others watch only the latest releases.
4. The genre or general type of movie. Some viewers like only comedies, others dramas or romances.
There are many other features of movies that could be used as well. Except for the last, genre, the information is readily available from descriptions of movies. Genre is a vaguer concept. However, movie reviews generally assign a genre from a set of commonly used terms. For example the Internet Movie Database (IMDB) assigns a genre or genres to every movie. We shall discuss mechanical construction of genres in Section 9.3.3.
Many other classes of items also allow us to obtain features from available data, even if that data must at some point be entered by hand. For instance, products often have descriptions written by the manufacturer, giving features relevant to that class of product (e.g., the screen size and cabinet color for a TV). Books have descriptions similar to those for movies, so we can obtain features such as author, year of publication, and genre. Music products such as CD’s and MP3 downloads have available features such as artist, composer, and genre.
9.2. CONTENT-BASED RECOMMENDATIONS 313 9.2.2 Discovering Features of Documents
There are other classes of items where it is not immediately apparent what the values of features should be. We shall consider two of them: document collec- tions and images. Documents present special problems, and we shall discuss the technology for extracting features from documents in this section. Images will be discussed in Section 9.2.3 as an important example where user-supplied features have some hope of success.
There are many kinds of documents for which a recommendation system can be useful. For example, there are many news articles published each day, and we cannot read all of them. A recommendation system can suggest articles on topics a user is interested in, but how can we distinguish among topics? Web pages are also a collection of documents. Can we suggest pages a user might want to see? Likewise, blogs could be recommended to interested users, if we could classify blogs by topics.
Unfortunately, these classes of documents do not tend to have readily avail- able information giving features. A substitute that has been useful in practice is the identification of words that characterize the topic of a document. How we do the identification was outlined in Section 1.3.1. First, eliminate stop words – the several hundred most common words, which tend to say little about the topic of a document. For the remaining words, compute the TF.IDF score for each word in the document. The ones with the highest scores are the words that characterize the document.
We may then take as the features of a document the n words with the highest TF.IDF scores. It is possible to pick n to be the same for all documents, or to let n be a fixed percentage of the words in the document. We could also choose to make all words whose TF.IDF scores are above a given threshold to be a part of the feature set.
Now, documents are represented by sets of words. Intuitively, we expect these words to express the subjects or main ideas of the document. For example, in a news article, we would expect the words with the highest TF.IDF score to include the names of people discussed in the article, unusual properties of the event described, and the location of the event. To measure the similarity of two documents, there are several natural distance measures we can use:
1. We could use the Jaccard distance between the sets of words (recall Sec- tion 3.5.3).
2. We could use the cosine distance (recall Section 3.5.4) between the sets, treated as vectors.
To compute the cosine distance in option (2), think of the sets of high- TF.IDF words as a vector, with one component for each possible word. The vector has 1 if the word is in the set and 0 if not. Since between two docu- ments there are only a finite number of words among their two sets, the infinite dimensionality of the vectors is unimportant. Almost all components are 0 in
314 CHAPTER 9. RECOMMENDATION SYSTEMS
   Two Kinds of Document Similarity
Recall that in Section 3.4 we gave a method for finding documents that were “similar,” using shingling, minhashing, and LSH. There, the notion of similarity was lexical – documents are similar if they contain large, identical sequences of characters. For recommendation systems, the notion of similarity is different. We are interested only in the occurrences of many important words in both documents, even if there is little lexical similarity between the documents. However, the methodology for finding similar documents remains almost the same. Once we have a distance measure, either Jaccard or cosine, we can use minhashing (for Jaccard) or random hyperplanes (for cosine distance; see Section 3.7.2) feeding data to an LSH algorithm to find the pairs of documents that are similar in the sense of sharing many common keywords.
 both, and 0’s do not impact the value of the dot product. To be precise, the dot product is the size of the intersection of the two sets of words, and the lengths of the vectors are the square roots of the numbers of words in each set. That calculation lets us compute the cosine of the angle between the vectors as the dot product divided by the product of the vector lengths.
9.2.3 Obtaining Item Features From Tags
Let us consider a database of images as an example of a way that features have been obtained for items. The problem with images is that their data, typically an array of pixels, does not tell us anything useful about their features. We can calculate simple properties of pixels, such as the average amount of red in the picture, but few users are looking for red pictures or especially like red pictures.
There have been a number of attempts to obtain information about features of items by inviting users to tag the items by entering words or phrases that describe the item. Thus, one picture with a lot of red might be tagged “Tianan- men Square,” while another is tagged “sunset at Malibu.” The distinction is not something that could be discovered by existing image-analysis programs.
Almost any kind of data can have its features described by tags. One of the earliest attempts to tag massive amounts of data was the site del.icio.us, later bought by Yahoo!, which invited users to tag Web pages. The goal of this tagging was to make a new method of search available, where users entered a set of tags as their search query, and the system retrieved the Web pages that had been tagged that way. However, it is also possible to use the tags as a recommendation system. If it is observed that a user retrieves or bookmarks many pages with a certain set of tags, then we can recommend other pages with the same tags.
The problem with tagging as an approach to feature discovery is that the
9.2. CONTENT-BASED RECOMMENDATIONS 315
   Tags from Computer Games
An interesting direction for encouraging tagging is the “games” approach pioneered by Luis von Ahn. He enabled two players to collaborate on the tag for an image. In rounds, they would suggest a tag, and the tags would be exchanged. If they agreed, then they “won,” and if not, they would play another round with the same image, trying to agree simultaneously on a tag. While an innovative direction to try, it is questionable whether sufficient public interest can be generated to produce enough free work to satisfy the needs for tagged data.
 process only works if users are willing to take the trouble to create the tags, and there are enough tags that occasional erroneous ones will not bias the system too much.
9.2.4 Representing Item Profiles
Our ultimate goal for content-based recommendation is to create both an item profile consisting of feature-value pairs and a user profile summarizing the pref- erences of the user, based of their row of the utility matrix. In Section 9.2.2 we suggested how an item profile could be constructed. We imagined a vector of 0’s and 1’s, where a 1 represented the occurrence of a high-TF.IDF word in the document. Since features for documents were all words, it was easy to represent profiles this way.
We shall try to generalize this vector approach to all sorts of features. It is easy to do so for features that are sets of discrete values. For example, if one feature of movies is the set of actors, then imagine that there is a component for each actor, with 1 if the actor is in the movie, and 0 if not. Likewise, we can have a component for each possible director, and each possible genre. All these features can be represented using only 0’s and 1’s.
There is another class of features that is not readily represented by boolean vectors: those features that are numerical. For instance, we might take the average rating for movies to be a feature,2 and this average is a real number. It does not make sense to have one component for each of the possible average ratings, and doing so would cause us to lose the structure implicit in numbers. That is, two ratings that are close but not identical should be considered more similar than widely differing ratings. Likewise, numerical features of products, such as screen size or disk capacity for PC’s, should be considered similar if their values do not differ greatly.
Numerical features should be represented by single components of vectors representing items. These components hold the exact value of that feature.
2The rating is not a very reliable feature, but it will serve as an example.
 
316 CHAPTER 9. RECOMMENDATION SYSTEMS
There is no harm if some components of the vectors are boolean and others are real-valued or integer-valued. We can still compute the cosine distance between vectors, although if we do so, we should give some thought to the appropri- ate scaling of the nonboolean components, so that they neither dominate the calculation nor are they irrelevant.
Example 9.2 : Suppose the only features of movies are the set of actors and the average rating. Consider two movies with five actors each. Two of the actors are in both movies. Also, one movie has an average rating of 3 and the other an average of 4. The vectors look something like
0 1 1 0 1 1 0 1 3α 1 1 0 1 0 1 1 0 4α
However, there are in principle an infinite number of additional components, each with 0’s for both vectors, representing all the possible actors that neither movie has. Since cosine distance of vectors is not affected by components in which both vectors have 0, we need not worry about the effect of actors that are in neither movie.
The last component shown represents the average rating. We have shown it as having an unknown scaling factor α. In terms of α, we can compute the cosine of the angle between the vectors. The dot product is 2 + 12α2, and the lengths of the vectors are √5 + 9α2 and √5 + 16α2. Thus, the cosine of the angle between the vectors is
√ 2+12α2
25 + 125α2 + 144α4
If we choose α = 1, that is, we take the average ratings as they are, then the value of the above expression is 0.816. If we use α = 2, that is, we double the ratings, then the cosine is 0.940. That is, the vectors appear much closer in direction than if we use α = 1. Likewise, if we use α = 1/2, then the cosine is 0.619, making the vectors look quite different. We cannot tell which value of α is “right,” but we see that the choice of scaling factor for numerical features affects our decision about how similar items are. ✷
9.2.5 User Profiles
We not only need to create vectors describing items; we need to create vectors with the same components that describe the user’s preferences. We have the utility matrix representing the connection between users and items. Recall the nonblank matrix entries could be just 1’s representing user purchases or a similar connection, or they could be arbitrary numbers representing a rating or degree of affection that the the user has for the item.
With this information, the best estimate we can make regarding which items the user likes is some aggregation of the profiles of those items. If the utility matrix has only 1’s, then the natural aggregate is the average of the components
    
9.2. CONTENT-BASED RECOMMENDATIONS 317 of the vectors representing the item profiles for the items in which the utility
matrix has 1 for that user.
Example 9.3 : Suppose items are movies, represented by boolean profiles with components corresponding to actors. Also, the utility matrix has a 1 if the user has seen the movie and is blank otherwise. If 20% of the movies that user U likes have Julia Roberts as one of the actors, then the user profile for U will have 0.2 in the component for Julia Roberts. ✷
If the utility matrix is not boolean, e.g., ratings 1–5, then we can weight the vectors representing the profiles of items by the utility value. It makes sense to normalize the utilities by subtracting the average value for a user. That way, we get negative weights for items with a below-average rating, and positive weights for items with above-average ratings. That effect will prove useful when we discuss in Section 9.2.6 how to find items that a user should like.
Example 9.4 : Consider the same movie information as in Example 9.3, but now suppose the utility matrix has nonblank entries that are ratings in the 1–5 range. Suppose user U gives an average rating of 3. There are three movies with Julia Roberts as an actor, and those movies got ratings of 3, 4, and 5. Then in the user profile of U, the component for Julia Roberts will have value that is the average of 3−3, 4−3, and 5−3, that is, a value of 1.
On the other hand, user V gives an average rating of 4, and has also rated three movies with Julia Roberts (it doesn’t matter whether or not they are the same three movies U rated). User V gives these three movies ratings of 2, 3, and 5. The user profile for V has, in the component for Julia Roberts, the average of 2−4, 3−4, and 5−4, that is, the value −2/3. ✷
9.2.6 Recommending Items to Users Based on Content
With profile vectors for both users and items, we can estimate the degree to which a user would prefer an item by computing the cosine distance between the user’s and item’s vectors. As in Example 9.2, we may wish to scale var- ious components whose values are not boolean. The random-hyperplane and locality-sensitive-hashing techniques can be used to place (just) item profiles in buckets. In that way, given a user to whom we want to recommend some items, we can apply the same two techniques – random hyperplanes and LSH – to determine in which buckets we must look for items that might have a small cosine distance from the user.
Example 9.5 : Consider first the data of Example 9.3. The user’s profile will have components for actors proportional to the likelihood that the actor will appear in a movie the user likes. Thus, the highest recommendations (lowest cosine distance) belong to the movies with lots of actors that appear in many
318 CHAPTER 9. RECOMMENDATION SYSTEMS
of the movies the user likes. As long as actors are the only information we have about features of movies, that is probably the best we can do.3
Now, consider Example 9.4. There, we observed that the vector for a user will have positive numbers for actors that tend to appear in movies the user likes and negative numbers for actors that tend to appear in movies the user doesn’t like. Consider a movie with many actors the user likes, and only a few or none that the user doesn’t like. The cosine of the angle between the user’s and movie’s vectors will be a large positive fraction. That implies an angle close to 0, and therefore a small cosine distance between the vectors.
Next, consider a movie with about as many actors that the user likes as those the user doesn’t like. In this situation, the cosine of the angle between the user and movie is around 0, and therefore the angle between the two vectors is around 90 degrees. Finally, consider a movie with mostly actors the user doesn’t like. In that case, the cosine will be a large negative fraction, and the angle between the two vectors will be close to 180 degrees – the maximum possible cosine distance. ✷
9.2.7 Classification Algorithms
A completely different approach to a recommendation system using item profiles and utility matrices is to treat the problem as one of machine learning. Regard the given data as a training set, and for each user, build a classifier that predicts the rating of all items. There are a great number of different classifiers, and it is not our purpose to teach this subject here. However, you should be aware of the option of developing a classifier for recommendation, so we shall discuss one common classifier – decision trees – briefly.
A decision tree is a collection of nodes, arranged as a binary tree. The leaves render decisions; in our case, the decision would be “likes” or “doesn’t like.” Each interior node is a condition on the objects being classified; in our case the condition would be a predicate involving one or more features of an item.
To classify an item, we start at the root, and apply the predicate at the root to the item. If the predicate is true, go to the left child, and if it is false, go to the right child. Then repeat the same process at the node visited, until a leaf is reached. That leaf classifies the item as liked or not.
Construction of a decision tree requires selection of a predicate for each interior node. There are many ways of picking the best predicate, but they all try to arrange that one of the children gets all or most of the positive examples in the training set (i.e, the items that the given user likes, in our case) and the other child gets all or most of the negative examples (the items this user does not like).
3Note that the fact all user-vector components will be small fractions does not affect the recommendation, since the cosine calculation involves dividing by the length of each vector. That is, user vectors will tend to be much shorter than movie vectors, but only the direction of vectors matters.
 
9.2. CONTENT-BASED RECOMMENDATIONS 319
Once we have selected a predicate for a node N, we divide the items into the two groups: those that satisfy the predicate and those that do not. For each group, we again find the predicate that best separates the positive and negative examples in that group. These predicates are assigned to the children of N. This process of dividing the examples and building children can proceed to any number of levels. We can stop, and create a leaf, if the group of items for a node is homogeneous; i.e., they are all positive or all negative examples.
However, we may wish to stop and create a leaf with the majority decision for a group, even if the group contains both positive and negative examples. The reason is that the statistical significance of a small group may not be high enough to rely on. For that reason a variant strategy is to create an ensemble of decision trees, each using different predicates, but allow the trees to be deeper than what the available data justifies. Such trees are called overfitted. To classify an item, apply all the trees in the ensemble, and let them vote on the outcome. We shall not consider this option here, but give a simple hypothetical example of a decision tree.
Example 9.6 : Suppose our items are news articles, and features are the high- TF.IDF words (keywords) in those documents. Further suppose there is a user U who likes articles about baseball, except articles about the New York Yankees. The row of the utility matrix for U has 1 if U has read the article and is blank if not. We shall take the 1’s as “like” and the blanks as “doesn’t like.” Predicates will be boolean expressions of keywords.
Since U generally likes baseball, we might find that the best predicate for the root is “homerun” OR (“batter” AND “pitcher”). Items that satisfy the predicate will tend to be positive examples (articles with 1 in the row for U in the utility matrix), and items that fail to satisfy the predicate will tend to be negative examples (blanks in the utility-matrix row for U). Figure 9.3 shows the root as well as the rest of the decision tree.
Suppose that the group of articles that do not satisfy the predicate includes sufficiently few positive examples that we can conclude all of these items are in the “don’t-like” class. We may then put a leaf with decision “don’t like” as the right child of the root. However, the articles that satisfy the predicate includes a number of articles that user U doesn’t like; these are the articles that mention the Yankees. Thus, at the left child of the root, we build another predicate. We might find that the predicate “Yankees” OR “Jeter” OR “Teixeira” is the best possible indicator of an article about baseball and about the Yankees. Thus, we see in Fig. 9.3 the left child of the root, which applies this predicate. Both children of this node are leaves, since we may suppose that the items satisfying this predicate are predominantly negative and those not satisfying it are predominantly positive. ✷
Unfortunately, classifiers of all types tend to take a long time to construct. For instance, if we wish to use decision trees, we need one tree per user. Con- structing a tree not only requires that we look at all the item profiles, but we
320
CHAPTER 9. RECOMMENDATION SYSTEMS
"homerun" OR ("batter" AND "pitcher")
"Yankees" OR "Jeter" OR "Teixeira"
Figure 9.3: A decision tree
    Doesn’t Like
    Doesn’t Like
Likes
have to consider many different predicates, which could involve complex com- binations of features. Thus, this approach tends to be used only for relatively small problem sizes.
9.2.8 Exercises for Section 9.2
Exercise 9.2.1: Three computers, A, B, and C, have the numerical features listed below:
 Feature
Processor Speed Disk Size Main-Memory Size
A B C
3.06 2.68 2.92 500 320 640 6 4 6
    We may imagine these values as defining a vector for each computer; for in- stance, A’s vector is [3.06, 500, 6]. We can compute the cosine distance between any two of the vectors, but if we do not scale the components, then the disk size will dominate and make differences in the other components essentially in- visible. Let us use 1 as the scale factor for processor speed, α for the disk size, and β for the main memory size.
(a) In terms of α and β, compute the cosines of the angles between the vectors for each pair of the three computers.
(b) What are the angles between the vectors if α = β = 1?
(c) What are the angles between the vectors if α = 0.01 and β = 0.5?
9.3. COLLABORATIVE FILTERING 321
! (d) One fair way of selecting scale factors is to make each inversely propor- tional to the average value in its component. What would be the values of α and β, and what would be the angles between the vectors?
Exercise 9.2.2: An alternative way of scaling components of a vector is to begin by normalizing the vectors. That is, compute the average for each com- ponent and subtract it from that component’s value in each of the vectors.
(a) !! (b)
Normalize the vectors for the three computers described in Exercise 9.2.1.
This question does not require difficult calculation, but it requires some serious thought about what angles between vectors mean. When all com- ponents are nonnegative, as they are in the data of Exercise 9.2.1, no vectors can have an angle greater than 90 degrees. However, when we normalize vectors, we can (and must) get some negative components, so the angles can now be anything, that is, 0 to 180 degrees. Moreover, averages are now 0 in every component, so the suggestion in part (d) of Exercise 9.2.1 that we should scale in inverse proportion to the average makes no sense. Suggest a way of finding an appropriate scale for each component of normalized vectors. How would you interpret a large or small angle between normalized vectors? What would the angles be for the normalized vectors derived from the data in Exercise 9.2.1?
Exercise 9.2.3 : A certain user has rated the three computers of Exercise 9.2.1 as follows: A: 4 stars, B: 2 stars, C: 5 stars.
(a) (b)
9.3
Normalize the ratings for this user.
Compute a user profile for the user, with components for processor speed, disk size, and main memory size, based on the data of Exercise 9.2.1.
Collaborative Filtering
We shall now take up a significantly different approach to recommendation. Instead of using features of items to determine their similarity, we focus on the similarity of the user ratings for two items. That is, in place of the item-profile vector for an item, we use its column in the utility matrix. Further, instead of contriving a profile vector for users, we represent them by their rows in the utility matrix. Users are similar if their vectors are close according to some distance measure such as Jaccard or cosine distance. Recommendation for a user U is then made by looking at the users that are most similar to U in this sense, and recommending items that these users like. The process of identifying similar users and recommending what similar users like is called collaborative filtering.
322 CHAPTER 9. RECOMMENDATION SYSTEMS 9.3.1 Measuring Similarity
The first question we must deal with is how to measure similarity of users or items from their rows or columns in the utility matrix. We have reproduced Fig. 9.1 here as Fig. 9.4. This data is too small to draw any reliable conclusions, but its small size will make clear some of the pitfalls in picking a distance measure. Observe specifically the users A and C. They rated two movies in common, but they appear to have almost diametrically opposite opinions of these movies. We would expect that a good distance measure would make them rather far apart. Here are some alternative measures to consider.
HP1 HP2 HP3 TW SW1 SW2 SW3 A4 51 B554
C 245 D33
Figure 9.4: The utility matrix introduced in Fig. 9.1
Jaccard Distance
We could ignore values in the matrix and focus only on the sets of items rated. If the utility matrix only reflected purchases, this measure would be a good one to choose. However, when utilities are more detailed ratings, the Jaccard distance loses important information.
Example 9.7 : A and B have an intersection of size 1 and a union of size 5. Thus, their Jaccard similarity is 1/5, and their Jaccard distance is 4/5; i.e., they are very far apart. In comparison, A and C have a Jaccard similarity of 2/4, so their Jaccard distance is the same, 1/2. Thus, A appears closer to C than to B. Yet that conclusion seems intuitively wrong. A and C disagree on the two movies they both watched, while A and B seem both to have liked the one movie they watched in common. ✷
Cosine Distance
We can treat blanks as a 0 value. This choice is questionable, since it has the effect of treating the lack of a rating as more similar to disliking the movie than liking it.
Example 9.8 : The cosine of the angle between A and B is
4×5
√42 +52 +12√52 +52 +42 =0.380
         
9.3. COLLABORATIVE FILTERING 323 The cosine of the angle between A and C is
5×2+1×4
√42 +52 +12√22 +42 +52 =0.322
Since a larger (positive) cosine implies a smaller angle and therefore a smaller distance, this measure tells us that A is slightly closer to B than to C. ✷
Rounding the Data
We could try to eliminate the apparent similarity between movies a user rates highly and those with low scores by rounding the ratings. For instance, we could consider ratings of 3, 4, and 5 as a “1” and consider ratings 1 and 2 as unrated. The utility matrix would then look as in Fig. 9.5. Now, the Jaccard distance between A and B is 3/4, while between A and C it is 1; i.e., C appears further from A than B does, which is intuitively correct. Applying cosine distance to Fig. 9.5 allows us to draw the same conclusion.
HP1 HP2 HP3 TW SW1 SW2 SW3 A11
B111 C11 D11
Figure 9.5: Utilities of 3, 4, and 5 have been replaced by 1, while ratings of 1 and 2 are omitted
Normalizing Ratings
If we normalize ratings, by subtracting from each rating the average rating of that user, we turn low ratings into negative numbers and high ratings into positive numbers. If we then take the cosine distance, we find that users with opposite views of the movies they viewed in common will have vectors in almost opposite directions, and can be considered as far apart as possible. However, users with similar opinions about the movies rated in common will have a relatively small angle between them.
Example 9.9 : Figure 9.6 shows the matrix of Fig. 9.4 with all ratings nor- malized. An interesting effect is that D’s ratings have effectively disappeared, because a 0 is the same as a blank when cosine distance is computed. Note that D gave only 3’s and did not differentiate among movies, so it is quite possible that D’s opinions are not worth taking seriously.
Let us compute the cosine of the angle between A and B:
(2/3) × (1/3)
 (2/3)2 + (5/3)2 + (−7/3)2 (1/3)2 + (1/3)2 + (−2/3)2 = 0.092
            
324
CHAPTER 9. RECOMMENDATION SYSTEMS
HP1 HP2 HP3 TW SW1 SW2 SW3
5/3 −7/3
−5/3 1/3 4/3
   A 2/3
B 1/3
C D00
 −2/3
Figure 9.6: The utility matrix introduced in Fig. 9.1
1/3
  The cosine of the angle between between A and C is (5/3) × (−5/3) + (−7/3) × (1/3)
 (2/3)2 + (5/3)2 + (−7/3)2 (−5/3)2 + (1/3)2 + (4/3)2 = −0.559
Notice that under this measure, A and C are much further apart than A and B, and neither pair is very close. Both these observations make intuitive sense, given that A and C disagree on the two movies they rated in common, while A and B give similar scores to the one movie they rated in common. ✷
9.3.2 The Duality of Similarity
The utility matrix can be viewed as telling us about users or about items, or both. It is important to realize that any of the techniques we suggested in Section 9.3.1 for finding similar users can be used on columns of the utility matrix to find similar items. There are two ways in which the symmetry is broken in practice.
1. We can use information about users to recommend items. That is, given a user, we can find some number of the most similar users, perhaps using the techniques of Chapter 3. We can base our recommendation on the decisions made by these similar users, e.g., recommend the items that the greatest number of them have purchased or rated highly. However, there is no symmetry. Even if we find pairs of similar items, we need to take an additional step in order to recommend items to users. This point is explored further at the end of this subsection.
2. There is a difference in the typical behavior of users and items, as it pertains to similarity. Intuitively, items tend to be classifiable in simple terms. For example, music tends to belong to a single genre. It is impossi- ble, e.g., for a piece of music to be both 60’s rock and 1700’s baroque. On the other hand, there are individuals who like both 60’s rock and 1700’s baroque, and who buy examples of both types of music. The consequence is that it is easier to discover items that are similar because they belong to the same genre, than it is to detect that two users are similar because they prefer one genre in common, while each also likes some genres that the other doesn’t care for.
   
9.3. COLLABORATIVE FILTERING 325
As we suggested in (1) above, one way of predicting the value of the utility- matrix entry for user U and item I is to find the n users (for some predetermined n) most similar to U and average their ratings for item I, counting only those among the n similar users who have rated I. It is generally better to normalize the matrix first. That is, for each of the n users subtract their average rating for items from their rating for i. Average the difference for those users who have rated I, and then add this average to the average rating that U gives for all items. This normalization adjusts the estimate in the case that U tends to give very high or very low ratings, or a large fraction of the similar users who rated I (of which there may be only a few) are users who tend to rate very high or very low.
Dually, we can use item similarity to estimate the entry for user U and item I. Find the m items most similar to I, for some m, and take the average rating, among the m items, of the ratings that U has given. As for user-user similarity, we consider only those items among the m that U has rated, and it is probably wise to normalize item ratings first.
Note that whichever approach to estimating entries in the utility matrix we use, it is not sufficient to find only one entry. In order to recommend items to a user U, we need to estimate every entry in the row of the utility matrix for U, or at least find all or most of the entries in that row that are blank but have a high estimated value. There is a tradeoff regarding whether we should work from similar users or similar items.
• If we find similar users, then we only have to do the process once for user U. From the set of similar users we can estimate all the blanks in the utility matrix for U. If we work from similar items, we have to compute similar items for almost all items, before we can estimate the row for U.
• On the other hand, item-item similarity often provides more reliable in- formation, because of the phenomenon observed above, namely that it is easier to find items of the same genre than it is to find users that like only items of a single genre.
Whichever method we choose, we should precompute preferred items for each user, rather than waiting until we need to make a decision. Since the utility matrix evolves slowly, it is generally sufficient to compute it infrequently and assume that it remains fixed between recomputations.
9.3.3 Clustering Users and Items
It is hard to detect similarity among either items or users, because we have little information about user-item pairs in the sparse utility matrix. In the perspective of Section 9.3.2, even if two items belong to the same genre, there are likely to be very few users who bought or rated both. Likewise, even if two users both like a genre or genres, they may not have bought any items in common.
326 CHAPTER 9. RECOMMENDATION SYSTEMS
One way of dealing with this pitfall is to cluster items and/or users. Select any of the distance measures suggested in Section 9.3.1, or any other distance measure, and use it to perform a clustering of, say, items. Any of the methods suggested in Chapter 7 can be used. However, we shall see that there may be little reason to try to cluster into a small number of clusters immediately. Rather, a hierarchical approach, where we leave many clusters unmerged may suffice as a first step. For example, we might leave half as many clusters as there are items.
HP TW SW A451
B 4.67
C 2 4.5 D33
Figure 9.7: Utility matrix for users and clusters of items
Example 9.10 : Figure 9.7 shows what happens to the utility matrix of Fig. 9.4 if we manage to cluster the three Harry-Potter movies into one cluster, denoted HP, and also cluster the three Star-Wars movies into one cluster SW. ✷
Having clustered items to an extent, we can revise the utility matrix so the columns represent clusters of items, and the entry for user U and cluster C is the average rating that U gave to those members of cluster C that U did rate. Note that U may have rated none of the cluster members, in which case the entry for U and C is still blank.
We can use this revised utility matrix to cluster users, again using the dis- tance measure we consider most appropriate. Use a clustering algorithm that again leaves many clusters, e.g., half as many clusters as there are users. Re- vise the utility matrix, so the rows correspond to clusters of users, just as the columns correspond to clusters of items. As for item-clusters, compute the entry for a user cluster by averaging the ratings of the users in the cluster.
Now, this process can be repeated several times if we like. That is, we can cluster the item clusters and again merge the columns of the utility matrix that belong to one cluster. We can then turn to the users again, and cluster the user clusters. The process can repeat until we have an intuitively reasonable number of clusters of each kind.
Once we have clustered the users and/or items to the desired extent and computed the cluster-cluster utility matrix, we can estimate entries in the orig- inal utility matrix as follows. Suppose we want to predict the entry for user U and item I:
(a) Find the clusters to which U and I belong, say clusters C and D, respec- tively.
      
9.3. COLLABORATIVE FILTERING 327
(b) If the entry in the cluster-cluster utility matrix for C and D is something other than blank, use this value as the estimated value for the U–I entry in the original utility matrix.
(c) IftheentryforC–Disblank,thenusethemethodoutlinedinSection9.3.2 to estimate that entry by considering clusters similar to C or D. Use the resulting estimate as the estimate for the U-I entry.
9.3.4 Exercises for Section 9.3
abcdefgh A45 51 32 B 343121 C2 13 453
Figure 9.8: A utility matrix for exercises
Exercise 9.3.1: Figure 9.8 is a utility matrix, representing the ratings, on a 1–5 star scale, of eight items, a through h, by three users A, B, and C. Compute the following from the data of this matrix.
(a) Treating the utility matrix as boolean, compute the Jaccard distance be- tween each pair of users.
(b) Repeat Part (a), but use the cosine distance.
(c) Treat ratings of 3, 4, and 5 as 1 and 1, 2, and blank as 0. Compute the Jaccard distance between each pair of users.
(d) Repeat Part (c), but use the cosine distance.
(e) Normalizethematrixbysubtractingfromeachnonblankentrytheaverage value for its user.
(f) Using the normalized matrix from Part (e), compute the cosine distance between each pair of users.
Exercise 9.3.2: In this exercise, we cluster items in the matrix of Fig. 9.8. Do the following steps.
(a) Cluster the eight items hierarchically into four clusters. The following method should be used to cluster. Replace all 3’s, 4’s, and 5’s by 1 and replace 1’s, 2’s, and blanks by 0. use the Jaccard distance to measure the distance between the resulting column vectors. For clusters of more than one element, take the distance between clusters to be the minimum distance between pairs of elements, one from each cluster.
     
328 (b)
(c)
9.4
CHAPTER 9. RECOMMENDATION SYSTEMS
Then, construct from the original matrix of Fig. 9.8 a new matrix whose rows correspond to users, as before, and whose columns correspond to clusters. Compute the entry for a user and cluster of items by averaging the nonblank entries for that user and all the items in the cluster.
Compute the cosine distance between each pair of users, according to your matrix from Part (b).
Dimensionality Reduction
An entirely different approach to estimating the blank entries in the utility matrix is to conjecture that the utility matrix is actually the product of two long, thin matrices. This view makes sense if there are a relatively small set of features of items and users that determine the reaction of most users to most items. In this section, we sketch one approach to discovering two such matrices; the approach is called “UV-decomposition,” and it is an instance of a more general theory called SVD (singular-value decomposition).
9.4.1 UV-Decomposition
Consider movies as a case in point. Most users respond to a small number of features; they like certain genres, they may have certain famous actors or actresses that they like, and perhaps there are a few directors with a significant following. If we start with the utility matrix M, with n rows and m columns (i.e., there are n users and m items), then we might be able to find a matrix U with n rows and d columns and a matrix V with d rows and m columns, such that UV closely approximates M in those entries where M is nonblank. If so, then we have established that there are d dimensions that allow us to characterize both users and items closely. We can then use the entry in the product UV to estimate the corresponding blank entry in utility matrix M. This process is called UV-decomposition of M.
 5  3
3   1

   v11 v12 v13 v14 v15   
 2  2
4= 5
   2  4  4
   1  2  4
      3  1
   5  4  3
4  4  5  4
u11 u12 u21 u22 u31 u32 u41 u42 u51 u52
 × v21 v22 v23 v24 v25 Figure 9.9: UV-decomposition of matrix M
Example 9.11 : We shall use as a running example a 5-by-5 matrix M with all but two of its entries known. We wish to decompose M into a 5-by-2 and 2-by-5 matrix, U and V, respectively. The matrices M, U, and V are shown in Fig. 9.9 with the known entries of M indicated and the matrices U and
9.4. DIMENSIONALITY REDUCTION 329
V shown with their entries as variables to be determined. This example is essentially the smallest nontrivial case where there are more known entries than there are entries in U and V combined, and we therefore can expect that the best decomposition will not yield a product that agrees exactly in the nonblank entries of M. ✷
9.4.2 Root-Mean-Square Error
While we can pick among several measures of how close the product UV is to M, the typical choice is the root-mean-square error (RMSE), where we
1. Sum, over all nonblank entries in M the square of the difference between that entry and the corresponding entry in the product U V .
2. Take the mean (average) of these squares by dividing by the number of terms in the sum (i.e., the number of nonblank entries in M).
3. Take the square root of the mean.
Minimizing the sum of the squares is the same as minimizing the square root of the average square, so we generally omit the last two steps in our running example.
11 22222 11 11111 22222
11× 11111 =22222  1 1   2 2 2 2 2 
11 22222 Figure 9.10: Matrices U and V with all entries 1
Example 9.12 : Suppose we guess that U and V should each have entries that are all 1’s, as shown in Fig. 9.10. This is a poor guess, since the product, consisting of all 2’s, has entries that are much below the average of the entries in M. Nonetheless, we can compute the RMSE for this U and V; in fact the regularity in the entries makes the calculation especially easy to follow. Consider the first rows of M and U V . We subtract 2 (each entry in U V ) from the entries in the first row of M, to get 3,0,2,2,1. We square and sum these to get 18. In the second row, we do the same to get 1, −1, 0, 2, −1, square and sum to get 7. In the third row, the second column is blank, so that entry is ignored when computing the RMSE. The differences are 0, 1, −1, 2 and the sum of squares is 6. For the fourth row, the differences are 0, 3, 2, 1, 3 and the sum of squares is 23. The fifth row has a blank entry in the last column, so the differences are 2, 2, 3, 2 and the sum of squares is 21. When we sum the sums fromeachofthefiverows,weget18+7+6+23+21=75. Generally,weshall
330 CHAPTER 9. RECOMMENDATION SYSTEMS
stop at this point, but if we want to compute the true RMSE, we divide by 23 (the number of nonblank entries in M) and take the square root. In this case  75/23 = 1.806 is the RMSE. ✷
9.4.3 Incremental Computation of a UV-Decomposition
Finding the UV-decomposition with the least RMSE involves starting with some arbitrarily chosen U and V , and repeatedly adjusting U and V to make the RMSE smaller. We shall consider only adjustments to a single element of U or V , although in principle, one could make more complex adjustments. Whatever adjustments we allow, in a typical example there will be many lo- cal minima – matrices U and V such that no allowable adjustment reduces the RMSE. Unfortunately, only one of these local minima will be the global minimum – the matrices U and V that produce the least possible RMSE. To increase our chances of finding the global minimum, we need to pick many dif- ferent starting points, that is, different choices of the initial matrices U and V . However, there is never a guarantee that our best local minimum will be the global minimum.
We shall start with the U and V of Fig. 9.10, where all entries are 1, and do a few adjustments to some of the entries, finding the values of those entries that give the largest possible improvement to the RMSE. From these examples, the general calculation should become obvious, but we shall follow the examples by the formula for minimizing the RMSE by changing a single entry. In what follows, we shall refer to entries of U and V by their variable names u11, and so on, as given in Fig. 9.9.
Example 9.13 : Suppose we start with U and V as in Fig. 9.10, and we decide to alter u11 to reduce the RMSE as much as possible. Let the value of u11 be x. Then the new U and V can be expressed as in Fig. 9.11.
x 1 x+1 x+1 x+1 x+1 x+1 11 11111 2 2 2 2 2 
1 1× 1 1 1 1 1 =2 2 2 2 2 11 2 2 2 2 2 
11 22222
Figure 9.11: Making u11 a variable
Notice that the only entries of the product that have changed are those in the first row. Thus, when we compare UV with M, the only change to the RMSE comes from the first row. The contribution to the sum of squares from the first row is
 5−(x+1) 2 + 2−(x+1) 2 + 4−(x+1) 2 + 4−(x+1) 2 + 3−(x+1) 2
 
9.4. DIMENSIONALITY REDUCTION 331
This sum simplifies to
(4−x)2 +(1−x)2 +(3−x)2 +(3−x)2 +(2−x)2
We want the value of x that minimizes the sum, so we take the derivative and set that equal to 0, as:
−2 ×  (4 − x) + (1 − x) + (3 − x) + (3 − x) + (2 − x)  = 0 or −2 × (13 − 5x) = 0, from which it follows that x = 2.6.
2.6 1 3.6 3.6 3.6 3.6 3.6 1 1 11111 2 2 2 2 2
1 1× 1 1 1 1 1 =2 2 2 2 2   1 1   2 2 2 2 2 
11 22222 Figure 9.12: The best value for u11 is found to be 2.6
Figure 9.12 shows U and V after u11 has been set to 2.6. Note that the sum of the squares of the errors in the first row has been reduced from 18 to 5.2, so the total RMSE (ignoring average and square root) has been reduced from 75 to 62.2.
2.6 1 2.6y+1 3.6 3.6 3.6 3.6 1 1 y1111 y+1 2 2 2 2 1 1×11111=y+1 2 222  1 1   y + 1 2 2 2 2 
11 y+12222 Figure 9.13: v11 becomes a variable y
Suppose our next entry to vary is v11. Let the value of this entry be y, as suggested in Fig. 9.13. Only the first column of the product is affected by y, so we need only to compute the sum of the squares of the differences between the entries in the first columns of M and U V . This sum is
 5−(2.6y+1) 2 + 3−(y+1) 2 + 2−(y+1) 2 + 2−(y+1) 2 + 4−(y+1) 2 This expression simplifies to
(4−2.6y)2 +(2−y)2 +(1−y)2 +(1−y)2 +(3−y)2
As before, we find the minimum value of this expression by differentiating and
equating to 0, as:
−2 ×  2.6(4 − 2.6y) + (2 − y) + (1 − y) + (1 − y) + (3 − y)  = 0
332 CHAPTER 9. RECOMMENDATION SYSTEMS
 2.6 1 z 1
1
1   5.204
1  1.617 1 1 1 1  2.617 1× 1 1 1 1 1 =1.617z+1 1 2.617
1 2.617
3.6 3.6 22 z+1 z+1 2 2
2 2
3.6 3.6  22 z+1 z+1 2 2  2 2
2.6 1  1  1
1
1 5.204 3.6 1 1.6171111 2.6172 1×1 1111=2.6172
1   2 . 6 1 7 2
1 2.617 2 Figure 9.14: Replace y by 1.617
3.6 3.6 2 2
2 2
2 2
2 2
3.6 2 2  2 
2
The solution for y is y = 17.4/10.76 = 1.617. The improved estimates of U and V are shown in Fig. 9.14.
We shall do one more change, to illustrate what happens when entries of M are blank. We shall vary u31, calling it z temporarily. The new U and V are shown in Fig. 9.15. The value of z affects only the entries in the third row.
Figure 9.15: u31 becomes a variable z We can express the sum of the squares of the errors as
 2−(1.617z+1) 2 + 3−(z+1) 2 + 1−(z+1) 2 + 4−(z+1) 2 Note that there is no contribution from the element in the second column of
the third row, since this element is blank in M. The expression simplifies to (1−1.617z)2 +(2−z)2 +(−z)2 +(3−z)2
The usual process of setting the derivative to 0 gives us
−2 ×  1.617(1 − 1.617z) + (2 − z) + (−z) + (3 − z)  = 0
whose solution is z = 6.617/5.615 = 1.178. The next estimate of the decompo- sition UV is shown in Fig. 9.16. ✷
9.4.4 Optimizing an Arbitrary Element
Having seen some examples of picking the optimum value for a single element in the matrix U or V , let us now develop the general formula. As before, assume
9.4.
 2.6 1
 1.178 1
333
3.6  2
2.178  2 
2
DIMENSIONALITY REDUCTION
1 5.204 3.6
1    1.617 1 1 1 1    2.617 2
1 × 1 1 1 1 1 = 2.905 2.178 1 2.6172
3.6 3.6
2 2 2.178 2.178 2 2
2 2
1 1 2.6172
Figure 9.16: Replace z by 1.178
that M is an n-by-m utility matrix with some entries blank, while U and V are matrices of dimensions n-by-d and d-by-m, for some d. We shall use mij, uij, and vij for the entries in row i and column j of M, U, and V , respectively. Also, let P = UV, and use pij for the element in row i and column j of the product matrix P.
Suppose we want to vary urs and find the value of this element that mini- mizes the RMSE between M and U V . Note that urs affects only the elements in row r of the product P = UV. Thus, we need only concern ourselves with the elements
d
prj = urkvkj = urkvkj +xvsj
k=1 k̸=s
for all values of j such that mrj is nonblank. In the expression above, we have replaced urs, the element we wish to vary, by a variable x, and we use the convention
•  k̸=s is shorthand for the sum for k = 1,2,...,d, except for k = s.
If mrj is a nonblank entry of the matrix M, then the contribution of this
element to the sum of the squares of the errors is
(mrj −prj)2 = mrj − urkvkj −xvsj 2
k̸=s
We shall use another convention:
•  j is shorthand for the sum over all j such that mrj is nonblank.
Then we can write the sum of the squares of the errors that are affected by the value of x = urs as
  mrj − urkvkj −xvsj 2 j k̸=s
Take the derivative of the above with respect to x, and set it equal to 0, in order to find the value of x that minimizes the RMSE. That is,
 −2vsj mrj − urkvkj −xvsj =0 j k̸=s
334 CHAPTER 9. RECOMMENDATION SYSTEMS As in the previous examples, the common factor −2 can be dropped. We solve
the above equation for x, and get
 j vsj mrj − k̸=s urkvkj 
x =   j v s2 j
There is an analogous formula for the optimum value of an element of V . If
 we want to vary vrs = y, then the value of y that minimizes the RMSE is  i uir mis −  k̸=r uikvks 
y =   i u 2i r
Here,  i is shorthand for the sum over all i such that mis is nonblank, and
  k̸=r is the sum over all values of k between 1 and d, except for k = r. 9.4.5 Building a Complete UV-Decomposition Algorithm
Now, we have the tools to search for the global optimum decomposition of a utility matrix M. There are four areas where we shall discuss the options.
1. Preprocessing of the matrix M.
2. Initializing U and V .
3. Ordering the optimization of the elements of U and V . 4. Ending the attempt at optimization.
Preprocessing
Because the differences in the quality of items and the rating scales of users are such important factors in determining the missing elements of the matrix M, it is often useful to remove these influences before doing anything else. The idea was introduced in Section 9.3.1. We can subtract from each nonblank element mij the average rating of user i. Then, the resulting matrix can be modified by subtracting the average rating (in the modified matrix) of item j. It is also possible to first subtract the average rating of item j and then subtract the average rating of user i in the modified matrix. The results one obtains from doing things in these two different orders need not be the same, but will tend to be close. A third option is to normalize by subtracting from mij the average of the average rating of user i and item j, that is, subtracting one half the sum of the user average and the item average.
If we choose to normalize M, then when we make predictions, we need to undo the normalization. That is, if whatever prediction method we use results in estimate e for an element mij of the normalized matrix, then the value we predict for mij in the true utility matrix is e plus whatever amount was subtracted from row i and from column j during the normalization process.
9.4. DIMENSIONALITY REDUCTION 335 Initialization
As we mentioned, it is essential that there be some randomness in the way we seek an optimum solution, because the existence of many local minima justifies our running many different optimizations in the hope of reaching the global minimum on at least one run. We can vary the initial values of U and V , or we can vary the way we seek the optimum (to be discussed next), or both.
A simple starting point for U and V is to give each element the same value, and a good choice for this value is that which gives the elements of the product U V the average of the nonblank elements of M . Note that if we have normalized M, then this value will necessarily be 0. If we have chosen d as the lengths of the short sides of U and V , and a is the average nonblank element of M , then the elements of U and V should be  a/d.
If we want many starting points for U and V , then we can perturb the value  a/d randomly and independently for each of the elements. There are many options for how we do the perturbation. We have a choice regarding the distribution of the difference. For example we could add to each element a normally distributed value with mean 0 and some chosen standard deviation. Or we could add a value uniformly chosen from the range −c to +c for some c.
Performing the Optimization
In order to reach a local minimum from a given starting value of U and V , we need to pick an order in which we visit the elements of U and V . The simplest thing to do is pick an order, e.g., row-by-row, for the elements of U and V , and visit them in round-robin fashion. Note that just because we optimized an element once does not mean we cannot find a better value for that element after other elements have been adjusted. Thus, we need to visit elements repeatedly, until we have reason to believe that no further improvements are possible.
Alternatively, we can follow many different optimization paths from a single starting value by randomly picking the element to optimize. To make sure that every element is considered in each round, we could instead choose a permuta- tion of the elements and follow that order for every round.
Converging to a Minimum
Ideally, at some point the RMSE becomes 0, and we know we cannot do better. In practice, since there are normally many more nonblank elements in M than there are elements in U and V together, we have no right to expect that we can reduce the RMSE to 0. Thus, we have to detect when there is little benefit to be had in revisiting elements of U and/or V . We can track the amount of improvement in the RMSE obtained in one round of the optimization, and stop when that improvement falls below a threshold. A small variation is to observe the improvements resulting from the optimization of individual elements, and stop when the maximum improvement during a round is below a threshold.
  
336 CHAPTER 9. RECOMMENDATION SYSTEMS
   Gradient Descent
The technique for finding a UV-decomposition discussed in Section 9.4 is an example of gradient descent. We are given some data points – the nonblank elements of the matrix M – and for each data point we find the direction of change that most decreases the error function: the RMSE between the current UV product and M. We shall have much more to say about gradient descent in Section 12.3.4. It is also worth noting that while we have described the method as visiting each nonblank point of M several times until we approach a minimum-error decomposition, that may well be too much work on a large matrix M. Thus, an alternative approach has us look at only a randomly chosen fraction of the data when seeking to minimize the error. This approach, called stochastic gradient descent is discussed in Section 12.3.5.
 Avoiding Overfitting
One problem that often arises when performing a UV-decomposition is that we arrive at one of the many local minima that conform well to the given data, but picks up values in the data that don’t reflect well the underlying process that gives rise to the data. That is, although the RMSE may be small on the given data, it doesn’t do well predicting future data. There are several things that can be done to cope with this problem, which is called overfitting by statisticians.
1. Avoid favoring the first components to be optimized by only moving the value of a component a fraction of the way, say half way, from its current value toward its optimized value.
2. Stop revisiting elements of U and V well before the process has converged.
3. Take several different UV decompositions, and when predicting a new entry in the matrix M, take the average of the results of using each decomposition.
9.4.6 Exercises for Section 9.4
Exercise 9.4.1 : Starting with the decomposition of Fig. 9.10, we may choose any of the 20 entries in U or V to optimize first. Perform this first optimization step assuming we choose: (a) u32 (b) v41.
Exercise 9.4.2: If we wish to start out, as in Fig. 9.10, with all U and V entries set to the same value, what value minimizes the RMSE for the matrix M of our running example?
9.5. THE NETFLIX CHALLENGE 337 Exercise 9.4.3: Starting with the U and V matrices in Fig. 9.16, do the
following in order:
(a) Reconsider the value of u11. Find its new best value, given the changes that have been made so far.
(b) Then choose the best value for u52.
(c) Then choose the best value for v22.
Exercise 9.4.4: Derive the formula for y (the optimum value of element vrs given at the end of Section 9.4.4.
Exercise 9.4.5 : Normalize the matrix M of our running example by:
(a) First subtracting from each element the average of its row, and then
subtracting from each element the average of its (modified) column.
(b) First subtracting from each element the average of its column, and then subtracting from each element the average of its (modified) row.
Are there any differences in the results of (a) and (b)?
9.5 The NetFlix Challenge
A significant boost to research into recommendation systems was given when NetFlix offered a prize of $1,000,000 to the first person or team to beat their own recommendation algorithm, called CineMatch, by 10%. After over three years of work, the prize was awarded in September, 2009.
The NetFlix challenge consisted of a published dataset, giving the ratings by approximately half a million users on (typically small subsets of) approximately 17,000 movies. This data was selected from a larger dataset, and proposed al- gorithms were tested on their ability to predict the ratings in a secret remainder of the larger dataset. The information for each (user, movie) pair in the pub- lished dataset included a rating (1–5 stars) and the date on which the rating was made.
The RMSE was used to measure the performance of algorithms. CineMatch has an RMSE of approximately 0.95; i.e., the typical rating would be off by almost one full star. To win the prize, it was necessary that your algorithm have an RMSE that was at most 90% of the RMSE of CineMatch.
The bibliographic notes for this chapter include references to descriptions of the winning algorithms. Here, we mention some interesting and perhaps unintuitive facts about the challenge.
• CineMatch was not a very good algorithm. In fact, it was discovered early that the obvious algorithm of predicting, for the rating by user u on movie m, the average of:
338
• •
•
•
CHAPTER 9. RECOMMENDATION SYSTEMS
1. The average rating given by u on all rated movies and
2. The average of the ratings for movie m by all users who rated that
movie.
was only 3% worse than CineMatch.
The UV-decomposition algorithm described in Section 9.4 was found by three students (Michael Harris, Jeffrey Wang, and David Kamm) to give a 7% improvement over CineMatch, when coupled with normalization and a few other tricks.
The winning entry was actually a combination of several different algo- rithms that had been developed independently. A second team, which submitted an entry that would have won, had it been submitted a few minutes earlier, also was a blend of independent algorithms. This strat- egy – combining different algorithms – has been used before in a number of hard problems and is something worth remembering.
Several attempts have been made to use the data contained in IMDB, the Internet movie database, to match the names of movies from the NetFlix challenge with their names in IMDB, and thus extract useful information not contained in the NetFlix data itself. IMDB has information about actors and directors, and classifies movies into one or more of 28 genres. It was found that genre and other information was not useful. One pos- sible reason is the machine-learning algorithms were able to discover the relevant information anyway, and a second is that the entity resolution problem of matching movie names as given in NetFlix and IMDB data is not that easy to solve exactly.
Time of rating turned out to be useful. It appears there are movies that are more likely to be appreciated by people who rate it immediately after viewing than by those who wait a while and then rate it. “Patch Adams” was given as an example of such a movie. Conversely, there are other movies that were not liked by those who rated it immediately, but were better appreciated after a while; “Memento” was cited as an example. While one cannot tease out of the data information about how long was the delay between viewing and rating, it is generally safe to assume that most people see a movie shortly after it comes out. Thus, one can examine the ratings of any movie to see if its ratings have an upward or downward slope with time.
Summary of Chapter 9
Utility Matrices: Recommendation systems deal with users and items. A utility matrix offers known information about the degree to which a user likes an item. Normally, most entries are unknown, and the essential
9.6
✦
9.6. SUMMARY OF CHAPTER 9 339 problem of recommending items to users is predicting the values of the
unknown entries based on the values of the known entries.
✦ Two Classes of Recommendation Systems: These systems attempt to pre- dict a user’s response to an item by discovering similar items and the response of the user to those. One class of recommendation system is content-based; it measures similarity by looking for common features of the items. A second class of recommendation system uses collaborative fil- tering; these measure similarity of users by their item preferences and/or measure similarity of items by the users who like them.
✦ Item Profiles: These consist of features of items. Different kinds of items have different features on which content-based similarity can be based. Features of documents are typically important or unusual words. Prod- ucts have attributes such as screen size for a television. Media such as movies have a genre and details such as actor or performer. Tags can also be used as features if they can be acquired from interested users.
✦ User Profiles: A content-based collaborative filtering system can con- struct profiles for users by measuring the frequency with which features appear in the items the user likes. We can then estimate the degree to which a user will like an item by the closeness of the item’s profile to the user’s profile.
✦ Classification of Items: An alternative to constructing a user profile is to build a classifier for each user, e.g., a decision tree. The row of the utility matrix for that user becomes the training data, and the classifier must predict the response of the user to all items, whether or not the row had an entry for that item.
✦ Similarity of Rows and Columns of the Utility Matrix: Collaborative fil- tering algorithms must measure the similarity of rows and/or columns of the utility matrix. Jaccard distance is appropriate when the matrix consists only of 1’s and blanks (for “not rated”). Cosine distance works for more general values in the utility matrix. It is often useful to normal- ize the utility matrix by subtracting the average value (either by row, by column, or both) before measuring the cosine distance.
✦ Clustering Users and Items: Since the utility matrix tends to be mostly blanks, distance measures such as Jaccard or cosine often have too little data with which to compare two rows or two columns. A preliminary step or steps, in which similarity is used to cluster users and/or items into small groups with strong similarity, can help provide more common components with which to compare rows or columns.
✦ UV-Decomposition: One way of predicting the blank values in a utility matrix is to find two long, thin matrices U and V , whose product is an approximation to the given utility matrix. Since the matrix product UV
340
✦
✦
✦
9.7
CHAPTER 9. RECOMMENDATION SYSTEMS
gives values for all user-item pairs, that value can be used to predict the value of a blank in the utility matrix. The intuitive reason this method makes sense is that often there are a relatively small number of issues (that number is the “thin” dimension of U and V ) that determine whether or not a user likes an item.
Root-Mean-Square Error: A good measure of how close the product UV is to the given utility matrix is the RMSE (root-mean-square error). The RMSE is computed by averaging the square of the differences between UV and the utility matrix, in those elements where the utility matrix is nonblank. The square root of this average is the RMSE.
Computing U and V: One way of finding a good choice for U and V in a UV-decomposition is to start with arbitrary matrices U and V . Repeat- edly adjust one of the elements of U or V to minimize the RMSE between the product UV and the given utility matrix. The process converges to a local optimum, although to have a good chance of obtaining a global optimum we must either repeat the process from many starting matrices, or search from the starting point in many different ways.
The NetFlix Challenge: An important driver of research into recommen- dation systems was the NetFlix challenge. A prize of $1,000,000 was offered for a contestant who could produce an algorithm that was 10% better than NetFlix’s own algorithm at predicting movie ratings by users. The prize was awarded in Sept., 2009.
References for Chapter 9
[1] is
ing the importance of the long tail in on-line systems is from [2], which was expanded into a book [3].
a survey of recommendation systems as of 2005. The argument regard-
[8] discusses the use of computer games to extract tags for items.
See [5] for a discussion of item-item similarity and how Amazon designed its collaborative-filtering algorithm for product recommendations.
There are three papers describing the three algorithms that, in combination, won the NetFlix challenge. They are [4], [6], and [7].
1. G. Adomavicius and A. Tuzhilin, “Towards the next generation of rec- ommender systems: a survey of the state-of-the-art and possible exten- sions,” IEEE Trans. on Data and Knowledge Engineering 17:6, pp. 734– 749, 2005.
2. C. Anderson,
http://www.wired.com/wired/archive/12.10/tail.html
9.7. REFERENCES FOR CHAPTER 9 341 2004.
3. C. Anderson, The Long Tail: Why the Future of Business is Selling Less of More, Hyperion Books, New York, 2006.
4. Y. Koren, “The BellKor solution to the Netflix grand prize,”
www.netflixprize.com/assets/GrandPrize2009 BPC BellKor.pdf
2009.
5. G. Linden, B. Smith, and J. York, “Amazon.com recommendations: item-
to-item collaborative filtering,” Internet Computing 7:1, pp. 76–80, 2003.
6. M. Piotte and M. Chabbert, ”The Pragmatic Theory solution to the Net-
flix grand prize,”
www.netflixprize.com/assets/ GrandPrize2009 BPC PragmaticTheory.pdf
2009.
7. A. Toscher, M. Jahrer, and R. Bell, “The BigChaos solution to the Netflix grand prize,”
www.netflixprize.com/assets/GrandPrize2009 BPC BigChaos.pdf
2009.
8. L. von Ahn, “Games with a purpose,” IEEE Computer Magazine, pp. 96– 98, June 2006.
      
342 CHAPTER 9. RECOMMENDATION SYSTEMS
Chapter 10
Mining Social-Network Graphs
There is much information to be gained by analyzing the large-scale data that is derived from social networks. The best-known example of a social network is the “friends” relation found on sites like Facebook. However, as we shall see there are many other sources of data that connect people or other entities.
In this chapter, we shall study techniques for analyzing such networks. An important question about a social network is how to identify “communities,” that is, subsets of the nodes (people or other entities that form the network) with unusually strong connections. Some of the techniques used to identify communities are similar to the clustering algorithms we discussed in Chapter 7. However, communities almost never partition the set of nodes in a network. Rather, communities usually overlap. For example, you may belong to several communities of friends or classmates. The people from one community tend to know each other, but people from two different communities rarely know each other. You would not want to be assigned to only one of the communities, nor would it make sense to cluster all the people from all your communities into one cluster.
Also in this chapter we explore efficient algorithms for discovering other properties of graphs. We look at “simrank,” a way to discover similarities among nodes of a graph. We explore triangle counting as a way to measure the connectedness of a community. We give efficient algorithms for exact and ap- proximate measurement of the neighborhood sizes of nodes in a graph. Finally, we look at efficient algorithms for computing the transitive closure.
10.1 Social Networks as Graphs
We begin our discussion of social networks by introducing a graph model. Not every graph is a suitable representation of what we intuitively regard as a social
343
344 CHAPTER 10. MINING SOCIAL-NETWORK GRAPHS
network. We therefore discuss the idea of “locality,” the property of social networks that says nodes and edges of the graph tend to cluster in communities. This section also looks at some of the kinds of social networks that occur in practice.
10.1.1 What is a Social Network?
When we think of a social network, we think of Facebook, Twitter, Google+, or another website that is called a “social network,” and indeed this kind of network is representative of the broader class of networks called “social.” The essential characteristics of a social network are:
1. There is a collection of entities that participate in the network. Typically, these entities are people, but they could be something else entirely. We shall discuss some other examples in Section 10.1.3.
2. There is at least one relationship between entities of the network. On Facebook or its ilk, this relationship is called friends. Sometimes the relationship is all-or-nothing; two people are either friends or they are not. However, in other examples of social networks, the relationship has a degree. This degree could be discrete; e.g., friends, family, acquaintances, or none as in Google+. It could be a real number; an example would be the fraction of the average day that two people spend talking to each other.
3. There is an assumption of nonrandomness or locality. This condition is the hardest to formalize, but the intuition is that relationships tend to cluster. That is, if entity A is related to both B and C, then there is a higher probability than average that B and C are related.
10.1.2 Social Networks as Graphs
Social networks are naturally modeled as graphs, which we sometimes refer to as a social graph. The entities are the nodes, and an edge connects two nodes if the nodes are related by the relationship that characterizes the network. If there is a degree associated with the relationship, this degree is represented by labeling the edges. Often, social graphs are undirected, as for the Facebook friends graph. But they can be directed graphs, as for example the graphs of followers on Twitter or Google+.
Example 10.1: Figure 10.1 is an example of a tiny social network. The entities are the nodes A through G. The relationship, which we might think of as “friends,” is represented by the edges. For instance, B is friends with A, C, and D.
Is this graph really typical of a social network, in the sense that it exhibits locality of relationships? First, note that the graph has nine edges out of the
10.1. SOCIAL NETWORKS AS GRAPHS 345 ABDE
C
GF
Figure 10.1: Example of a small social network
 72  = 21 pairs of nodes that could have had an edge between them. Suppose X, Y, and Z are nodes of Fig. 10.1, with edges between X and Y and also between X and Z. What would we expect the probability of an edge between Y and Z to be? If the graph were large, that probability would be very close to the fraction of the pairs of nodes that have edges between them, i.e., 9/21 = .429 in this case. However, because the graph is small, there is a noticeable difference between the true probability and the ratio of the number of edges to the number of pairs of nodes. Since we already know there are edges (X, Y ) and (X, Z), there are only seven edges remaining. Those seven edges could run between any of the 19 remaining pairs of nodes. Thus, the probability of an edge (Y, Z) is 7/19 = .368.
Now, we must compute the probability that the edge (Y,Z) exists in Fig. 10.1, given that edges (X, Y ) and (X, Z) exist. What we shall actually count is pairs of nodes that could be Y and Z, without worrying about which node isY andwhichisZ. IfXisA,thenY andZmustbeBandC,insome order. Since the edge (B, C) exists, A contributes one positive example (where the edge does exist) and no negative examples (where the edge is absent). The cases where X is C, E, or G are essentially the same. In each case, X has only two neighbors, and the edge between the neighbors exists. Thus, we have seen four positive examples and zero negative examples so far.
Now, consider X = F . F has three neighbors, D, E, and G. There are edges between two of the three pairs of neighbors, but no edge between G and E. Thus, we see two more positive examples and we see our first negative example. If X = B, there are again three neighbors, but only one pair of neighbors, A and C, has an edge. Thus, we have two more negative examples, and one positive example, for a total of seven positive and three negative. Finally, when X = D, there are four neighbors. Of the six pairs of neighbors, only two have edges between them.
Thus, the total number of positive examples is nine and the total number of negative examples is seven. We see that in Fig. 10.1, the fraction of times the third edge exists is thus 9/16 = .563. This fraction is considerably greater than the .368 expected value for that fraction. We conclude that Fig. 10.1 does indeed exhibit the locality expected in a social network. ✷
                
346 CHAPTER 10. MINING SOCIAL-NETWORK GRAPHS 10.1.3 Varieties of Social Networks
There are many examples of social networks other than “friends” networks. Here, let us enumerate some of the other examples of networks that also exhibit locality of relationships.
Telephone Networks
Here the nodes represent phone numbers, which are really individuals. There is an edge between two nodes if a call has been placed between those phones in some fixed period of time, such as last month, or “ever.” The edges could be weighted by the number of calls made between these phones during the period. Communities in a telephone network will form from groups of people that communicate frequently: groups of friends, members of a club, or people working at the same company, for example.
Email Networks
The nodes represent email addresses, which are again individuals. An edge represents the fact that there was at least one email in at least one direction between the two addresses. Alternatively, we may only place an edge if there were emails in both directions. In that way, we avoid viewing spammers as “friends” with all their victims. Another approach is to label edges as weak or strong. Strong edges represent communication in both directions, while weak edges indicate that the communication was in one direction only. The com- munities seen in email networks come from the same sorts of groupings we mentioned in connection with telephone networks. A similar sort of network involves people who text other people through their cell phones.
Collaboration Networks
Nodes represent individuals who have published research papers. There is an edge between two individuals who published one or more papers jointly. Option- ally, we can label edges by the number of joint publications. The communities in this network are authors working on a particular topic.
An alternative view of the same data is as a graph in which the nodes are papers. Two papers are connected by an edge if they have at least one author in common. Now, we form communities that are collections of papers on the same topic.
There are several other kinds of data that form two networks in a similar way. For example, we can look at the people who edit Wikipedia articles and the articles that they edit. Two editors are connected if they have edited an article in common. The communities are groups of editors that are interested in the same subject. Dually, we can build a network of articles, and connect articles if they have been edited by the same person. Here, we get communities of articles on similar or related subjects.
10.1. SOCIAL NETWORKS AS GRAPHS 347
In fact, the data involved in Collaborative filtering, as was discussed in Chapter 9, often can be viewed as forming a pair of networks, one for the customers and one for the products. Customers who buy the same sorts of products, e.g., science-fiction books, will form communities, and dually, prod- ucts that are bought by the same customers will form communities, e.g., all science-fiction books.
Other Examples of Social Graphs
Many other phenomena give rise to graphs that look something like social graphs, especially exhibiting locality. Examples include: information networks (documents, web graphs, patents), infrastructure networks (roads, planes, water pipes, powergrids), biological networks (genes, proteins, food-webs of animals eating each other), as well as other types, like product co-purchasing networks (e.g., Groupon).
10.1.4 Graphs With Several Node Types
There are other social phenomena that involve entities of different types. We just discussed under the heading of “collaboration networks,” several kinds of graphs that are really formed from two types of nodes. Authorship networks can be seen to have author nodes and paper nodes. In the discussion above, we built two social networks by eliminating the nodes of one of the two types, but we do not have to do that. We can rather think of the structure as a whole.
For a more complex example, users at a site like deli.cio.us place tags on Web pages. There are thus three different kinds of entities: users, tags, and pages. We might think that users were somehow connected if they tended to use the same tags frequently, or if they tended to tag the same pages. Similarly, tags could be considered related if they appeared on the same pages or were used by the same users, and pages could be considered similar if they had many of the same tags or were tagged by many of the same users.
The natural way to represent such information is as a k-partite graph for some k > 1. We met bipartite graphs, the case k = 2, in Section 8.3. In general, a k-partite graph consists of k disjoint sets of nodes, with no edges between nodes of the same set.
Example 10.2 : Figure 10.2 is an example of a tripartite graph (the case k = 3 of a k-partite graph). There are three sets of nodes, which we may think of as users {U1,U2}, tags {T1,T2,T3,T4}, and Web pages {W1,W2,W3}. Notice that all edges connect nodes from two different sets. We may assume this graph represents information about the three kinds of entities. For example, the edge (U1,T2) means that user U1 has placed the tag T2 on at least one page. Note that the graph does not tell us a detail that could be important: who placed which tag on which page? To represent such ternary information would require a more complex representation, such as a database relation with three columns corresponding to users, tags, and pages. ✷
348 CHAPTER 10. MINING SOCIAL-NETWORK GRAPHS
T1 W1 U1 T2 W2 U2 T3 W3
T4
Figure 10.2: A tripartite graph representing users, tags, and Web pages
10.1.5 Exercises for Section 10.1
Exercise 10.1.1: It is possible to think of the edges of one graph G as the nodes of another graph G′. We construct G′ from G by the dual construction:
          1.
2.
(a) (b)
!(c) !! (d) ! (e)
If (X, Y ) is an edge of G, then X Y , representing the unordered set of X and Y is a node of G′. Note that XY and Y X represent the same node of G′, not two different nodes.
If (X,Y) and (X,Z) are edges of G, then in G′ there is an edge between XY and XZ. That is, nodes of G′ have an edge between them if the edges of G that these nodes represent have a node (of G) in common.
If we apply the dual construction to a network of friends, what is the interpretation of the edges of the resulting graph?
Apply the dual construction to the graph of Fig. 10.1. HowisthedegreeofanodeXY inG′ relatedtothedegreesofXandY
in G?
The number of edges of G′ is related to the degrees of the nodes of G by
a certain formula. Discover that formula.
What we called the dual is not a true dual, because applying the con- struction to G′ does not necessarily yield a graph isomorphic to G. Give an example graph G where the dual of G′ is isomorphic to G and another example where the dual of G′ is not isomorphic to G.
10.2. CLUSTERING OF SOCIAL-NETWORK GRAPHS 349 10.2 Clustering of Social-Network Graphs
An important aspect of social networks is that they contain communities of entities that are connected by many edges. These typically correspond to groups of friends at school or groups of researchers interested in the same topic, for example. In this section, we shall consider clustering of the graph as a way to identify communities. It turns out that the techniques we learned in Chapter 7 are generally unsuitable for the problem of clustering social-network graphs.
10.2.1 Distance Measures for Social-Network Graphs
If we were to apply standard clustering techniques to a social-network graph, our first step would be to define a distance measure. When the edges of the graph have labels, these labels might be usable as a distance measure, depending on what they represented. But when the edges are unlabeled, as in a “friends” graph, there is not much we can do to define a suitable distance.
Our first instinct is to assume that nodes are close if they have an edge between them and distant if not. Thus, we could say that the distance d(x, y) is 0 if there is an edge (x,y) and 1 if there is no such edge. We could use any other two values, such as 1 and ∞, as long as the distance is closer when there is an edge.
Neither of these two-valued “distance measures” – 0 and 1 or 1 and ∞ – is a true distance measure. The reason is that they violate the triangle inequality when there are three nodes, with two edges between them. That is, if there are edges (A,B) and (B,C), but no edge (A,C), then the distance from A to C exceeds the sum of the distances from A to B to C. We could fix this problem by using, say, distance 1 for an edge and distance 1.5 for a missing edge. But the problem with two-valued distance functions is not limited to the triangle inequality, as we shall see in the next section.
10.2.2 Applying Standard Clustering Methods
Recall from Section 7.1.2 that there are two general approaches to clustering: hierarchical (agglomerative) and point-assignment. Let us consider how each of these would work on a social-network graph. First, consider the hierarchical methods covered in Section 7.2. In particular, suppose we use as the intercluster distance the minimum distance between nodes of the two clusters.
Hierarchical clustering of a social-network graph starts by combining some two nodes that are connected by an edge. Successively, edges that are not between two nodes of the same cluster would be chosen randomly to combine the clusters to which their two nodes belong. The choices would be random, because all distances represented by an edge are the same.
Example 10.3 : Consider again the graph of Fig. 10.1, repeated here as Fig. 10.3. First, let us agree on what the communities are. At the highest level,
350 CHAPTER 10. MINING SOCIAL-NETWORK GRAPHS
it appears that there are two communities {A, B, C} and {D, E, F, G}. How- ever, we could also view {D,E,F} and {D,F,G} as two subcommunities of {D, E, F, G}; these two subcommunities overlap in two of their members, and thus could never be identified by a pure clustering algorithm. Finally, we could consider each pair of individuals that are connected by an edge as a community of size 2, although such communities are uninteresting.
ABDE
C
GF
Figure 10.3: Repeat of Fig. 10.1
The problem with hierarchical clustering of a graph like that of Fig. 10.3 is that at some point we are likely to chose to combine B and D, even though they surely belong in different clusters. The reason we are likely to combine B and D is that D, and any cluster containing it, is as close to B and any cluster containing it, as A and C are to B. There is even a 1/9 probability that the first thing we do is to combine B and D into one cluster.
There are things we can do to reduce the probability of error. We can run hierarchical clustering several times and pick the run that gives the most coherent clusters. We can use a more sophisticated method for measuring the distance between clusters of more than one node, as discussed in Section 7.2.3. But no matter what we do, in a large graph with many communities there is a significant chance that in the initial phases we shall use some edges that connect two nodes that do not belong together in any large community. ✷
Now, consider a point-assignment approach to clustering social networks. Again, the fact that all edges are at the same distance will introduce a number of random factors that will lead to some nodes being assigned to the wrong cluster. An example should illustrate the point.
Example 10.4: Suppose we try a k-means approach to clustering Fig. 10.3. As we want two clusters, we pick k = 2. If we pick two starting nodes at random, they might both be in the same cluster. If, as suggested in Section 7.3.2, we start with one randomly chosen node and then pick another as far away as possible, we don’t do much better; we could thereby pick any pair of nodes not connected by an edge, e.g., E and G in Fig. 10.3.
However, suppose we do get two suitable starting nodes, such as B and F. We shall then assign A and C to the cluster of B and assign E and G to the clusterofF. ButDisasclosetoBasitistoF,soitcouldgoeitherway,even though it is “obvious” that D belongs with F.
                
10.2. CLUSTERING OF SOCIAL-NETWORK GRAPHS 351
If the decision about where to place D is deferred until we have assigned some other nodes to the clusters, then we shall probably make the right decision. For instance, if we assign a node to the cluster with the shortest average distance to all the nodes of the cluster, then D should be assigned to the cluster of F , as long as we do not try to place D before any other nodes are assigned. However, in large graphs, we shall surely make mistakes on some of the first nodes we place. ✷
10.2.3 Betweenness
Since there are problems with standard clustering methods, several specialized clustering techniques have been developed to find communities in social net- works. In this section we shall consider one of the simplest, based on finding the edges that are least likely to be inside a community.
Define the betweenness of an edge (a, b) to be the number of pairs of nodes x and y such that the edge (a,b) lies on the shortest path between x and y. To be more precise, since there can be several shortest paths between x and y, edge (a, b) is credited with the fraction of those shortest paths that include the edge (a, b). As in golf, a high score is bad. It suggests that the edge (a, b) runs between two different communities; that is, a and b do not belong to the same community.
Example 10.5: In Fig. 10.3 the edge (B,D) has the highest betweenness, as should surprise no one. In fact, this edge is on every shortest path between any of A, B, and C to any of D, E, F, and G. Its betweenness is therefore 3 × 4 = 12. In contrast, the edge (D, F ) is on only four shortest paths: those fromA,B,C,andDtoF. ✷
10.2.4 The Girvan-Newman Algorithm
In order to exploit the betweenness of edges, we need to calculate the number of shortest paths going through each edge. We shall describe a method called the Girvan-Newman (GN) Algorithm, which visits each node X once and computes the number of shortest paths from X to each of the other nodes that go through each of the edges. The algorithm begins by performing a breadth-first search (BFS) of the graph, starting at the node X. Note that the level of each node in the BFS presentation is the length of the shortest path from X to that node. Thus, the edges that go between nodes at the same level can never be part of a shortest path from X.
Edges between levels are called DAG edges (“DAG” stands for directed, acyclic graph). Each DAG edge will be part of at least one shortest path from root X. If there is a DAG edge (Y,Z), where Y is at the level above Z (i.e., closer to the root), then we shall call Y a parent of Z and Z a child of Y , although parents are not necessarily unique in a DAG as they would be in a tree.
352
CHAPTER 10.
MINING SOCIAL-NETWORK GRAPHS
     Level 1
Level 2
Level 3
1
E
11 DF
B G 2
11 AC
    1
       Figure 10.4: Step 1 of the Girvan-Newman Algorithm
Example 10.6 : Figure 10.4 is a breadth-first presentation of the graph of Fig. 10.3, starting at node E. Solid edges are DAG edges and dashed edges connect nodes at the same level. ✷
The second step of the GN algorithm is to label each node by the number of shortest paths that reach it from the root. Start by labeling the root 1. Then, from the top down, label each node Y by the sum of the labels of its parents.
Example 10.7 : In Fig. 10.4 are the labels for each of the nodes. First, label therootEwith1. Atlevel1arethenodesDandF. EachhasonlyEasa parent, so they too are labeled 1. Nodes B and G are at level 2. B has only D as a parent, so B’s label is the same as the label of D, which is 1. However, G has parents D and F, so its label is the sum of their labels, or 2. Finally, at level 3, A and C each have only parent B, so their labels are the label of B, which is 1. ✷
The third and final step is to calculate for each edge e the sum over all nodes Y of the fraction of shortest paths from the root X to Y that go through e. This calculation involves computing this sum for both nodes and edges, from the bottom. Each node other than the root is given a credit of 1, representing the shortest path to that node. This credit may be divided among nodes and edges above, since there could be several different shortest paths to the node. The rules for the calculation are as follows:
1. Each leaf in the DAG (a leaf is a node with no DAG edges to nodes at levels below) gets a credit of 1.
2. Each node that is not a leaf gets a credit equal to 1 plus the sum of the credits of the DAG edges from that node to the level below.
10.2.
3.
CLUSTERING OF SOCIAL-NETWORK GRAPHS 353
A DAG edge e entering node Z from the level above is given a share of the credit of Z proportional to the fraction of shortest paths from the root to Z that go through e. Formally, let the parents of Z be Y1,Y2,...,Yk. Let pi be the number of shortest paths from the root to Yi; this number was computed in Step 2 and is illustrated by the labels in Fig. 10.4. Then the credit for the edge (Yi,Z) is the credit of Z times pi divided by  kj=1 pj.
After performing the credit calculation with each node as the root, we sum the credits for each edge. Then, since each shortest path will have been discov- ered twice – once when each of its endpoints is the root – we must divide the credit for each edge by 2.
Example 10.8 : Let us perform the credit calculation for the BFS presentation of Fig. 10.4. We shall start from level 3 and proceed upwards. First, A and C, being leaves, get credit 1. Each of these nodes have only one parent, so their credit is given to the edges (B, A) and (B, C), respectively.
E
DF
3B G1 11
1A C1
Figure 10.5: Final step of the Girvan-Newman Algorithm – levels 3 and 2
Atlevel2,Gisaleaf,soitgetscredit1. Bisnotaleaf,soitgetscredit equal to 1 plus the credits on the DAG edges entering it from below. Since both these edges have credit 1, the credit of B is 3. Intuitively 3 represents the fact that all shortest paths from E to A, B, and C go through B. Figure 10.5 shows the credits assigned so far.
Now, let us proceed to level 1. B has only one parent, D, so the edge (D, B) gets the entire credit of B, which is 3. However, G has two parents, D and F . We therefore need to divide the credit of 1 that G has between the edges (D, G) and (F, G). In what proportion do we divide? If you examine the labels of Fig. 10.4, you see that both D and F have label 1, representing the fact that there is one shortest path from E to each of these nodes. Thus, we give half the credit of G to each of these edges; i.e., their credit is each 1/(1 + 1) = 0.5. Had the labels of D and F in Fig. 10.4 been 5 and 3, meaning there were five
                
354 CHAPTER 10. MINING SOCIAL-NETWORK GRAPHS shortest paths to D and only three to F , then the credit of edge (D, G) would
have been 5/8 and the credit of edge (F, G) would have been 3/8.
E
4.5 1.5
     4.5 D 3
F 1.5 0.5 0.5
      3B G1 11
1A C1
Figure 10.6: Final step of the Girvan-Newman Algorithm – completing the credit calculation
Now, we can assign credits to the nodes at level 1. D gets 1 plus the credits of the edges entering it from below, which are 3 and 0.5. That is, the credit of D is 4.5. The credit of F is 1 plus the credit of the edge (F, G), or 1.5. Finally, the edges (E, D) and (E, F ) receive the credit of D and F , respectively, since each of these nodes has only one parent. These credits are all shown in Fig. 10.6.
The credit on each of the edges in Fig. 10.6 is the contribution to the be- tweenness of that edge due to shortest paths from E. For example, this contri- bution for the edge (E, D) is 4.5. ✷
To complete the betweenness calculation, we have to repeat this calculation for every node as the root and sum the contributions. Finally, we must divide by 2 to get the true betweenness, since every shortest path will be discovered twice, once for each of its endpoints.
10.2.5 Using Betweenness to Find Communities
The betweenness scores for the edges of a graph behave something like a distance measure on the nodes of the graph. It is not exactly a distance measure, because it is not defined for pairs of nodes that are unconnected by an edge, and might not satisfy the triangle inequality even when defined. However, we can cluster by taking the edges in order of increasing betweenness and add them to the graph one at a time. At each step, the connected components of the graph form some clusters. The higher the betweenness we allow, the more edges we get, and the larger the clusters become.
More commonly, this idea is expressed as a process of edge removal. Start with the graph and all its edges; then remove edges with the highest between-
     
10.2. CLUSTERING OF SOCIAL-NETWORK GRAPHS 355 ness, until the graph has broken into a suitable number of connected compo-
nents.
Example 10.9 : Let us start with our running example, the graph of Fig. 10.1. We see it with the betweenness for each edge in Fig. 10.7. The calculation of the betweenness will be left to the reader. The only tricky part of the count is to observe that between E and G there are two shortest paths, one going through D and the other through F. Thus, each of the edges (D,E), (E,F), (D, G), and (G, F ) are credited with half a shortest path.
5 12 4.5 ABDE
154 4.5
C
GF 1.5
Figure 10.7: Betweenness scores for the graph of Fig. 10.1
Clearly, edge (B,D) has the highest betweenness, so it is removed first. That leaves us with exactly the communities we observed make the most sense, namely: {A, B, C} and {D, E, F, G}. However, we can continue to remove edges. Next to leave are (A, B) and (B, C) with a score of 5, followed by (D, E) and (D, G) with a score of 4.5. Then, (D, F ), whose score is 4, would leave the graph. We see in Fig. 10.8 the graph that remains.
ABDE
C
GF
Figure 10.8: All the edges with betweenness 4 or more have been removed
The “communities” of Fig. 10.8 look strange. One implication is that A and C are more closely knit to each other than to B. That is, in some sense B is a “traitor” to the community {A,B,C} because he has a friend D outside that community. Likewise, D can be seen as a “traitor” to the group {D, E, F, G}, which is why in Fig. 10.8, only E, F, and G remain connected. ✷
            1.5
              
356 CHAPTER 10. MINING SOCIAL-NETWORK GRAPHS
   Speeding Up the Betweenness Calculation
If we apply the method of Section 10.2.4 to a graph of n nodes and e edges, it takes O(ne) running time to compute the betweenness of each edge. That is, BFS from a single node takes O(e) time, as do the two labeling steps. We must start from each node, so there are n of the computations described in Section 10.2.4.
If the graph is large – and even a million nodes is large when the algorithm takes O(ne) time – we cannot afford to execute it as suggested. However, if we pick a subset of the nodes at random and use these as the roots of breadth-first searches, we can get an approximation to the betweenness of each edge that will serve in most applications.
 10.2.6 Exercises for Section 10.2
Exercise 10.2.1: Figure 10.9 is an example of a social-network graph. Use the Girvan-Newman approach to find the number of shortest paths from each
of the following nodes that pass through each of the edges. (a) A A
BC
HD
IGEF
Figure 10.9: Graph for exercises
(b) B.
                     Exercise 10.2.2: Using symmetry, the calculations of Exercise 10.2.1 are all you need to compute the betweenness of each edge. Do the calculation.
Exercise 10.2.3: Using the betweenness values from Exercise 10.2.2, deter- mine reasonable candidates for the communities in Fig. 10.9 by removing all edges with a betweenness above some threshold.
10.3. DIRECT DISCOVERY OF COMMUNITIES 357 10.3 Direct Discovery of Communities
In the previous section we searched for communities by partitioning all the in- dividuals in a social network. While this approach is relatively efficient, it does have several limitations. It is not possible to place an individual in two different communities, and everyone is assigned to a community. In this section, we shall see a technique for discovering communities directly by looking for subsets of the nodes that have a relatively large number of edges among them. Interest- ingly, the technique for doing this search on a large graph involves finding large frequent itemsets, as was discussed in Chapter 6.
10.3.1 Finding Cliques
Our first thought about how we could find sets of nodes with many edges between them is to start by finding a large clique (a set of nodes with edges between any two of them). However, that task is not easy. Not only is finding maximal cliques NP-complete, but it is among the hardest of the NP-complete problems in the sense that even approximating the maximal clique is hard. Further, it is possible to have a set of nodes with almost all edges between them, and yet have only relatively small cliques.
Example 10.10 : Suppose our graph has nodes numbered 1, 2, . . . , n and there is an edge between two nodes i and j unless i and j have the same remain- der when divided by k. Then the fraction of possible edges that are actually present is approximately (k − 1)/k. There are many cliques of size k, of which {1,2,...,k} is but one example.
Yet there are no cliques larger than k. To see why, observe that any set of k + 1 nodes has two that leave the same remainder when divided by k. This point is an application of the “pigeonhole principle.” Since there are only k different remainders possible, we cannot have distinct remainders for each of k+1 nodes. Thus, no set of k+1 nodes can be a clique in this graph. ✷
10.3.2 Complete Bipartite Graphs
Recall our discussion of bipartite graphs from Section 8.3. A complete bipartite graph consists of s nodes on one side and t nodes on the other side, with all st possible edges between the nodes of one side and the other present. We denote this graph by Ks,t. You should draw an analogy between complete bipartite graphs as subgraphs of general bipartite graphs and cliques as subgraphs of general graphs. In fact, a clique of s nodes is often referred to as a complete graph and denoted Ks, while a complete bipartite subgraph is sometimes called a bi-clique.
While as we saw in Example 10.10, it is not possible to guarantee that a graph with many edges necessarily has a large clique, it is possible to guar- antee that a bipartite graph with many edges has a large complete bipartite
358 CHAPTER 10. MINING SOCIAL-NETWORK GRAPHS
subgraph.1 We can regard a complete bipartite subgraph (or a clique if we discovered a large one) as the nucleus of a community and add to it nodes with many edges to existing members of the community. If the graph itself is k-partite as discussed in Section 10.1.4, then we can take nodes of two types and the edges between them to form a bipartite graph. In this bipartite graph, we can search for complete bipartite subgraphs as the nuclei of communities. For instance, in Example 10.2, we could focus on the tag and page nodes of a graph like Fig. 10.2 and try to find communities of tags and Web pages. Such a community would consist of related tags and related pages that deserved many or all of those tags.
However, we can also use complete bipartite subgraphs for community find- ing in ordinary graphs where nodes all have the same type. Divide the nodes into two equal groups at random. If a community exists, then we would expect about half its nodes to fall into each group, and we would expect that about half its edges would go between groups. Thus, we still have a reasonable chance of identifying a large complete bipartite subgraph in the community. To this nucleus we can add nodes from either of the two groups, if they have edges to many of the nodes already identified as belonging to the community.
10.3.3 Finding Complete Bipartite Subgraphs
Suppose we are given a large bipartite graph G , and we want to find instances of Ks,t within it. It is possible to view the problem of finding instances of Ks,t within G as one of finding frequent itemsets. For this purpose, let the “items” be the nodes on one side of G, which we shall call the left side. We assume that the instance of Ks,t we are looking for has t nodes on the left side, and we shall also assume for efficiency that t ≤ s. The “baskets” correspond to the nodes on the other side of G (the right side). The members of the basket for node v are the nodes of the left side to which v is connected. Finally, let the support threshold be s, the number of nodes that the instance of Ks,t has on the right side.
We can now state the problem of finding instances of Ks,t as that of finding frequent itemsets F of size t. That is, if a set of t nodes on the left side is frequent, then they all occur together in at least s baskets. But the baskets are the nodes on the right side. Each basket corresponds to a node that is connected to all t of the nodes in F. Thus, the frequent itemset of size t and s of the baskets in which all those items appear form an instance of Ks,t.
Example 10.11 : Recall the bipartite graph of Fig. 8.1, which we repeat here as Fig. 10.10. The left side is the nodes {1, 2, 3, 4} and the right side is {a, b, c, d}. The latter are the baskets, so basket a consists of “items” 1 and 4; that is, a = {1,4}. Similarly, b = {2,3}, c = {1} and d = {3}.
1It is important to understand that we do not mean a generated subgraph – one formed by selecting some nodes and including all edges. In this context, we only require that there be edges between any pair of nodes on different sides. It is also possible that some nodes on the same side are connected by edges as well.
 
10.3. DIRECT DISCOVERY OF COMMUNITIES 359
   1
2
3
4
a
b
c
d
Figure 10.10: The bipartite graph from Fig. 8.1
If s = 2 and t = 1, we must find itemsets of size 1 that appear in at least two baskets. {1} is one such itemset, and {3} is another. However, in this tiny example there are no itemsets for larger, more interesting values of s and t, suchass=t=2. ✷
10.3.4 Why Complete Bipartite Graphs Must Exist
We must now turn to the matter of demonstrating that any bipartite graph with a sufficiently high fraction of the edges present will have an instance of Ks,t. In what follows, assume that the graph G has n nodes on the left and another n nodes on the right. Assume the two sides have the same number of nodes simplifies the calculation, but the argument generalizes to sides of any size. Finally, let d be the average degree of all nodes.
The argument involves counting the number of frequent itemsets of size t that a basket with d items contributes to. When we sum this number over all nodes on the right side, we get the total frequency of all the subsets of size t on the left. When we divide by  nt  , we get the average frequency of all itemsets of size t. At least one must have a frequency that is at least average, so if this average is at least s, we know an instance of Ks,t exists.
Now, we provide the detailed calculation. Suppose the degree of the ith
node on the right is di; that is, di is the size of the ith basket. Then this
basket contributes to  di  itemsets of size t. The total contribution of the n t
nodes on the right is    di . The value of this sum depends on the di’s, of it
course. However, we know that the average value of di is d. It is known that
this sum is minimized when each di is d. We shall not prove this point, but a
simple example will suggest the reasoning: since  di  grows roughly as the tth t
360 CHAPTER 10. MINING SOCIAL-NETWORK GRAPHS power of di, moving 1 from a large di to some smaller dj will reduce the sum
of  di + dj . tt
Example 10.12: Suppose there are only two nodes, t = 2, and the average degree of the nodes is 4. Then d1 +d2 = 8, and the sum of interest is  d1 + d2 .
22 Ifd1 =d2 =4,thenthesumis 42 + 42 =6+6=12. However,ifd1 =5and
d2 =3,thesumis 52 + 32 =10+3=13. Ifd1 =6andd1 =2,thenthesum is 62 + 2 =15+1=16. ✷
Thus, in what follows, we shall assume that all nodes have the average degree d. So doing minimizes the total contribution to the counts for the itemsets, and thus makes it least likely that there will be a frequent itemset (itemset with with support s or more) of size t. Observe the following:
• The total contribution of the n nodes on the right to the counts of the itemsets of size t is n dt .
• The number of itemsets of size t is  nt .
• Thus, the average count of an itemset of size t is n dt  / nt  ; this expression
must be at least s if we are to argue that an instance of Ks,t exists.
If we expand the binomial coefficients in terms of factorials, we find
n dt / nt =nd!(n−t)!t!/ (d−t)!t!n!  = n(d)(d − 1) · · · (d − t + 1)/ n(n − 1) · · · (n − t + 1) 
To simplify the formula above, let us assume that n is much larger than d, and d is much larger than t. Then d(d − 1)···(d − t + 1) is approximately dt, and n(n−1)···(n−t+1) is approximately nt. We thus require that
n(d/n)t ≥ s
That is, if there is a community with n nodes on each side, the average degree of the nodes is d, and n(d/n)t ≥ s, then this community is guaranteed to have a complete bipartite subgraph Ks,t. Moreover, we can find the instance of Ks,t efficiently, using the methods of Chapter 6, even if this small community is embedded in a much larger graph. That is, we can treat all nodes in the entire graph as baskets and as items, and run A-priori or one of its improvements on the entire graph, looking for sets of t items with support s.
Example 10.13 : Suppose there is a community with 100 nodes on each side, and the average degree of nodes is 50; i.e., half the possible edges exist. This community will have an instance of Ks,t, provided 100(1/2)t ≥ s. For example, ift=2,thenscanbeaslargeas25. Ift=3,scanbe11,andift=4,scan be 6.
10.4. PARTITIONING OF GRAPHS 361
Unfortunately, the approximation we made gives us a bound on s that is a
little too high. If we revert to the original formula n dt / nt  ≥ s, we see that
for the case t = 4 we need 100 50 / 100  ≥ s. That is, 44
100×50×49×48×47 ≥ s 100×99×98×97
The expression on the left is not 6, but only 5.87. However, if the average support for an itemset of size 4 is 5.87, then it is impossible that all those itemsets have support 5 or less. Thus, we can be sure that at least one itemset of size 4 has support 6 or more, and an instance of K6.4 exists in this community. ✷
10.3.5 Exercises for Section 10.3
Exercise 10.3.1 : For the running example of a social network from Fig. 10.1, how many instances of Ks,t are there for:
(a) s=1andt=3. (b) s=2andt=2. (c) s=2andt=3.
Exercise 10.3.2: Suppose there is a community of 2n nodes. Divide the community into two groups of n members, at random, and form the bipartite graph between the two groups. Suppose that the average degree of the nodes of the bipartite graph is d. Find the set of maximal pairs (t, s), with t ≤ s, such that an instance of Ks,t is guaranteed to exist, for the following combinations of n and d:
(a) n=20andd=5. (b) n=200andd=150.
(c) n=1000andd=400.
By “maximal,” we mean there is no different pair (s′, t′) such that both s′ ≥ s
and t′ ≥ t hold.
10.4 Partitioning of Graphs
In this section, we examine another approach to organizing social-network graphs. We use some important tools from matrix theory (“spectral meth- ods”) to formulate the problem of partitioning a graph to minimize the number of edges that connect different components. The goal of minimizing the “cut” size needs to be understood carefully before proceeding. For instance, if you
 
362 CHAPTER 10. MINING SOCIAL-NETWORK GRAPHS
just joined Facebook, you are not yet connected to any friends. We do not want to partition the friends graph with you in one group and the rest of the world in the other group, even though that would partition the graph without there being any edges that connect members of the two groups. This cut is not desirable because the two components are too unequal in size.
10.4.1 What Makes a Good Partition?
Given a graph, we would like to divide the nodes into two sets so that the cut, or set of edges that connect nodes in different sets is minimized. However, we also want to constrain the selection of the cut so that the two sets are approximately equal in size. The next example illustrates the point.
Example 10.14 : Recall our running example of the graph in Fig. 10.1. There, it is evident that the best partition puts {A, B, C} in one set and {D, E, F, G} in the other. The cut consists only of the edge (B,D) and is of size 1. No nontrivial cut can be smaller.
ABDE
C HGF
                   Smallest cut
Best cut
Figure 10.11: The smallest cut might not be the best cut
In Fig. 10.11 is a variant of our example, where we have added the node H and two extra edges, (H,C) and (C,G). If all we wanted was to minimize the size of the cut, then the best choice would be to put H in one set and all the other nodes in the other set. But it should be apparent that if we reject partitions where one set is too small, then the best we can do is to use the cut consisting of edges (B, D) and (C, G), which partitions the graph into two equal-sized sets {A, B, C, H} and {D, E, F, G}. ✷
10.4.2 Normalized Cuts
A proper definition of a “good” cut must balance the size of the cut itself against the difference in the sizes of the sets that the cut creates. One choice
10.4. PARTITIONING OF GRAPHS 363
that serves well is the “normalized cut.” First, define the volume of a set S of nodes, denoted Vol(S), to be the number of edges with at least one end in S.
Suppose we partition the nodes of a graph into two disjoint sets S and T. Let Cut(S,T) be the number of edges that connect a node in S to a node in T. Then the normalized cut value for S and T is
Cut(S,T) + Cut(S,T) Vol(S) Vol(T)
Example 10.15 : Again consider the graph of Fig. 10.11. If we choose S = {H} and T = {A,B,C,D,E,F,G}, then Cut(S,T) = 1. Vol(S) = 1, because there is only one edge connected to H. On the other hand, Vol(T) = 11, because all the edges have at least one end at a node of T. Thus, the normalized cut for this partition is 1/1 + 1/11 = 1.09.
Now, consider the preferred cut for this graph consisting of the edges (B, D) and (C,G). Then S = {A,B,C,H} and T = {D,E,F,G}. Cut(S,T) = 2, Vol(S) = 6, and Vol(T) = 7. The normalized cut for this partition is thus only 2/6 + 2/7 = 0.62. ✷
10.4.3 Some Matrices That Describe Graphs
To develop the theory of how matrix algebra can help us find good graph partitions, we first need to learn about three different matrices that describe aspects of a graph. The first should be familiar: the adjacency matrix that has a 1 in row i and column j if there is an edge between nodes i and j, and 0 otherwise.
ABDE
C
GF
Figure 10.12: Repeat of the graph of Fig. 10.1
Example 10.16: We repeat our running example graph in Fig. 10.12. Its adjacency matrix appears in Fig. 10.13. Note that the rows and columns cor- respond to the nodes A, B, . . . , G in that order. For example, the edge (B, D) is reflected by the fact that the entry in row 2 and column 4 is 1 and so is the entry in row 4 and column 2. ✷
The second matrix we need is the degree matrix for a graph. This graph has nonzero entries only on the diagonal. The entry for row and column i is the degree of the ith node.
                  
364
CHAPTER 10. MINING SOCIAL-NETWORK GRAPHS
0110000 1011000  1 1 0 0 0 0 0   0 1 0 0 1 1 1   0 0 0 1 0 1 0   0 0 0 1 1 0 1  0001010
Figure 10.13: The adjacency matrix for Fig. 10.12
Example 10.17 : The degree matrix for the graph of Fig. 10.12 is shown in Fig. 10.14. We use the same order of the nodes as in Example 10.16. For instance, the entry in row 4 and column 4 is 4 because node D has edges to four other nodes. The entry in row 4 and column 5 is 0, because that entry is not on the diagonal. ✷
2000000 0300000  0 0 2 0 0 0 0   0 0 0 4 0 0 0   0 0 0 0 2 0 0   0 0 0 0 0 3 0  0000002
Figure 10.14: The degree matrix for Fig. 10.12
Suppose our graph has adjacency matrix A and degree matrix D. Our third matrix, called the Laplacian matrix, is L = D − A, the difference between the degree matrix and the adjacency matrix. That is, the Laplacian matrix L has the same entries as D on the diagonal. Off the diagonal, at row i and column j, L has −1 if there is an edge between nodes i and j and 0 if not.
Example 10.18 : The Laplacian matrix for the graph of Fig. 10.12 is shown in Fig. 10.15. Notice that each row and each column sums to zero, as must be the case for any Laplacian matrix. ✷
10.4.4 Eigenvalues of the Laplacian Matrix
We can get a good idea of the best way to partition a graph from the eigenvalues and eigenvectors of its Laplacian matrix. In Section 5.1.2 we observed how the principal eigenvector (eigenvector associated with the largest eigenvalue) of the transition matrix of the Web told us something useful about the importance of Web pages. In fact, in simple cases (no taxation) the principal eigenvector is the
10.4. PARTITIONING OF GRAPHS 365
 2 -1 -1 0 0 0 0   -1 3 -1 -1 0 0 0   -1 -1 2 0 0 0 0  0-1 0 4-1-1-1 0 0 0-1 2-1 0  0 0 0 - 1 - 1 3 - 1 
0 0 0 -1 0 -1 2
Figure 10.15: The Laplacian matrix for Fig. 10.12
PageRank vector. When dealing with the Laplacian matrix, however, it turns out that the smallest eigenvalues and their eigenvectors reveal the information we desire.
The smallest eigenvalue for every Laplacian matrix is 0, and its correspond- ing eigenvector is [1, 1, . . . , 1]. To see why, let L be the Laplacian matrix for a graph of n nodes, and let 1 be the column vector of all 1’s with length n. We claim L1 is a column vector of all 0’s. To see why, consider row i of L. Its diagonal element has the degree d of node i. Row i also will have d occurrences of −1, and all other elements of row i are 0. Multiplying row i by column vector 1 has the effect of summing the row, and this sum is clearly d + (−1)d = 0. Thus, we can conclude L1 = 01, which demonstrates that 0 is an eigenvalue and 1 its corresponding eigenvector.
There is a simple way to find the second-smallest eigenvalue for any matrix, such as the Laplacian matrix, that is symmetric (the entry in row i and column j equals the entry in row j and column i). While we shall not prove this fact, the second-smallest eigenvalue of L is the minimum of xTLx, where x = [x1,x2,...,xn] is a column vector with n components, and the minimum is taken under the constraints:
1. The length of x is 1; that is  ni=1 x2i = 1.
2. x is orthogonal to the eigenvector associated with the smallest eigenvalue.
Moreover, the value of x that achieves this minimum is the second eigenvector. When L is a Laplacian matrix for an n-node graph, we know something more. The eigenvector associated with the smallest eigenvalue is 1. Thus, if x
is orthogonal to 1, we must have
xT1= xi =0
In addition for the Laplacian matrix, the expression xTLx has a useful equiv- alent expression. Recall that L = D − A, where D and A are the degree and adjacency matrices of the same graph. Thus, xTLx = xTDx − xTAx. Let us
n i=1
366 CHAPTER 10. MINING SOCIAL-NETWORK GRAPHS
evaluate the term with D and then the term for A. Here, Dx is the column vec- tor [d1x1, d2x2, . . . , dnxn], where di is the degree of the ith node of the graph. Thus, xTDx is  ni=1 dix2i .
Now, turn to xTAx. The ith component of the column vector Ax is the sum of xj over all j such that there is an edge (i, j) in the graph. Thus, −xTAx is the sum of −2xixj over all pairs of nodes {i, j} such that there is an edge between them. Note that the factor 2 appears because each set {i, j} corresponds to two terms, −xixj and −xjxi.
We can group the terms of xTLx in a way that distributes the terms to each pair {i,j}. From −xTAx, we already have the term −2xixj. From xTDx, we distribute the term dix2i to the di pairs that include node i. As a result, we can associate with each pair {i, j} that has an edge between nodes i and j the terms x2i −2xixj +x2j. This expression is equivalent to (xi −xj)2. Therefore, we have proved that xTLx equals the sum over all graph edges (i,j) of (xi −xj)2.
Recall that the second-smallest eigenvalue is the minimum of this expression under the constraint that  ni=1 x2i = 1. Intuitively, we minimize it by making xi and xj close whenever there is an edge betwe√en nodes i and j in the graph. We might imagine that we could choose xi = 1/ n for all i and thus make this sum 0. However, recall that we are constrained to choose x to be orthogonal to 1,whichmeansthesumofthexi’sis0.Wearealsoforcedtomake ni=1x2i be 1, so all components cannot be 0. As a consequence, x must have some positive and some negative components.
We can obtain a partition of the graph by taking one set to be the nodes i whose corresponding vector component xi is positive and the other set to be those whose components are negative. This choice does not guarantee a partition into sets of equal size, but the sizes are likely to be close. We believe that the cut between the two sets will have a small number of edges because (xi−xj)2 islikelytobesmallerifbothxi andxj havethesamesignthanifthey have different signs. Thus, minimizing xTLx under the required constraints will tend to give xi and xj the same sign if there is an edge (i, j).
14 25 36
Figure 10.16: Graph for illustrating partitioning by spectral analysis
Example 10.19 : Let us apply the above technique to the graph of Fig. 10.16. The Laplacian matrix for this graph is shown in Fig. 10.17. By standard meth- ods or math packages we can find all the eigenvalues and eigenvectors of this matrix. We shall simply tabulate them in Fig. 10.18, from lowest eigenvalue to
               
10.4. PARTITIONING OF GRAPHS 367 highest. Note that we have not scaled the eigenvectors to have length 1, but
could do so easily if we wished.
 3 -1 -1 -1 0 0   -1 2 -1 0 0 0  -1-1 3 0 0-1 -1 0 0 3-1-1  0 0 0 - 1 2 - 1 
0 0-1-1-1 3
Figure 10.17: The Laplacian matrix for Fig. 10.16
The second eigenvector has three positive and three negative components. It makes the unsurprising suggestion that one group should be {1,2,3}, the nodes with positive components, and the other group should be {4, 5, 6}. ✷
  Eigenvalue   0   1   3   3 Eigenvector   1   1   −5   −1 1   2   4   −2 1   1   1   3 1   −1   −5   −1 1   −2   4   −2 1   −1   1   3
Figure 10.18: Eigenvalues and eigenvectors for
4   5 −1   −1 1   0 −1   1 1   1 −1   0 1   −1
the matrix of Fig. 10.17
                  10.4.5 Alternative Partitioning Methods
The method of Section 10.4.4 gives us a good partition of the graph into two pieces that have a small cut between them. There are several ways we can use the same eigenvectors to suggest other good choices of partition. First, we are not constrained to put all the nodes with positive components in the eigenvector into one group and those with negative components in the other. We could set the threshold at some point other than zero.
For instance, suppose we modified Example 10.19 so that the threshold was not zero, but −1.5. Then the two nodes 4 and 6, with components −1 in the second eigenvector of Fig. 10.18, would join 1, 2, and 3, leaving five nodes in one component and only node 5 in the other. That partition would have a cut of size two, as did the choice based on the threshold of zero, but the two components have radically different sizes, so we would tend to prefer our original choice. However, there are other cases where the threshold zero gives unequally sized components, as would be the case if we used the third eigenvector in Fig. 10.18.
368 CHAPTER 10. MINING SOCIAL-NETWORK GRAPHS
We may also want a partition into more than two components. One approach is to use the method described above to split the graph into two, and then use it repeatedly on the components to split them as far as desired. A second approach is to use several of the eigenvectors, not just the second, to partition the graph. If we use m eigenvectors, and set a threshold for each, we can get a partition into 2m groups, each group consisting of the nodes that are above or below threshold for each of the eigenvectors, in a particular pattern.
It is worth noting that each eigenvector except the first is the vector x that minimizes xTLx, subject to the constraint that it is orthogonal to all previous eigenvectors. This constraint generalizes the constraints we described for the second eigenvector in a natural way. As a result, while each eigenvector tries to produce a minimum-sized cut, the fact that successive eigenvectors have to satisfy more and more constraints generally causes the cuts they describe to be progressively worse.
Example 10.20 : Let us reconsider the graph of Fig. 10.16, for which the eigenvectors of its Laplacian matrix were tabulated in Fig. 10.18. The third eigenvector, with a threshold of 0, puts nodes 1 and 4 in one group and the other four nodes in the other. That is not a bad partition, but its cut size is four, compared with the cut of size two that we get from the second eigenvector.
If we use both the second and third eigenvectors, we put nodes 2 and 3 in one group, because their components are positive in both eigenvectors. Nodes 5 and 6 are in another group, because their components are negative in the second eigenvector and positive in the third. Node 1 is in a group by itself because it is positive in the second eigenvector and negative in the third, while node 4 is also in a group by itself because its component is negative in both eigenvectors. This partition of a six-node graph into four groups is too fine a partition to be meaningful. But at least the groups of size two each have an edge between the nodes, so it is as good as we could ever get for a partition into groups of these sizes. ✷
10.4.6 Exercises for Section 10.4
Exercise 10.4.1 : For the graph of Fig. 10.9, construct: (a) The adjacency matrix.
(b) The degree matrix.
(c) The Laplacian matrix.
! Exercise 10.4.2 : For the Laplacian matrix constructed in Exercise 10.4.1(c), find the second-smallest eigenvalue and its eigenvector. What partition of the nodes does it suggest?
!! Exercise 10.4.3 : For the Laplacian matrix constructed in Exercise 10.4.1(c), construct the third and subsequent smallest eigenvalues and their eigenvectors.
10.5. FINDING OVERLAPPING COMMUNITIES 369 10.5 Finding Overlapping Communities
So far, we have concentrated on clustering a social graph to find communities. But communities are in practice rarely disjoint. In this section, we explain a method for taking a social graph and fitting a model to it that best explains how it could have been generated by a mechanism that assumes the probability that two individuals are connected by an edge (are “friends”) increases as they become members of more communities in common. An important tool in this analysis is “maximum-likelihood estimation,” which we shall explain before getting to the matter of finding overlapping communities.
10.5.1 The Nature of Communities
To begin, let us consider what we would expect two overlapping communities to look like. Our data is a social graph, where nodes are people and there is an edge between two nodes if the people are “friends.” Let us imagine that this graph represents students at a school, and there are two clubs in this school: the Chess Club and the Spanish Club. It is reasonable to suppose that each of these clubs forms a community, as does any other club at the school. It is also reasonable to suppose that two people in the Chess Club are more likely to be friends in the graph because they know each other from the club. Likewise, if two people are in the Spanish Club, then there is a good chance they know each other, and are likely to be friends.
What if two people are in both clubs? They now have two reasons why they might know each other, and so we would expect an even greater probability that they will be friends in the social graph. Our conclusion is that we expect edges to be dense within any community, but we expect edges to be even denser in the intersection of two communities, denser than that in the intersection of three communities, and so on. The idea is suggested by Fig. 10.19.
10.5.2 Maximum-Likelihood Estimation
Before we see the algorithm for finding communities that have overlap of the kind suggested in Section 10.5.1, let us digress and learn a useful modeling tool called maximum-likelihood estimation, or MLE. The idea behind MLE is that we make an assumption about the generative process (the model) that creates instances of some artifact, for example, “friends graphs.” The model has parameters that determine the probability of generating any particular instance of the artifact; this probability is called the likelihood of those parameter values. We assume that the value of the parameters that gives the largest value of the likelihood is the correct model for the observed artifact.
An example should make the MLE principle clear. For instance, we might wish to generate random graphs. We suppose that each edge is present with probability p and not present with probability 1−p, with the presence or absence of each edge chosen independently. The only parameter we can adjust is p. For
370 CHAPTER 10. MINING SOCIAL-NETWORK GRAPHS
  Chess Club
Spanish Club
                       Intersection
Figure 10.19: The overlap of two communities is denser than the nonoverlapping parts of these communities
each value of p there is a small but nonzero probability that the graph generated will be exactly the one we see. Following the MLE principle, we shall declare that the true value of p is the one for which the probability of generating the observed graph is the highest.
Example 10.21: Consider the graph of Fig. 10.19. There are 15 nodes and 23 edges. As there are  15  = 105 pairs of 15 nodes, we see that if each edge
2
is chosen with probability p, then the probability (likelihood) of generating exactly the graph of Fig. 10.19 is given by the function p23(1−p)82. No matter what value p has between 0 and 1, that is an incredibly tiny number. But the function does have a maximum, which we can determine by taking its derivative and setting that to 0. That is:
23p22(1 − p)82 − 82p23(1 − p)81 = 0 We can group terms to rewrite the above as
p22(1 − p)81 23(1 − p) − 82p  = 0
The only way the right side can be 0 is if p is 0 or 1, or the last factor,
 23(1 − p) − 82p 
is 0. When p is 0 or 1, the value of the likelihood function p23(1 − p)82 is minimized, not maximized, so it must be the last factor that is 0. That is, the
10.5. FINDING OVERLAPPING COMMUNITIES 371
   Prior Probabilities
When we do an MLE analysis, we generally assume that the parameters can take any value in their range, and there is no bias in favor of particular values. However, if that is not the case, then we can multiply the formula we get for the probability of the observed artifact being generated, as a function of the parameter values, by the function that represents the relative likelihood of those values of the parameter being the true values. The exercises offer examples of MLE with assumptions about the prior distribution of the parameters.
 likelihood of generating the graph of Fig. 10.19 is maximized when 23 − 23p − 82p = 0
or p = 23/105.
That outcome is hardly a surprise. It says the most likely value for p is the
observed fraction of possible edges that are present in the graph. However, when we use a more complicated mechanism to generate graphs or other artifacts, the value of the parameters that produce the observed artifact with maximum likelihood is far from obvious. ✷
10.5.3 The Affiliation-Graph Model
We shall now introduce a reasonable mechanism, called the affiliation-graph model, to generate social graphs from communities. Once we see how the pa- rameters of the model influence the likelihood of seeing a given graph, we can address how one would solve for the values of the parameters that give the maximum likelihood. The mechanism, called community-affiliation graphs.
1. There is a given number of communities, and there is a given number of individuals (nodes of the graph).
2. Each community can have any set of individuals as members. That is, the memberships in the communities are parameters of the model.
3. Each community C has a probability pC associated with it, the probability that two members of community C are connected by an edge because they are both members of C. These probabilities are also parameters of the model.
4. If a pair of nodes is in two or more communities, then there is an edge be- tween them if any of the communities of which both are members justifies that edge according to rule (3).
372 CHAPTER 10. MINING SOCIAL-NETWORK GRAPHS
We must compute the likelihood that a given graph with the proper number of nodes is generated by this mechanism. The key observation is how the edge probabilities are computed, given an assignment of individuals to communities and values of the pC ’s. Consider an edge (u, v) between nodes u and v. Suppose u and v are members of communities C and D, but not any other communities. Then the probability that there is no edge between u and v is the product of the probabilities that there is no edge due to community C and no edge due to community D. That is, with probability (1 − pC )(1 − pD ) there is no edge (u, v) in the graph, and of course the probability that there is such an edge is 1 minus that.
More generally, if u and v are members of a nonempty set of communities M and not any others, then puv, the probability of an edge between u and v is given by:
puv=1−   (1−pC) C in M
As an important special case, if u and v are not in any communities together, then we take puv to be ǫ, some very tiny number. We have to choose this probability to be nonzero, or else we can never assign a nonzero likelihood to any set of communities that do not have every pair of individuals sharing a community. But by taking the probability to be very small, we bias our computation to favor solutions such that every observed edge is explained by joint membership in some community.
If we know which nodes are in which communities, then we can compute the likelihood of the given graph for these edge probabilities using a simple generalization of Example 10.21. Let Muv be the set of communities to which both u and v are assigned. Then the likelihood of E being exactly the set of edges in the observed graph is
  puv   (1−puv) (u,v) in E (u,v) not in E
 w
x
yz
   Figure 10.20: A social graph
Example 10.22 : Consider the tiny social graph in Fig. 10.20. Suppose there are two communities C and D, with associated probabilities pC and pD. Also, suppose that we have determined (or are using as a temporary hypothesis) that C = {w,x,y} and D = {w,y,z}. To begin, consider the pair of nodes w and
10.5. FINDING OVERLAPPING COMMUNITIES 373
x. Mwx = {C}; that is, this pair is in community C but not in community D. Therefore,pwx =1−(1−pC)=pC.
Similarly, x and y are only together in C, y and z are only together in D, and likewise w and z are only together in D. Thus, we find pxy = pC and pyz = pwz = pD. Now the pair w and y are together in both communities, so pwy = 1−(1−pC)(1−pD) = pC +pD −pCpD. Finally, x and z are not together in either community, so pxz = ǫ.
Now, we can compute the likelihood of the graph in Fig. 10.20, given our assumptions about membership in the two communities. This likelihood is the product of the probabilities associated with each of the four pairs of nodes whose edges appear in the graph, times one minus the probability for each of the two pairs whose edges are not there. That is, we want
pwxpwypxypyz(1 − pwz)(1 − pxz)
Substituting the expressions we developed above for each of these probabilities,
we get
(pC)2pD(pC + pD − pCpD)(1 − pD)(1 − ǫ)
Note that ǫ is very small, so the last factor is essentially 1 and can be dropped. We must find the values of pC and pD that maximize the above expression. First, notice that all factors are either independent of pC or grow with pC. The
only hard step in this argument is to remember that pD ≤ 1, so pC +pD −pCpD
must grow positively with pC. It follows that the likelihood is maximized when pC is as large as possible; that is, pC = 1.
Next, we must find the value of pD that maximizes the expression, given that pC = 1. The expression becomes pD(1 − pD), and it is easy to see that this expression has its maximum at pD = 0.5. That is, given C = {w, x, y} and D = {w, y, z}, the maximum likelihood for the graph in Fig. 10.20 occurs when members of C are certain to have an edge between them and there is a 50% chance that joint membership in D will cause an edge between the members. ✷
However, Example 10.22 reflects only part of the solution. We also need to find an assignment of members to communities such that the maximum likelihood solution for that assignment is the largest solution for any assignment. Once we fix on an assignment, we can find the probabilities, pC , associated with each community, even for very large graphs with large numbers of communities. The general method for doing so is called “gradient descent,” a technique that we introduced in Section 9.4.5 and that will be discussed further starting in Section 12.3.4.
Unfortunately, it is not obvious how one incorporates the set of members of each community into the gradient-descent solution, since changes to the com- position of communities is by discrete steps, not according to some continuous
374 CHAPTER 10. MINING SOCIAL-NETWORK GRAPHS
   Log Likelihood
Usually, we compute the logarithm of the likelihood function (the log like- lihood), rather than the function itself. Doing so offers several advantages. Products become sums, which often simplifies the expression. Also, sum- ming many numbers is less prone to numerical rounding errors than is taking the product of many tiny numbers.
 function, as is required for gradient descent. The only feasible way to search the space of possible assignments of members to communities is by starting with an assignment and making small changes, say insertion or deletion of one member for one community. For each such assignment, we can solve for the best community probabilities (the pC’s) by gradient descent. However, figuring out what changes to membership lead us in the right direction is tricky, and there is no guarantee you can even get to the best assignment by making incremental changes from a starting assignment.
10.5.4 Avoiding the Use of Discrete Membership Changes
There is a solution to the problem caused by the mechanism of Section 10.5.3, where membership of individuals in communities is discrete; either you are a member of the community or not. We can think of “strength of membership” of individuals in communities. Intuitively, the stronger the membership of two individuals in the same community, the more likely it is that this community will cause them to have an edge between them. In this model, we can adjust the strength of membership for an individual in a community continuously, just as we can adjust the probability associated with a community in the affiliation- graph model. That improvement allows us to use standard methods, such as gradient descent, to maximize the expression for likelihood. In the improved model, we have
1. Fixed sets of communities and individuals, as before.
2. For each community C and individual x, there is a strength of membership parameter FxC . These parameters can take any nonnegative value, and a value of 0 means the individual is definitely not in the community.
3. The probability that community C causes there to be an edge between nodes u and v is
pC (u, v) = 1 − e−FuC FvC
As before, the probability of there being an edge between u and v is 1 minus the probability that none of the communities causes there to be an edge between them. That is, each community independently causes edges, and an edge exists
10.5. FINDING OVERLAPPING COMMUNITIES 375 between two nodes if any community causes it to exist. More formally, puv, the
probability of an edge between nodes u and v can be calculated as puv =1−  1−pC(u,v) 
C
If we substitute the formula for pC (u, v) that is assumed in the model, we get
puv =1−e− C FuCFvC
Finally, let E be the set of edges in the observed graph. As before, we can write the formula for the likelihood of the observed graph as the product of the expression for puv for each edge (u, v) that is in E, times the product of 1 − puv for each edge (u,v) that is not in E. Thus, in the new model, the formula for the likelihood of the graph with edges E is
  (1−e− C FuCFvC )   e− C FuCFvC (u,v) in E (u,v) not in E
We can simplify this expression somewhat by taking its logarithm. Remem- ber that maximizing a function also maximizes the logarithm of that function, and vice versa. So we can take the natural logarithm of the above expression to replace the products by sums. We also get simplification from the fact that log(ex) = x.
  log(1−e− C FuCFvC )−    FuCFvC (10.1) (u,v) in E (u,v) not in E C
We can now find the values for the FxC’s that maximizes the expression (10.1). One way is to use gradient descent in a manner similar to what was done in Section 9.4.5. That is, we pick one node x, and adjust all the values of the FxC ’s in the direction that most improves the value of (10.1). Notice that the only factors whose values change in response to changes to the FxC’s are those where one of u and v is x and the other of u and v is a node adjacent to x. Since the degree of a node is typically much less than the number of edges in the graph, we can avoid looking at most of the terms in (10.1) at each step.
10.5.5 Exercises for Section 10.5
Exercise 10.5.1 : Suppose graphs are generated by picking a probability p and choosing each edge independently with probability p, as in Example 10.21. For the graph of Fig. 10.20, what value of p gives the maximum likelihood of seeing that graph? What is the probability this graph is generated?
Exercise 10.5.2 : Compute the MLE for the graph in Example 10.22 for the following guesses of the memberships of the two communities.
376 CHAPTER 10. MINING SOCIAL-NETWORK GRAPHS (a) C = {w,x}; C = {y,z}.
(b) C = {w,x,y,z}; C = {x,y,z}.
Exercise 10.5.3: Suppose we have a coin, which may not be a fair coin, and we flip it some number of times, seeing h heads and t tails.
(a) !((b)
!! (c)
If the probability p of getting a head on any flip is p, what is the MLE for p, in terms of h and t?
Suppose we are told that there is a 90% probability that the coin is fair (i.e., p = 0.5), and a 10% chance that p = 0.1. For what values of h and t is it more likely that the coin is fair?
Suppose the a-priori likelihood that p has a particular value is proportional to |p − 0.5|. That is, p is more likely to be near 0 or 1, than around 1/2. If we see h heads and t tails, what is the maximum likelihood estimate of p?
10.6 Simrank
In this section, we shall take up another approach to analyzing social-network graphs. This technique, called “simrank,” applies best to graphs with several types of nodes, although it can in principle be applied to any graph. The purpose of simrank is to measure the similarity between nodes of the same type, and it does so by seeing where random walkers on the graph wind up when starting at a particular node. Because calculation must be carried out once for each starting node, it is limited in the sizes of graphs that can be analyzed completely in this manner.
10.6.1 Random Walkers on a Social Graph
Recall our view of PageRank in Section 5.1 as reflecting what a “random surfer” would do if they walked on the Web graph. We can similarly think of a per- son “walking” on a social network. The graph of a social network is generally undirected, while the Web graph is directed. However, the difference is unim- portant. A walker at a node N of an undirected graph will move with equal probability to any of the neighbors of N (those nodes with which N shares an edge).
Suppose, for example, that such a walker starts out at node T1 of Fig. 10.2, which we reproduce here as Fig. 10.21. At the first step, it would go either to U1 or W1. If to W1, then it would next either come back to T1 or go to T2. If the walker first moved to U1, it would wind up at either T1, T2, or T3 next.
We conclude that, starting at T1, there is a good chance the walker would visit T2, at least initially, and that chance is better than the chance it would visit T3 or T4. It would be interesting if we could infer that tags T1 and T2 are
10.6. SIMRANK
377
  T1 W1 U1 T2 W2 U2 T3 W3
T4
Figure 10.21: Repeat of Fig. 10.2
        therefore related or similar in some way. The evidence is that they have both been placed on a common Web page, W1, and they have also been used by a common tagger, U1.
However, if we allow the walker to continue traversing the graph at random, then the probability that the walker will be at any particular node does not depend on where it starts out. This conclusion comes from the theory of Markov processes that we mentioned in Section 5.1.2, although the independence from the starting point requires additional conditions besides connectedness that the graph of Fig. 10.21 does satisfy.
10.6.2 Random Walks with Restart
We see from the observations above that it is not possible to measure similar- ity to a particular node by looking at the limiting distribution of the walker. However, we have already seen, in Section 5.1.5, the introduction of a small probability that the walker will stop walking at random. Later, we saw in Sec- tion 5.3.2 that there were reasons to select only a subset of Web pages as the teleport set, the pages that the walker would go to when they stopped surfing the Web at random.
Here, we take this idea to the extreme. As we are focused on one particular node N of a social network, and want to see where the random walker winds up on short walks from that node, we modify the matrix of transition probabilities to have a small additional probability of transitioning to N from any node. Formally, let M be the transition matrix of the graph G. That is, the entry in row i and column j of M is 1/k if node j of G has degree k, and one of the adjacent nodes is i. Otherwise, this entry is 0. We shall discuss teleporting in a moment, but first, let us look at a simple example of a transition matrix.
378 CHAPTER 10. MINING SOCIAL-NETWORK GRAPHS
Example 10.23 : Figure 10.22 is an example of a very simple network involving three pictures, and two tags, “Sky” and “Tree” that have been placed on some of them. Pictures 1 and 3 have both tags, while Picture 2 has only the tag “Sky.” Intuitively, we expect that Picture 3 is more similar to Picture 1 than Picture 2 is, and an analysis using a random walker with restart at Picture 1 will support that intuition.
Picture 1 Picture 2 Picture 3
Sky Tree
Figure 10.22: A simple bipartite social graph
Let us order the nodes as Picture 1, Picture 2, Picture 3, Sky, Tree. Then the transition matrix for the graph of Fig. 10.22 is
  0 0  0 0 0 0  1 / 2 1
1/2 0
0 1/3 1/2 0 1/3 0  01/31/2
1 / 2 0 0  1/2 0 0
For example, the fourth column corresponds to the node “Sky,” and this node connects to each of the tree picture nodes. It therefore has degree three, so the nonzero entries in its column must be 1/3. The picture nodes correspond to the first three rows and columns, so the entry 1/3 appears in the first three rows of column 4. Since the “Sky” node does not have an edge to either itself or the “Tree” node, the entries in the last two rows of column 4 are 0. ✷
As before, let us use β as the probability that the walker continues at ran- dom, so 1 − β is the probability the walker will teleport to the initial node N . Let eN be the column vector that has 1 in the row for node N and 0’s elsewhere. Then if v is the column vector that reflects the probability the walker is at each of the nodes at a particular round, and v′ is the probability the walker is at each of the nodes at the next round, then v′ is related to v by:
v′ =βMv+(1−β)eN
Example 10.24 : Assume M is the matrix of Example 10.23 and β = 0.8. Also, assume that node N is for Picture 1; that is, we want to compute the similarity of other pictures to Picture 1. Then the equation for the new value
10.6. SIMRANK 379 v′ of the distribution that we must iterate is
0 0 04/152/5 1/5 ′  0 0 0 4/15 0   0  v= 0 0 0 4/15 2/5v+ 0   2 / 5 4 / 5 2 / 5 0 0   0 
2/5 0 2/5 0 0 0
Since the graph of Fig. 10.22 is connected, the original matrix M is stochas- tic, and we can deduce that if the initial vector v has components that sum to 1, then v′ will also have components that sum to 1. As a result, we can simplify the above equation by adding 1/5 to each of the entries in the first row of the matrix. That is, we can iterate the matrix-vector multiplication
 1/5 1/5 1/5 7/15 3/5  ′  0 0 0 4/15 0 
v = 0 0 0 4/15 2/5 v 2/54/52/5 0 0
2/5 0 2/5 0 0
If we start with v = eN , then the sequence of estimates of the distribution of
the walker that we get is
1 1/5 35/75  95/375 2353/5625 .345 0  0   8/75  8/375  568/5625 .066  0 , 0 , 20/75 , 20/375 , 1228/5625 ,..., .145  0 2/5  6/75 142/375  786/5625 .249 0 2/5 6/75 110/375 690/5625 .196
We observe from the above that in the limit, the walker is more than twice as likely to be at Picture 3 than at Picture 2. This analysis confirms the intuition that Picture 3 is more like Picture 1 than Picture 2 is. ✷
There are several additional observations that we may take away from Ex- ample 10.24. First, remember that this analysis applies only to Picture 1. If we wanted to know what pictures were most similar to another picture, we would have to start the analysis over for that picture. Likewise, if we wanted to know about which tags were most closely associated with the tag “Sky” (an uninter- esting question in this small example, since there is only one other tag), then we would have to arrange to have the walker teleport only to the “Sky” node.
Second, notice that convergence takes time, since there is an initial oscil- lation. That is, initially, all the weight is at the pictures, and at the second step most of the weight is at the tags. At the third step, most weight is back at the pictures, but at the fourth step much of the weight moves to the tags again. However, in the limit there is convergence, with 5/9 of the weight at the pictures and 4/9 of the weight at the tags. In general, the process converges for any connected k-partite graph.
380 CHAPTER 10. MINING SOCIAL-NETWORK GRAPHS 10.6.3 Exercises for Section 10.6
Exercise 10.6.1 : If, in Fig. 10.22 you start the walk from Picture 2, what will be the similarity to Picture 2 of the other two pictures? Which do you expect to be more similar to Picture 2?
Exercise 10.6.2 : If, in Fig. 10.22 you start the walk from Picture 3, what do you expect the similarity to the other two pictures to be?
! Exercise 10.6.3: Repeat the analysis of Example 10.24, and compute the similarities of Picture 1 to the other pictures, if the following modifications are made to Fig. 10.22:
(a) The tag “Tree” is added to Picture 2.
(b) A third tag “Water” is added to Picture 3.
(c) A third tag “Water” is added to both Picture 1 and Picture 2.
Note: the changes are independently done for each part; they are not cumula- tive.
10.7 Counting Triangles
One of the most useful properties of social-network graphs is the count of tri- angles and other simple subgraphs. In this section we shall give methods for estimating or getting an exact count of triangles in a very large graph. We be- gin with a motivation for such counts and then give some methods for counting efficiently.
10.7.1 Why Count Triangles?
If we start with n nodes and add m edges to a graph at random, there will be an expected number of triangles in the graph. We can calculate this number without too much difficulty. There are  n3  sets of three nodes, or approximately n3/6 sets of three nodes that might be a triangle. The probability of an edge between any two given nodes being added is m/ n2 , or approximately 2m/n2. The probability that any set of three nodes has edges between each pair, if those edges are independently chosen to be present or absent is approximately (2m/n2)3 = 8m3/n6. Thus, the expected number of triangles in a graph of n nodes and m randomly selected edges is approximately (8m3/n6)(n3/6) =
4 (m/n)3.
If a graph is a social network with n participants and m pairs of “friends,” we would expect the number of triangles to be much greater than the value for a random graph. The reason is that if A and B are friends, and A is also a friend of C, there should be a much greater chance than average that B and
 3
10.7. COUNTING TRIANGLES 381
C are also friends. Thus, counting the number of triangles helps us to measure the extent to which a graph looks like a social network.
We can also look at communities within a social network. It has been demonstrated that the age of a community is related to the density of triangles. That is, when a group has just formed, people pull in their like-minded friends, but the number of triangles is relatively small. If A brings in friends B and C, it may well be that B and C do not know each other. As the community matures, B and C may interact because of their membership in the community. Thus, there is a good chance that at sometime the triangle {A,B,C} will be completed.
10.7.2 An Algorithm for Finding Triangles
We shall begin our study with an algorithm that has the fastest possible running time on a single processor. Suppose we have a graph of n nodes and m ≥ n edges. For convenience, assume the nodes are integers 1, 2, . . . , n.
m. A heavy-hitter Call a node a heavy hitter if its degree is at least √
 triangle is a triangle all three of whose nodes are heavy hitters. We use separate algorithms to count the heavy-hitter triangles and all other triangles. Note that
the number of heavy-hitter nodes is no more than 2√
m, since otherwise the sum
 of the degrees of the heavy hitter nodes would be more than 2m. Since each edge contributes to the degree of only two nodes, there would then have to be more than m edges.
Assuming the graph is represented by its edges, we preprocess the graph as follows:
1. Compute the degree of each node. This part requires only that we examine each edge and add 1 to the count of each of its two nodes. The total time required is O(m).
2. Create an index on edges, with the pair of nodes at its ends as the key. That is, the index allows us to determine, given two nodes, whether the edge between them exists. A hash table suffices. It can be constructed in O(m) time, and the expected time to answer a query about the existence of an edge is a constant, at least in the expected sense.2
3. Create another index of edges, this one with key equal to a single node. Given a node v, we can retrieve the nodes adjacent to v in time propor- tional to the number of those nodes. Again, a hash table, this time with single nodes as the key, suffices in the expected sense.
2Thus, technically, our algorithm is only optimal in the sense of expected running time, not worst-case running time. However, hashing of large numbers of items has an extremely high probability of behaving according to expectation, and if we happened to choose a hash function that made some buckets too big, we could rehash until we found a good hash function.
 
382 CHAPTER 10. MINING SOCIAL-NETWORK GRAPHS
We shall order the nodes as follows. First, order nodes by degree. Then, if v and u have the same degree, recall that both v and u are integers, so order them numerically. That is, we say v ≺ u if and only if either
(i) The degree of v is less than the degree of u, or (ii) The degrees of u and v are the same, and v < u.
Heavy-Hitter Triangles: There are only O(√m) heavy-hitter nodes, so we can consider all sets of three of these nodes. There are O(m3/2) possible heavy- hitter triangles, and using the index on edges we can check if all three edges exist in O(1) time. Therefore, O(m3/2) time is needed to find all the heavy-hitter triangles.
Other Triangles: We find the other triangles a different way. Consider each edge (v1,v2). If both v1 and v2 are heavy hitters, ignore this edge. Suppose, however, that v1 is not a heavy hitter and moreover v1 ≺ v2. Let u1, u2, . . . , uk be the nodes adjacent to v1. Note that k < √m. We can find these nodes, using the index on nodes, in O(k) time, which is surely O(√m) time. For each ui we can use the first index to check whether edge (ui,v2) exists in O(1) time. We can also determine the degree of ui in O(1) time, because we have counted all the nodes’ degrees. We count the triangle {v1,v2,ui} if and only if the edge (ui, v2) exists, and v1 ≺ ui. In that way, a triangle is counted only once – when v1 is the node of the triangle that precedes both other nodes of the triangle according to the ≺ ordering. Thus, the time to process all the nodes adjacent to v1 is O(√m). Since there are m edges, the total time spent counting other triangles is O(m3/2).
We now see that preprocessing takes O(m) time. The time to find heavy- hitter triangles is O(m3/2), and so is the time to find the other triangles. Thus, the total time of the algorithm is O(m3/2).
10.7.3 Optimality of the Triangle-Finding Algorithm
It turns out that the algorithm described in Section 10.7.2 is, to within an order of magnitude the best possible. To see why, consider a complete graph on n nodes. This graph has m =  n2  edges and the number of triangles is  n3 . Since we cannot enumerate triangles in less time than the number of those triangles, we know any algorithm will take Ω(n3) time on this graph. However, m = O(n2), so any algorithm takes Ω(m3/2) time on this graph.
One might wonder if there were a better algorithm that worked on sparser graphs than the complete graph. However, we can add to the complete graph a chain of nodes with any length up to n2. This chain adds no more triangles. It no more than doubles the number of edges, but makes the number of nodes as large as we like, in effect lowering the ratio of edges to nodes to be as close to 1 as we like. Since there are still Ω(m3/2) triangles, we see that this lower bound holds for the full range of possible ratios of m/n.
    
10.7. COUNTING TRIANGLES 383 10.7.4 Finding Triangles Using MapReduce
For a very large graph, we want to use parallelism to speed the computation. We can express triangle-finding as a multiway join and use the technique of Section 2.5.3 to optimize the use of a single MapReduce job to count triangles. It turns out that this use is one where the multiway join technique of that section is generally much more efficient than taking two two-way joins. Moreover, the total execution time of the parallel algorithm is essentially the same as the execution time on a single processor using the algorithm of Section 10.7.2.
To begin, assume that the nodes of a graph are numbered 1, 2, . . . , n. We use a relation E to represent edges. To avoid representing each edge twice, we assume that if E(A,B) is a tuple of this relation, then not only is there an edge between nodes A and B, but also, as integers, we have A < B.3 This requirement also eliminates loops (edges from a node to itself), which we generally assume do not exist in social-network graphs anyway, but which could lead to “triangles” that really do not involve three different nodes.
Using this relation, we can express the set of triangles of the graph whose edges are E by the natural join
E(X, Y ) ⊲⊳ E(X, Z) ⊲⊳ E(Y, Z) (10.2)
To understand this join, we have to recognize that the attributes of the relation E are given different names in each of the three uses of E. That is, we imagine there are three copies of E, each with the same tuples, but with a different schemas. In SQL, this join would be written using a single relation E(A, B) as follows:
SELECT e1.A, e1.B, e2.B
FROM E e1, E e2, E e3
WHERE e1.A = e2.A AND e1.B = e3.A AND e2.B = e3.B
In this query, the equated attributes e1.A and e2.A are represented in our join by the attribute X. Also, e1.B and e3.A are each represented by Y ; e2.B and e3.B are represented by Z.
Notice that each triangle appears once in this join. The triangle consisting of nodes v1, v2, and v3 is generated when X, Y , and Z are these three nodes in numerical order, i.e., X < Y < Z. For instance, if the numerical order of the nodes is v1 < v2 < v3, then X can only be v1, Y is v2, and Z is v3.
The technique of Section 2.5.3 can be used to optimize the join of Equa- tion 10.2. Recall the ideas in Example 2.9, where we considered the number of ways in which the values of each attribute should be hashed. In the present example, the matter is quite simple. The three occurrences of relation E surely have the same size, so by symmetry, attributes X, Y , and Z will each be hashed
3Do not confuse this simple numerical ordering of the nodes with the order ≺ that we discussed in Section 10.7.2 and which involved the degrees of the nodes. Here, node degrees are not computed and are not relevant.
 
384 CHAPTER 10. MINING SOCIAL-NETWORK GRAPHS
to the same number of buckets. In particular, if we hash nodes to b buckets, then there will be b3 reducers. Each Reduce task is associated with a sequence of three bucket numbers (x, y, z), where each of x, y, and z is in the range 1 to b.
The Map tasks divide the relation E into as many parts as there are Map tasks. Suppose one Map task is given the tuple E(u,v) to send to certain Reduce tasks. First, think of (u, v) as a tuple of the join term E(X, Y ). We can hash u and v to get the bucket numbers for X and Y , but we don’t know the bucket to which Z hashes. Thus, we must send E(u, v) to all Reducer tasks that correspond to a sequence of three bucket numbers  h(u),h(v),z  for any of the b possible buckets z.
But the same tuple E(u,v) must also be treated as a tuple for the term E(X,Z). We therefore also send the tuple E(u,v) to all Reduce tasks that correspond to a triple  h(u), y, h(v)  for any y. Finally, we treat E(u, v) as a tuple of the term E(Y, Z) and send that tuple to all Reduce tasks corresponding to a triple  x, h(u), h(v)  for any x. The total communication required is thus 3b key-value pairs for each of the m tuples of the edge relation E. That is, the minimum communication cost is O(mb) if we use b3 Reduce tasks.
Next, let us compute the total execution cost at all the Reduce tasks. As- sume that the hash function distributes edges sufficiently randomly that the Reduce tasks each get approximately the same number of edges. Since the total number of edges distributed to the b3 Reduce tasks is O(mb), it follows that each task receives O(m/b2) edges. If we use the algorithm of Section 10.7.2 at each Reduce task, the total computation at a task is O (m/b2)3/2 , or O(m3/2/b3). Since there are b3 Reduce tasks, the total computation cost is O(m3/2), exactly as for the one-processor algorithm of Section 10.7.2.
10.7.5 Using Fewer Reduce Tasks
By a judicious ordering of the nodes, we can lower the number of reduce tasks by approximately a factor of 6. Think of the “name” of the node i as the pair  h(i), i , where h is the hash function that we used in Section 10.7.4 to hash nodes to b buckets. Order nodes by their name, considering only the first component (i.e., the bucket to which the node hashes), and only using the second component to break ties among nodes that are in the same bucket.
If we use this ordering of nodes, then the Reduce task corresponding to list of buckets (i,j,k) will be needed only if i ≤ j ≤ k. If b is large, then approximately 1/6 of all b3 sequences of integers, each in the range 1 to b, will satisfy these inequalities. For any b, the number of such sequences is  b+2  (see Exercise 10.7.4). Thus, the exact ratio is (b + 2)(b + 1)/(6b2). 3
As there are fewer reducers, we get a substantial decrease in the number of key-value pairs that must be communicated. Instead of having to send each of the m edges to 3b Reduce tasks, we need to send each edge to only b tasks. Specifically, consider an edge e whose two nodes hash to i and j; these buckets could be the same or different. For each of the b values of k between 1 and b,
10.7. COUNTING TRIANGLES 385
consider the list formed from i, j, and k in sorted order. Then the Reduce task that corresponds to this list requires the edge e. But no other Reduce tasks require e.
To compare the communication cost of the method of this section with that of Section 10.7.4, let us fix the number of Reduce tasks, say k. Then the method of Section 10.7.4 hashes nodes to √3 k buckets, and therefore communicates 3m√3 k key-value pairs. On the other hand, the method of this section hashes nodes to approximately √3 6k buckets, thus requiring m√3 6√3 k communication. Thus, the ratio of the communication needed by the method of Section 10.7.4 to what is needed here is 3/√3 6 = 1.65.
Example 10.25 : Consider the straightforward algorithm of Section 10.7.4
with b = 6. That is, there are b3 = 216 Reduce tasks and the communication
cost is 3mb = 18m. We cannot use exactly 216 Reduce tasks with the method of
this section, but we can come very close if we choose b = 10. Then, the number
of Reduce tasks is  12  = 220, and the communication cost is mb = 10m. That 3
is, the communication cost is 5/9th of the cost of the straightforward method.
✷
10.7.6 Exercises for Section 10.7
Exercise 10.7.1 : How many triangles are there in the graphs: (a) Figure 10.1.
(b) Figure 10.9.
! (c) Figure 10.2.
Exercise 10.7.2 : For each of the graphs of Exercise 10.7.1 determine:
(i) What is the minimum degree for a node to be considered a “heavy hitter”? (ii) Which nodes are heavy hitters?
(iii) Which triangles are heavy-hitter triangles?
! Exercise 10.7.3 : In this exercise we consider the problem of finding squares in a graph. That is, we want to find quadruples of nodes a, b, c, d such that the four edges (a, b), (b, c), (c, d), and (a, d) exist in the graph. Assume the graph is represented by a relation E as in Section 10.7.4. It is not possible to write a single join of four copies of E that expresses all possible squares in the graph, but we can write three such joins. Moreover, in some cases, we need to follow the join by a selection that eliminates “squares” where one pair of opposite corners are really the same node. We can assume that node a is numerically lower than its neighbors b and d, but there are three cases,depending on whether c is
      
386
(i) (ii) (iii) (a)
(b) !! (c)
(d)
CHAPTER 10. MINING SOCIAL-NETWORK GRAPHS
Also lower than b and d, Between b and d, or Higher than both b and d.
Write the natural joins that produce squares satisfying each of the three conditions above. You can use four different attributes W, X, Y , and Z, and assume that there are four copies of relation E with different schemas, so the joins can each be expressed as natural joins.
For which of these joins do we need a selection to assure that opposite corners are really different nodes?
Assume we plan to use k Reduce tasks. For each of your joins from (a), into how many buckets should you hash each of W, X, Y , and Z in order to minimize the communication cost?
Unlike the case of triangles, it is not guaranteed that each square is pro- duced only once, although we can be sure that each square is produced by only one of the three joins. For example, a square in which the two nodes at opposite corners are each lower numerically than each of the other two nodes will only be produced by the join (i). For each of the three joins, how many times does it produce any square that it produces at all?
! Exercise 10.7.4 : Show that the number of sequences of integers 1 ≤ i ≤ j ≤ k ≤ b is  b+2 . Hint: show that these sequences can be placed in a 1-to-1
3
correspondence with the binary strings of length b + 2 having exactly three 1’s. 10.8 Neighborhood Properties of Graphs
There are several important properties of graphs that relate to the number of nodes one can reach from a given node along a short path. In this section we look at algorithms for solving problems about paths and neighborhoods for very large graphs. In some cases, exact solutions are not feasible for graphs with millions of nodes. We therefore look at approximation algorithms as well as exact algorithms.
10.8.1 Directed Graphs and Neighborhoods
In this section we shall use a directed graph as a model of a network. A directed graph has a set of nodes and a set of arcs; the latter is a pair of nodes written u→v. Wecalluthesourceandvthetargetofthearc. Thearcissaidtobe from u to v.
Many kinds of graphs can be modeled by directed graphs. The Web is a major example, where the arc u → v is a link from page u to page v. Or, the arc u → v could mean that telephone subscriber u has called subscriber v in
10.8. NEIGHBORHOOD PROPERTIES OF GRAPHS 387
the past month. For another example, the arc could mean that individual u is following individual v on Twitter. In yet another graph, the arc could mean that research paper u references paper v.
Moreover, all undirected graphs can be represented by directed graphs. In- stead of the undirected edge (u, v), use two arcs u → v and v → u. Thus, the material of this section also applies to graphs that are inherently undirected, such as a friends graph in a social network.
A path in a directed graph is a sequence of nodes v0, v1, . . . , vk such that therearearcsvi →vi+1 foralli=0,1,...,k−1. Thelengthofthispathisk, the number of arcs along the path. Note that there are k + 1 nodes in a path of length k, and a node by itself is considered a path of length 0.
The neighborhood of radius d for a node v is the set of nodes u for which there is a path of length at most d from v to u. We denote this neighborhood by N(v, d). For example, N(v, 0) is always {v}, and N(v, 1) is v plus the set of nodes to which there is an arc from v. More generally, if V is a set of nodes, then N(V,d) is the set of nodes u for which there is a path of length d or less from at least one node in the set V .
The neighborhood profile of a node v is the sequence of sizes of its neighbor- hoods |N (v, 1)|, |N (v, 2)|, . . . . We do not include the neighborhood of distance 0, since its size is always 1.
ABDE
C
GF
Figure 10.23: Our small social network; think of it as a directed graph
Example 10.26 : Consider the undirected graph of Fig. 10.1, which we re- produce here as Fig. 10.23. To turn it into a directed graph, think of each edge as a pair of arcs, one in each direction. For instance, the edge (A,B) becomes the arcs A → B and B → A. First, consider the neighborhoods of node A. We know N(A, 0) = {A}. Moreover, N(A, 1) = {A, B, C}, since there are arcs from A only to B and C. Furthermore, N(A,2) = {A,B,C,D} and N(A,3) = {A,B,C,D,E,F,G}. Neighborhoods for larger radius are all the same as N(A,3).
On the other hand, consider node B. We find N(B,0) = {B}, N(B,1) = {A,B,C,D}, and N(B,2) = {A,B,C,D,E,F,G}. We know that B is more central to the network than A, and this fact is reflected by the neighborhood profiles of the two nodes. Node A has profile 3, 4, 7, 7, . . ., while B has profile 4, 7, 7, . . . . Evidently, B is more central than A, because at every distance, its
                
388 CHAPTER 10. MINING SOCIAL-NETWORK GRAPHS
neighborhood is at least as large as that of A. In fact, D is even more central than B, because its neighborhood profile 5, 7, 7, . . . dominates the profile of each of the nodes. ✷
10.8.2 The Diameter of a Graph
The diameter of a directed graph is the smallest integer d such that for every twonodesuandvthereisapathoflengthdorlessfromutov. Inadirected graph, this definition only makes sense if the graph is strongly connected; that is, there is a path from any node to any other node. Recall our discussion of the Web in Section 5.1.3, where we observed that there is a large strongly connected subset of the Web in the “center,” but that the Web as a whole is not strongly connected. Rather, there are some pages that reach nowhere by links, and some pages that cannot be reached by links.
If the graph is undirected, the definition of diameter is the same as for directed graphs, but the path may traverse the undirected edges in either di- rection. That is, we treat an undirected edge as a pair of arcs, one in each direction. The notion of diameter makes sense in an undirected graph as long as that graph is connected.
Example 10.27 : For the graph of Fig. 10.23, the diameter is 3. There are some pairs of nodes, such as A and E, that have no path of length less than 3. But every pair of nodes has a path from one to the other with length at most 3. ✷
We can compute the diameter of a graph by computing the sizes of its neighborhoods of increasing radius, until at some radius we fail to add any more nodes. That is, for each node v, find the smallest d such that |N (v, d)| = |N (v, d + 1)|. This d is the tight upper bound on the length of the shortest path from v to any node it can reach. Call it d(v). For instance, we saw from Example 10.26 that d(A) = 3 and d(B) = 2. If there is any node v such that |N v,d(v) | is not the number of nodes in the entire graph, then the graph is not strongly connected, and we cannot offer any finite integer as its diameter. However, if the graph is strongly connected, then the diameter of the graph is maxv  d(v) .
The reason this computation works is that one way to express N (v, d + 1) is the union of N(v,d) and the set of all nodes w such that for some u in N(v,d) there is an arc u → w. That is, we start with N(v,d) and add to it the targets of all arcs that have a source in N(v,d). If all the arcs with source in N(v,d) are already in N(v,d), then not only is N(v,d+1) equal to N(v,d), but all of N(v,d+2),N(v,d+3),...willequalN(v,d). Finally,weobservethatsince N(v, d) ⊆ N(v, d+1) the only way |N(v, d)| can equal |N(v, d+1)| is for N(v, d) and N (v, d + 1) to be the same set. Thus, if d is the smallest integer such that |N (v, d)| = |N (v, d + 1)|, it follows that every node v can reach is reached by a path of length at most d.
10.8. NEIGHBORHOOD PROPERTIES OF GRAPHS 389
   Six Degrees of Separation
There is a famous game called “six degrees of Kevin Bacon,” the object of which is to find paths of length at most six in the graph whose nodes are movie stars and whose edges connect stars that played in the same movie. The conjecture is that in this graph, no movie star is of distance more than six from Kevin Bacon. More generally, any two movie stars can be connected by a path of length at most six; i.e., the diameter of the graph is six. A small diameter makes computation of neighborhoods more efficient, so it would be nice if all social-network graphs exhibited a similar small diameter. In fact, the phrase “six degrees of separation,” refers to the conjecture that in the network of all people in the world, where an edge means that the two people know each other, the diameter is six. Unfortunately, as we shall discuss in Section 10.8.3, not all important graphs exhibit such tight connections.
 10.8.3 Transitive Closure and Reachability
The transitive closure of a graph is the set of pairs of nodes (u,v) such that there is a path from u to v of length zero or more. We shall sometimes write this assertion as P ath(u, v).4 A related concept is that of reachability. We say node u reaches node v if Path(u,v). The problem of computing the transitive closure is to find all pairs of nodes u and v in a graph for which Path(u,v) is true. The reachability problem is, given a node u in the graph, find all v such that P ath(u, v) is true.
These two concepts relate to the notions of neighborhoods that we have seen earlier. In f act, Path(u,v) is true if and only if v is in N(u,∞), which we define to be i≥0 N(u,i). Thus, the reachability problem is to compute the union of all the neighborhoods of any radius for a given node u. The discussion in Section 10.8.2 reminds us that we can compute the reachable set for u by computing its neighborhoods up to that smallest radius d for which N (u, d) = N (u, d + 1).
The two problems – transitive closure and reachability – are related, but there are many examples of graphs where reachability is feasible and transitive closure is not. For instance, suppose we have a Web graph of a billion nodes. If we want to find the pages (nodes) reachable from a given page, we can do so, even on a single machine with a large main memory. However, just to produce the transitive closure of the graph could involve 1018 pairs of nodes, which is not practical, even using a large cluster of computers.5
4Technically, this definition gives us the reflexive and transitive closure of the graph, since P ath(v, v) is always considered true, even if there is no cycle that contains v.
5While we cannot compute the transitive closure completely, we can still learn a great deal about the structure of a graph, provided there are large strongly connected components.
 
390 CHAPTER 10. MINING SOCIAL-NETWORK GRAPHS 10.8.4 Transitive Closure Via MapReduce
When it comes to parallel implementation, transitive closure is actually more readily parallelizable than is reachability. If we want to compute N(v,∞), the set of nodes reachable from node v, without computing the entire transitive closure, we have no option but to compute the sequence of neighborhoods, which is essentially a breadth-first search of the graph from v. In relational terms, suppose we have a relation Arc(X, Y ) containing those pairs (x, y) such that there is an arc x → y. We want to compute iteratively a relation Reach(X) that is the set of nodes reachable from node v. After i rounds, Reach(X) will contain all those nodes in N(v,i).
Initially, Reach(X) contains only the node v. Suppose it contains all the nodes in N (v, i) after some round of MapReduce. To construct N (v, i + 1) we need to join Reach with the Arc relation, then project onto the second component and perform a union of the result with the old value of Reach. In SQL terms, we perform
SELECT DISTINCT Arc.Y FROM Reach, Arc
WHERE Arc.X = Reach.X;
This query asks us to compute the natural join of Reach(X) and Arc(X,Y), which we can do by MapReduce as explained in Section 2.3.7. Then, we have to group the result by Y and eliminate duplicates, a process that can be done by another MapReduce job as in Section 2.3.8.
How many rounds this process requires depends on how far from v is the furthest node that v can reach. In many social-network graphs, the diameter is small, as discussed in the box on “Six Degrees of Separation.” If so, computing reachability in parallel, using MapReduce or another approach is feasible. Few rounds of computation will be needed and the space requirements are not greater than the space it takes to represent the graph.
However, there are some graphs where the number of rounds is a serious impediment. For instance, in a typical portion of the Web, it has been found that most pages reachable from a given page are reachable by paths of length 10–15. Yet there are some pairs of pages such that the first reaches the second, but only through paths whose length is measured in the hundreds. For instance, blogs are sometimes structured so each response is reachable only through the comment to which it responds. Running arguments lead to long paths with no way to ‘shortcut” around that path. Or a tutorial on the Web, with 50 chapters, may be structured so you can only get to Chapter i through the page for Chapter i − 1.
Interestingly, the transitive closure can be computed much faster in parallel than can strict reachability. By a recursive-doubling technique, we can double
For example, the Web graph experiments discussed in Section 5.1.3 were done on a graph of about 200 million nodes. Although they never listed all the pairs of nodes in the transitive closure, they were able to describe the structure of the Web.
 
10.8. NEIGHBORHOOD PROPERTIES OF GRAPHS 391
the length of paths we know about in a single round. Thus, on a graph of diameter d, we need only log2 d rounds, rather than d rounds. If d = 6, the difference is not important, but if d = 1000, log2 d is about 10, so there is a hundredfold reduction in the number of rounds. The problem, as discussed above, is that while we can compute the transitive closure quickly, we must compute many more facts than are needed for a reachability computation on the same graph, and therefore the space requirements for transitive closure can greatly exceed the space requirements for reachability. That is, if all we want is the set Reach(v), we can compute the transitive closure of the entire graph, and then throw away all pairs that do not have v as their first component. But we cannot throw away all those pairs until we are done. During the computation of the transitive closure, we could wind up computing many facts Path(x,y), where neither x nor y is reachable from v, and even if they are reachable from v, we may not need to know x can reach y.
Assuming the graph is small enough that we can compute the transitive closure in its entirety, we still must be careful how we do so using MapReduce or another parallelism approach. The simplest recursive-doubling approach is to start the the relation P ath(X, Y ) equal to the arc relation Arc(X, Y ). Suppose that after i rounds, Path(X,Y) contains all pairs (x,y) such that there is a path from x to y of length at most 2i. Then if we join Path with itself at the next round, we shall discover all those pairs (x,y) such that there is a path from x to y of length at most 2 × 2i = 2i+1. The recursive-doubling query in SQL is
SELECT DISTINCT p1.X, p2.Y FROM Path p1, Path p2 WHERE p1.Y = p2.X;
After computing this query, we get all pairs connected by a path of length between 2 and 2i+1, assuming Path contains pairs connected by paths of length between 1 and 2i. If we take the union of the result of this query with the Arc relation itself, then we get all paths of length between 1 and 2i+1 and can use the union as the P ath relation in the next round of recursive doubling. The query itself can be implemented by two MapReduce jobs, one to do the join and the other to do the union and eliminate duplicates. As we observed for the parallel reachability computation, the methods of Sections 2.3.7 and 2.3.8 suffice. The union, discussed in Section 2.3.6 doesn’t really require a MapReduce job of its own; it can be combined with the duplicate-elimination.
If a graph has diameter d, then after log2 d rounds of the above algorithm Path contains all pairs (x,y) connected by a path of length at most d; that is, it contains all pairs in the transitive closure. Unless we already know d, one more round will be necessary to verify that no more pairs can be found, but for large d, this process takes many fewer rounds than the breadth-first search that we used for reachability.
However, the above recursive-doubling method does a lot of redundant work. An example should make the point clear.
392 CHAPTER 10. MINING SOCIAL-NETWORK GRAPHS
Example 10.28 : Suppose the shortest path from x0 to x17 is of length 17; in particular, let there be a path x0 → x1 → · · · → x17 . We shall discover the fact Path(x0,x17) on the fifth round, when Path contains all pairs connected by paths of length up to 16. The same path from x0 to x17 will be discovered 16 times when we join P ath with itself. That is, we can take the fact P ath(x0 , x16 ) and combine it with P ath(x16 , x17 ) to obtain P ath(x0 , x17 ). Or we can combine Path(x0,x15) with Path(x15,x17) to discover the same fact, and so on. ✷
10.8.5 Smart Transitive Closure
A variant of recursive doubling that avoids discovering the same path more than once is called smart transitive closure. Every path of length greater than 1 can be broken into a head whose length is a power of 2, followed by a tail whose length is no greater than the length of the head.
Example 10.29 : A path of length 13 has a head consisting of the first 8 arcs, followed by a tail consisting of the last 5 arcs. A path of length 2 is a head of length 1 followed by a tail of length 1. Note that 1 is a power of 2 (the 0th power), and the tail will be as long as the head whenever the path itself has a length that is a power of 2. ✷
To implement smart transitive closure in SQL, we introduce a relation Q(X, Y ) whose function after the ith round is to hold all pairs of nodes (x, y) such that the shortest path from x to y is of length exactly 2i. Also, after the ith round, Path(x,y) will be true if the shortest path from x to y is of length at most 2i+1 − 1. Note that this interpretation of P ath is slightly different from the interpretation of P ath in the simple recursive-doubling method given in Section 10.8.4.
Initially, set both Q and Path to be copies of the relation Arc. After the ith round, assume that Q and P ath have the contents described in the previous paragraph. Note that for the round i = 1, the initial values of Q and Path initially satisfy the conditions as described for i = 0. On the (i + 1)st round, we do the following:
1. Compute a new value for Q by joining it with itself, using the SQL query
SELECT DISTINCT q1.X, q2.Y FROM Q q1, Q q2
WHERE q1.Y = q2.X;
2. Subtract P ath from the relation Q computed in step 1. Note that step (1) discovers all paths of length 2i+1 . But some pairs connected by these paths may also have shorter paths. The result of step (2) is to leave in Q all and only those pairs (u,v) such that the shortest path from u to v has length exactly 2i+1.
10.8. NEIGHBORHOOD PROPERTIES OF GRAPHS 393
3. Join P ath with the new value of Q computed in 2, using the SQL query
SELECT DISTINCT Q.X, Path.Y FROM Q, Path
WHERE Q.Y = Path.X
At the beginning of the round P ath contains all (y, z) such that the short- est path from y to z has a length up to 2i+1 −1 from y to z, and the new value of Q contains all pairs (x, y) for which the shortest path from x to y is of length 2i+1. Thus, the result of this query is the set of pairs (x,y) such that the shortest path from x to y has a length between 2i+1 + 1 and 2i+2 − 1, inclusive.
4. Set the new value of Path to be the union of the relation computed in step 3, the new value of Q computed in step (1), and the old value of P ath. These three terms give us all pairs (x, y) whose shortest path is of length 2i+1 + 1 through 2i+2 − 1, exactly 2i+1, and 1 through 2i+1 − 1, respectively. Thus, the union gives us all shortest paths up to length 2i+2 − 1, as required by the inductive hypothesis about what is true after each round.
Each round of the smart transitive-closure algorithm uses steps that are joins, aggregations (duplicate eliminations), or unions. A round can thus be imple- mented as a short sequence of MapReduce jobs. Further, a good deal of work can be saved if these operations can be combined, say by using the more general patterns of communication permitted by a workflow system (see Section 2.4.1).
10.8.6 Transitive Closure by Graph Reduction
A typical directed graph such as the Web contains many strongly connected components (SCC’s). We can collapse an SCC to a single node as far as com- puting the transitive closure is concerned, since all the nodes in an SCC reach exactly the same nodes. There is an elegant algorithm for finding the SCC’s of a graph in time linear in the size of the graph, due to J.E. Hopcroft and R.E. Tarjan. However, this algorithm is inherently sequential, based on depth-first search, and so not well suited to parallel impelementation on large graphs.
We can find most of the SCC’s in a graph by some random node selections followed by two breadth-first searches. Moreover, the larger an SCC is, the more likely it is to be collapsed early, thus reducing the size of the graph quickly. The algorithm for reducing SCC’s to single nodes is as follows. Let G be the graph to be reduced, and let G′ be G with all the arcs reversed.
1. Pick a node v from G at random.
2. Find NG(v, ∞), the set of nodes reachable from v in G.
394 CHAPTER 10. MINING SOCIAL-NETWORK GRAPHS
   Path Facts Versus Paths
We should be careful to distinguish between a path, which is a sequence of arcs, and a path fact, which is a statement that there exists a path from some node x to some node y. The path fact has been shown typically as P ath(x, y). Smart transitive closure discovers each path only once, but it may discover a path fact more than once. The reason is that often a graph will have many paths from x to y, and may even have many different paths from x to y that are of the same length.
Not all paths are discovered independently by smart transitive closure. For instance, if there are arcs w → x → y → z and also arcs x → u → z, then the path fact P ath(w, z) will be discovered twice, once by combining Path(w,y) with Path(y,z) and again when combining Path(w,u) with Path(u,z). On the other hand, if the arcs were w → x → y → z and w → v → y, then Path(w,z) would be discovered only once, when combining P ath(w, y) with P ath(y, z).
 3. Find NG′ (v, ∞), the set of nodes that v reaches in the graph G′ that has the arcs of G reversed. Equivalently, this set is those nodes that reach v in G.
4. Construct the SCC S containing v, which is NG(v, ∞) ∩ NG′ (v, ∞). That is, v and u are in the same SCC of G if and only if v can reach u and u can reach v.
5. Replace SCC S by a single node s in G. To do so, delete all nodes in S fromGandaddstothenodesetofG. DeletefromGallarcsoneor bothendsofwhichareinS. Then,addtothearcsetofGanarcs→x whenever there was an arc in G from any member of S to x. Finally, add an arc x → s if there was an arc from x to any member of S.
We can iterate the above steps a fixed number of times. We can alternatively iterate until the graph becomes sufficiently small, or we could examine all nodes v in turn and not stop until each node is in an SCC by itself; i.e.,
NG(v, ∞) ∩ NG′ (v, ∞) = {v}
for all remaining nodes v. If we make the latter choice, the resulting graph is called the transitive reduction of the original graph G. The transitive reduction is always acyclic, since if it had a cycle there would remain an SCC of more than one node. However, it is not necessary to reduce to an acyclic graph, as long as the resulting graph has sufficiently few nodes that it is feasible to compute the full transitive closure of this graph; that is, the number of nodes is small enough that we can deal with a result whose size is proportional to the square of that number of nodes.
10.8. NEIGHBORHOOD PROPERTIES OF GRAPHS 395
While the transitive closure of the reduced graph is not exactly the same as the transitive closure of the original graph, the former, plus the information about what SCC each original node belongs to is enough to tell us anything that the transitive closure of the original graph tells us. If we want to know whether Path(u,v) is true in the original graph, find the SCC’s containing u and v. If one or both of these nodes have never been combined into an SCC, then treat that node as an SCC by itself. If u and v belong to the same SCC, then surely u can reach v. If they belong to different SCC’s s and t, respectively, determine whether s reaches t in the reduced graph. If so, then u reaches v in the original graph, and if not, then not.
Example 10.30: Let us reconsider the “bowtie” picture of the Web from Section 5.1.3. The number of nodes in the part of the graph examined was over 200 million; surely too large to deal with data of size proportional to the square of that number. There was one large set of nodes called “the SCC” that was regarded as the center of the graph. Since about one node in four was in this SCC, it would be collapsed to a single node as soon as any one of its members was chosen at random. But there are many other SCC’s in the Web, even though they were not shown explicitly in the “bowtie.” For instance, the in-component would have within it many large SCC’s. The nodes in one of these SCC’s can reach each other, and can reach some of the other nodes in the in-component, and of course they can reach all the nodes in the central SCC. The SCC’s in the in- and out-components, the tubes, and other structures can all be collapsed, leading to a far smaller graph. ✷
10.8.7 Approximating the Sizes of Neighborhoods
In this section we shall take up the problem of computing the neighborhood profile for each node of a large graph. A variant of the problem, which yields to the same technique, is to find the size of the reachable set for each node v, i.e., the set we have called N(v,∞). Recall that for a graph of a billion nodes, it is totally infeasible to compute the neighborhoods for each node, even using a very large cluster of machines. However, even if we want only a count of the nodes in each neighborhood, we need to remember the nodes found so far as we explore the graph, or else we shall not know whether a found node is new or one we have seen already.
On the other hand, it is not so hard to find an approximation to the size of each neighborhood, using the Flajolet-Martin technique discussed in Sec- tion 4.4.2. Recall that this technique uses some large number of hash functions; in this case, the hash functions are applied to the nodes of the graph. The important property of the bit string we get when we apply hash function h to node v is the “tail length” – the number of 0’s at the end of the string. For any set of nodes, an estimate of the size of the set is 2R, where R is the length of the longest tail for any member of the set. Thus, instead of storing all the members of the set, we can instead record only the value of R for that set. Of
396 CHAPTER 10. MINING SOCIAL-NETWORK GRAPHS course, there are many hash functions, so we need to record values of R for each
hash function.
Example 10.31 : If we use a hash function that produces a 64-bit string, then six bits are all that are needed to store each value of R. For instance, if there are a billion nodes, and we want to estimate the size of the neighborhood for each, we can store the value of R for 20 hash functions for each node in 15 gigabytes. ✷
If we store tail lengths for each neighborhood, we can use this information to compute estimates for the larger neighborhoods from our estimates for the smaller neighborhoods. That is, suppose we have computed our estimates for |N(v,d)| for all nodes v, and we want to compute estimates for the neighbor- hoods of radius d + 1. For each hash function h, the value of R for N(v, d + 1) is the largest of:
1. The tail of v itself and
2. The values of R associated with h and N(u,d), where v → u is an arc of
the graph.
Notice that it doesn’t matter whether a node is reachable through only one successor of v in the graph, or through many different successors. We get the same estimate in either case. This useful property was the same one we exploited in Section 4.4.2 to avoid having to know whether a stream element appeared once or many times in the stream.
We shall now describe the complete algorithm, called ANF (Approximate Neighborhood Function). We choose K hash functions h1, h2, . . . , hk. For each node v and radius d, let Ri(v,d) denote the maximum tail length of any node in N(v,d) using hash function hi. To initialize, let Ri(v,0) be the length of the tail of hi(v), for all i and v.
For the inductive step, suppose we have computed Ri(v,d) for all i and v. Initialize Ri(v, d + 1) to be Ri(v, d), for all i and v. Then, consider all arcs x → y in the graph, in any order. For each x → y, set Ri(x,d+1) to the larger of its current value and Ri(y,d). Observe that the fact we can consider the arcs in any order may provide a big speedup in the case that we can store the Ri’s in main memory, while the set of arcs is so large it must be stored on disk. We can stream all the disk blocks containing arcs one at a time, thus using only one disk access per iteration per disk block used for arc storage. This advantage is similar to the one we observed in Section 6.2.1, where we pointed out how frequent-itemset algorithms like A-priori could take advantage of reading market-basket data in a stream, where each disk block was read only once for each round.
To estimate the size of N(v,d), combine the values of the Ri(v,d) for i = 1, 2, . . . , k, as discussed in Section 4.4.3. That is, group the R’s into small groups, take the average, and take the median of the averages.
10.8. NEIGHBORHOOD PROPERTIES OF GRAPHS 397
Another improvement to the ANF Algorithm can be had if we are only interested in estimating the sizes of the reachable sets, N(v,∞). It is not then necessary to save Ri(v, d) for different radii d. We can maintain one value Ri(v) for each hash function hi and each node v. When on any round, we consider arc x → y, we simply assign
Ri(x) := max Ri(x), Ri(y) 
We can stop the iteration when at some round no value of any Ri(v) changes.
Or if we know d is the diameter of the graph, we can just iterate d times. A
BC
HD
IGEF
Figure 10.24: A graph for exercises on neighborhoods and transitive closure
10.8.8 Exercises for Section 10.8
Exercise 10.8.1 : For the graph of Fig. 10.9, which we repeat here as Fig. 10.24:
(a) If the graph is represented as a directed graph, how many arcs are there?
(b) What are the neighborhood profiles for nodes A and B?
(c) What is the diameter of the graph?
(d) How many pairs are in the transitive closure? Hint: Do not forget that there are paths of length greater than zero from a node to itself in this graph.
(e) If we compute the transitive closure by recursive doubling, how many rounds are needed?
Exercise 10.8.2 : The smart transitive closure algorithm breaks paths of any length into head and tail of specific lengths. What are the head and tail lengths for paths of length 7, 8, and 9?
                     
398 CHAPTER 10. MINING SOCIAL-NETWORK GRAPHS
Exercise 10.8.3: Consider the running example of a social network, last shown in Fig. 10.23. Suppose we use one hash function h which maps each node (capital letter) to its ASCII code. Note that the ASCII code for A is 01000001, and the codes for B, C, . . . are sequential, 01000010, 01000011, . . . .
(a) Using this hash function, compute the values of R for each node and radius 1. What are the estimates of the sizes of each neighborhood? How do the estimates compare with reality?
(b) Next, compute the values of R for each node and radius 2. Again compute the estimates of neighborhood sizes and compare with reality.
(c) The diameter of the graph is 3. Compute the value of R and the size estimate for the set of reachable nodes for each of the nodes of the graph.
(d) Another hash function g is one plus the ASCII code for a letter. Repeat (a) through (c) for hash function g. Take the estimate of the size of a neighborhood to be the average of the estimates given by h and g. How close are these estimates?
10.9 Summary of Chapter 10
✦ Social-Network Graphs: Graphs that represent the connections in a social network are not only large, but they exhibit a form of locality, where small subsets of nodes (communities) have a much higher density of edges than the average density.
✦ Communities and Clusters: While communities resemble clusters in some ways, there are also significant differences. Individuals (nodes) normally belong to several communities, and the usual distance measures fail to represent closeness among nodes of a community. As a result, standard algorithms for finding clusters in data do not work well for community finding.
✦ Betweenness: One way to separate nodes into communities is to measure the betweenness of edges, which is the sum over all pairs of nodes of the fraction of shortest paths between those nodes that go through the given edge. Communities are formed by deleting the edges whose betweenness is above a given threshold.
✦ The Girvan-Newman Algorithm: The Girvan-Newman Algorithm is an efficient technique for computing the betweenness of edges. A breadth- first search from each node is performed, and a sequence of labeling steps computes the share of paths from the root to each other node that go through each of the edges. The shares for an edge that are computed for each root are summed to get the betweenness.
10.9. SUMMARY OF CHAPTER 10 399
✦ Communities and Complete Bipartite Graphs: A complete bipartite graph has two groups of nodes, all possible edges between pairs of nodes chosen one from each group, and no edges between nodes of the same group. Any sufficiently dense community (a set of nodes with many edges among them) will have a large complete bipartite graph.
✦ Finding Complete Bipartite Graphs: We can find complete bipartite graphs by the same techniques we used for finding frequent itemsets. Nodes of the graph can be thought of both as the items and as the baskets. The basket corresponding to a node is the set of adjacent nodes, thought of as items. A complete bipartite graph with node groups of size t and s can be thought of as finding frequent itemsets of size t with support s.
✦ Graph Partitioning: One way to find communities is to partition a graph repeatedly into pieces of roughly similar sizes. A cut is a partition of the nodes of the graph into two sets, and its size is the number of edges that have one end in each set. The volume of a set of nodes is the number of edges with at least one end in that set.
✦ Normalized Cuts: We can normalize the size of a cut by taking the ratio of the size of the cut and the volume of each of the two sets formed by the cut. Then add these two ratios to get the normalized cut value. Normalized cuts with a low sum are good, in the sense that they tend to divide the nodes into two roughly equal parts, and have a relatively small size.
✦ Adjacency Matrices: These matrices describe a graph. The entry in row i and column j is 1 if there is an edge between nodes i and j, and 0 otherwise.
✦ Degree Matrices: The degree matrix for a graph has d in the ith diagonal entry if d is the degree of the ith node. Off the diagonal, all entries are 0.
✦ Laplacian Matrices: The Laplacian matrix for a graph is its degree matrix minus its adjacency matrix. That is, the entry in row i and column i of the Laplacian matrix is the degree of the ith node of the graph, and the entry in row i and column j, for i ̸= j, is −1 if there is an edge between nodes i and j, and 0 otherwise.
✦ Spectral Method for Partitioning Graphs: The lowest eigenvalue for any Laplacian matrix is 0, and its corresponding eigenvector consists of all 1’s. The eigenvectors corresponding to small eigenvalues can be used to guide a partition of the graph into two parts of similar size with a small cut value. For one example, putting the nodes with a positive component in the eigenvector with the second-smallest eigenvalue into one set and those with a negative component into the other is usually good.
400 CHAPTER 10. MINING SOCIAL-NETWORK GRAPHS
✦ Overlapping Communities: Typically, individuals are members of several communities. In graphs describing social networks, it is normal for the probability that two individuals are friends to rise as the number of com- munities of which both are members grows.
✦ The Affiliation-Graph Model: An appropriate model for membership in communities is to assume that for each community there is a probability that because of this community two members become friends (have an edge in the social network graph). Thus, the probability that two nodes have an edge is 1 minus the product of the probabilities that none of the communities of which both are members cause there to be an edge between them. We then find the assignment of nodes to communities and the values of those probabilities that best describes the observed social graph.
✦ Maximum-Likelihood Estimation: An important modeling technique, use- ful for modeling communities as well as many other things, is to compute, as a function of all choices of parameter values that the model allows, the probability that the observed data would be generated. The values that yield the highest probability are assumed to be correct, and called the maximum-likelihood estimate (MLE).
✦ Use of Gradient Descent: If we know membership in communities, we can find the MLE by gradient descent or other methods. However, we cannot find the best membership in communities by gradient descent, because membership is discrete, not continuous.
✦ Improved Community Modeling by Strength of Membership: We can for- mulate the problem of finding the MLE of communities in a social graph by assuming individuals have a strength of membership in each commu- nity, possibly 0 if they are not a member. If we define the probability of an edge between two nodes to be a function of their membership strengths in their common communities, we can turn the problem of finding the MLE into a continuous problem and solve it using gradient descent.
✦ Simrank: One way to measure the similarity of nodes in a graph with several types of nodes is to start a random walker at one node and allow it to wander, with a fixed probability of restarting at the same node. The distribution of where the walker can be expected to be is a good measure of the similarity of nodes to the starting node. This process must be repeated with each node as the starting node if we are to get all-pairs similarity.
✦ Triangles in Social Networks: The number of triangles per node is an important measure of the closeness of a community and often reflects its maturity. We can enumerate or count the triangles in a graph with m edges in O(m3/2) time, but no more efficient algorithm exists in general.
10.9. SUMMARY OF CHAPTER 10 401
✦ Triangle Finding by MapReduce: We can find triangles in a single round of MapReduce by treating it as a three-way join. Each edge must be sent to a number of reducers proportional to the cube root of the total number of reducers, and the total computation time spent at all the reducers is proportional to the time of the serial algorithm for triangle finding.
✦ Neighborhoods: The neighborhood of radius d for a node v in a directed or undirected graph is the set of nodes reachable from v along paths of length at most d. The neighborhood profile of a node is the sequence of neighborhood sizes for all distances from 1 upwards. The diameter of a connected graph is the smallest d for which the neighborhood of radius d for any starting node includes the entire graph.
✦ Transitive Closure: A node v can reach node u if u is in the neighborhood of v for some radius. The transitive closure of a graph is the set of pairs of nodes (v, u) such that v can reach u.
✦ Computing Transitive Closure: Since the transitive closure can have a number of facts equal to the square of the number of nodes of a graph, it is infeasible to compute transitive closure directly for large graphs. One approach is to find strongly connected components of the graph and col- lapse them each to a single node before computing the transitive closure.
✦ Transitive Closure and MapReduce: We can view transitive closure com- putation as the iterative join of a path relation (pairs of nodes v and u such that u is known to be reachable from v) and the arc relation of the graph. Such an approach requires a number of MapReduce rounds equal to the diameter of the graph.
✦ Transitive Closure by Recursive Doubling: An approach that uses fewer MapReduce rounds is to join the path relation with itself at each round. At each round, we double the length of paths that are able to contribute to the transitive closure. Thus, the number of needed rounds is only the base-2 logarithm of the diameter of the graph.
✦ Smart Transitive Closure: While recursive doubling can cause the same path to be considered many times, and thus increases the total compu- tation time (compared with iteratively joining paths with single arcs), a variant called smart transitive closure avoids discovering the same path more than once. The trick is to require that when joining two paths, the first has a length that is a power of 2.
✦ Approximating Neighborhood Sizes: By using the Flajolet-Martin tech- nique for approximating the number of distinct elements in a stream, we can find the neighborhood sizes at different radii approximately. We maintain a set of tail lengths for each node. To increase the radius by 1, we examine each edge (u,v) and for each tail length for u we set it
402 CHAPTER 10. MINING SOCIAL-NETWORK GRAPHS equal to the corresponding tail length for v if the latter is larger than the
former.
10.10 References for Chapter 10
Simrank comes from [8]. An alternative approach in [11] views similarity of two nodes as the propability that random walks from the two nodes will be at the same node. [3] combines random walks with node classification to predict links in a social-network graph. [16] looks at the efficiency of computing simrank as a personalized PageRank.
The Girvan-Newman Algorithm is from [6]. Finding communities by search- ing for complete bipartite graphs appears in [9].
Normalized cuts for spectral analysis were introduced in [13]. [19] is a sur- vey of spectral methods for finding clusters, and [5] is a more general survey of community finding in graphs. [10] is an analysis of communities in many networks encountered in practice.
Detection of overlapping communities is explored in [20], [21], and [22].
Counting triangles using MapReduce was discussed in [15]. The method descirbed here is from [1], which also gives a technique that works for any subgraph. [17] discusses randomized algorithms for triangle finding.
The ANF Algorithm was first investigated in [12]. [4] gives an additional speedup to ANF.
The Smart Transitive-Closure Algorithm was discovered by [7] and [18] in- dependently. Implementation of transitive closure using MapReduce or similar systems is discussed in [2].
An open-source C++ implementation of many of the algorithms described in this chapter can be found in the SNAP library [14].
1. F. N. Afrati, D. Fotakis, and J. D. Ullman, “Enumerating subgraph in- stances by map-reduce,”
http://ilpubs.stanford.edu:8090/1020
2. F.N. Afrati and J.D. Ullman, “Transitive closure and recursive Datalog implemented on clusters,” in Proc. EDBT (2012).
3. L. Backstrom and J. Leskovec, “Supervised random walks: predicting and recommending links in social networks,” Proc. Fourth ACM Intl. Conf. on Web Search and Data Mining (2011), pp. 635–644.
4. P. Boldi, M. Rosa, and S. Vigna, “HyperANF: approximating the neigh- bourhood function of very large graphs on a budget,” Proc. WWW Con- ference (2011), pp. 625–634.
5. S. Fortunato, “Community detection in graphs,” Physics Reports 486:3–5 (2010), pp. 75–174.
10.10. REFERENCES FOR CHAPTER 10 403
6. M. Girvan and M.E.J. Newman, “Community structure in social and bi-
ological networks,” Proc. Natl. Acad. Sci. 99 (2002), pp. 7821–7826.
7. Y.E. Ioannidis, “On the computation of the transitive closure of relational
operators,” Proc. 12th Intl. Conf. on Very Large Data Bases, pp. 403–411.
8. G. Jeh and J. Widom, “Simrank: a measure of structural-context similar- ity,” Proceedings of the eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (2002), pp. 538–543.
9. R. Kumar, P. Raghavan, S. Rajagopalan, and A. Tomkins, “Trawling the Web for emerging cyber-communities, Computer Networks 31:11–16 (May, 1999), pp. 1481–1493.
10. J. Leskovec, K.J. Lang, A. Dasgupta, and M.W. Mahoney, “Community structure in large networks: natural cluster sizes and the absence of large well-defined clusters,” http://arxiv.org/abs/0810.1355.
11. S. Melnik, H. Garcia-Molina, and E. Rahm, “Similarity flooding: a ver- satile graph matching algorithm and its application to schema matching, Proc. Intl. Conf. on Data Engineering (2002), pp. 117–128.
12. C.R. Palmer, P.B. Gibbons, and C. Faloutsos, “ANF: a fast and scalable tool for data mining in massive graphs,” Proc. Eighth ACM SIGKDD Intl. Conf. on Knowledge Discovery and Data Mining (2002), pp. 81–90.
13. J. Shi and J. Malik, “Normalized cuts and image segmentation,” IEEE Trans. on Pattern Analysis and Machine Intelligence,” 22:8 (2000), pp. 888– 905.
14. Stanford Network Analysis Platform, http://snap.stanford.edu.
15. S. Suri and S. Vassilivitskii, “Counting triangles and the curse of the last
reducer,” Proc. WWW Conference (2011).
16. H. Tong, C. Faloutsos, and J.-Y. Pan, “Fast random walk with restart
and its applications,” ICDM 2006, pp. 613–622.
17. C.E. Tsourakakis, U. Kang, G.L. Miller, and C. Faloutsos, “DOULION: counting triangles in massive graphs with a coin,” Proc. Fifteenth ACM SIGKDD Intl. Conf. on Knowledge Discovery and Data Mining (2009).
18. P. Valduriez and H. Boral, “Evaluation of recursive queries using join indices,” Expert Database Conf. (1986), pp. 271–293.
19. U. von Luxburg, “A tutorial on spectral clustering,” Statistics and Com- puting bf17:4 (2007), 2007, pp. 395–416.
20. J. Yang and J. Leskovec, “Overlapping community detection at scale: a nonnegative matrix factorization approach,” ACM International Confer- ence on Web Search and Data Mining, 2013.
404 CHAPTER 10. MINING SOCIAL-NETWORK GRAPHS
21. J. Yang, J. McAuley, J. Leskovec, “Detecting cohesive and 2-mode com- munities in directed and undirected networks,” ACM International Con- ference on Web Search and Data Mining, 2014.
22. J. Yang, J. McAuley, J. Leskovec, “Community detection in networks with node attributes,” IEEE International Conference On Data Mining, 2013.
Chapter 11
Dimensionality Reduction
There are many sources of data that can be viewed as a large matrix. We saw in Chapter 5 how the Web can be represented as a transition matrix. In Chapter 9, the utility matrix was a point of focus. And in Chapter 10 we examined matrices that represent social networks. In many of these matrix applications, the matrix can be summarized by finding “narrower” matrices that in some sense are close to the original. These narrow matrices have only a small number of rows or a small number of columns, and therefore can be used much more efficiently than can the original large matrix. The process of finding these narrow matrices is called dimensionality reduction.
We saw a preliminary example of dimensionality reduction in Section 9.4. There, we discussed UV-decomposition of a matrix and gave a simple algorithm for finding this decomposition. Recall that a large matrix M was decomposed into two matrices U and V whose product UV was approximately M. The matrix U had a small number of columns whereas V had a small number of rows, so each was significantly smaller than M, and yet together they represented most of the information in M that was useful in predicting ratings of items by individuals.
In this chapter we shall explore the idea of dimensionality reduction in more detail. We begin with a discussion of eigenvalues and their use in “prin- cipal component analysis” (PCA). We cover singular-value decomposition, a more powerful version of UV-decomposition. Finally, because we are always interested in the largest data sizes we can handle, we look at another form of decomposition, called CUR-decomposition, which is a variant of singular- value decomposition that keeps the matrices of the decomposition sparse if the original matrix is sparse.
405
406 CHAPTER 11. DIMENSIONALITY REDUCTION 11.1 Eigenvalues and Eigenvectors of Symmet-
ric Matrices
We shall assume that you are familiar with the basics of matrix algebra: multi- plication, transpose, determinants, and solving linear equations for example. In this section, we shall define eigenvalues and eigenvectors of a symmetric matrix and show how to find them. Recall a matrix is symmetric if the element in row i and column j equals the element in row j and column i.
11.1.1 Definitions
Let M be a square matrix. Let λ be a constant and e a nonzero column vector with the same number of rows as M. Then λ is an eigenvalue of M and e is the corresponding eigenvector of M if Me = λe.
If e is an eigenvector of M and c is any constant, then it is also true that c e is an eigenvector of M with the same eigenvalue. Multiplying a vector by a constant changes the length of a vector, but not its direction. Thus, to avoid ambiguity regarding the length, we shall require that every eigenvector be a unit vector, meaning that the sum of the squares of the components of the vector is 1. Even that is not quite enough to make the eigenvector unique, since we may still multiply by −1 without changing the sum of squares of the components. Thus, we shall normally require that the first nonzero component of an eigenvector be positive.
Example 11.1 : Let M be the matrix
 3 2 
26
One of the eigenvectors of M is
  1/√5  
 2/√5
and its corresponding eigenvalue is 7. The equation
   3 2    1/√5     1/√5   26 2/√5=72/√5
    demonstrates the truth of this claim. Note that both sides are equal to   7/√5 
Also observe that the eigenvector is a unit vector, because (1/√5)2 + (2/√5)2 = 1/5+4/5=1. ✷
 14/√5
   
11.1. EIGENVALUESANDEIGENVECTORSOFSYMMETRICMATRICES407 11.1.2 Computing Eigenvalues and Eigenvectors
We have already seen one approach to finding an eigenpair (an eigenvalue and its corresponding eigenvector) for a suitable matrix M in Section 5.1: start with any unit vector v of the appropriate length and compute Miv iteratively until it converges.1 When M is a stochastic matrix, the limiting vector is the principal eigenvector (the eigenvector with the largest eigenvalue), and its corresponding eigenvalueis1.2 Thismethodforfindingtheprincipaleigenvector,calledpower iteration, works quite generally, although if the principal eigenvalue (eigenvalue associated with the principal eigenvector) is not 1, then as i grows, the ratio of Mi+1v to Miv approaches the principal eigenvalue while Miv approaches a vector (probably not a unit vector) with the same direction as the principal eigenvector.
We shall take up the generalization of the power-iteration method to find all eigenpairs in Section 11.1.3. However, there is an O(n3)-running-time method for computing all the eigenpairs of a symmetric n × n matrix exactly, and this method will be presented first. There will always be n eigenpairs, although in some cases, some of the eigenvalues will be identical. The method starts by restating the equation that defines eigenpairs, Me = λe as (M − λI)e = 0, where
1. I is the n × n identity matrix with 1’s along the main diagonal and 0’s elsewhere.
2. 0 is a vector of all 0’s.
A fact of linear algebra is that in order for (M − λI)e = 0 to hold for a vector e ̸= 0, the determinant of M − λI must be 0. Notice that (M − λI) looks almost like the matrix M, but if M has c in one of its diagonal elements, then (M − λI) has c − λ there. While the determinant of an n × n matrix has n! terms, it can be computed in various ways in O(n3) time; an example is the method of “pivotal condensation.”
The determinant of (M − λI) is an nth-degree polynomial in λ, from which we can get the n values of λ that are the eigenvalues of M. For any such value, say c, we can then solve the equation Me = ce. There are n equations in n unknowns (the n components of e), but since there is no constant term in any equation, we can only solve for e to within a constant factor. However, using any solution, we can normalize it so the sum of the squares of the components is 1, thus obtaining the eigenvector that corresponds to eigenvalue c.
Example 11.2 : Let us find the eigenpairs for the 2 × 2 matrix M from Ex- ample 11.1. Recall M =
 3 2  26
1Recall Mi denotes multiplying by the matrix M i times, as discussed in Section 5.1.2.
2Note that a stochastic matrix is not generally symmetric. Symmetric matrices and stochastic matrices are two classes of matrices for which eigenpairs exist and can be exploited. In this chapter, we focus on techniques for symmetric matrices.
 
408 CHAPTER 11. DIMENSIONALITY REDUCTION
Then M − λI is
 3−λ2  2 6−λ
The determinant of this matrix is (3 − λ)(6 − λ) − 4, which we must set to 0. The equation in λ to solve is thus λ2 − 9λ + 14 = 0. The roots of this equation are λ = 7 and λ = 2; the first is the principal eigenvalue, since it is the larger. Let e be the vector of unknowns
We must solve
  xy  
 3 2  x =7 x 
26yy
When we multiply the matrix and vector we get two equations
3x+2y = 7x 2x+6y = 7y
Notice that both of these equations really say the same thing: y = 2x. Thus, a possible eigenvector is
  12  
But that vector is not a unit vector, since the sum of the squares of its compo- nents is 5, not 1. Thus to get the unit vector in the same direction, we divide each component by √5. That is, the principal eigenvector is
  1/√5   2/√5
and its eigenvalue is 7. Note that this was the eigenpair we explored in Exam- ple 11.1.
For the second eigenpair, we repeat the above with eigenvalue 2 in place of 7. The equation involving the components of e is x = −2y, and the second
   eigenvector is
  2/√5  −1/√5
  Its corresponding eigenvalue is 2, of course. ✷
11.1.3 Finding Eigenpairs by Power Iteration
We now examine the generalization of the process we used in Section 5.1 to find the principal eigenvector, which in that section was the PageRank vector – all we needed from among the various eigenvectors of the stochastic matrix of the Web. We start by computing the principal eigenvector by a slight general- ization of the approach used in Section 5.1. We then modify the matrix to, in
11.1. EIGENVALUESANDEIGENVECTORSOFSYMMETRICMATRICES409
effect, remove the principal eigenvector. The result is a new matrix whose prin- cipal eigenvector is the second eigenvector (eigenvector with the second-largest eigenvalue) of the original matrix. The process proceeds in that manner, re- moving each eigenvector as we find it, and then using power iteration to find the principal eigenvector of the matrix that remains.
Let M be the matrix whose eigenpairs we would like to find. Start with any nonzero vector x0 and then iterate:
xk+1 := Mxk ∥Mxk∥
where ∥N∥ for a matrix or vector N denotes the Frobenius norm; that is, the square root of the sum of the squares of the elements of N. We multiply the current vector xk by the matrix M until convergence (i.e., ∥xk − xk+1∥ is less than some small, chosen constant). Let x be xk for that value of k at which convergence is obtained. Then x is (approximately) the principal eigenvector of M. To obtain the corresponding eigenvalue we simply compute λ1 = xTMx, which is the equation Mx = λx solved for λ, since x is a unit vector.
Example 11.3 : Take the matrix from Example 11.2: M= 32 
26
and let us start with x0 a vector with 1 for both components. To compute x1, we multiply Mx0 to get
 3 2  1 = 5 
2618
The Frobenius norm of the result is √52 + 82 = √89 = 9.434. We obtain x1 by dividing 5 and 8 by 9.434; that is:
x1 =  0.530   0.848
For the next iteration, we compute
 3 2  0.530 = 3.286 
2 6 0.848 6.148
The Frobenius norm of the result is 6.971, so we divide to obtain
x2 =  0.471   0.882
We are converging toward a normal vector whose second component is twice the first. That is, the limiting value of the vector that we obtain by power iteration is the principal eigenvector:
x=  0.447   0.894
   
410 CHAPTER 11. DIMENSIONALITY REDUCTION Finally, we compute the principal eigenvalue by
λ=xTMx=  0.447 0.894    3 2    0.447  =6.993 2 6 0.894
Recall from Example 11.2 that the true principal eigenvalue is 7. Power iteration will introduce small errors due either to limited precision, as was the case here, or due to the fact that we stop the iteration before reaching the exact value of the eigenvector. When we computed PageRank, the small inaccuracies did not matter, but when we try to compute all eigenpairs, inaccuracies accumulate if we are not careful. ✷
To find the second eigenpair we create a new matrix M∗ = M − λ1xxT. Then, use power iteration on M∗ to compute its largest eigenvalue. The ob- tained x∗ and λ∗ correspond to the second largest eigenvalue and the corre- sponding eigenvector of matrix M.
Intuitively, what we have done is eliminate the influence of a given eigen- vector by setting its associated eigenvalue to zero. The formal justification is the following two observations. If M∗ = M − λxxT, where x and λ are the eigenpair with the largest eigenvalue, then:
1. x is also an eigenvector of M∗, and its corresponding eigenvalue is 0. In proof, observe that
M∗x = (M − λxxT)x = Mx − λxxTx = Mx − λx = 0
At the next-to-last step we use the fact that xTx = 1 because x is a unit
vector.
2. Conversely, if v and λv are an eigenpair of a symmetric matrix M other than the first eigenpair (x,λ), then they are also an eigenpair of M∗. Proof :
M∗v = (M∗)Tv = (M − λxxT)Tv = MTv − λx(xTv) = MTv = λvv This sequence of equalities needs the following justifications:
(a) If M is symmetric, then M = MT.
(b) The eigenvectors of a symmetric matrix are orthogonal. That is, the dot product of any two distinct eigenvectors of a matrix is 0. We do not prove this statement here.
Example 11.4 : Continuing Example 11.3, we compute
M∗ =  3 2  −6.993  0.447 0.894    0.447  =
2 6 0.894
  3 2  −  1.397 2.795  =  1.603 −0.795  
2 6 2.795 5.589 −0.795 0.411
We may find the second eigenpair by processing the matrix above as we did the original matrix M. ✷
11.1. EIGENVALUESANDEIGENVECTORSOFSYMMETRICMATRICES411 11.1.4 The Matrix of Eigenvectors
Suppose we have an n × n symmetric matrix M whose eigenvectors, viewed as column vectors, are e1,e2,...,en. Let E be the matrix whose ith column is ei. Then EET = ETE = I. The explanation is that the eigenvectors of a symmetric matrix are orthonormal. That is, they are orthogonal unit vectors.
ET is therefore
  4/5+1/5 −2/5+2/5  =  1 0   −2/5+2/5 1/5+4/5 0 1
The calculation is similar when we compute ETE. Notice that the 1’s along the main diagonal are the sums of the squares of the components of each of the eigenvectors, which makes sense because they are unit vectors. The 0’s off the diagonal reflect the fact that the entry in the ith row and jth column is the dot product of the ith and jth eigenvectors. Since eigenvectors are orthogonal, these dot products are 0. ✷
11.1.5 Exercises for Section 11.1
Exercise 11.1.1: Find the unit vector in the same direction as the vector [1, 2, 3].
Exercise 11.1.2: Complete Example 11.4 by computing the principal eigen- vector of the matrix that was constructed in this example. How close to the correct solution (from Example 11.2) are you?
Exercise 11.1.3 : For any symmetric 3 × 3 matrix
a−λbc b d−λ e
cef−λ
there is a cubic equation in λ that says the determinant of this matrix is 0. In terms of a through f, find this equation.
Example 11.5 : For the matrix M of Example 11.2, the matrix E is   2/√5 1/√5  
  −1/√5 2/√5
  2/√5 −1/√5  
    1/√5 2/√5 When we compute EET we get
  
412 CHAPTER 11. DIMENSIONALITY REDUCTION Exercise 11.1.4 : Find the eigenpairs for the following matrix:
111 123 135
using the method of Section 11.1.2.
! Exercise 11.1.5 : Find the eigenpairs for the following matrix:
111 123 136
using the method of Section 11.1.2.
Exercise 11.1.6 : For the matrix of Exercise 11.1.4:
(a) Starting with a vector of three 1’s, use power iteration to find an approx- imate value of the principal eigenvector.
(b) Compute an estimate the principal eigenvalue for the matrix.
(c) Construct a new matrix by subtracting out the effect of the principal eigenpair, as in Section 11.1.3.
(d) From your matrix of (c), find the second eigenpair for the original matrix of Exercise 11.1.4.
(e) Repeat (c) and (d) to find the third eigenpair for the original matrix.
Exercise 11.1.7 : Repeat Exercise 11.1.6 for the matrix of Exercise 11.1.5.
11.2 Principal-Component Analysis
Principal-component analysis, or PCA, is a technique for taking a dataset con- sisting of a set of tuples representing points in a high-dimensional space and finding the directions along which the tuples line up best. The idea is to treat the set of tuples as a matrix M and find the eigenvectors for MMT or MTM. The matrix of these eigenvectors can be thought of as a rigid rotation in a high- dimensional space. When you apply this transformation to the original data, the axis corresponding to the principal eigenvector is the one along which the points are most “spread out,” More precisely, this axis is the one along which the variance of the data is maximized. Put another way, the points can best be viewed as lying along this axis, with small deviations from this axis. Likewise, the axis corresponding to the second eigenvector (the eigenvector correspond- ing to the second-largest eigenvalue) is the axis along which the variance of distances from the first axis is greatest, and so on.
11.2. PRINCIPAL-COMPONENT ANALYSIS 413
We can view PCA as a data-mining technique. The high-dimensional data can be replaced by its projection onto the most important axes. These axes are the ones corresponding to the largest eigenvalues. Thus, the original data is approximated by data that has many fewer dimensions and that summarizes well the original data.
11.2.1 An Illustrative Example
We shall start the exposition with a contrived and simple example. In this example, the data is two-dimensional, a number of dimensions that is too small to make PCA really useful. Moreover, the data, shown in Fig. 11.1 has only four points, and they are arranged in a simple pattern along the 45-degree line to make our calculations easy to follow. That is, to anticipate the result, the points can best be viewed as lying along the axis that is at a 45-degree angle, with small deviations in the perpendicular direction.
 (1,2)
(3,4)
(2,1)
(4,3)
Figure 11.1: Four points in a two-dimensional space
To begin, let us represent the points by a matrix M with four rows – one for each point – and two columns, corresponding to the x-axis and y-axis. This
matrix is
1 2 M=21
 3 4  43
Compute MTM, which is
1 2
MTM= 1 2 3 4 2 1= 30 28 
214334 2830 43
We may find the eigenvalues of the matrix above by solving the equation (30 − λ)(30 − λ) − 28 × 28 = 0
414 CHAPTER 11. DIMENSIONALITY REDUCTION aswedidinExample11.2. Thesolutionisλ=58andλ=2.
Following the same procedure as in Example 11.2, we must solve  30 28  x =58 x 
2830y y
When we multiply out the matrix and vector we get two equations
30x+28y = 58x 28x+30y = 58y
Both equations tell us the same thing: x = y. Thus, the unit eigenvector corresponding to the principal eigenvalue 58 is
  1/√2   1/√2
For the second eigenvalue, 2, we perform the same process. Multiply out  30 28  x =2 x 
  to get the two equations
2830y y 30x+28y = 2x
28x+30y = 2y
Both equations tell us the same thing: x = −y. Thus, the unit eigenvector corresponding to the principal eigenvalue 2 is
  −1/√2   1/√2
While we promised to write eigenvectors with their first component positive, we choose the opposite here because it makes the transformation of coordinates easier to follow in this case.
Now, let us construct E, the matrix of eigenvectors for the matrix MTM. Placing the principal eigenvector first, the matrix of eigenvectors is
  1/√2 −1/√2   E= 1/√2 1/√2
Any matrix of orthonormal vectors (unit vectors that are orthogonal to one another) represents a rotation of the axes of a Euclidean space. The matrix above can be viewed as a rotation 45 degrees counterclockwise. For example, let us multiply the matrix M that represents each of the points of Fig. 11.1 by E. The product is
      12 3/√2 1/√2 2 1   1/√2 −1/√2   3/√2 −1/√2
      ME= 3 4  1/√2 1/√2 = 7/√2 1/√2  4 3 7/√2 −1/√2
      
11.2. PRINCIPAL-COMPONENT ANALYSIS 415 (3,4)
 (1,2)
(3.5,3.5) (4,3)
(1.5,1.5) (2,1)
Figure 11.2: Figure 11.1 with the axes rotated 45 degrees counterclockwise
We see the first point, [1, 2], has been transformed into the point [3/√2, 1/√2]
If we examine Fig. 11.2, with the dashed line representing the new x-axis, we see that the projection of the first point onto that axis places it at distance 3/√2 from the origin. To check this fact, notice that the point of projection for both the first and second points is [1.5, 1.5] in the original coordinate system, and the distance from the origin to this point is
 (1.5)2 + (1.5)2 =  9/2 = 3/√2
Moreover, the new y-axis is, of course, perpendicular to the dashed line. The first point is at distance 1/√2 above the new x-axis in the direction of the y-axis. That is, the distance between the points [1, 2] and [1.5, 1.5] is
 (1 − 1.5)2 + (2 − 1.5)2 =  (−1/2)2 + (1/2)2 =  1/2 = 1/√2 Figure 11.3 shows the four points in the rotated coordinate system.
(3/   2 , 1/   2 ) (7/   2 , 1/   2 )
(3/ 2 , −1/ 2 ) (7/ 2 , −1/ 2 )
Figure 11.3: The points of Fig. 11.1 in the new coordinate system
The second point, [2,1] happens by coincidence to project onto the same point of the new x-axis. It is 1/√2 below that axis along the new y-axis, as is
                     
416 CHAPTER 11. DIMENSIONALITY REDUCTION
confirmed by the fact that the second row in the matrix of transformed points is [3/√2, −1/√2]. The third point, [3, 4] is transformed into [7/√2, 1/√2] and the fourth point, [4,3], is transformed to [7/√2,−1/√2]. That is, they both project onto the same point of the new x-axis, and that point is at distance 7/√2 from the origin, while they are 1/√2 above and below the new x-axis in the direction of the new y-axis.
11.2.2 Using Eigenvectors for Dimensionality Reduction
From the example we have just worked out, we can see a general principle. If M is a matrix whose rows each represent a point in a Euclidean space with any number of dimensions, we can compute MTM and compute its eigenpairs. Let E be the matrix whose columns are the eigenvectors, ordered as largest eigenvalue first. Define the matrix L to have the eigenvalues of MTM along the diagonal, largest first, and 0’s in all other entries. Then, since MTMe = λe = eλ for each eigenvector e and its corresponding eigenvalue λ, it follows that MTME = EL.
We observed that ME is the points of M transformed into a new coordi- nate space. In this space, the first axis (the one corresponding to the largest eigenvalue) is the most significant; formally, the variance of points along that axis is the greatest. The second axis, corresponding to the second eigenpair, is next most significant in the same sense, and the pattern continues for each of the eigenpairs. If we want to transform M to a space with fewer dimen- sions, then the choice that preserves the most significance is the one that uses the eigenvectors associated with the largest eigenvalues and ignores the other eigenvalues.
That is, let Ek be the first k columns of E. Then MEk is a k-dimensional representation of M.
Example 11.6 : Let M be the matrix from Section 11.2.1. This data has only two dimensions, so the only dimensionality reduction we can do is to use k = 1; i.e., project the data onto a one dimensional space. That is, we compute ME1 by
        1 2 3/√2 2 1   1/√2   3/√2
   3 4 1/√2 =7/√2 4 3 7/√2
   The effect of this transformation is to replace the points of M by their pro- jections onto the x-axis of Fig. 11.3. While the first two points project to the same point, as do the third and fourth points, this representation makes the best possible one-dimensional distinctions among the points. ✷
11.2. PRINCIPAL-COMPONENT ANALYSIS 417 11.2.3 The Matrix of Distances
Let us return to the example of Section 11.2.1, but instead of starting with MTM, let us examine the eigenvalues of MMT. Since our example M has more rows than columns, the latter is a bigger matrix than the former, but if M had more columns than rows, we would actually get a smaller matrix. In the running example, we have
 1 2   5 4 11 10  MMT=2 1 1 2 3 4 =4 5 10 11  3 4  2 1 4 3  1 1 1 0 2 5 2 4 
4 3 10 11 24 25
Like MTM, we see that MMT is symmetric. The entry in the ith row and jth column has a simple interpretation; it is the dot product of the vectors represented by the ith and jth points (rows of M).
There is a strong relationship between the eigenvalues of MTM and MMT. Suppose e is an eigenvector of MTM; that is,
MTMe = λe
Multiply both sides of this equation by M on the left. Then
MMT(Me) = Mλe = λ(Me)
Thus, as long as Me is not the zero vector 0, it will be an eigenvector of MMT and λ will be an eigenvalue of MMT as well as of MTM.
The converse holds as well. That is, if e is an eigenvector of MMT with corresponding eigenvalue λ, then start with MMTe = λe and multiply on the left by MT to conclude that MTM(MTe) = λ(MTe). Thus, if MTe is not 0, then λ is also an eigenvalue of MTM.
We might wonder what happens when MTe = 0. In that case, MMTe is also 0, but e is not 0 because 0 cannot be an eigenvector. However, since 0 = λe, we conclude that λ = 0.
We conclude that the eigenvalues of MMT are the eigenvalues of MTM plus additional 0’s. If the dimension of MMT were less than the dimension of MTM, then the opposite would be true; the eigenvalues of MTM would be those of MMT plus additional 0’s.
 3/√116 3/√116  7/√116  7/√116
Figure 11.4:
1/2 7/√116 1/2  −1/2 7/√116 −1/2
    1/2 −3/√116 −1/2  −1/2 −3/√116 1/2 
    Eigenvector matrix for MMT
418 CHAPTER 11. DIMENSIONALITY REDUCTION
Example 11.7: The eigenvalues of MMT for our running example must in- clude 58 and 2, because those are the eigenvalues of MTM as we observed in Section 11.2.1. Since MMT is a 4 × 4 matrix, it has two other eigenvalues, which must both be 0. The matrix of eigenvectors corresponding to 58, 2, 0, and 0 is shown in Fig. 11.4. ✷
11.2.4 Exercises for Section 11.2
Exercise 11.2.1 : Let M be the matrix of data points 1 1
2 4  3 9 
4 16
(b) Compute the eigenpairs for MTM.
! (c) What do you expect to be the eigenvalues of MMT?
! (d) Find the eigenvectors of MMT, using your eigenvalues from part (c).
! Exercise 11.2.2: Prove that if M is any matrix, then MTM and MMT are
symmetric.
11.3 Singular-Value Decomposition
We now take up a second form of matrix analysis that leads to a low-dimensional representation of a high-dimensional matrix. This approach, called singular- value decomposition (SVD), allows an exact representation of any matrix, and also makes it easy to eliminate the less important parts of that representation to produce an approximate representation with any desired number of dimensions. Of course the fewer the dimensions we choose, the less accurate will be the approximation.
We begin with the necessary definitions. Then, we explore the idea that the SVD defines a small number of “concepts” that connect the rows and columns of the matrix. We show how eliminating the least important concepts gives us a smaller representation that closely approximates the original matrix. Next, we see how these concepts can be used to query the original matrix more efficiently, and finally we offer an algorithm for performing the SVD itself.
11.3.1 Definition of SVD
Let M be an m×n matrix, and let the rank of M be r. Recall that the rank of a matrix is the largest number of rows (or equivalently columns) we can choose
(a) What are MTM and MMT?
11.3. SINGULAR-VALUE DECOMPOSITION 419
for which no nonzero linear combination of the rows is the all-zero vector 0 (we say a set of such rows or columns is independent). Then we can find matrices U, Σ, and V as shown in Fig. 11.5 with the following properties:
1. U is an m × r column-orthonormal matrix ; that is, each of its columns is a unit vector and the dot product of any two columns is 0.
2. V is an n × r column-orthonormal matrix. Note that we always use V in its transposed form, so it is the rows of V T that are orthonormal.
3. Σ is a diagonal matrix; that is, all elements not on the main diagonal are
0. The elements of Σ are called the singular values of M.
nrrn
=
Figure 11.5: The form of a singular-value decomposition
r
                          m
M
U
  Example 11.8 : Figure 11.6 gives a rank-2 matrix representing ratings of movies by users. In this contrived example there are two “concepts” underlying the movies: science-fiction and romance. All the boys rate only science-fiction, and all the girls rate only romance. It is this existence of two strictly adhered to concepts that gives the matrix a rank of 2. That is, we may pick one of the first four rows and one of the last three rows and observe that there is no nonzero linear sum of these rows that is 0. But we cannot pick three independent rows. For example, if we pick rows 1, 2, and 7, then three times the first minus the second, plus zero times the seventh is 0.
We can make a similar observation about the columns. We may pick one of the first three columns and one of the last two coluns, and they will be independent, but no set of three columns is independent.
The decomposition of the matrix M from Fig. 11.6 into U , Σ, and V , with all elements correct to two significant digits, is shown in Fig. 11.7. Since the rank of M is 2, we can use r = 2 in the decomposition. We shall see how to compute this decomposition in Section 11.3.6. ✷
Σ
T
V
420
CHAPTER 11.
Joe
Jim John Jack
Jill Jenny Jane
Figure 11.6: Ratings of movies by users
 11100 .14 0 33300 .42 0
4 4 4 0 0 .56 0  12.4 5 5 5 0 0=.70 0  0 00044 0 .60 00055 0 .75
0   .58 .58 .58 9.5 0 0 0
0   .71 .71
0 0 0 2 2 0 .30 MUΣVT
Figure 11.7: SVD for the matrix M of Fig. 11.6 11.3.2 Interpretation of SVD
DIMENSIONALITY REDUCTION
11100 33300 44400 55500 00044 00055 00022
The key to understanding what SVD offers is in viewing the r columns of U, Σ, and V as representing concepts that are hidden in the original matrix M. In Example 11.8, these concepts are clear; one is “science fiction” and the other is “romance.” Let us think of the rows of M as people and the columns of M as movies. Then matrix U connects people to concepts. For example, the person Joe, who corresponds to row 1 of M in Fig. 11.6, likes only the concept science fiction. The value 0.14 in the first row and first column of U is smaller than some of the other entries in that column, because while Joe watches only science fiction, he doesn’t rate those movies highly. The second column of the first row of U is 0, because Joe doesn’t rate romance movies at all.
The matrix V relates movies to concepts. The 0.58 in each of the first three columns of the first row of V T indicates that the first three movies – The Matrix, Alien, and Star Wars – each are of the science-fiction genre, while the 0’s in the last two columns of the first row say that these movies do not partake of the concept romance at all. Likewise, the second row of V T tells us that the
0
Titanic Casablanca
Star Wars Alien Matrix
11.3. SINGULAR-VALUE DECOMPOSITION 421
movies Casablanca and Titanic are exclusively romances.
Finally, the matrix Σ gives the strength of each of the concepts. In our
example, the strength of the science-fiction concept is 12.4, while the strength of the romance concept is 9.5. Intuitively, the science-fiction concept is stronger because the data provides more information about the movies of that genre and the people who like them.
In general, the concepts will not be so clearly delineated. There will be fewer 0’s in U and V , although Σ is always a diagonal matrix and will always have 0’s off the diagonal. The entities represented by the rows and columns of M (analogous to people and movies in our example) will partake of several different concepts to varying degrees. In fact, the decomposition of Example 11.8 was especially simple, since the rank of the matrix M was equal to the desired number of columns of U, Σ, and V. We were therefore able to get an exact decomposition of M with only two columns for each of the three matrices U, Σ, and V ; the product UΣV T, if carried out to infinite precision, would be exactly M. In practice, life is not so simple. When the rank of M is greater than the number of columns we want for the matrices U , Σ, and V , the decomposition is not exact. We need to eliminate from the exact decomposition those columns of U and V that correspond to the smallest singular values, in order to get the best approximation. The following example is a slight modification of Example 11.8 that will illustrate the point.
Joe
Jim John Jack
Jill Jenny Jane
Figure 11.8: The new matrix M′, with ratings for Alien by two additional raters
Example 11.9 : Figure 11.8 is almost the same as Fig. 11.6, but Jill and Jane rated Alien, although neither liked it very much. The rank of the matrix in Fig. 11.8 is 3; for example the first, sixth, and seventh rows are independent, but you can check that no four rows are independent. Figure 11.9 shows the decomposition of the matrix from Fig. 11.8.
We have used three columns for U, Σ, and V because they decompose a matrix of rank three. The columns of U and V still correspond to concepts. The first is still “science fiction” and the second is “romance.” It is harder to
 11100 33300 44400 55500 02044 00055 01022
Titanic Casablanca
Star Wars Alien Matrix
422
CHAPTER 11. DIMENSIONALITY REDUCTION
11100 33300
 4 4 4 0 0  5 5 5 0 0=  0 2 0 4 4 
 0 0 0 5 5  01022
M′
.02 −.01 
.07 −.03 
.09 −.0412.4 0 0 .56 .59 .56 .09 .09 .11 −.05 0 9.5 0 .12 −.02 .12 −.69 −.69
−.59 .65  0 0 1.3 .40 −.80 .40 .09 .09 −.73 −.67 
−.29 .32
UΣVT Figure 11.9: SVD for the matrix M′ of Fig. 11.8
 .13  .41  .55 .68  .15  .07
.07
explain the third column’s concept, but it doesn’t matter all that much, because its weight, as given by the third nonzero entry in Σ, is very low compared with the weights of the first two concepts. ✷
In the next section, we consider eliminating some of the least important concepts. For instance, we might want to eliminate the third concept in Ex- ample 11.9, since it really doesn’t tell us much, and the fact that its associated singular value is so small confirms its unimportance.
11.3.3 Dimensionality Reduction Using SVD
Suppose we want to represent a very large matrix M by its SVD components U , Σ, and V , but these matrices are also too large to store conveniently. The best way to reduce the dimensionality of the three matrices is to set the smallest of the singular values to zero. If we set the s smallest singular values to 0, then we can also eliminate the corresponding s columns of U and V .
Example 11.10 : The decomposition of Example 11.9 has three singular val- ues. Suppose we want to reduce the number of dimensions to two. Then we set the smallest of the singular values, which is 1.3, to zero. The effect on the expression in Fig. 11.9 is that the third column of U and the third row of V T are
11.3. SINGULAR-VALUE DECOMPOSITION 423
multiplied only by 0’s when we perform the multiplication, so this row and this column may as well not be there. That is, the approximation to M′ obtained by using only the two largest singular values is that shown in Fig. 11.10.
 .13  .41
 .55
 .68
 .15
 . 0 7
.07
.02 .07 .09 .11
−.59 − . 7 3 −.29
 
  12.4 0    .56  0 9.5 .12
.59 .56 .09 .09   −.02 .12 −.69 −.69
.014 .014  .000 .000  .026 .026  .040 .040  4.04 4.04  4.87 4.87  1.98 1.98
The resulting matrix is quite close to the matrix M ′ of Fig. 11.8. Ideally, the entire difference is the result of making the last singular value be 0. However, in this simple example, much of the difference is due to rounding error caused by the fact that the decomposition of M′ was only correct to two significant digits. ✷
11.3.4 Why Zeroing Low Singular Values Works
The choice of the lowest singular values to drop when we reduce the number of dimensions can be shown to minimize the root-mean-square error between the original matrix M and its approximation. Since the number of entries is fixed, and the square root is a monotone operation, we can simplify and compare the Frobenius norms of the matrices involved. Recall that the Frobenius norm of a matrix M, denoted ∥M∥, is the square root of the sum of the squares of the elements of M. Note that if M is the difference between one matrix and its approximation, then ∥M∥ is proportional to the RMSE (root-mean-square error) between the matrices.
To explain why choosing the smallest singular values to set to 0 minimizes the RMSE or Frobenius norm of the difference between M and its approxima- tion, let us begin with a little matrix algebra. Suppose M is the product of three matrices M = PQR. Let mij, pij, qij, and rij be the elements in row i and column j of M, P, Q, and R, respectively. Then the definition of matrix
 
= 
 0.93 0.95
 2.93 2.99
 3.92 4.01
0.93 2.93 3.92 4.84 0.37 0.35 0.16
4.84 4.96  0.37 1.21  0.35 0.65
0.16 0.57
Figure 11.10: Dropping the lowest singular value from the decomposition of
Fig. 11.7
424
CHAPTER 11. DIMENSIONALITY REDUCTION
   How Many Singular Values Should We Retain?
A useful rule of thumb is to retain enough singular values to make up 90% of the energy in Σ. That is, the sum of the squares of the retained singular values should be at least 90% of the sum of the squares of all the singular values. In Example 11.10, the total energy is (12.4)2 + (9.5)2 + (1.3)2 = 245.70, while the retained energy is (12.4)2 + (9.5)2 = 244.01. Thus, we have retained over 99% of the energy. However, were we to eliminate the second singular value, 9.5, the retained energy would be only (12.4)2/245.70 or about 63%.
 multiplication tells us
Then
mij =  pikqklrlj kl
∥M∥2 =   (mij)2 =      pikqklrlj 2 ij ijkl
When we square a sum of terms, as we do on the right side of Equation 11.1, we effectively create two copies of the sum (with different indices of summation) and multiply each term of the first sum by each term of the second sum. That is,
   pikqklrlj 2 =     pikqklrljpinqnmrmj kl klmn
we can thus rewrite Equation 11.1 as
∥M∥2 =       pikqklrljpinqnmrmj
ijklnm
(11.2)
Now, let us examine the case where P, Q, and R are really the SVD of M. That is, P is a column-orthonormal matrix, Q is a diagonal matrix, and R is the transpose of a column-orthonormal matrix. That is, R is row-orthonormal; its rows are unit vectors and the dot product of any two different rows is 0. To begin, since Q is a diagonal matrix, qkl and qnm will be zero unless k = l and n = m. We can thus drop the summations for l and m in Equation 11.2 and set k = l and n = m. That is, Equation 11.2 becomes
∥M∥2 =     pikqkkrkjpinqnnrnj (11.3) ijkn
Next, reorder the summation, so i is the innermost sum. Equation 11.3 has only two factors pik and pin that involve i; all other factors are constants as far as summation over i is concerned. Since P is column-orthonormal, We know
(11.1)
11.3. SINGULAR-VALUE DECOMPOSITION 425
that  i pikpin is 1 if k = n and 0 otherwise. That is, in Equation 11.3 we can set k = n, drop the factors pik and pin, and eliminate the sums over i and n, yielding
∥M∥2 =   qkkrkjqkkrkj (11.4) jk
Since R is row-orthonormal,  j rkjrkj is 1. Thus, we can eliminate the terms rkj and the sum over j, leaving a very simple formula for the Frobenius
norm:
∥M∥2 =  (qkk)2 (11.5) k
Next, let us apply this formula to a matrix M whose SVD is M = UΣV T. Let the ith diagonal element of Σ be σi, and suppose we preserve the first n of the r diagonal elements of Σ, setting the rest to 0. Let Σ′ be the resulting diagonal matrix. Let M′ = UΣ′V T be the resulting approximation to M. Then M − M′ = U(Σ − Σ′)V T is the matrix giving the errors that result from our approximation.
If we apply Equation 11.5 to the matrix M − M′, we see that ∥M − M′∥2 equals the sum of the squares of the diagonal elements of Σ − Σ′. But Σ − Σ′ has 0 for the first n diagonal elements and σi for the ith diagonal element, where n < i ≤ r. That is, ∥M −M′∥2 is the sum of the squares of the elements of Σ that were set to 0. To minimize ∥M − M′∥2, pick those elements to be the smallest in Σ. Doing so gives the least possible value of ∥M − M′∥2 under the constraint that we preserve n of the diagonal elements, and it therefore minimizes the RMSE under the same constraint.
11.3.5 Querying Using Concepts
In this section we shall look at how SVD can help us answer certain queries efficiently, with good accuracy. Let us assume for example that we have de- composed our original movie-rating data (the rank-2 data of Fig. 11.6) into the SVD form of Fig. 11.7. Quincy is not one of the people represented by the original matrix, but he wants to use the system to know what movies he would like. He has only seen one movie, The Matrix, and rated it 4. Thus, we can represent Quincy by the vector q = [4, 0, 0, 0, 0], as if this were one of the rows of the original matrix.
If we used a collaborative-filtering approach, we would try to compare Quincy with the other users represented in the original matrix M. Instead, we can map Quincy into “concept space” by multiplying him by the matrix V of the decomposition. We find qV = [2.32, 0].3 That is to say, Quincy is high in science-fiction interest, and not at all interested in romance.
We now have a representation of Quincy in concept space, derived from, but different from his representation in the original “movie space.” One useful thing we can do is to map his representation back into movie space by multiplying
3Note that Fig. 11.7 shows V T, while this multiplication requires V .
 
426 CHAPTER 11. DIMENSIONALITY REDUCTION
[2.32, 0] by V T. This product is [1.35, 1.35, 1.35, 0, 0]. It suggests that Quincy would like Alien and Star Wars, but not Casablanca or Titanic.
Another sort of query we can perform in concept space is to find users similar to Quincy. We can use V to map all users into concept space. For example, Joe maps to [1.74,0], and Jill maps to [0,5.68]. Notice that in this simple example, all users are either 100% science-fiction fans or 100% romance fans, so each vector has a zero in one component. In reality, people are more complex, and they will have different, but nonzero, levels of interest in various concepts. In general, we can measure the similarity of users by their cosine distance in concept space.
Example 11.11 : For the case introduced above, note that the concept vectors for Quincy and Joe, which are [2.32,0] and [1.74,0], respectively, are not the same, but they have exactly the same direction. That is, their cosine distance is 0. On the other hand, the vectors for Quincy and Jill, which are [2.32, 0] and [0, 5.68], respectively, have a dot product of 0, and therefore their angle is 90 degrees. That is, their cosine distance is 1, the maximum possible. ✷
11.3.6 Computing the SVD of a Matrix
The SVD of a matrix M is strongly connected to the eigenvalues of the symmet- ric matrices MTM and MMT. This relationship allows us to obtain the SVD of M from the eigenpairs of the latter two matrices. To begin the explanation, start with M = UΣV T, the expression for the SVD of M. Then
MT = (UΣVT)T = (VT)TΣTUT = VΣTUT
Since Σ is a diagonal matrix, transposing it has no effect. Thus, MT = V ΣUT. Now, MTM = V ΣUTUΣV T. Remember that U is an orthonormal matrix,
so UTU is the identity matrix of the appropriate size. That is, MTM =VΣ2VT
Multiply both sides of this equation on the right by V to get MTMV =VΣ2VTV
Since V is also an orthonormal matrix, we know that V TV is the identity. Thus MTMV =VΣ2 (11.6)
Since Σ is a diagonal matrix, Σ2 is also a diagonal matrix whose entry in the ith row and column is the square of the entry in the same position of Σ. Now, Equation (11.6) should be familiar. It says that V is the matrix of eigenvectors of MTM and Σ2 is the diagonal matrix whose entries are the corresponding eigenvalues.
11.3. SINGULAR-VALUE DECOMPOSITION 427
Thus, the same algorithm that computes the eigenpairs for MTM gives us the matrix V for the SVD of M itself. It also gives us the singular values for this SVD; just take the square roots of the eigenvalues for MTM.
Only U remains to be computed, but it can be found in the same way we found V . Start with
MMT =UΣVT(UΣVT)T =UΣVTVΣUT =UΣ2UT
Then by a series of manipulations analogous to the above, we learn that
MMTU = UΣ2
That is, U is the matrix of eigenvectors of MMT.
A small detail needs to be explained concerning U and V . Each of these
matrices have r columns, while MTM is an n×n matrix and MMT is an m×m matrix. Both n and m are at least as large as r. Thus, MTM and MMT should have an additional n − r and m − r eigenpairs, respectively, and these pairs do not show up in U, V, and Σ. Since the rank of M is r, all other eigenvalues will be 0, and these are not useful.
11.3.7 Exercises for Section 11.3
Exercise 11.3.1 : In Fig. 11.11 is a matrix M . It has rank 2, as you can see by observing that the first column plus the third column minus twice the second column equals 0.
(a) ! (b) (c) (d)
(e)
123 345
5 4 3  0 2 4  135
Figure 11.11: Matrix M for Exercise 11.3.1
Compute the matrices MTM and MMT.
Find the eigenvalues for your matrices of part (a). Find the eigenvectors for the matrices of part (a).
Find the SVD for the original matrix M from parts (b) and (c). Note that there are only two nonzero eigenvalues, so your matrix Σ should have only two singular values, while U and V have only two columns.
Set your smaller singular value to 0 and compute the one-dimensional approximation to the matrix M from Fig. 11.11.
428 CHAPTER 11. DIMENSIONALITY REDUCTION (f) How much of the energy of the original singular values is retained by the
one-dimensional approximation?
Exercise 11.3.2 : Use the SVD from Fig. 11.7. Suppose Leslie assigns rating 3 to Alien and rating 4 to Titanic, giving us a representation of Leslie in “movie space” of [0, 3, 0, 0, 4]. Find the representation of Leslie in concept space. What does that representation predict about how well Leslie would like the other movies appearing in our example data?
! Exercise 11.3.3 : Demonstrate that the rank of the matrix in Fig. 11.8 is 3.
! Exercise 11.3.4: Section 11.3.5 showed how to guess the movies a person would most like. How would you use a similar technique to guess the people that would most like a given movie, if all you had were the ratings of that movie by a few people?
11.4 CUR Decomposition
There is a problem with SVD that does not show up in the running example of Section 11.3. In large-data applications, it is normal for the matrix M being decomposed to be very sparse; that is, most entries are 0. For example, a matrix representing many documents (as rows) and the words they contain (as columns) will be sparse, because most words are not present in most documents. Similarly, a matrix of customers and products will be sparse because most people do not buy most products.
We cannot deal with dense matrices that have millions or billions of rows and/or columns. However, with SVD, even if M is sparse, U and V will be dense.4 Since Σ is diagonal, it will be sparse, but Σ is usually much smaller than U and V , so its sparseness does not help.
In this section, we shall consider another approach to decomposition, called CUR-decomposition. The merit of this approach lies in the fact that if M is sparse, then the two large matrices (called C and R for “columns” and “rows”) analogous to U and V in SVD are also sparse. Only the matrix in the middle (analogous to Σ in SVD) is dense, but this matrix is small so the density does not hurt too much.
Unlike SVD, which gives an exact decomposition as long as the parameter r is taken to be at least as great as the rank of the matrix M , CUR-decomposition is an approximation no matter how large we make r. There is a theory that guarantees convergence to M as r gets larger, but typically you have to make r so large to get, say within 1% that the method becomes impractical. Neverthe- less, a decomposition with a relatively small value of r has a good probability of being a useful and accurate decomposition.
4In Fig. 11.7, it happens that U and V have a significant number of 0’s. However, that is an artifact of the very regular nature of our example matrix M and is not the case in general.
 
11.4. CUR DECOMPOSITION 429
   Why the Pseudoinverse Works
In general suppose a matrix M is equal to a product of matrices XZY . If all the inverses exist, then the rule for inverse of a product tell us M−1 = Y−1Z−1X−1. Since in the case we are interested in, XZY is an SVD, we know X is column-orthonormal and Y is row-orthonormal. In either of these cases, the inverse and the transpose are the same. That is, XXT is an identity matrix of the appropriate size, and so is YYT. Thus,M−1 =YTZ−1XT.
We also know Z is a diagonal matrix. If there are no 0’s along the diagonal, then Z−1 is formed from Z by taking the numerical inverse of each diagonal element. It is only when there are 0’s along the diagonal of Z that we are unable to find an element for the same position in the inverse such that we can get an identity matrix when we multiply Z by its inverse. That is why we resort to a “pseudoinverse,” accepting the fact that the product ZZ+ will not be an identity matrix, but rather a diagonal matrix where the ith diagonal entry is 1 if the ith element of Z is nonzero and 0 if the ith element of Z is 0.
 11.4.1 Definition of CUR
Let M be a matrix of m rows and n columns. Pick a target number of “concepts” r to be used in the decomposition. A CUR-decomposition of M is a randomly chosen set of r columns of M , which form the m × r matrix C , and a randomly chosen set of r rows of M, which form the r × n matrix R. There is also an
r × r 1.
2. 3.
4.
matrix U that is constructed from C and R as follows:
Let W be the r × r matrix that is the intersection of the chosen columns of C and the chosen rows of R. That is, the element in row i and column j of W is the element of M whose column is the jth column of C and whose row is the ith row of R.
Compute the SVD of W; say W = XΣYT.
Compute Σ+, the Moore-Penrose pseudoinverse of the diagonal matrix Σ. That is, if the ith diagonal element of Σ is σ ̸= 0, then replace it by 1/σ. But if the ith element is 0, leave it as 0.
Let U = Y (Σ+)2XT.
We shall defer to Section 11.4.3 an example where we illustrate the entire CUR process, including the important matter of how the matrices C and R should be chosen to make the approximation to M have a small expected value.
430 CHAPTER 11. DIMENSIONALITY REDUCTION 11.4.2 Choosing Rows and Columns Properly
Recall that the choice of rows and columns is random. However, this choice must be biased so that the more important rows and columns have a better chance of being picked. The measure of importance we must use is the square of the Frobenius norm, that is, the sum of the squares of the elements of the row or column. Let f =  i,j m2ij, the square of the Frobenius norm of M. Then each time we select a row, the probability pi with which we select row i is  j m2ij/f. Each time we select a column, the probability qj with which we select column j is  i m2ij/f.
Joe
Jim John Jack
Jill Jenny Jane
Figure 11.12: Matrix M, repeated from Fig. 11.6
Example 11.12 : Let us reconsider the matrix M from Fig. 11.6, which we repeat here as Fig. 11.12. The sum of the squares of the elements of M is 243. The three columns for the science-fiction movies The Matrix, Alien, and Star Wars each have a squared Frobenius norm of 12 + 32 + 42 + 52 = 51, so their probabilities are each 51/243 = .210. The remaining two columns each have a squared Frobenius norm of 42 + 52 + 22 = 45, and therefore their probabilities are each 45/243 = .185.
The seven rows of M have squared Frobenius norms of 3, 27, 48, 75, 32, 50, and 8, respectively. Thus, their respective probabilities are .012, .111, .198, .309, .132, .206, and .033. ✷
Now, let us select r columns for the matrix C. For each column, we choose randomly from the columns of M. However, the selection is not with uniform probability; rather, the jth column is selected with probability qj. Recall that probability is the sum of the squares of the elements in that column divided by the sum of the squares of all the elements of the matrix. Each column of C is chosen independently from the columns of M, so there is some chance that a column will be selected more than once. We shall discuss how to deal with this situation after explaining the basics of CUR-decomposition.
 11100 33300 44400 55500 00044 00055 00022
Titanic Casablanca
Star Wars Alien Matrix
11.4. CUR DECOMPOSITION 431
Having selected each of the columns of M , we scale each column by dividing its elements by the square root of the expected number of times this column would be picked. That is, we divide the elements of the jth column of M, if it is selected, by √rqj. The scaled column of M becomes a column of C.
Rows of M are selected for R in the analogous way. For each row of R we select from the rows of M, choosing row i with probability pi. Recall pi is the sum of the squares of the elements of the ith row divided by the sum of the squares of all the elements of M. We then scale each chosen row by dividing
 by √
rpi if it is the ith row of M that was chosen.
 Example 11.13: Let r = 2 for our CUR-decomposition. Suppose that our random selection of columns from matrix M of Fig. 11.12 is first Alien (the second column) and then Casablanca (the fourth column). The column for Alien
is [1, 3, 4, 5, 0, 0, 0]T, and we must scale this column by dividing by √
rq2. Recall from Example 11.12 that the probability associated with the Alien column is .210, so the division is by √2 × 0.210 = 0.648. To two decimal places, the scaled column for Alien is [1.54, 4.63, 6.17, 7.72, 0, 0, 0]T. This column becomes
  the first column of C.
The second column of C is constructed by taking the column of M for
Casablanca, which is [0, 0, 0, 0, 4, 5, 2]T, and dividing it by √rp4 = √2 × 0.185 = 0.608. Thus, the second column of C is [0, 0, 0, 0, 6.58, 8.22, 3.29]T to two deci- mal places.
  Now, let us choose the rows for R. The most likely rows to be chosen are those for Jenny and Jack, so let’s suppose these rows are indeed chosen, Jenny first. The unscaled rows for R are thus
 00055  55500
To scale the row for Jenny, we note that its associated probability is 0.206, so we divide by √2 × 0.206 = 0.642. To scale the row for Jack, whose associated probability is 0.309, we divide by √2 × 0.309 = 0.786. Thus, the matrix R is
 0 0 07.797.79  6.36 6.36 6.36 0 0
✷
11.4.3 Constructing the Middle Matrix
Finally, we must construct the matrix U that connects C and R in the decom- position. Recall that U is an r × r matrix. We start the construction of U with another matrix of the same size, which we call W . The entry in row i and column j of W is the entry of M whose row is the one from which we selected the ith row of R and whose column is the one from which we selected the jth column of C.
  
432 CHAPTER 11. DIMENSIONALITY REDUCTION
Example 11.14 : Let us follow the selections of rows and columns made in Example 11.13. We claim
W= 05  50
The first row of W corresponds to the first row of R, which is the row for Jenny in the matrix M of Fig. 11.12. The 0 in the first column is there because that is the entry in the row of M for Jenny and the column for Alien; recall that the first column of C was constructed from the column of M for Alien. The 5 in the second column reflects the 5 in M’s row for Jenny and column for Casablanca; the latter is the column of M from which the second column of C was derived. Similarly, the second row of W is the entries in the row for Jack and columns for Alien and Casablanca, respectively. ✷
The matrix U is constructed from W by the Moore-Penrose pseudoin- verse described in Section 11.4.1. It consists of taking the SVD of W, say W = XΣYT, and replacing all nonzero elements in the matrix Σ of singu- lar values by their numerical inverses, to obtain the pseudoinverse Σ+. Then U = Y (Σ+)2XT.
Example 11.15 : Let us construct U from the matrix W that we constructed in Example 11.14. First, here is the SVD for W :
W= 0 5 = 0 1  5 0  1 0  50100501
That is, the three matrices on the right are X, Σ, and Y T, respectively. The matrix Σ has no zeros along the diagonal, so each element is replaced by its numerical inverse to get its Moore-Penrose pseudoinverse:
Σ+= 1/5 0   0 1/5
Now X and Y are symmetric, so they are their own transposes. Thus, U=Y(Σ+)2XT= 1 0  1/5 0  2 0 1 =  0 1/25 
0 1 0 1/5 1 0 1/25 0
✷
11.4.4 The Complete CUR Decomposition
We now have a method to select randomly the three component matrices C, U, and R. Their product will approximate the original matrix M. As we mentioned at the beginning of the discussion, the approximation is only for- mally guaranteed to be close when very large numbers of rows and columns are selected. However, the intuition is that by selecting rows and columns that tend to have high “importance” (i.e., high Frobenius norm), we are extracting
11.4. CUR DECOMPOSITION
433
0 0 11.01 11.01   8.99 8.99 0 0
0 0
0  0 
CUR= 7.72  0
0  0 1/25   0 01/2508.99
 1.54  4.63  6.17
0 0
9.30   011.63
0 4.65
=  2.78 2.78  0 0  0 0
0 0
Figure 11.13: CUR-decomposition of the matrix of Fig. 11.12
the most significant parts of the original matrix, even with a small number of rows and columns. As an example, let us see how well we do with the running example of this section.
Example 11.16: For our running example, the decomposition is shown in Fig. 11.13. While there is considerable difference between this result and the original matrix M, especially in the science-fiction numbers, the values are in proportion to their originals. This example is much too small, and the selection of the small numbers of rows and columns was arbitrary rather than random, for us to expect close convergence of the CUR decomposition to the exact values. ✷
11.4.5 Eliminating Duplicate Rows and Columns
It is quite possible that a single row or column is selected more than once. There is no great harm in using the same row twice, although the rank of the matrices of the decomposition will be less than the number of row and column choices made. However, it is also possible to combine k rows of R that are each the same row of the matrix M into a single row of R, thus leaving R with fewer rows. Likewise, k columns of C that each come from the same column of M can be combined into one column of C. However, for either rows or columns, the remaining vector should have each of its elements multiplied by √k.
When we merge some rows and/or columns, it is possible that R has fewer rows than C has columns, or vice versa. As a consequence, W will not be a square matrix. However, we can still take its pseudoinverse by decomposing it into W = XΣY T, where Σ is now a diagonal matrix with some all-0 rows or
 0.55 0.55  1.67 1.67  2.22 2.22
0.55 0 1.67 0 2.22 0 2.78 0
04.104.10 05.125.12 0 2.05 2.05
 
434 CHAPTER 11. DIMENSIONALITY REDUCTION
columns, whichever it has more of. To take the pseudoinverse of such a diagonal matrix, we treat each element on the diagonal as usual (invert nonzero elements and leave 0 as it is), but then we must transpose the result.
Example 11.17: Suppose
Then
✷
2000 Σ=0000
0030
 1/2 0 0  Σ+= 0 0 0   0 0 1 / 3 
000
11.4.6 Exercises for Section 11.4
Exercise 11.4.1 : The SVD for the matrix M= 48 14 
14 −48
 48 14 = 3/5 4/5  50 0   4/5 −3/5 
is
Find the Moore-Penrose pseudoinverse of M.
14 −48 4/5 −3/5 0 25 3/5 4/5
! Exercise 11.4.2: Find the CUR-decomposition of the matrix of Fig. 11.12
when we pick two “random” rows and columns as follows:
(a) The columns for The Matrix and Alien and the rows for Jim and John. (b) The columns for Alien and Star Wars and the rows for Jack and Jill.
(c) The columns for The Matrix and Titanic and the rows for Joe and Jane.
! Exercise 11.4.3: Find the CUR-decomposition of the matrix of Fig. 11.12 if the two “random” rows are both Jack and the two columns are Star Wars and Casablanca.
11.5 Summary of Chapter 11
✦ Dimensionality Reduction: The goal of dimensionality reduction is to re- place a large matrix by two or more other matrices whose sizes are much smaller than the original, but from which the original can be approxi- mately reconstructed, usually by taking their product.
11.5. SUMMARY OF CHAPTER 11 435
✦ Eigenvalues and Eigenvectors: A matrix may have several eigenvectors such that when the matrix multiplies the eigenvector, the result is a con- stant multiple of the eigenvector. That constant is the eigenvalue asso- ciated with this eigenvector. Together the eigenvector and its eigenvalue are called an eigenpair.
✦ Finding Eigenpairs by Power Iteration: We can find the principal eigen- vector (eigenvector with the largest eigenvalue) by starting with any vec- tor and repeatedly multiplying the current vector by the matrix to get a new vector. When the changes to the vector become small, we can treat the result as a close approximation to the principal eigenvector. By mod- ifying the matrix, we can then use the same iteration to get the second eigenpair (that with the second-largest eigenvalue), and similarly get each of the eigenpairs in turn, in order of decreasing value of the eigenvalue.
✦ Principal-Component Analysis: This technique for dimensionality reduc- tion views data consisting of a collection of points in a multidimensional space as a matrix, with rows corresponding to the points and columns to the dimensions. The product of this matrix and its transpose has eigen- pairs, and the principal eigenvector can be viewed as the direction in the space along which the points best line up. The second eigenvector repre- sents the direction in which deviations from the principal eigenvector are the greatest, and so on.
✦ Dimensionality Reduction by PCA: By representing the matrix of points by a small number of its eigenvectors, we can approximate the data in a way that minimizes the root-mean-square error for the given number of columns in the representing matrix.
✦ Singular-Value Decomposition: The singular-value decomposition of a ma- trix consists of three matrices, U , Σ, and V . The matrices U and V are column-orthonormal, meaning that as vectors, the columns are orthogo- nal, and their lengths are 1. The matrix Σ is a diagonal matrix, and the values along its diagonal are called singular values. The product of U, Σ, and the transpose of V equals the original matrix.
✦ Concepts: SVD is useful when there are a small number of concepts that connect the rows and columns of the original matrix. For example, if the original matrix represents the ratings given by movie viewers (rows) to movies (columns), the concepts might be the genres of the movies. The matrix U connects rows to concepts, Σ represents the strengths of the concepts, and V connects the concepts to columns.
✦ Queries Using the Singular-Value Decomposition: We can use the decom- position to relate new or hypothetical rows of the original matrix to the concepts represented by the decomposition. Multiply a row by the matrix V of the decomposition to get a vector indicating the extent to which that row matches each of the concepts.
436 CHAPTER 11. DIMENSIONALITY REDUCTION
✦ Using SVD for Dimensionality Reduction: In a complete SVD for a ma- trix, U and V are typically as large as the original. To use fewer columns for U and V , delete the columns corresponding to the smallest singular values from U , V , and Σ. This choice minimizes the error in reconstruct- ing the original matrix from the modified U , Σ, and V .
✦ Decomposing Sparse Matrices: Even in the common case where the given matrix is sparse, the matrices constructed by SVD are dense. The CUR decomposition seeks to decompose a sparse matrix into sparse, smaller matrices whose product approximates the original matrix.
✦ CUR Decomposition: This method chooses from a given sparse matrix a set of columns C and a set of rows R, which play the role of U and V T in SVD; the user can pick any number of rows and columns. The choice of rows and columns is made randomly with a distribution that depends on the Frobenius norm, or the square root of the sum of the squares of the elements. Between C and R is a square matrix called U that is constructed by a pseudo-inverse of the intersection of the chosen rows and columns.
11.6 References for Chapter 11
A well regarded text on matrix algebra is [4].
Principal component analysis was first discussed over a century ago, in [6]. SVD is from [3]. There have been many applications of this idea. Two
worth mentioning are [1] dealing with document analysis and [8] dealing with applications in Biology.
The CUR decomposition is from [2] and [5]. Our description follows a later work [7].
1. S. Deerwester, S.T. Dumais, G.W. Furnas, T.K. Landauer, and R. Harsh- man, “Indexing by latent semantic analysis,” J. American Society for Information Science 41:6 (1990).
2. P. Drineas, R. Kannan, and M.W. Mahoney, “Fast Monte-Carlo algo- rithms for matrices III: Computing a compressed approximate matrix decomposition,” SIAM J. Computing 36:1 (2006), pp. 184–206.
3. G.H. Golub and W. Kahan, “Calculating the singular values and pseudo- inverse of a matrix,” J. SIAM Series B 2:2 (1965), pp. 205–224.
4. G.H.Golub and C.F. Van Loan, Matrix Computations, JHU Press, 1996.
5. M.W. Mahoney, M. Maggioni, and P. Drineas, Tensor-CUR decomposi-
tions For tensor-based data, SIGKDD, pp. 327–336, 2006.
6. K. Pearson, “On lines and planes of closest fit to systems of points in space,” Philosophical Magazine 2:11 (1901), pp. 559–572.
11.6.
7.
8.
REFERENCES FOR CHAPTER 11 437
J. Sun, Y. Xie, H. Zhang, and C. Faloutsos, “Less is more: compact matrix decomposition for large sparse graphs,” Proc. SIAM Intl. Conf. on Data Mining, 2007.
M.E. Wall, A. Reichtsteiner and L.M. Rocha, “Singular value decom- position and principal component analysis,” in A Practical Approach to Microarray Data Analysis (D.P. Berrar, W. Dubitzky, and M. Granzow eds.), pp. 91–109, Kluwer, Norwell, MA, 2003.
438 CHAPTER 11. DIMENSIONALITY REDUCTION
Chapter 12
Large-Scale Machine Learning
Many algorithms are today classified as “machine learning.” These algorithms share, with the other algorithms studied in this book, the goal of extracting information from data. All algorithms for analysis of data are designed to produce a useful summary of the data, from which decisions are made. Among many examples, the frequent-itemset analysis that we did in Chapter 6 produces information like association rules, which can then be used for planning a sales strategy or for many other purposes.
However, algorithms called “machine learning” not only summarize our data; they are perceived as learning a model or classifier from the data, and thus discover something about data that will be seen in the future. For instance, the clustering algorithms discussed in Chapter 7 produce clusters that not only tell us something about the data being analyzed (the training set), but they allow us to classify future data into one of the clusters that result from the clustering algorithm. Thus, machine-learning enthusiasts often speak of clustering with the neologism “unsupervised learning”; the term unsupervised refers to the fact that the input data does not tell the clustering algorithm what the clusters should be. In supervised machine learning,, which is the subject of this chapter, the available data includes information about the correct way to classify at least some of the data. The data classified already is called the training set.
In this chapter, we do not attempt to cover all the different approaches to machine learning. We concentrate on methods that are suitable for very large data and that have the potential for parallel implementation. We consider the classical “perceptron” approach to learning a data classifier, where a hyperplane that separates two classes is sought. Then, we look at more modern techniques involving support-vector machines. Similar to perceptrons, these methods look for hyperplanes that best divide the classes, so that few, if any, members of the training set lie close to the hyperplane. We end with a discussion of nearest- neighbor techniques, where data is classified according to the class(es) of their
439
440 CHAPTER 12. LARGE-SCALE MACHINE LEARNING nearest neighbors in some space.
12.1 The Machine-Learning Model
In this brief section we introduce the framework for machine-learning algorithms and give the basic definitions.
12.1.1 Training Sets
The data to which a machine-learning (often abbreviated ML) algorithm is applied is called a training set. A training set consists of a set of pairs (x,y), called training examples, where
• x is a vector of values, often called a feature vector. Each value, or feature, can be categorical (values are taken from a set of discrete values, such as {red, blue, green}) or numerical (values are integers or real numbers).
• y is the label, the classification value for x.
The objective of the ML process is to discover a function y = f(x) that best predicts the value of y associated with each value of x. The type of y is in principle arbitrary, but there are several common and important cases.
1. y is a real number. In this case, the ML problem is called regression.
2. y is a boolean value true-or-false, more commonly written as +1 and −1,
respectively. In this class the problem is binary classification.
3. y is a member of some finite set. The members of this set can be thought of as “classes,” and each member represents one class. The problem is multiclass classification.
4. y is a member of some potentially infinite set, for example, a parse tree for x, which is interpreted as a sentence.
12.1.2 Some Illustrative Examples
Example 12.1: Recall Fig. 7.1, repeated as Fig. 12.1, where we plotted the height and weight of dogs in three classes: Beagles, Chihuahuas, and Dachs- hunds. We can think of this data as a training set, provided the data includes the variety of the dog along with each height-weight pair. Each pair (x,y) in the training set consists of a feature vector x of the form [height, weight]. The associated label y is the variety of the dog. An example of a training-set pair would be ([5 inches, 2 pounds], Chihuahua).
An appropriate way to implement the decision function f would be to imag- ine two lines, shown dashed in Fig. 12.1. The horizontal line represents a height
12.1. THE MACHINE-LEARNING MODEL
441
 weight = 3
 Beagles
            Height
height = 7
Chihuahuas
  Dachshunds
              Weight
  Figure 12.1: Repeat of Fig. 7.1, indicating the heights and weights of certain dogs
of 7 inches and separates Beagles from Chihuahuas and Dachshunds. The verti- cal line represents a weight of 3 pounds and separates Chihuahuas from Beagles and Dachshunds. The algorithm that implements f is:
if (height > 7) print Beagle
else if (weight < 3) print Chihuahua else print Dachshund;
Recall that the original intent of Fig. 7.1 was to cluster points without knowing which variety of dog they represented. That is, the label associated with a given height-weight vector was not available. Here, we are performing supervised learning with the same data augmented by classifications for the training data. ✷
Example 12.2 : As an example of supervised learning, the four points (1, 2), (2, 1), (3, 4), and (4, 3) from Fig.11.1 (repeated here as Fig. 12.2), can be thought of as a training set, where the vectors are one-dimensional. That is, the point (1, 2) can be thought of as a pair ([1], 2), where [1] is the one-dimensional feature vector x, and 2 is the associated label y; the other points can be interpreted similarly.
Suppose we want to “learn” the linear function f(x) = ax + b that best represents the points of the training set. A natural interpretation of “best” is that the RMSE of the value of f(x) compared with the given value of y is
442
CHAPTER 12. LARGE-SCALE MACHINE LEARNING
 (1,2)
(3,4)
(2,1)
(4,3)
Figure 12.2: Repeat of Fig. 11.1, to be used as a training set minimized. That is, we want to minimize
4  (ax+b−yx)2 x=1
where yx is the y-value associated with x. This sum is
(a+b−2)2 +(2a+b−1)2 +(3a+b−4)2 +(4a+b−3)2
Simplifying, the sum is 30a2 + 4b2 + 20ab − 56a − 20b + 30. If we then take the derivatives with respect to a and b and set them to 0, we get
60a+20b−56 = 0 20a+8b−20 = 0
The solution to these equations is a = 3/5 and b = 1. For these values the RMSE is 3.2.
Note that the learned straight line is not the principal axis that was dis-
covered for these points in Section 11.2.1. That axis was the line with slope
1, going through the origin, i.e., the line y = x. For this line, the RMSE is
4. The difference is that PCA discussed in Section 11.2.1 minimizes the sum
of the squares of the lengths of the projections onto the chosen axis, which is
constrained to go through the origin. Here, we are minimizing the sum of the
squares of the vertical distances between the points and the line. In fact, even
had we tried to learn the line through the origin with the least RMSE, we would
notchoosey=x.Youcancheckthaty=14xhasalowerRMSEthan4. ✷ 15
Example 12.3 : A common application of machine learning involves a training set where the feature vectors x are boolean-valued and of very high dimension. We shall focus on data consisting of documents, e.g., emails, Web pages, or newspaper articles. Each component represents a word in some large dictio- nary. We would probably eliminate stop words (very common words) from this dictionary, because these words tend not to tell us much about the subject mat- ter of the document. Similarly, we might also restrict the dictionary to words
 
12.1. THE MACHINE-LEARNING MODEL 443
with high TF.IDF scores (see Section 1.3.1) so the words considered would tend to reflect the topic or substance of the document.
The training set consists of pairs, where the vector x represents the presence or absence of each dictionary word in document. The label y could be +1 or −1, with +1 representing that the document (an email, e.g.) is spam. Our goal would be to train a classifier to examine future emails and decide whether or not they are spam. We shall illustrate this use of machine learning in Example 12.4.
Alternatively, y could be chosen from some finite set of topics, e.g., “sports,” “politics,” and so on. Again, x could represent a document, perhaps a Web page. The goal would be to create a classifier for Web pages that assigned a topic to each. ✷
12.1.3 Approaches to Machine Learning
There are many forms of ML algorithms, and we shall not cover them all here. Here are the major classes of such algorithms, each of which is distinguished by the form by which the function f is represented.
1. Decision trees were discussed briefly in Section 9.2.7. The form of f is a tree, and each node of the tree has a function of x that determines to which child or children the search must proceed. While we saw only binary trees in Section 9.2.7, in general a decision tree can have any number of children for each node. Decision trees are suitable for binary and multiclass classification, especially when the dimension of the feature vector is not too large (large numbers of features can lead to overfitting).
2. Perceptrons are threshold functions applied to the components of the vec- tor x = [x1, x2, . . . , xn]. A weight wi is associated with the ith component, for each i = 1,2,...,n, and there is a threshold θ. The output is +1 if
n
 wixi ≥θ i=1
and the output is −1 otherwise. A perceptron is suitable for binary classi- fication, even when the number of features is very large, e.g., the presence or absence of words in a document. Perceptrons are the topic of Sec- tion 12.2.
3. Neural nets are acyclic networks of perceptrons, with the outputs of some perceptrons used as inputs to others. These are suitable for binary or multiclass classification, since there can be several perceptrons used as output, with one or more indicating each class.
4. Instance-based learning uses the entire training set to represent the func- tion f. The calculation of the label y associated with a new feature vector x can involve examination of the entire training set, although usually some
444
CHAPTER 12. LARGE-SCALE MACHINE LEARNING
preprocessing of the training set enables the computation of f(x) to pro- ceed efficiently. We shall consider an important kind of instance-based learning, k-nearest-neighbor, in Section 12.4. For example, 1-nearest- neighbor classifies data by giving it the same class as that of its nearest training example. There are k-nearest-neighbor algorithms that are ap- propriate for any kind of classification, although we shall concentrate on the case where y and the components of x are real numbers.
5. Support-vector machines are an advance over the algorithms traditionally used to select the weights and threshold. The result is a classifier that tends to be more accurate on unseen data. We discuss support-vector machines in Section 12.3.
12.1.4 Machine-Learning Architecture
Machine-learning algorithms can be classified not only by their general algorith- mic approach as we discussed in Section 12.1.3. but also by their underlying architecture – the way data is handled and the way it is used to build the model.
Training and Testing
One general issue regarding the handling of data is that there is a good reason to withhold some of the available data from the training set. The remaining data is called the test set. The problem addressed is that many machine-learning algorithms tend to overfit the data; they pick up on artifacts that occur in the training set but that are atypical of the larger population of possible data. By using the test data, and seeing how well the classifier works on that, we can tell if the classifier is overfitting the data. If so, we can restrict the machine-learning algorithm in some way. For instance, if we are constructing a decision tree, we can limit the number of levels of the tree.
Training Set
 Test Set
       Model Generation
Error Rate
    Figure 12.3: The training set helps build the model, and the test set validates it
Figure 12.3 illustrates the train-and-test architecture. We assume all the data is suitable for training (i.e., the class information is attached to the data),
Model
12.1. THE MACHINE-LEARNING MODEL 445
   Generalization
We should remember that the purpose of creating a model or classifier is not to classify the training set, but to classify the data whose class we do not know. We want that data to be classified correctly, but often we have no way of knowing whether or not the model does so. If the nature of the data changes over time, for instance, if we are trying to detect spam emails, then we need to measure the performance over time, as best we can. For example, in the case of spam emails, we can note the rate of reports of spam emails that were not classified as spam.
 but we separate out a small fraction of the available data as the test set. We use the remaining data to build a suitable model or classifier. Then we feed the test data to this model. Since we know the class of each element of the test data, we can tell how well the model does on the test data. If the error rate on the test data is not much worse than the error rate of the model on the training data itself, then we expect there is little, if any, overfitting, and the model can be used. On the other hand, if the classifier performs much worse on the test data than on the training data, we expect there is overfitting and need to rethink the way we construct the classifier.
There is nothing special about the selection of the test data. In fact, we can repeat the train-then-test process several times using the same data, if we divide the data into k equal-sized chunks. In turn, we let each chunk be the test data, and use the remaining k − 1 chunks as the training data. This training architecture is called cross-validation.
Batch Versus On-Line Learning
Often, as in Examples 12.1 and 12.2 we use a batch learning architecture. That is, the entire training set is available at the beginning of the process, and it is all used in whatever way the algorithm requires to produce a model once and for all. The alternative is on-line learning, where the training set arrives in a stream and, like any stream, cannot be revisited after it is processed. In on-line learning, we maintain a model at all times. As new training examples arrive, we may choose to modify the model to account for the new examples. On-line learning has the advantages that it can
1. Deal with very large training sets, because it does not access more than one training example at a time.
2. Adapt to changes in the population of training examples as time goes on. For instance, Google trains its spam-email classifier this way, adapting the classifier for spam as new kinds of spam email are sent by spammers and indicated to be spam by the recipients.
446 CHAPTER 12. LARGE-SCALE MACHINE LEARNING
An enhancement of on-line learning, suitable in some cases, is active learn- ing. Here, the classifier may receive some training examples, but it primarily receives unclassified data, which it must classify. If the classifier is unsure of the classification (e.g., the newly arrived example is very close to the bound- ary), then the classifier can ask for ground truth at some significant cost. For instance, it could send the example to Mechanical Turk and gather opinions of real people. In this way, examples near the boundary become training examples and can be used to modify the classifier.
Feature Selection
Sometimes, the hardest part of designing a good model or classifier is figuring out what features to use as input to the learning algorithm. Let us reconsider Example 12.3, where we suggested that we could classify emails as spam or not spam by looking at the words contained in the email. In fact, we explore in detail such a classifier in Example 12.4. As discussed in Example 12.3, it may make sense to focus on certain words and not others; e.g., we should eliminate stop words.
But we should also ask whether there is other information available that would help us make a better decision about spam. For example, spam is often generated by particular hosts, either those belonging to the spammers, or hosts that have been coopted into a “botnet” for the purpose of generating spam. Thus, including the originating host or originating email address into the feature vector describing an email might enable us to design a better classifier and lower the error rate.
Creating a Training Set
It is reasonable to ask where the label information that turns data into a train- ing set comes from. The obvious method is to create the labels by hand, having an expert look at each feature vector and classify it properly. Recently, crowd- sourcing techniques have been used to label data. For example, in many appli- cations it is possible to use Mechanical Turk to label data. Since the “Turkers” are not necessarily reliable, it is wise to use a system that allows the question to be asked of several different people, until a clear majority is in favor of one label.
One often can find data on the Web that is implicitly labeled. For example, the Open Directory (DMOZ) has millions of pages labeled by topic. That data, used as a training set, can enable one to classify other pages or documents according to their topic, based on the frequency of word occurrence. Another approach to classifying by topic is to look at the Wikipedia page for a topic and see what pages it links to. Those pages can safely be assumed to be relevant to the given topic.
In some applications we can use the stars that people use to rate products or services on sites like Amazon or Yelp. For example, we might want to estimate the number of stars that would be assigned to reviews or tweets about a product,
12.2. PERCEPTRONS 447
even if those reviews do not have star ratings. If we use star-labeled reviews as a training set, we can deduce the words that are most commonly associated with positive and negative reviews (called sentiment analysis). The presence of these words in other reviews can tell us the sentiment of those reviews.
12.1.5 Exercises for Section 12.1
Exercise 12.1.1 : Redo Example 12.2 for the following different forms of f (x). (a) Require f(x) = ax; i.e., a straight line through the origin. Is the line
y = 14 x that we discussed in the example optimal? 15
(b) Require f(x) to be a quadratic, i.e., f(x) = ax2 + bx + c. 12.2 Perceptrons
A perceptron is a linear binary classifier. Its input is a vector x = [x1, x2, . . . , xd] with real-valued components. Associated with the perceptron is a vector of weights w = [w1, w2, . . . , wd], also with real-valued components. Each percep- tron has a threshold θ. The output of the perceptron is +1 if w.x > θ, and the output is −1 if w.x < θ. The special case where w.x = θ will always be regarded as “wrong,” in the sense that we shall describe in detail when we get to Section 12.2.1.
The weight vector w defines a hyperplane of dimension d − 1 – the set of all points x such that w.x = θ, as suggested in Fig. 12.4. Points on the positive side of the hyperplane are classified +1 and those on the negative side are clas- sified −1. A perceptron classifier works only for data that is linearly separable, in the sense that there is some hyperplane that separates all the positive points from all the negative points. If there are many such hyperplanes, the percep- tron will converge to one of them, and thus will correctly classify all the training points. If no such hyperplane exists, then the perceptron cannot converge to any particular one. In the next section, we discuss support-vector machines, which do not have this limitation; they will converge to some separator that, although not a perfect classifier, will do as well as possible under the metric to be described in Section 12.3.
12.2.1 Training a Perceptron with Zero Threshold
To train a perceptron, we examine the training set and try to find a weight vector w and threshold θ such that all the feature vectors with y = +1 (the positive examples) are on the positive side of the hyperplane and all those with y = −1 (the negative examples) are on the negative side. It may or may not be possible to do so, since there is no guarantee that any hyperplane separates all the positive and negative examples in the training set.
 
448 CHAPTER 12. LARGE-SCALE MACHINE LEARNING
w
w.x = θ
Figure 12.4: A perceptron divides a space by a hyperplane into two half-spaces
We begin by assuming the threshold is 0; the simple augmentation needed to handle an unknown threshold is discussed in Section 12.2.4. The follow- ing method will converge to some hyperplane that separates the positive and negative examples, provided one exists.
1. Initialize the weight vector w to all 0’s.
2. Pick a learning-rate parameter η, which is a small, positive real number. The choice of η affects the convergence of the perceptron. If η is too small, then convergence is slow; if it is too big, then the decision boundary will “dance around” and again will converge slowly, if at all.
3. Consider each training example t = (x, y) in turn.
(a) Let y′ = w.x.
(b) If y′ and y have the same sign, then do nothing; t is properly classi- fied.
(c) However, if y′ and y have different signs, or y′ = 0, replace w by w + ηyx. That is, adjust w slightly in the direction of x.
The two-dimensional case of this transformation on w is suggested in Fig. 12.5. Notice how moving w in the direction of x moves the hyperplane that is perpendicular to w in such a direction that it makes it more likely that x will be on the correct side of the hyperplane, although it does not guarantee that to be the case.
Example 12.4 : Let us consider training a perceptron to recognize spam email. The training set consists of pairs (x, y) where x is a vector of 0’s and 1’s, with each component xi corresponding to the presence (xi = 1) or absence (xi = 0)
                  
12.2. PERCEPTRONS
449
ηx1 w’
w
                    x
1
    of a particular word in the email. The value of y is +1 if the email is known to be spam and −1 if it is known not to be spam. While the number of words found in the training set of emails is very large, we shall use a simplified example where there are only five words: “and,” “viagra,” “the,” “of,” and “nigeria.” Figure 12.6 gives the training set of six vectors and their corresponding classes.
and viagra the of nigeria y a 1 1 0 1 1 +1 b 0 0 1 1 0 −1 c 0 1 1 0 0 +1 d 1 0 0 1 0 −1 e 1 0 1 0 1 +1 f 1 0 1 1 0 −1
Figure 12.6: Training data for spam emails
w’.x = 0
  w.x = 0
Figure 12.5: A misclassified point x1 moves the vector w
               In this example, we shall use learning rate η = 1/2, and we shall visit each training example once, in the order shown in Fig. 12.6. We begin with w = [0,0,0,0,0] and compute w.a = 0. Since 0 is not positive, we move w in the direction of a by performing w := w + (1/2)(+1)a. The new value of w is thus
w = [0, 0, 0, 0, 0] + [ 1 , 1 , 0, 1 , 1 ] = [ 1 , 1 , 0, 1 , 1 ] 22222222
        Next, consider b. w.b = [1, 1,0, 1, 1].[0,0,1,1,0] = 1. Since the associated 2222 2
y for b is −1, b is misclassified. We thus assign
w := w+(1/2)(−1)b=[1,1,0,1,1]−[0,0,1,1,0]=[1,1,−1,0,1] 2222222222
               
450 CHAPTER 12. LARGE-SCALE MACHINE LEARNING
   Pragmatics of Training on Emails
When we represent emails or other large documents as training examples, we would not really want to construct the vector of 0’s and 1’s with a component for every word that appears even once in the collection of emails. Doing so would typically give us sparse vectors with millions of components. Rather, create a table in which all the words appearing in the emails are assigned integers 1, 2, . . ., indicating their component. When we process an email in the training set, make a list of the components in which the vector has 1; i.e., use the standard sparse representation for the vector. If we eliminate stop words from the representation, or even eliminate words with a low TF.IDF score, then we make the vectors representing emails significantly sparser and thus compress the data even more. Only the vector w needs to have all its components listed, since it will not be sparse after a small number of training examples have been processed.
 Training example c is next. We compute
w.c = [1, 1,−1,0, 1].[0,1,1,0,0] = 0
    2222
Since the associated y for c is +1, c is also misclassified. We thus assign
w := w+(1/2)(+1)c=[1,1,−1,0,1]+[0,1,1,0,0]=[1,1,0,0,1] 22222222
        Training example d is next to be considered:
w.d = [1,1,0,0, 1].[1,0,0,1,0] = 1
  22
Since the associated y for d is −1, d is misclassified as well. We thus assign
w := w+(1/2)(−1)d=[1,1,0,0,1]−[1,0,0,1,0]=[0,1,0,−1,1] 222222
For training example e we compute w.e = [0,1,0,−1, 1].[1,0,1,0,1] = 1. 222
Since the associated y for e is +1, e is classified correctly, and no change to w is made. Similarly, for f we compute
w.f = [0,1,0,−1, 1].[1,0,1,1,0] = −1 222
so f is correctly classified. If we check a through d, we find that this w cor- rectly classifies them as well. Thus, we have converged to a perceptron that classifies all the training set examples correctly. It also makes a certain amount of sense: it says that “viagra” and “nigeria” are indicative of spam, while “of” is indicative of nonspam. It considers “and” and “the” neutral,” although we would probably prefer to give “and,” “of,” and “the” the same weight. ✷
            
12.2. PERCEPTRONS 451 12.2.2 Convergence of Perceptrons
As we mentioned at the beginning of this section, if the data points are linearly separable, then the perceptron algorithm will converge to a separator. However, if the data is not linearly separable, then the algorithm will eventually repeat a weight vector and loop infinitely. Unfortunately, it is often hard to tell, during the running of the algorithm, which of these two cases applies. When the data is large, it is not feasible to remember all previous weight vectors to see whether we are repeating a vector, and even if we could, the period of repetition would most likely be so large that we would want to terminate the algorithm long before we repeated.
A second issue regarding termination is that even if the training data is linearly separable, the entire dataset might not be linearly separable. The consequence is that there might not be any value in running the algorithm for a very large number of rounds, in the hope of converging to a separator. We therefore need a strategy for deciding when to terminate the perceptron algorithm, assuming convergence has not occurred. Here are some common tests for termination.
1. Terminate after a fixed number of rounds.
2. Terminate when the number of misclassified training points stops chang- ing.
3. Withhold a test set from the training data, and after each round, run the perceptron on the test data. Terminate the algorithm when the number of errors on the test set stops changing.
Another technique that will aid convergence is to lower the training rate as the number of rounds increases. For example, we could allow the training rate η to start at some initial η0 and lower it to η0/(1 + ct) after the tth round, where c is some small constant.
12.2.3 The Winnow Algorithm
There are many other rules one could use to adjust weights for a perceptron. Not all possible algorithms are guaranteed to converge, even if there is a hyperplane separating positive and negative examples. One that does converge is called Winnow, and that rule will be described here. Winnow assumes that the feature vectors consist of 0’s and 1’s, and the labels are +1 or −1. Unlike the basic perceptron algorithm, which can produce positive or negative components in the weight vector w, Winnow produces only positive weights.
The general Winnow Algorithm allows for a variety of parameters to be selected, and we shall only consider one simple variant. However, all variants have in common the idea that there is a positive threshold θ. If w is the current weight vector, and x is the feature vector in the training set that we are currently considering, we compute w.x and compare it with θ. If the dot
452 CHAPTER 12. LARGE-SCALE MACHINE LEARNING
product is too low, and the class for x is +1, then we have to raise the weights of w in those components where x has 1. We multiply these weights by a number greater than 1. The larger this number, the greater the training rate, so we want to pick a number that is not too close to 1 (or convergence will be too slow) but also not too large (or the weight vector may oscillate). Similarly, if w.x ≥ θ, but the class of x is −1, then we want to lower the weights of w in those components where x is 1. We multiply those weights by a number greater than 0 but less than 1. Again, we want to pick a number that is not too close to 1 but also not too small, to avoid slow convergence or oscillation.
We shall give the details of the algorithm using the factors 2 and 1/2, for the cases where we want to raise weights and lower weights, respectively. Start the Winnow Algorithm with a weight vector w = [w1, w2, . . . , wd] all of whose components are 1, and let the threshold θ equal d, the number of dimensions of the vectors in the training examples. Let (x,y) be the next training example to be considered, where x = [x1, x2, . . . , xd].
1. If w.x > θ and y = +1, or w.x < θ and y = −1, then the example is correctly classified, so no change to w is made.
2. If w.x ≤ θ, but y = +1, then the weights for the components where x has 1 are too low as a group. Double each of the corresponding components ofw. Thatis,ifxi =1thensetwi := 2wi.
3. If w.x ≥ θ, but y = −1, then the weights for the components where x has 1 are too high as a group. Halve each of the corresponding components ofw. Thatis,ifxi =1thensetwi := wi/2.
Example 12.5 : Let us reconsider the training data from Fig. 12.6. Initialize w = [1,1,1,1,1] and let θ = 5. First, consider feature vector a = [1,1,0,1,1]. w.a = 4, which is less than θ. Since the associated label for a is +1, this example is misclassified. When a +1-labeled example is misclassified, we must double all the components where the example has 1; in this case, all but the third component of a is 1. Thus, the new value of w is [2, 2, 1, 2, 2].
Next, we consider training example b = [0, 0, 1, 1, 0]. w.b = 3, which is less than θ. However, the associated label for b is −1, so no change to w is needed. For c = [0,1,1,0,0] we find w.c = 3 < θ, while the associated label is +1. Thus, we double the components of w where the corresponding components of c are 1. These components are the second and third, so the new value of w is
[2,4,2,2,2].
The next two training examples, d and e require no change, since they are
correctly classified. However, there is a problem with f = [1, 0, 1, 1, 0], since w.f = 6 > θ, while the associated label for f is −1. Thus, we must divide the first, third, and fourth components of w by 2, since these are the components where f has 1. The new value of w is [1,4,1,1,2].
We still have not converged. It turns out we must consider each of the training examples a through f again. At the end of this process, the algorithm
12.2. PERCEPTRONS
x y w.x
453
 OK? and viagra the of nigeria 11111
   a +1 b−1 c +1 d−1 e +1 f −1 a +1 b−1 c +1 d−1 e +1 f −1
4 no 3 yes 3 no 4 yes 6 yes 6 no 8 yes 2 yes 5 no 2 yes 5 no 7 no
2 2 1 2 2 2 4 2 2 2
1 4 1 1 2
1 8 2 1 2 2 8 4 1 4
1 8 2 1 4 2
            Figure 12.7: Sequence of updates to w performed by the Winnow Algorithm on the training set of Fig. 12.6
has converged to a weight vector w = [1, 8, 2, 1 , 4], which with threshold θ = 5 2
correctly classifies all of the training examples in Fig. 12.6. The details of the twelve steps to convergence are shown in Fig. 12.7. This figure gives the associated label y and the computed dot product of w and the given feature vector. The last five columns are the five components of w after processing each training example. ✷
12.2.4 Allowing the Threshold to Vary
Suppose now that the choice of threshold 0, as in Section 12.2.1, or threshold d, as in Section 12.2.3 is not desirable, or that we don’t know the best threshold to use. At the cost of adding another dimension to the feature vectors, we can treat θ as one of the components of the weight vector w. That is:
1. Replace the vector of weights w = [w1,w2,...,wd] by w′ = [w1,w2,...,wd,θ]
2. Replace every feature vector x = [x1, x2, . . . , xd] by x′ = [x1,x2,...,xd,−1]
Then, for the new training set and weight vector, we can treat the threshold as 0 and use the algorithm of Section 12.2.1. The justification is that w′.x′ ≥ 0 is equivalent to di=1 wixi +θ×−1 = w.x−θ ≥ 0, which in turn is equivalent to w.x ≥ θ. The latter is the condition for a positive response from a perceptron with threshold θ.
 
454 CHAPTER 12. LARGE-SCALE MACHINE LEARNING
We can also apply the Winnow Algorithm to the modified data. Winnow requires all feature vectors to have 0’s and 1’s, as components. However, we can allow a −1 in the feature vector component for θ if we treat it in the manner opposite to the way we treat components that are 1. That is, if the training example is positive, and we need to increase the other weights, we instead divide the component for the threshold by 2. And if the training example is negative, and we need to decrease the other weights we multiply the threshold component by 2.
Example 12.6: Let us modify the training set of Fig. 12.6 to incorporate a
sixth “word” that represents the negative −θ of the threshold. is shown in Fig. 12.8.
and viagra the of nigeria θ y a 1 1 0 1 1 −1 +1 b 0 0 1 1 0 −1 −1 c 0 1 1 0 0 −1 +1 d 1 0 0 1 0 −1 −1 e 1 0 1 0 1 −1 +1 f 1 0 1 1 0 −1 −1
The new data
               Figure 12.8: Training data for spam emails, with a sixth component representing the negative of the threshold
We begin with a weight vector w with six 1’s, as shown in the first line
of Fig. 12.9. When we compute w.a = 3, using the first feature vector a, we
are happy because the training example is positive, and so is the dot product.
However, for the second training example, we compute w.b = 1. Since the
example is negative and the dot product is positive, we must adjust the weights.
Since b has 1’s in the third and fourth components, the 1’s in the corresponding
components of w are replaced by 1/2. The last component, corresponding to θ,
must be doubled. These adjustments give the new weight vector [1, 1, 1 , 1 , 1, 2]
  22
Figure 12.9: Sequence of updates to w performed by the Winnow Algorithm on the training set of Fig. 12.8
The feature vector c is a positive example, but w.c = −1. Thus, we must 2
shown in the third line of Fig. 12.9.
 x y w.x OK? and viagra the of nigeria θ 111111
   a +1 3 yes
b−1 1no 1 1 1 1
12 1 1 1 2
   c +1 −1 no 1 2 1 1 22
d −1 1 no 1 2 1 1 224
22
        
12.2. PERCEPTRONS 455
double the second and third components of w, because c has 1 in the cor-
responding components, and we must halve the last component of w, which
corresponds to θ. The resulting w = [1, 2, 1, 1 , 1, 1] is shown in the fourth line 21
 of Fig. 12.9. Next, d is a negative example. Since w.d = 2, we must again
 adjust weights. We halve the weights in the first and fourth components and
double the last component, yielding w = [ 1 , 2, 1, 1 , 1, 2]. Now, all positive ex- 24
amples have a positive dot product with the weight vector, and all negative examples have a negative dot product, so there are no further changes to the weights.
The designed perceptron has a threshold of 2. It has weights 2 and 1 for “viagra” and “nigeria” and smaller weights for “and” and “of.” It also has weight 1 for “the,” which suggests that “the” is as indicative of spam as “nige- ria,” something we doubt is true. Nevertheless, this perceptron does classify all examples correctly. ✷
12.2.5 Multiclass Perceptrons
There are several ways in which the basic idea of the perceptron can be ex- tended. We shall discuss transformations that enable hyperplanes to serve for more complex boundaries in the next section. Here, we look at how perceptrons can be used to classify data into many classes.
Suppose we are given a training set with labels in k different classes. Start by training a perceptron for each class; these perceptrons should each have the same threshold θ. That is, for class i treat a training example (x, i) as a positive example, and all examples (x, j), where j ̸= i, as a negative example. Suppose that the weight vector of the perceptron for class i is determined to be wi after training.
Given a new vector x to classify, we compute wi.x for all i = 1,2,...,k. We take the class of x to be the value of i for which wi.x is the maximum, provided that value is at least θ. Otherwise, x is assumed not to belong to any of the k classes.
For example, suppose we want to classify Web pages into a number of topics, such as sports, politics, medicine, and so on. We can represent Web pages by a vector with 1 for each word present in the page and 0 for words not present (of course we would only visualize the pages that way; we wouldn’t construct the vectors in reality). Each topic has certain words that tend to indicate that topic. For instance, sports pages would be full of words like “win,” “goal,” “played,” and so on. The weight vector for that topic would give higher weights to the words that characterize that topic.
A new page could be classified as belonging to the topic that gives the highest score when the dot product of the page’s vector and the weight vectors for the topics are computed. An alternative interpretation of the situation is to classify a page as belonging to all those topics for which the dot product is above some threshold (presumably a threshold higher than the θ used for training).
  
456 CHAPTER 12. LARGE-SCALE MACHINE LEARNING 12.2.6 Transforming the Training Set
While a perceptron must use a linear function to separate two classes, it is always possible to transform the vectors of a training set before applying a perceptron-based algorithm to separate the classes. An example should give the basic idea.
             Figure 12.10: Transforming from rectangular to polar coordinates turns this training set into one with a separating hyperplane
Example 12.7 : In Fig. 12.10 we see a plot of places to visit from my home. The horizontal and vertical coordinates represent latitude and longitude of places. Some of the places have been classified into “day trips” – places close enough to visit in one day – and “excursions,” which require more than a day to visit. These are the circles and squares, respectively. Evidently, there is no straight line that separates day trips from excursions. However, if we replace the Cartesian coordinates by polar coordinates, then in the transformed space of polar coordinates, the dashed circle shown in Fig. 12.10 becomes a hyperplane. Formally, we transform the vector x = [x1, x2] into [ x21 + x2, arctan(x2/x1)].
In fact, we can also do dimensionality reduction of the data. The angle of the point is irrelevant, and only the radius  x21 + x2 matters. Thus, we can turn the point vectors into one-component vectors giving the distance of the point from the origin. Associated with the small distances will be the class label “day trip,” while the larger distances will all be associated with the label “excursion.” Training the perceptron is extremely easy. ✷
  
12.2. PERCEPTRONS 457 12.2.7 Problems With Perceptrons
Despite the extensions discussed above, there are some limitations to the ability of perceptrons to classify some data. The biggest problem is that sometimes the data is inherently not separable by a hyperplane. An example is shown in Fig. 12.11. In this example, points of the two classes mix near the boundary so that any line through the points will have points of both classes on at least one of the sides.
                        Figure 12.11: A training set may not allow the existence of any separating hyperplane
One might argue that, based on the observations of Section 12.2.6 it should be possible to find some function on the points that would transform them to another space where they were linearly separable. That might be the case, but if so, it would probably be an example of overfitting, the situation where the classifier works very well on the training set, because it has been carefully designed to handle each training example correctly. However, because the clas- sifier is exploiting details of the training set that do not apply to other examples that must be classified in the future, the classifier will not perform well on new data.
Another problem is illustrated in Fig. 12.12. Usually, if classes can be sep- arated by one hyperplane, then there are many different hyperplanes that will separate the points. However, not all hyperplanes are equally good. For in- stance, if we choose the hyperplane that is furthest clockwise, then the point indicated by “?” will be classified as a circle, even though we intuitively see it as closer to the squares. When we meet support-vector machines in Section 12.3, we shall see that there is a way to insist that the hyperplane chosen be the one that in a sense divides the space most fairly.
Yet another problem is illustrated by Fig. 12.13. Most rules for training
458 CHAPTER 12. LARGE-SCALE MACHINE LEARNING ?
                  Figure 12.12: Generally, more that one hyperplane can separate the classes if they can be separated at all
a perceptron stop as soon as there are no misclassified points. As a result, the chosen hyperplane will be one that just manages to classify some of the points correctly. For instance, the upper line in Fig. 12.13 has just managed to accommodate two of the squares, and the lower line has just managed to accommodate one of the circles. If either of these lines represent the final weight vector, then the weights are biased toward one of the classes. That is, they correctly classify the points in the training set, but the upper line would classify new squares that are just below it as circles, while the lower line would classify circles just above it as squares. Again, a more equitable choice of separating hyperplane will be shown in Section 12.3.
12.2.8 Parallel Implementation of Perceptrons
The training of a perceptron is an inherently sequential process. If the num- ber of dimensions of the vectors involved is huge, then we might obtain some parallelism by computing dot products in parallel. However, as we discussed in connection with Example 12.4, high-dimensional vectors are likely to be sparse and can be represented more succinctly than would be expected from their length.
In order to get significant parallelism, we have to modify the perceptron algorithm slightly, so that many training examples are used with the same esti- mated weight vector w. As an example, let us formulate the parallel algorithm as a MapReduce job.
The Map Function: Each Map task is given a chunk of training examples, and each Map task knows the current weight vector w. The Map task computes w.x for each feature vector x = [x1, x2, . . . , xk] in its chunk and compares that
12.2. PERCEPTRONS 459
                Figure 12.13: Perceptrons converge as soon as the separating hyperplane reaches the region between classes
dot product with the label y, which is +1 or −1, associated with x. If the signs agree, no key-value pairs are produced for this training example. However, if the signs disagree, then for each nonzero component xi of x the key-value pair (i,ηyxi) is produced; here, η is the learning-rate constant used to train this perceptron. Notice that ηyxi is the increment we would like to add to the current ith component of w, and if xi = 0, then there is no need to produce a key-value pair. However, in the interests of parallelism, we defer that change until we can accumulate many changes in the Reduce phase.
The Reduce Function: For each key i, the Reduce task that handles key i adds all the associated increments and then adds that sum to the ith component of w.
Probably, these changes will not be enough to train the perceptron. If any changes to w occur, then we need to start a new MapReduce job that does the same thing, perhaps with different chunks from the training set. However, even if the entire training set was used on the first round, it can be used again, since its effect on w will be different if w has changed.
12.2.9 Exercises for Section 12.2
Exercise 12.2.1: Modify the training set of Fig. 12.6 so that example b also includes the word “nigeria” (yet remains a negative example – perhaps someone telling about their trip to Nigeria). Find a weight vector that separates the positive and negative examples, using:
(a) The basic training method of Section 12.2.1. (b) The Winnow method of Section 12.2.3.
460
CHAPTER 12. LARGE-SCALE MACHINE LEARNING
   Perceptrons on Streaming Data
While we have viewed the training set as stored data, available for repeated use on any number of passes, Perceptrons can also be used in a stream setting. That is, we may suppose there is an infinite sequence of training examples, but that each may be used only once. Detecting email spam is a good example of a training stream. Users report spam emails and also report emails that were classified as spam but are not. Each email, as it arrives, is treated as a training example, and modifies the current weight vector, presumably by a very small amount.
If the training set is a stream, we never really converge, and in fact the data points may well not be linearly separable. However, at all times, we have an approximation to the best possible separator. Moreover, if the examples in the stream evolve over time, as would be the case for email spam, then we have an approximation that values recent examples more than examples from the distant past, much like the exponentially decaying windows technique from Section 4.7.
 (c) (d)
Thebasicmethodwithavariablethreshold,assuggestedinSection12.2.4.
The Winnow method with a variable threshold, as suggested in Section 12.2.4.
! Exercise 12.2.2 : For the following training set: ([1, 2], +1) ([2, 1], +1)
([2, 3], −1) ([3, 2], −1)
describe all the vectors w and thresholds θ such that the hyperplane (really a
line) defined by w.x − θ = 0 separates the points correctly.
! Exercise 12.2.3: Suppose the following four examples constitute a training
set:
(a) !! (b) (c)
([1, 2], −1) ([2, 3], +1) ([2, 1], +1) ([3, 2], −1)
What happens when you attempt to train a perceptron to classify these points using 0 as the threshold?
Is it possible to change the threshold and obtain a perceptron that cor- rectly classifies these points?
Suggest a transformation using quadratic polynomials that will transform these points so they become linearly separable.
12.3. SUPPORT-VECTOR MACHINES 461 12.3 Support-Vector Machines
We can view a support-vector machine, or SVM, as an improvement on the perceptron that is designed to address the problems mentioned in Section 12.2.7. An SVM selects one particular hyperplane that not only separates the points in the two classes, but does so in a way that maximizes the margin – the distance between the hyperplane and the closest points of the training set.
12.3.1 The Mechanics of an SVM
The goal of an SVM is to select a hyperplane w.x + b = 01 that maximizes the distance γ between the hyperplane and any point of the training set. The idea is suggested by Fig. 12.14. There, we see the points of two classes and a hyperplane dividing them.
                          Support vectors
γ γ
Figure 12.14: An SVM selects the hyperplane with the greatest possible margin γ between the hyperplane and the training points
Intuitively, we are more certain of the class of points that are far from the separating hyperplane than we are of points near to that hyperplane. Thus, it is desirable that all the training points be as far from the hyperplane as possible (but on the correct side of that hyperplane, of course). An added advantage of choosing the separating hyperplane to have as large a margin as possible is that there may be points closer to the hyperplane in the full data set but not in the training set. If so, we have a better chance that these points will be classified properly than if we chose a hyperplane that separated the training points but allowed some points to be very close to the hyperplane itself. In that case, there is a fair chance that a new point that was near a training point that
1Constant b in this formulation of a hyperplane is the same as the negative of the threshold θ in our treatment of perceptrons in Section 12.2.
w.x + b = 0
 
462 CHAPTER 12. LARGE-SCALE MACHINE LEARNING
was also near the hyperplane would be misclassified. This issue was discussed in Section 12.2.7 in connection with Fig. 12.13.
We also see in Fig. 12.14 two parallel hyperplanes at distance γ from the central hyperplane w.x + b = 0, and these each touch one or more of the support vectors. The latter are the points that actually constrain the dividing hyperplane, in the sense that they are all at distance γ from the hyperplane. In most cases, a d-dimensional set of points has d + 1 support vectors, as is the case in Fig. 12.14. However, there can be more support vectors if too many points happen to lie on the parallel hyperplanes. We shall see an example based on the points of Fig. 11.1, where it turns out that all four points are support vectors, even though two-dimensional data normally has three.
A tentative statement of our goal is:
• Givenatrainingset(x1,y1),(x2,y2),...,(xn,yn),maximizeγ(byvarying
w and b) subject to the constraint that for all i = 1,2,...,n, yi(w.xi + b) ≥ γ
Notice that yi, which must be +1 or −1, determines which side of the hyperplane the point xi must be on, so the ≥ relationship to γ is always correct. However, it may be easier to express this condition as two cases: if y = +1, then w.x+b ≥ γ, and if y = −1, then w.x + b ≤ −γ.
Unfortunately, this formulation doesn’t really work properly. The problem is that by increasing w and b, we can always allow a larger value of γ. For example, suppose that w and b satisfy the constraint above. If we replace w by 2w and b by 2b, we observe that for all i, yi (2w).xi + 2b  ≥ 2γ. Thus, 2w and 2b is always a better choice that w and b, so there is no best choice and no maximum γ.
12.3.2 Normalizing the Hyperplane
The solution to the problem that we described intuitively above is to normalize the weight vector w. That is, the unit of measure perpendicular to the sepa- rating hyperplane is the unit vector w/∥w∥. Recall that ∥w∥ is the Frobenius norm, or the square root of the sum of the squares of the components of w. We shall require that w be such that the parallel hyperplanes that just touch the support vectors are described by the equations w.x+b = +1 and w.x+b = −1, as suggested by Fig. 12.15.
Our goal becomes to maximize γ, which is now the multiple of the unit vector w/∥w∥ between the separating hyperplane and the parallel hyperplanes through the support vectors. Consider one of the support vectors, say x2 shown in Fig. 12.15. Let x1 be the projection of x2 onto the far hyperplane, also as suggested by Fig. 12.15. Note that x1 need not be a support vector or even a point of the training set. The distance from x2 to x1 in units of w/∥w∥ is 2γ. That is,
12.3. SUPPORT-VECTOR MACHINES
463
  x
1
       x
2
      w / || w ||
γ γ
         Figure 12.15: Normalizing the weight vector for an SVM
x1 = x2 + 2γ w (12.1) ∥w∥
Since x1 is on the hyperplane defined by w.x + b = +1, we know that w.x1 + b = 1. If we substitute for x1 using Equation 12.1, we get
w.x
w.x + b = +1 w.x+b=0
+ b = −1
  Regrouping terms, we see
w. x2 +2γ w  +b=1 ∥w∥
w.x2 +b+2γw.w =1 ∥w∥
(12.2)
 But the first two terms of Equation 12.2, w.x2 + b, sum to −1, since we know that x2 is on the hyperplane w.x + b = −1. If we move this −1 from left to right in Equation 12.2 and then divide through by 2, we conclude that
γ w.w = 1 (12.3) ∥w∥
Notice also that w.w is the sum of the squares of the components of w. That is, w.w = ∥w∥2. We conclude from Equation 12.3 that γ = 1/∥w∥.
This equivalence gives us a way to reformulate the optimization problem originally stated in Section 12.3.1. Instead of maximizing γ, we want to mini- mize ∥w∥, which is the inverse of γ if we insist on normalizing the scale of w. That is:
• Givenatrainingset(x1,y1),(x2,y2),...,(xn,yn),minimize∥w∥(byvary- ing w and b) subject to the constraint that for all i = 1,2,...,n,
yi(w.xi + b) ≥ 1
 
464 CHAPTER 12. LARGE-SCALE MACHINE LEARNING
Example 12.8: Let us consider the four points of Fig. 11.1, supposing that they alternate as positive and negative examples. That is, the training set consists of
([1, 2], +1) ([2, 1], −1) ([3, 4], +1) ([4, 3], −1)
Let w = [u,v]. Our goal is to minimize √u2 +v2 subject to the constraints we derive from the four training examples. For the first, where x1 = [1, 2] and y1 =+1,theconstraintis(+1)(u+2v+b)=u+2v+b≥1. Forthesecond, where x2 = [2,1] and y2 = −1, the constraint is (−1)(2u+v+b) ≥ 1, or 2u + v + b ≤ −1. The last two points are analogously handled, and the four constraints we derive are:
u + 2v + b ≥ 1 2u + v + b ≤ −1 3u+4v+b≥1 4u+3v+b≤−1
We shall cover in detail the subject of how one optimizes with constraints; the subject is broad and many packages are available for you to use. Sec- tion 12.3.4 discusses one method – gradient descent – in connection with a more general application of SVM, where there is no separating hyperplane. An illustration of how this method works will appear in Example 12.9.
In this simple example, the solution is easy to see: b = 0 and w = [u, v] = [−1,+1]. It happens that all four constraints are satisfied exactly; i.e., each of the four points is a support vector. That case is unusual, since when the data is two-dimensional, we expect only three support vectors. However, the fact that the positive and negative examples lie on parallel lines allows all four constraints to be satisfied exactly. ✷
12.3.3 Finding Optimal Approximate Separators
We shall now consider finding an optimal hyperplane in the more general case, where no matter which hyperplane we chose, there will be some points on the wrong side, and perhaps some points that are on the correct side, but too close to the separating hyperplane itself, so the margin requirement is not met. A typical situation is shown in Fig. 12.16. We see two points that are misclassified; they are on the wrong side of the separating hyperplane w.x + b = 0. We also see two points that, while they are classified correctly, are too close to the separating hyperplane. We shall call all these points bad points.
Each bad point incurs a penalty when we evaluate a possible hyperplane. The amount of the penalty, in units to be determined as part of the optimization process, is shown by the arrow leading to the bad point from the hyperplane on the wrong side of which the bad point lies. That is, the arrows measure the distance from the hyperplane w.x + b = 1 or w.x + b = −1. The former is the baseline for training examples that are supposed to be above the separating hyperplane (because the label y is +1), and the latter is the baseline for points that are supposed to be below (because y = −1).
 
12.3. SUPPORT-VECTOR MACHINES
Misclassified
465
                          Too close to boundary
Figure 12.16: Points that are misclassified or are too close to the separating hyperplane incur a penalty; the amount of the penalty is proportional to the length of the arrow leading to that point
We have many options regarding the exact formula that we wish to mini- mize. Intuitively, we want ∥w∥ to be as small as possible, as we discussed in Section 12.3.2. But we also want the penalties associated with the bad points to be as small as possible. The most common form of a tradeoff is expressed by a formula that involves the term ∥w∥2/2 and another term that involves a constant times the sum of the penalties.
To see why minimizing the term ∥w∥2/2 makes sense, note that minimizing
∥w∥ is the same as minimizing any monotone function of ∥w∥, so it is at least an
option to choose a formula in which we try to minimize ∥w∥2/2. It turns out to
be desirable because its derivative with respect to any component of w is that
component. That is, if w = [w1,w2,...,wd], then ∥w∥2/2 is 1  n wi2, so its 2 i=1
partial derivative ∂/∂wi is wi. This situation makes sense because, as we shall see, the derivative of the penalty term with respect to wi is a constant times each xi, the corresponding component of each feature vector whose training example incurs a penalty. That in turn means that the vector w and the vectors of the training set are commensurate in the units of their components.
Thus, we shall consider how to minimize the particular function
1dn d 
f(w,b)= 2 wi2 +C max 0, 1−yi  wjxij +b  (12.4)
j=1 i=1 j=1
The first term encourages small w, while the second term, involving the con- stant C that must be chosen properly, represents the penalty for bad points
w.x + b = +1 w.x + b = 0
+ b = −1
  w.x
  
466 CHAPTER 12. LARGE-SCALE MACHINE LEARNING
in a manner to be explained below. We assume there are n training exam- ples (xi,yi) for i = 1,2,...,n, and xi = [xi1,xi2,...,xid]. Also, as before, w = [w1, w2, . . . , wd]. Note that the two summations  dj=1 express the dot product of vectors.
The constant C, called the regularization parameter, reflects how important misclassification is. Pick a large C if you really do not want to misclassify points, but you would accept a narrow margin. Pick a small C if you are OK with some misclassified points, but want most of the points to be far away from the boundary (i.e., the margin is large).
We must explain the penalty function (second term) in Equation 12.4. The summation over i has one term
d L(xi,yi)=max 0, 1−yi  wjxij +b  
j=1
for each training example xi. The quantity L is a hinge function, suggested in Fig. 12.17, and we call its value the hinge loss. Let zi = yi( dj=1 wj xij + b). When zi is 1 or more, the value of L is 0. But for smaller values of zi, L rises linearly as zi decreases.
  2
1
0
max{0, 1− z }
−2 −1 0 1 2 3
z=y(w.x+b) ii
Figure 12.17: The hinge function decreases linearly for z ≤ 1 and then remains 0
Since we shall have need to take the derivative with respect to each wj of L(xi,yi), note that the derivative of the hinge function is discontinuous. It is −yixij forzi <1and0forzi >1. Thatis,ifyi =+1(i.e.,theithtraining example is positive), then
∂L  d
∂w =if wjxij+b≥1then0else−xij
j j=1
  
12.3. SUPPORT-VECTOR MACHINES 467 Moreover, if yi = −1 (i.e., the ith training example is negative), then
∂L  d
∂w =if wjxij +b≤−1then0elsexij
j j=1
The two cases can be summarized as one, if we include the value of yi, as:
∂L  d
∂w =if yi( wjxij +b)≥1then0else −yixij (12.5)
j j=1
12.3.4 SVM Solutions by Gradient Descent
A common approach to solving Equation 12.4 is to use quadratic programming. For large-scale data, another approach, gradient descent has an advantage. We can allow the data to reside on disk, rather than keeping it all in memory, which is normally required for quadratic solvers. To implement gradient descent, we compute the derivative of the equation with respect to b and each component wj of the vector w. Since we want to minimize f(w,b), we move b and the components wj in the direction opposite to the direction of the gradient. The amount we move each component is proportional to the derivative with respect to that component.
Our first step is to use the trick of Section 12.2.4 to make b part of the weight vector w. Notice that b is really the negative of a threshold on the dot product w.x, so we can append a (d + 1)st component b to w and append an extra component with value +1 to every feature vector in the training set (not −1 as we did in Section 12.2.4).
We must choose a constant η to be the fraction of the gradient that we move w in each round. That is, we assign
wj := wj − η ∂f ∂wj
for all j = 1, 2, . . . , d + 1.
The derivative ∂f of the first term in Equation 12.4, 1  d
∂f  n  d  
∂w =wj +C if yi( wjxij +b)≥1then0else −yixij (12.6)
   w2, is easy; i
  ∂wj 2 j=1
it is wj. However, the second term involves the hinge function, so it is harder to express. We shall use an if-then expression to describe these derivatives, as in Equation 12.5. That is:
 j i=1 j=1
Note that this formula gives us a partial derivative with respect to each compo- nent of w, including wd+1, which is b, as well as to the weights w1, w2, . . . , wd. We continue to use b instead of the equivalent wd+1 in the if-then condition to remind us of the form in which the desired hyperplane is described.
To execute the gradient-descent algorithm on a training set, we pick:
468 CHAPTER 12. LARGE-SCALE MACHINE LEARNING
1. Values for the parameters C and η.
2. Initial values for w, including the (d + 1)st component b. Then, we repeatedly:
(a) Compute the partial derivatives of f(w,b) with respect to the wj’s. (b) Adjust the values of w by subtracting η ∂f from each wj.
∂wj
Example 12.9 : Figure 12.18 shows six points, three positive and three nega- tive. We expect that the best separating line will be horizontal, and the only question is whether or not the separating hyperplane and the scale of w allows the point (2, 2) to be misclassified or to lie too close to the boundary. Initially, we shall choose w = [0,1], a vertical vector with a scale of 1, and we shall choose b = −2. As a result, we see in Fig. 12.18 that the point (2, 2) lies on the initial hyperplane and the three negative points are right at the margin. The parameter values we shall choose for gradient descent are C = 0.1, and η = 0.2.
 (1,4)
(3,4)
  Margin
Initial hyperplane
Margin
w
   (2,2)
   (1,1) (2,1)
(3,1)
Figure 12.18: Six points for a gradient-descent example
We begin by incorporating b as the third component of w, and for notational convenience, we shall use u and v as the first two components, rather than the customary w1 and w2. That is, we take w = [u, v, b]. We also expand the two- dimensional points of the training set with a third component that is always 1. That is, the training set becomes
([1, 4, 1], +1) ([2, 2, 1], +1) ([3, 4, 1], +1) ([1, 1, 1], −1) ([2, 1, 1], −1) ([3, 1, 1], −1)
In Fig. 12.19 we tabulate the if-then conditions and the resulting contri- butions to the summations over i in Equation 12.6. The summation must be
12.3. SUPPORT-VECTOR MACHINES
469 −1 −4 −1
 if if if if if if
u+4v+b≥+1 2u+2v+b≥+1 3u+4v+b≥+1
u+v+b≤−1 2u+v+b≤−1 3u+v+b≤−1
then 0 else then 0 else then 0 else then 0 else then 0 else then 0 else
foru forv forb −2 −2 −1
Figure 12.19: Sum each of these terms and of bad points to the derivatives of f with
multiplied by C and added to u, v, or b, tion 12.6.
multiply by C to get the contribution respect to u, v, and b
as appropriate, to implement Equa-
The truth or falsehood of each of the six conditions in Fig. 12.19 determines the contribution of the terms in the summations over i in Equation 12.6. We shall represent the status of each condition by a sequence of x’s and o’s, with x
representing a condition that does The first few iterations of gradient
not hold and o representing one that does. descent are shown in Fig. 12.20.
−3 −4 −1 +1 +1 +1 +2 +1 +1 +3 +1 +1
 (1) (2) (3) (4) (5) (6)
w = [u, v] [0.000,1.000] [0.040,0.840]
[−0.048,0.652] [−0.118,0.502] [−0.094,0.542] [−0.155, 0.414]
b −2.000 −1.580 −1.304 −1.083 −0.866 −0.733
Bad oxoooo oxoxxx oxoxxx xxxxxx oxoxxx xxxxxx
∂/∂u −0.200 0.440 0.352 −0.118 0.306
∂/∂v ∂/∂b 0.800 −2.100 0.940 −1.380 0.752 −1.104
−0.198 −1.083 0.642 −0.666
Figure 12.20: Beginning of the process of gradient descent
Consider line (1). It shows the initial value of w = [0, 1]. Recall that we use uandvforthecomponentsofw,sou=0andv=1. Wealsoseetheinitial value of b = −2. We must use these values of u and v to evaluate the conditions in Fig. 12.19. The first of the conditions in Fig. 12.19 is u + 4v + b ≥ +1. The left side is 0 + 4 + (−2) = 2, so the condition is satisfied. However, the second condition,2u+2v+b≥+1fails. Theleftsideis0+2+(−2)=0. Thefact that the sum is 0 means the second point (2,2) is exactly on the separating hyperplane, and not outside the margin. The third condition is satisfied, since 0 + 4 + (−2) = 2 ≥ +1. The last three conditions are also satisfied, and in fact are satisfied exactly. For instance, the fourth condition is u + v + b ≤ −1. The left side is 0 + 1 + (−2) = −1. Thus, the pattern oxoooo represents the outcome of these six conditions, as we see in the first line of Fig. 12.20.
We use these conditions to compute the partial derivatives. For ∂f/∂u, we
470 CHAPTER 12. LARGE-SCALE MACHINE LEARNING use u in place of wj in Equation 12.6. This expression thus becomes
u+C 0+(−2)+0+0+0+0 =0+ 1 (−2)=−0.2 10
The sum multiplying C can be explained this way. For each of the six conditions
of Fig. 12.19, take 0 if the condition is satisfied, and take the value in the
column labeled “for u” if it is not satisfied. Similarly, for v in place of wj we
get ∂f/∂v = 1+ 1  0+(−2)+0+0+0+0  = 0.8. Finally, for b we get 10
∂f/∂b=−2+ 1  0+(−1)+0+0+0+0 =−2.1. 10
   We can now compute the new w and b that appear on line (2) of Fig. 12.20. Since we chose η = 1/5, the new value of u is 0 − 1(−0.2) = −0.04, the new
 151
value of v is 1− 5(0.8) = 0.84, and the new value of b is −2− 5(−2.1) = −1.58.
  To compute the derivatives shown in line (2) of Fig. 12.20 we must first check
the conditions of Fig. 12.19. While the outcomes of the first three conditions
have not changed, the last three are no longer satisfied. For example, the
fourth condition is u + v + b ≤ −1, but 0.04 + 0.84 + (−1.58) = −0.7, which is
not less than −1. Thus, the pattern of bad points becomes oxoxxx. We now
have more nonzero terms in the expressions for the derivatives. For example
∂f/∂u=0.04+ 1  0+(−2)+0+1+2+3 =0.44. 10
The values of w and b in line (3) are computed from the derivatives of line (2) in the same way as they were computed in line (2). The new values do not change the pattern of bad points; it is still oxoxxx. However, when we repeat the process for line (4), we find that all six conditions are unsatisfied. For instance, the first condition, u + 4v + b ≥ +1 is not satisfied, because (−0.118 + 4 × 0.502 + (−1.083) = 0.807, which is less than 1. In effect, the first point has become too close to the separating hyperplane, even though it is properly classified.
We can see that in line (5) of Fig. 12.20, the problems with the first and third points are corrected, and we go back to pattern oxoxxx of bad points. However, at line (6), the points have again become too close to the separating hyperplane, so we revert to the xxxxxx pattern of bad points. You are invited to continue the sequence of updates to w and b for several more iterations.
One might wonder why the gradient-descent process seems to be converging on a solution where at least some of the points are inside the margin, when there is an obvious hyperplane (horizontal, at height 1.5) with a margin of 1/2, that separates the positive and negative points. The reason is that when we picked C = 0.1 we were saying that we really don’t care too much whether there are points inside the margins, or even if points are misclassified. We were saying also that what was important was a large margin (which corresponds to a small ∥w∥), even if some points violated that same margin. ✷
12.3.5 Stochastic Gradient Descent
The gradient-descent algorithm described in Section 12.3.4 is often called batch gradient descent, because at each round, all the training examples are consid-
 
12.3. SUPPORT-VECTOR MACHINES 471
ered as a “batch.” While it is effective on small datasets, it can be too time- consuming to execute on a large dataset, where we must visit every training example, often many times before convergence.
An alternative, called stochastic gradient descent, considers one training ex- ample, or a few training examples at a time and adjusts the current estimate of the error function (w in the SVM example) in the direction indicated by only the small set of training examples considered. Additional rounds are possible, using other sets of training examples; these can be selected randomly or accord- ing to some fixed strategy. Note that it is normal that some members of the training set are never used in a stochastic gradient descent algorithm.
Example 12.10: Recall the UV-decomposition algorithm discussed in Sec- tion 9.4.3. This algorithm was described as an example of batch gradient de- scent. We can regard each of the nonblank entries in the matrix M we are trying to approximate by the product UV as a training example, and the error function is the root-mean-square error between the product of the current ma- trices U and V and the matrix M, considering only those elements where M is nonblank.
However, if M has a very large number of nonblank entries, as would be the case if M represented, say, purchases of items by Amazon customers or movies that Netflix customers had rated, then it is not practical to make repeated passes over the entire set of nonblank entries of M when adjusting the entries in U and V . A stochastic gradient descent would look at a single nonblank entry of M and compute the change to each element of U and V that would make the product UV agree with that element of M. We would not make that change to the elements of U and V completely, but rather choose some learning rate η less than 1 and change each element of U and V by the fraction η of the amount that would be necessary to make UV equal M in the chosen entry. ✷
12.3.6 Parallel Implementation of SVM
One approach to parallelism for SVM is analogous to what we suggested for perceptrons in Section 12.2.8. You can start with the current w and b, and in parallel do several iterations based on each training example. Then average the changes for each of the examples to create a new w and b. If we distribute w and b to each mapper, then the Map tasks can do as many iterations as we wish to do in one round, and we need use the Reduce tasks only to average the results. One iteration of MapReduce is needed for each round.
A second approach is to follow the prescription given here, but implement the computation of the second term in Equation 12.4 in parallel. The contribu- tion from each training example can then be summed. This approach requires one round of MapReduce for each iteration of gradient descent.
472 CHAPTER 12. LARGE-SCALE MACHINE LEARNING 12.3.7 Exercises for Section 12.3
Exercise 12.3.1: Continue the iterations of Fig. 12.20 for three more itera- tions.
Exercise 12.3.2: The following training set obeys the rule that the positive examples all have vectors whose components sum to 10 or more, while the sum is less than 10 for the negative examples.
(a) ! (b)
! (c)
([3, 4, 5], +1) ([2, 7, 2], +1) ([5, 5, 5], +1) ([1, 2, 3], −1) ([3, 3, 2], −1) ([2, 4, 1], −1)
Which of these six vectors are the support vectors?
Suggest a vector w and constant b such that the hyperplane defined by w.x + b = 0 is a good separator for the positive and negative examples. Make sure that the scale of w is such that all points are outside the margin; that is, for each training example (x, y), you have y(w.x + b) ≥ +1.
Starting with your answer to part (b), use gradient descent to find the optimum w and b. Note that if you start with a separating hyperplane, and you scale w properly, then the second term of Equation 12.4 will always be 0, which simplifies your work considerably.
! Exercise 12.3.3: The following training set obeys the rule that the positive examples all have vectors whose components have an odd sum, while the sum is even for the negative examples.
(a) !! (b)
([1, 2], +1) ([3, 4], +1) ([5, 2], +1) ([2, 4], −1) ([3, 1], −1) ([7, 3], −1)
Suggest a starting vector w and constant b that classifies at least three of the points correctly.
Starting with your answer to (a), use gradient descent to find the optimum w and b.
12.4 Learning from Nearest Neighbors
In this section we consider several examples of “learning,” where the entire training set is stored, perhaps, preprocessed in some useful way, and then used to classify future examples or to compute the value of the label that is most likely associated with the example. The feature vector of each training example is treated as a data point in some space. When a new point arrives and must be classified, we find the training example or examples that are closest to the new point, according to the distance measure for that space. The estimated label is then computed by combining the closest examples in some way.
12.4. LEARNING FROM NEAREST NEIGHBORS 473 12.4.1 The Framework for Nearest-Neighbor Calculations
The training set is first preprocessed and stored. The decisions take place when a new example, called the query example arrives and must be classified.
There are several decisions we must make in order to design a nearest- neighbor-based algorithm that will classify query examples. We enumerate them here.
1. What distance measure do we use?
2. How many of the nearest neighbors do we look at?
3. How do we weight the nearest neighbors? Normally, we provide a function (the kernel function) of the distance between the query example and its nearest neighbors in the training set, and use this function to weight the neighbors.
4. How do we define the label to associate with the query? This label is some function of the labels of the nearest neighbors, perhaps weighted by the kernel function, or perhaps not. If there is no weighting, then the kernel function need not be specified.
12.4.2 Learning with One Nearest Neighbor
The simplest cases of nearest-neighbor learning are when we choose only the one neighbor that is nearest the query example. In that case, there is no use for weighting the neighbors, so the kernel function is omitted. There is also typically only one possible choice for the labeling function: take the label of the query to be the same as the label of the nearest neighbor.
Example 12.11 : Figure 12.21 shows some of the examples of dogs that last appeared in Fig. 12.1. We have dropped most of the examples for simplicity, leaving only three Chihuahuas, two Dachshunds, and two Beagles. Since the height-weight vectors describing the dogs are two-dimensional, there is a simple and efficient way to construct a Voronoi diagram for the points, in which the perpendicular bisectors of the lines between each pair of points is constructed. Each point gets a region around it, containing all the points to which it is the nearest. These regions are always convex, although they may be open to infinityinonedirection.2 Itisalsoasurprisingfactthat,eventhoughthereare O(n2) perpendicular bisectors for n points, the Voronoi diagram can be found in O(n log n) time.
In Fig. 12.21 we see the Voronoi diagram for the seven points. The bound- aries that separate dogs of different breeds are shown solid, while the boundaries
2While the region belonging to any one point is convex, the union of the regions for two or more points might not be convex. Thus, in Fig. 12.21 we see that the region for all Dachshunds and the region for all Beagles are not convex. That is, there are points p1 and p2 that are both classified Dachshunds, but the midpoint of the line between p1 and p2 is classified as a Beagle, and vice versa.
 
474
CHAPTER 12. LARGE-SCALE MACHINE LEARNING
Beagles
Dachshunds
Figure 12.21: Voronoi diagram for the three breeds of dogs
 Chihuahuas
between dogs of the same breed are shown dashed. Suppose a query example q is provided. Note that q is a point in the space of Fig. 12.21. We find the region into which q falls, and give q the label of the training example to which that region belongs. Note that it is not too hard to find the region of q. We have to determine to which side of certain lines q falls. This process is the same as we used in Sections 12.2 and 12.3 to compare a vector x with a hyperplane perpendicular to a vector w. In fact, if the lines that actually form parts of the Voronoi diagram are preprocessed properly, we can make the determination in O(log n) comparisons; it is not necessary to compare q with all of the O(n log n) lines that form part of the diagram. ✷
12.4.3 Learning One-Dimensional Functions
Another simple and useful case of nearest-neighbor learning has one-dimensional data. In this situation, the training examples are of the form ([x],y), and we shall write them as (x,y), identifying a one-dimensional vector with its lone component. In effect, the training set is a collection of samples of the value of a function y = f(x) for certain values of x, and we must interpolate the function f at all points. There are many rules that could be used, and we shall only outline some of the popular approaches. As discussed in Section 12.4.1, the approaches vary in the number of neighbors they use, whether or not the
12.4. LEARNING FROM NEAREST NEIGHBORS 475
neighbors are weighted, and if so, how the weight varies with distance. Suppose we use a method with k nearest neighbors, and x is the query point. Let x1, x2, . . . , xk be the k nearest neighbors of x, and let the weight associated with training point (xi,yi) be wi. Then the estimate of the label y for x is  ki=1 wiyi/  ki=1 wi. Note that this expression gives the weighted average of
the labels of the k nearest neighbors.
Example 12.12 : We shall illustrate four simple rules, using the training set (1, 1), (2, 2), (3, 4), (4, 8), (5, 4), (6, 2), and (7, 1). These points represent a function that has a peak at x = 4 and decays exponentially on both sides. Note that this training set has values of x that are evenly spaced. There is no requirement that the points have any regular pattern. Some possible ways to interpolate values are:
                  1234567 1234567
(a) One nearest neighbor (b) Average of two nearest neighbors
Figure 12.22: Results of applying the first two rules in Example 12.12
1. Nearest Neighbor. Use only the one nearest neighbor. There is no need for a weighting. Just take the value of any f(x) to be the label y associ- ated with the training-set point nearest to query point x. The result of using this rule on the example training set described above is shown in Fig. 12.22(a).
2. Average of the Two Nearest Neighbors. Choose 2 as the number of nearest neighbors to use. The weights of these two are each 1/2, regardless of how far they are from the query point x. The result of this rule on the example training set is in Fig. 12.22(b).
3. Weighted Average of the Two Nearest Neighbors. We again choose two nearest neighbors, but we weight them in inverse proportion to their dis- tance from the query point. Suppose the two neighbors nearest to query
476
CHAPTER 12. LARGE-SCALE MACHINE LEARNING
point x are x1 and x2. Suppose first that x1 < x < x2. Then the weight of x1, the inverse of its distance from x, is 1/(x − x1), and the weight of x2 is 1/(x2 − x). The weighted average of the labels is
  y1 + y2  /  1 + 1   x−x1 x2 −x x−x1 x2 −x
which, when we multiply numerator and denominator by (x−x1)(x2 −x), simplifies to
y1(x2 −x)+y2(x−x1) x2 − x1
This expression is the linear interpolation of the two nearest neighbors, as shown in Fig. 12.23(a). When both nearest neighbors are on the same side of the query x, the same weights make sense, and the resulting estimate is an extrapolation. We see extrapolation in Fig. 12.23(a) in the range x = 0 to x = 1. In general, when points are unevenly spaced, we can find query points in the interior where both neighbors are on one side.
     ✷
4. Average of Three Nearest Neighbors. We can average any number of the nearest neighbors to estimate the label of a query point. Figure 12.23(b) shows what happens on our example training set when the three nearest neighbors are used.
                  1234567 1234567
(a) Weighted average of two neighbors (b) Average of three neighbors
Figure 12.23: Results of applying the last two rules in Example 12.12
12.4. LEARNING FROM NEAREST NEIGHBORS 477 12.4.4 Kernel Regression
A way to construct a continuous function that represents the data of a training set well is to consider all points in the training set, but weight the points using a kernel function that decays with distance. A popular choice is to use a normal distribution (or “bell curve”), so the weight of a training point x when the query is q is e−(x−q)2/σ2 . Here σ is the standard deviation of the distribution and the query q is the mean. Roughly, points within distance σ of q are heavily weighted, and those further away have little weight. The advantage of using a kernel function that is itself continuous and that is defined for all points in the training set is to be sure that the resulting function learned from the data is itself continuous (see Exercise 12.4.6 for a discussion of the problem when a simpler weighting is used).
Example 12.13 : Let us use the seven training examples of Example 12.12. To make calculation simpler, we shall not use the normal distribution as the kernel function, but rather another continuous function of distance, namely w = 1/(x − q)2. That is, weights decay as the square of the distance. Suppose the query q is 3.5. The weights w1,w2,...w7 of the seven training examples (xi,yi) = (i,8/2|i−4|) for i = 1,2,...,7 are shown in Fig. 12.24.
(1)xi 1234567 (2)yi 1248421 (3) wi 4/25 4/9 4 4 4/9 4/25 4/49 (4) wiyi 4/25 8/9 16 32 16/9 8/25 4/49
Figure 12.24: Weights of points when the query is q = 3.5
Lines (1) and (2) of Fig. 12.24 give the seven training points. The weight of each when the query is q = 3.5 is given in line (3). For instance, for x1 = 1, the weight w1 = 1/(1 − 3.5)2 = 1/(−2.5)2 = 4/25. Then, line (4) shows each yi weighted by the weight from line (3). For instance, the column for x2 has value 8/9 because w2y2 = 2 × (4/9).
To compute the label for the query q = 3.5 we sum the weighted values of the labels in the training set, as given by line (4) of Fig. 12.24; this sum is 51.23. We then divide by the sum of the weights in line (3). This sum is 9.29, so the ratio is 51.23/9.29 = 5.51. That estimate of the value of the label for q = 3.5 seems intuitively reasonable, since q lies midway between two points with labels 4 and 8. ✷
12.4.5 Dealing with High-Dimensional Euclidean Data
We saw in Section 12.4.2 that the two-dimensional case of Euclidean data is fairly easy. There are several large-scale data structures that have been de- veloped for finding near neighbors when the number of dimensions grows, and
     
478 CHAPTER 12. LARGE-SCALE MACHINE LEARNING
   Problems in the Limit for Example 12.13
Suppose q is exactly equal to one of the training examples x. If we use the normal distribution as the kernel function, there is no problem with the weight of x; it is 1. However, with the kernel function discussed in Example 12.13, the weight of x is 1/(x−q)2 = ∞. Fortunately, this weight appears in both the numerator and denominator of the expression that estimates the label of q. It can be shown that in the limit as q approaches x, the label of x dominates all the other terms in both numerator and denominator, so the estimated label of q is the same as the label of x. That makes excellent sense, since q = x in the limit.
 the training set is large. We shall not cover these structures here, because the subject could fill a book by itself, and there are many places available to learn about these techniques, collectively called multidimensional index structures. The references for this chapter give some of these sources for information about such structures as kd-Trees, R-Trees, and Quad Trees.
Unfortunately, for high-dimensional data, there is little that can be done to avoid searching a large portion of the data. This fact is another manifestation of the “curse of dimensionality” from Section 7.1.3. Two ways to deal with the “curse” are the following:
1. VA Files. Since we must look at a large fraction of the data anyway in order to find the nearest neighbors of a query point, we could avoid a complex data structure altogether. Accept that we must scan the entire file, but do so in a two-stage manner. First, a summary of the file is created, using only a small number of bits that approximate the values of each component of each training vector. For example, if we use only the high-order (1/4)th of the bits in numerical components, then we can create a file that is (1/4)th the size of the full dataset. However, by scanning this file we can construct a list of candidates that might be among the k nearest neighbors of the query q, and this list may be a small fraction of the entire dataset. We then look up only these candidates in the complete file, in order to determine which k are nearest to q.
2. Dimensionality Reduction. We may treat the vectors of the training set as a matrix, where the rows are the vectors of the training example, and the columns correspond to the components of these vectors. Apply one of the dimensionality-reduction techniques of Chapter 11, to compress the vectors to a small number of dimensions, small enough that the techniques for multidimensional indexing can be used. Of course, when processing a query vector q, the same transformation must be applied to q before searching for q’s nearest neighbors.
12.4. LEARNING FROM NEAREST NEIGHBORS 479 12.4.6 Dealing with Non-Euclidean Distances
To this point, we have assumed that the distance measure is Euclidean. How- ever, most of the techniques can be adapted naturally to an arbitrary distance function d. For instance, in Section 12.4.4 we talked about using a normal dis- tribution as a kernel function. Since we were thinking about a one-dimensional training set in a Euclidean space, we wrote the exponent as −(x−q)2. However, for any distance function d, we can use as the weight of a point x at distance d(x, q) from the query point q the value of
   2 2 e− d(x−q) /σ
Note that this expression makes sense if the data is in some high-dimensional Euclidean space and d is the usual Euclidean distance or Manhattan distance or any other distance discussed in Section 3.5.2. It also makes sense if d is Jaccard distance or any other distance measure.
However, for Jaccard distance and the other distance measures we consid- ered in Section 3.5 we also have the option to use locality-sensitive hashing, the subject of Chapter 3. Recall these methods are only approximate, and they could yield false negatives – training examples that were near neighbors to a query but that do not show up in a search.
If we are willing to accept such errors occasionally, we can build the buckets for the training set and keep them as the representation of the training set. These buckets are designed so we can retrieve all (or almost all, since there can be false negatives) training-set points that are have a minimum similarity to a given query q. Equivalently, one of the buckets to which the query hashes will contain all those points within some maximum distance of q. We hope that as many nearest neighbors of q as our method requires will be found among those buckets.
Yet if different queries have radically different distances to their nearest neighbors, all is not lost. We can pick several distances d1 < d2 < d3 < · · · . Build the buckets for locality-sensitive hashing using each of these distances. For a query q, start with the buckets for distance d1. If we find enough near neighbors, we are done. Otherwise, repeat the search using the buckets for d2, and so on, until enough nearest neighbors are found.
12.4.7 Exercises for Section 12.4
Exercise 12.4.1: Suppose we modified Example 12.11 to look at the two nearest neighbors of a query point q. Classify q with the common label if those two neighbors have the same label, and leave q unclassified if the labels of the neighbors are different.
(a) Sketch the boundaries of the regions for the three dog breeds on Fig. 12.21.
! (b) Would the boundaries always consist of straight line segments for any training data?
480 CHAPTER 12. LARGE-SCALE MACHINE LEARNING Exercise 12.4.2 : Suppose we have the following training set
([1, 2], +1) ([2, 1], −1) ([3, 4], −1) ([4, 3], +1)
which is the training set used in Example 12.9. If we use nearest-neighbor learning with the single nearest neighbor as the estimate of the label of a query point, which query points are labeled +1?
Exercise 12.4.3 : Consider the one-dimensional training set (1, 1), (2, 2), (4, 3), (8, 4), (16, 5), (32, 6)
Describe the function f(q), the label that is returned in response to the query q, when the interpolation used is:
(a) The label of the nearest neighbor.
(b) The average of the labels of the two nearest neighbors.
! (c) The average, weighted by distance, of the two nearest neighbors.
(d) The (unweighted) average of the three nearest neighbors.
! Exercise 12.4.4: Apply the kernel function of Example 12.13 to the data of
Exercise 12.4.3. For queries q in the range 2 < q < 4, what is the label of q? Exercise 12.4.5 : What is the function that estimates the label of query points
using the data of Example 12.12 and the average of the four nearest neighbors?
!! Exercise 12.4.6 : Simple weighting functions such as those in Example 12.12 need not define a continuous function. We can see that the constructed functions in Fig. 12.22 and Fig. 12.23(b) are not continuous, but Fig. 12.23(a) is. Does the weighted average of two nearest neighbors always give a continuous function?
12.5 Comparison of Learning Methods
Each of the methods discussed in this chapter and elsewhere has its advantages. In this closing section, we shall consider:
• Does the method deal with categorical features or only with numerical features?
• Does the method deal effectively with high-dimensional feature vectors?
• Is the model that the method constructs intuitively understandable?
12.6. SUMMARY OF CHAPTER 12 481
Perceptrons and Support-Vector Machines: These methods can handle millions of features, but they only make sense if the features are numerical. They only are effective if there is a linear separator, or at least a hyperplane that approximately separates the classes. However, we can separate points by a nonlinear boundary if we first transform the points to make the separator be linear. The model is expressed by a vector, the normal to the separating hyperplane. Since this vector is often of very high dimension, it can be very hard to interpret the model.
Nearest-Neighbor Classification and Regression: Here, the model is the training set itself, so we expect it to be intuitively understandable. The ap- proach can deal with multidimensional data, although the larger the number of dimensions, the sparser the training set will be, and therefore the less likely it is that we shall find a training point very close to the point we need to classify. That is, the “curse of dimensionality” makes nearest-neighbor methods ques- tionable in high dimensions. These methods are really only useful for numerical features, although one could allow categorical features with a small number of values. For instance, a binary categorical feature like {male, female} could have the values replaced by 0 and 1, so there was no distance in this dimension between individuals of the same gender and distance 1 between other pairs of individuals. However, three or more values cannot be assigned numbers that are equidistant. Finally, nearest-neighbor methods have many parameters to set, including the distance measure we use (e.g., cosine or Euclidean), the number of neighbors to choose, and the kernel function to use. Different choices result in different classification, and in many cases it is not obvious which choices yield the best results.
Decision Trees: We have not discussed this commonly used method in this chapter, although it was introduced briefly in Section 9.2.7. Unlike the methods of this chapter, decision trees are useful for both categorical and numerical features. The models produced are generally quite understandable, since each decision is represented by one node of the tree. However, this approach is only useful for low-dimension feature vectors. The reason is that building decision trees with many levels leads to overfitting, where below the top levels, the decisions are based on peculiarities of small fractions of the training set, rather than fundamental properties of the data. But if a decision tree has few levels, then it cannot even mention more than a small number of features. As a result, the best use of decision trees is often to create an ensemble of many, low-depth trees and combine their decision in some way.
12.6 Summary of Chapter 12
✦ Training Sets: A training set consists of a feature vector, each component of which is a feature, and a label indicating the class to which the object represented by the feature vector belongs. Features can be categorical – belonging to an enumerated list of values – or numerical.
482 CHAPTER 12. LARGE-SCALE MACHINE LEARNING
✦ Test Sets and Overfitting: When training some classifier on a training set, it is useful to remove some of the training set and use the removed data as a test set. After producing a model or classifier without using the test set, we can run the classifier on the test set to see how well it does. If the classifier does not perform as well on the test set as on the training set used, then we have overfit the training set by conforming to peculiarities of the training-set data which is not present in the data as a whole.
✦ Batch Versus On-Line Learning: In batch learning, the training set is available at any time and can be used in repeated passes. On-line learning uses a stream of training examples, each of which can be used only once.
✦ Perceptrons: This machine-learning method assumes the training set has only two class labels, positive and negative. Perceptrons work when there is a hyperplane that separates the feature vectors of the positive examples from those of the negative examples. We converge to that hyperplane by adjusting our estimate of the hyperplane by a fraction – the learning rate – of the direction that is the average of the currently misclassified points.
✦ The Winnow Algorithm: This algorithm is a variant of the perceptron algorithm that requires components of the feature vectors to be 0 or 1. Training examples are examined in a round-robin fashion, and if the cur- rent classification of a training example is incorrect, the components of the estimated separator where the feature vector has 1 are adjusted up or down, in the direction that will make it more likely this training example is correctly classified in the next round.
✦ Nonlinear Separators: When the training points do not have a linear func- tion that separates two classes, it may still be possible to use a perceptron to classify them. We must find a function we can use to transform the points so that in the transformed space, the separator is a hyperplane.
✦ Support-Vector Machines: The SVM improves upon perceptrons by find- ing a separating hyperplane that not only separates the positive and nega- tive points, but does so in a way that maximizes the margin – the distance perpendicular to the hyperplane to the nearest points. The points that lie exactly at this minimum distance are the support vectors. Alterna- tively, the SVM can be designed to allow points that are too close to the hyperplane, or even on the wrong side of the hyperplane, but minimize the error due to such misplaced points.
✦ Solving the SVM Equations: We can set up a function of the vector that is normal to the hyperplane, the length of the vector (which determines the margin), and the penalty for points on the wrong side of the margins. The regularization parameter determines the relative importance of a wide margin and a small penalty. The equations can be solved by several methods, including gradient descent and quadratic programming.
12.7.
✦
✦
REFERENCES FOR CHAPTER 12 483
Nearest-Neighbor Learning: In this approach to machine learning, the entire training set is used as the model. For each (“query”) point to be classified, we search for its k nearest neighbors in the training set. The classification of the query point is some function of the labels of these k neighbors. The simplest case is when k = 1, in which case we can take the label of the query point to be the label of the nearest neighbor.
Regression: A common case of nearest-neighbor learning, called regres- sion, occurs when the there is only one feature vector, and it, as well as the label, are real numbers; i.e., the data defines a real-valued function of one variable. To estimate the label, i.e., the value of the function, for an unlabeled data point, we can perform some computation involving the k nearest neighbors. Examples include averaging the neighbors or taking a weighted average, where the weight of a neighbor is some decreasing func- tion of its distance from the point whose label we are trying to determine.
12.7 References for Chapter 12
The perceptron was introduced in [11]. [7] introduces the idea of maximizing the margin around the separating hyperplane. A well-known book on the subject is [10].
The Winnow algorithm is from [9]. Also see the analysis in [1].
Support-vector machines appeared in [6]. [5] and [4] are useful surveys. [8] talks about a more efficient algorithm for the case of sparse features (most com- ponents of the feature vectors are zero). The use of gradient-descent methods is found in [2, 3].
1. A. Blum, “Empirical support for winnow and weighted-majority algo- rithms: results on a calendar scheduling domain,” Machine Learning 26 (1997), pp. 5–23.
2. L. Bottou, “Large-scale machine learning with stochastic gradient de- scent,” Proc. 19th Intl. Conf. on Computational Statistics (2010), pp. 177– 187, Springer.
3. L. Bottou, “Stochastic gradient tricks, neural networks,” in Tricks of the Trade, Reloaded, pp. 430–445, Edited by G. Montavon, G.B. Orr and K.- R. Mueller, Lecture Notes in Computer Science (LNCS 7700), Springer, 2012.
4. C.J.C. Burges, “A tutorial on support vector machines for pattern recog- nition,” Data Mining and Knowledge Discovery 2 (1998), pp. 121–167.
5. N. Cristianini and J. Shawe-Taylor, An Introduction to Support Vector Machines and Other Kernel-Based Learning Methods, Cambridge Univer- sity Press, 2000.
484 CHAPTER 12. LARGE-SCALE MACHINE LEARNING
6. C. Cortes and V.N. Vapnik, “Support-vector networks,” Machine Learn-
ing 20 (1995), pp. 273–297.
7. Y. Freund and R.E. Schapire, “Large margin classification using the per-
ceptron algorithm,” Machine Learning 37 (1999), pp. 277–296.
8. T. Joachims, “Training linear SVMs in linear time.” Proc. 12th ACM
SIGKDD (2006), pp. 217–226.
9. N. Littlestone, “Learning quickly when irrelevant attributes abound: a new linear-threshold algorithm,” Machine Learning 2 (1988), pp. 285– 318.
10. M. Minsky and S. Papert, Perceptrons: An Introduction to Computational Geometry (2nd edition), MIT Press, Cambridge MA, 1972.
11. F. Rosenblatt, “The perceptron: a probabilistic model for information storage and organization in the brain,” Psychological Review 65:6 (1958), pp. 386–408.
Chapter One
Linear Systems I Solving Linear Systems
Systems of linear equations are common in science and mathematics. These two examples from high school science [Onan] give a sense of how they arise.
The first example is from Statics. Suppose that we have three objects, we know that one has a mass of 2 kg, and we want to find the two unknown masses. Experimentation with a meter stick produces these two balances.
40 50 25 50
hc2c2h
15 25
For the masses to balance we must have that the sum of moments on the left equals the sum of moments on the right, where the moment of an object is its mass times its distance from the balance point. That gives a system of two linear equations.
40h + 15c = 100
25c = 50 + 50h
The second example is from Chemistry. We can mix, under controlled conditions, toluene C7H8 and nitric acid HNO3 to produce trinitrotoluene C7H5O6N3 along with the byproduct water (conditions have to be very well controlled — trinitrotoluene is better known as TNT). In what proportion should we mix them? The number of atoms of each element present before the reaction
xC7H8 + yHNO3 −→ zC7H5O6N3 + wH2O
                        
2 Chapter One. Linear Systems must equal the number present afterward. Applying that in turn to the elements
C, H, N, and O gives this system.
7x = 7z
8x + 1y = 5z + 2w 1y = 3z
3y = 6z + 1w
Both examples come down to solving a system of equations. In each system, the equations involve only the first power of each variable. This chapter shows how to solve any such system of equations.
I.1 Gauss’s Method
  1.1 Definition A linear combination of x1 , . . . , xn has the form a1x1 +a2x2 +a3x3 +···+anxn
where the numbers a1, . . . , an ∈ R are the combination’s coefficients. A linear equation in the variables x1, ..., xn has the form a1x1 +a2x2 +a3x3 +···+ anxn = d where d ∈ R is the constant.
An n-tuple (s1, s2, . . . , sn) ∈ Rn is a solution of, or satisfies, that equation if substituting the numbers s1, . . . , sn for the variables gives a true statement: a1s1 + a2s2 + · · · + ansn = d. A system of linear equations
a1,1x1 + a1,2x2 +···+ a1,nxn = d1 a2,1x1 + a2,2x2 +···+ a2,nxn = d2
.
am,1x1 +am,2x2 +···+am,nxn = dm
has the solution (s1 , s2 , . . . , sn ) if that n-tuple is a solution of all of the equations.
1.2 Example The combination 3x1 + 2x2 of x1 and x2 is linear. The combination 3x21 + 2x2 is not a linear function of x1 and x2, nor is 3x1 + 2 sin(x2).
We usually take x1, . . . , xn to be unequal to each other because in a sum with repeats we can rearrange to make the elements unique, as with 2x + 3y + 4x = 6x + 3y. We sometimes include terms with a zero coefficient, as in x − 2y + 0z, and at other times omit them, depending on what is convenient.
Section I. Solving Linear Systems 3 1.3 Example The ordered pair (−1, 5) is a solution of this system.
3x1 +2x2 =7 −x1+ x2=6
In contrast, (5, −1) is not a solution.
Finding the set of all solutions is solving the system. We don’t need guesswork or good luck, there is an algorithm that always works. This algorithm is Gauss’s Method (or Gaussian elimination or linear elimination).
 1.4 Example To solve this system
x1 +5x2 −2x3 =2
3x3 = 9 1 x1 + 2x2 = 3
 3
we transform it, step by step, until it is in a form that we can easily solve. The first transformation rewrites the system by interchanging the first and
third row.
 swap row 1 with row 3
1x1 +2x2 =3 3
x1 + 5x2 − 2x3 = 2 3x3 = 9
−→
The second transformation rescales the first row by a factor of 3.
x1 + 6x2 = 9 −→ x1 + 5x2 − 2x3 = 2 3x3 = 9
The third transformation is the only nontrivial one in this example. We mentally multiply both sides of the first row by −1, mentally add that to the second row, and write the result in as the new second row.
multiply row 1 by 3
x1+6x2 =9 −→ −x2 − 2x3 = −7 3x3= 9
These steps have brought the system to a form where we can easily find the value of each variable. The bottom equation shows that x3 = 3. Substituting 3 for x3 in the middle equation shows that x2 = 1. Substituting those two into the top equation gives that x1 = 3. Thus the system has a unique solution; the solution set is {(3,1,3)}.
We will use Gauss’s Method throughout the book. It is fast and easy. We will now show that it is also safe: Gauss’s Method never loses solutions nor does it ever pick up extraneous solutions, so that a tuple is a solution to the system before we apply the method if and only if it is a solution after.
add −1 times row 1 to row 2
4 Chapter One. Linear Systems
  1.5 Theorem (Gauss’s Method) If a linear system is changed to another by one of these operations
(1) an equation is swapped with another
(2) an equation has both sides multiplied by a nonzero constant
(3) an equation is replaced by the sum of itself and a multiple of another
then the two systems have the same set of solutions.
Each of the three operations has a restriction. Multiplying a row by 0 is not allowed because obviously that can change the solution set. Similarly, adding a multiple of a row to itself is not allowed because adding −1 times the row to itself has the effect of multiplying the row by 0. And we disallow swapping a row with itself, to make some results in the fourth chapter easier. Besides, it’s pointless.
Proof We will cover the equation swap operation here. The other two cases are similar and are Exercise 33.
Consider a linear system.
a1,1x1 + a1,2x2 +···+ a1,nxn = d1 .
ai,1x1 + ai,2x2 +···+ ai,nxn = di .
aj,1x1 + aj,2x2 +···+ aj,nxn = dj .
am,1x1 +am,2x2 +···+am,nxn = dm
The tuple (s1 , . . . , sn ) satisfies this system if and only if substituting the values for the variables, the s’s for the x’s, gives a conjunction of true statements: a1,1s1+a1,2s2+···+a1,nsn =d1 and... ai,1s1+ai,2s2+···+ai,nsn =di and ... aj,1s1 +aj,2s2 +···+aj,nsn = dj and ... am,1s1 +am,2s2 +···+am,nsn = dm.
In a list of statements joined with ‘and’ we can rearrange the order of the statements. Thus this requirement is met if and only if a1,1 s1 + a1,2 s2 + · · · + a1,nsn = d1 and ... aj,1s1 +aj,2s2 +···+aj,nsn = dj and ... ai,1s1 +ai,2s2 + ···+ai,nsn = di and ... am,1s1 +am,2s2 +···+am,nsn = dm. This is exactly the requirement that (s1 , . . . , sn ) solves the system after the row swap. QED
Section I. Solving Linear Systems 5
  1.6 Definition The three operations from Theorem 1.5 are the elementary re- duction operations, or row operations, or Gaussian operations. They are swapping, multiplying by a scalar (or rescaling), and row combination.
When writing out the calculations, we will abbreviate ‘row i’ by ‘ρi’. For instance, we will denote a row combination operation by kρi + ρj, with the row that changes written second. To save writing we will often combine addition steps when they use the same ρi, as in the next example.
1.7 Example Gauss’s Method systematically applies the row operations to solve a system. Here is a typical case.
x+y=0 2x− y+3z=3 x−2y− z=3
We begin by using the first row to eliminate the 2x in the second row and the x in the third. To get rid of the 2x we mentally multiply the entire first row by −2, add that to the second row, and write the result in as the new second row. To eliminate the x in the third row we multiply the first row by −1, add that to the third row, and write the result in as the new third row.
−3y− z=3
We finish by transforming the second system into a third, where the bottom equation involves only one unknown. We do that by using the second row to eliminate the y term from the third row.
x+y=0 −→ −3y+3z=3
−2ρ1 +ρ2 −ρ1 +ρ3
x+y=0 −→ −3y+ 3z=3 −4z=0
Now finding the system’s solution is easy. The third row gives z = 0. Substitute that back into the second row to get y = −1. Then substitute back into the first row to get x = 1.
1.8 Example For the Physics problem from the start of this chapter, Gauss’s Method gives this.
40h + 15c = 100 5/4ρ1+ρ2 40h + 15c = 100 −50h + 25c = 50 −→ (175/4)c = 175
So c = 4, and back-substitution gives that h = 1. (We will solve the Chemistry problem later.)
−ρ2 +ρ3
6
1.9 Example The reduction
x+y+z=9
2x+4y−3z=1 −→ 2y−5z=−17 3x+6y−5z=0 −3ρ1+ρ3 3y−8z=−27
−(3/2)ρ2 +ρ3
Chapter One. Linear Systems
 −2ρ1 +ρ2
x+y+z= 9
x+y+ z= 9 −→ 2y−5z=−17 −(1/2)z = −(3/2)
shows that z = 3, y = −1, and x = 7.
As illustrated above, the point of Gauss’s Method is to use the elementary
reduction operations to set up back-substitution.
1.11 Example The prior three examples only used the operation of row combina- tion. This linear system requires the swap operation to get it into echelon form because after the first combination
x−y =0 x−y =0 2x−2y+ z+2w=4 −2ρ1+ρ2 z+2w=4 y+w=0−→ y+w=0 2z+ w=5 2z+ w=5
the second equation has no leading y. We exchange it for a lower-down row that has a leading y.
 1.10Definition Ineachrowofasystem,thefirstvariablewithanonzerocoefficient is the row’s leading variable. A system is in echelon form if each leading variable is to the right of the leading variable in the row above it, except for the leading variable in the first row, and any all-zero rows are at the bottom.
ρ2↔ρ3 −→
x−y =0 y + w=0 z+2w=4 2z+ w=5
(Had there been more than one suitable row below the second then we could have used any one.) With that, Gauss’s Method proceeds as before.
x−y =0 −2ρ3+ρ4 y + w= 0
−→
z+ 2w= 4 −3w=−3
Back-substitution gives w = 1, z = 2 , y = −1, and x = −1.
Section I. Solving Linear Systems 7
 Strictly speaking, to solve linear systems we don’t need the row rescaling operation. We have introduced it here because it is convenient and because we will use it later in this chapter as part of a variation of Gauss’s Method, the Gauss-Jordan Method.
All of the systems so far have the same number of equations as unknowns. All of them have a solution and for all of them there is only one solution. We finish this subsection by seeing other things that can happen.
1.12 Example This system has more equations than variables.
x+3y= 1 2x+ y=−3 2x+2y=−2
Gauss’s Method helps us understand this system also, since this
−4y=−4
shows that one of the equations is redundant. Echelon form
x+ 3y= 1 −→ −5y=−5
−2ρ1 +ρ2 −2ρ1 +ρ3
x+ 3y= 1 −→ −5y=−5 0=0
gives that y = 1 and x = −2. The ‘0 = 0’ reflects the redundancy.
Gauss’s Method is also useful on systems with more variables than equations. The next subsection has many examples.
Another way that linear systems can differ from the examples shown above is that some linear systems do not have a unique solution. This can happen in two ways. The first is that a system can fail to have any solution at all.
1.13 Example Contrast the system in the last example with this one.
−(4/5)ρ2 +ρ3
x+3y= 1
2x+ y=−3 −→ 2x+2y= 0 −2ρ1+ρ3
x+ 3y= 1 −5y=−5 −4y=−2
−2ρ1 +ρ2
Here the system is inconsistent: no pair of numbers (s1,s2) satisfies all three equations simultaneously. Echelon form makes the inconsistency obvious.
The solution set is empty.
x+ 3y= 1 −→ −5y=−5 0=2
−(4/5)ρ2 +ρ3
8 Chapter One. Linear Systems
 1.14 Example The prior system has more equations than unknowns but that is not what causes the inconsistency — Example 1.12 has more equations than unknowns and yet is consistent. Nor is having more equations than unknowns necessary for inconsistency, as we see with this inconsistent system that has the same number of equations as unknowns.
x+2y=8 −2ρ1+ρ2 x+2y= 8 2x+4y=8 −→ 0=−8
Instead, inconsistency has to do with the interaction of the left and right sides; in the first system above the left side’s second equation is twice the first but the right side’s second constant is not twice the first. Later we will have more to say about dependencies between a system’s parts.
The other way that a linear system can fail to have a unique solution, besides having no solutions, is to have many solutions.
1.15 Example In this system
x+ y=4 2x+2y=8
any pair of numbers satisfying the first equation also satisfies the second. The solution set { (x, y) | x + y = 4 } is infinite; some example member pairs are (0, 4), (−1, 5), and (2.5, 1.5).
The result of applying Gauss’s Method here contrasts with the prior example because we do not get a contradictory equation.
−2ρ1+ρ2 x+y=4
0=0
Don’t be fooled by that example: a 0 = 0 equation is not the signal that a
system has many solutions.
1.16 Example The absence of a 0 = 0 equation does not keep a system from having many different solutions. This system is in echelon form, has no 0 = 0, but has infinitely many solutions, including (0, 1, −1), (0, 1/2, −1/2), (0, 0, 0), and (0, −π, π) (any triple whose first component is 0 and whose second component is the negative of the third is a solution).
x+y+z=0 y+z=0
Nor does the presence of 0 = 0 mean that the system must have many solutions. Example 1.12 shows that. So does this system, which does not have
−→
Section I. Solving Linear Systems 9 any solutions at all despite that in echelon form it has a 0 = 0 row.
2x −2z=6 2x −2z=6 y+ z=1 −ρ1+ρ3 y+ z=1 2x+ y− z=7 −→ y+ z=1 3y+3z=0 3y+3z=0
2x −2z= 6
 −ρ2+ρ3 y+ z= 1 −→
−3ρ2+ρ4 0= 0 0=−3
In summary, Gauss’s Method uses the row operations to set a system up for back substitution. If any step shows a contradictory equation then we can stop with the conclusion that the system has no solutions. If we reach echelon form without a contradictory equation, and each variable is a leading variable in its row, then the system has a unique solution and we find it by back substitution. Finally, if we reach echelon form without a contradictory equation, and there is not a unique solution — that is, at least one variable is not a leading variable — then the system has many solutions.
The next subsection explores the third case. We will see that such a system must have infinitely many solutions and we will describe the solution set.
Note. In the exercises here, and in the rest of the book, you must justify all of your answers. For instance, if a question asks whether a system has a solution then you must justify a yes response by producing the solution and must justify a no response by showing that no solution exists.
Exercises
  1.17 Use Gauss’s Method to find the unique solution for each system. (a) 2x+3y= 13 (b) x −z=0
x− y=−1 3x+y =1 −x+y+z=4
1.18 Each system is in echelon form. For each, say whether the system has a unique solution, no solution, or infinitely many solutions.
(a) −3x+ 2y=0 (b) x+y =4
(c) x+y =4 y −z=0 0=0
(g) 2x+2y=4 y=1 0=4
(d) x+y=4 0=4
(h) 2x+y=0
−2y=0
(e) 3x+6y+ z=−0.5
−z= 2.5
y−z=0 (f) x−3y=2
0=0
(i) x−y=−1 (j) x+y−3z=−1 0=0 y−z=2 0=4 z=0 0=0
10 Chapter One. Linear Systems
   1.19 Use Gauss’s Method to solve each system or conclude ‘many solutions’ or ‘no solutions’.
(a) 2x+2y=5 (b) −x+y=1 (c) x−3y+ z= 1 x−4y=0 x+y=2 x+ y+2z=14
(e) 4y+z=20(f)2x+z+w=5 2x−2y+z= 0 y −w=−1 x +z=5 3x −z−w=0 x+ y−z=10 4x+y+2z+w= 9
(d) −x− y=1 −3x−3y=2
1.20 Solve each system or conclude ‘many solutions’ or ‘no solutions’. Use Gauss’s Method.
(a) x+y+ z=5 (b) 3x + z= 7 (c) x+3y+ z=0 x−y =0 x− y+3z= 4 −x− y =2 y+2z=7 x+2y−5z=−1 −x+ y+2z=8
  1.21 We can solve linear systems by methods other than Gauss’s. One often taught in high school is to solve one of the equations for a variable, then substitute the resulting expression into other equations. Then we repeat that step until there is an equation with only one variable. From that we get the first number in the solution and then we get the rest with back-substitution. This method takes longer than Gauss’s Method, since it involves more arithmetic operations, and is also more likely to lead to errors. To illustrate how it can lead to wrong conclusions, we will use the system
x+3y= 1 2x+ y=−3 2x+2y= 0
from Example 1.13.
(a) Solve the first equation for x and substitute that expression into the second
equation. Find the resulting y.
(b) Again solve the first equation for x, but this time substitute that expression
into the third equation. Find this y.
What extra step must a user of this method take to avoid erroneously concluding a system has a solution?
  1.22 For which values of k are there no solutions, many solutions, or a unique solution to this system?
x− y=1 3x−3y=k
1.23 This system is not linear in that it says sin α instead of α
2sinα − cosβ + 3tanγ = 3 4sinα + 2cosβ − 2tanγ = 10 6sinα − 3cosβ + tanγ = 9
and yet we can apply Gauss’s Method. Do so. Does the system have a solution?
  1.24 What conditions must the constants, the b’s, satisfy so that each of these systems has a solution? Hint. Apply Gauss’s Method and see what happens to the right side.
Section I. Solving Linear Systems 11
 (a) x−3y=b1 3x+ y=b2 x+7y=b3 2x+4y=b4
(b)
x1 +2x2 +3x3 =b1 2x1 +5x2 +3x3 =b2 x1 +8x3 =b3
1.25 True or false: a system with more unknowns than equations has at least one solution. (As always, to say ‘true’ you must prove it, while to say ‘false’ you must produce a counterexample.)
1.26 Must any Chemistry problem like the one that starts this subsection — a balance the reaction problem — have infinitely many solutions?
  1.27 Find the coefficients a, b, and c so that the graph of f(x) = ax2 + bx + c passes through the points (1, 2), (−1, 6), and (2, 3).
1.28 After Theorem 1.5 we note that multiplying a row by 0 is not allowed because that could change a solution set. Give an example of a system with solution set S0 where after multiplying a row by 0 the new system has a solution set S1 and S0 is a proper subset of S1, that is, S0 ̸= S1. Give an example where S0 = S1.
1.29 Gauss’s Method works by combining the equations in a system to make new equations.
(a) Can we derive the equation 3x − 2y = 5 by a sequence of Gaussian reduction steps from the equations in this system?
x+y=1 4x−y=6
(b) Can we derive the equation 5x − 3y = 2 with a sequence of Gaussian reduction steps from the equations in this system?
2x+2y=5
3x+ y=4
(c) Can we derive 6x − 9y + 5z = −2 by a sequence of Gaussian reduction steps
from the equations in the system?
2x+ y−z=4
6x−3y+z=5
1.30 Prove that, where a, b, c, d, e are real numbers with a ̸= 0, if this linear equation
ax + by = c has the same solution set as this one
ax+dy = e then they are the same equation. What if a = 0?
1.31 Showthatifad−bc̸=0then
has a unique solution.   1.32 In the system
ax+by= j cx+dy=k
ax+by=c
dx+ ey= f
each of the equations describes a line in the xy-plane. By geometrical reasoning, show that there are three possibilities: there is a unique solution, there is no
solution, and there are infinitely many solutions.
12 Chapter One. Linear Systems
 1.33 Finish the proof of Theorem 1.5.
1.34 Is there a two-unknowns linear system whose solution set is all of R2?
  1.35 Are any of the operations used in Gauss’s Method redundant? That is, can we
make any of the operations from a combination of the others?
1.36 Prove that each operation of Gauss’s Method is reversible. That is, show that if two systems are related by a row operation S1 → S2 then there is a row operation to go back S2 → S1.
? 1.37 [Anton] A box holding pennies, nickels and dimes contains thirteen coins with a total value of 83 cents. How many coins of each type are in the box? (These are US coins; a penny is 1 cent, a nickel is 5 cents, and a dime is 10 cents.)
? 1.38 [Con. Prob. 1955] Four positive integers are given. Select any three of the integers, find their arithmetic average, and add this result to the fourth integer. Thus the numbers 29, 23, 21, and 17 are obtained. One of the original integers is:
(a) 19 (b) 21 (c) 23 (d) 29 (e) 17
? 1.39 [Am. Math. Mon., Jan. 1935] Laugh at this: AHAHA + TEHE = TEHAW. It resulted from substituting a code letter for each digit of a simple example in addition, and it is required to identify the letters and prove the solution unique.
? 1.40 [Wohascum no. 2] The Wohascum County Board of Commissioners, which has 20 members, recently had to elect a President. There were three candidates (A, B, and C); on each ballot the three candidates were to be listed in order of preference, with no abstentions. It was found that 11 members, a majority, preferred A over B (thus the other 9 preferred B over A). Similarly, it was found that 12 members preferred C over A. Given these results, it was suggested that B should withdraw, to enable a runoff election between A and C. However, B protested, and it was then found that 14 members preferred B over C! The Board has not yet recovered from the resulting confusion. Given that every possible order of A, B, C appeared on at least one ballot, how many members voted for B as their first choice?
? 1.41 [Am. Math. Mon., Jan. 1963] “This system of n linear equations with n un- knowns,” said the Great Mathematician, “has a curious property.”
“Good heavens!” said the Poor Nut, “What is it?”
“Note,” said the Great Mathematician, “that the constants are in arithmetic progression.”
“It’s all so clear when you explain it!” said the Poor Nut. “Do you mean like 6x + 9y = 12 and 15x + 18y = 21?”
“Quite so,” said the Great Mathematician, pulling out his bassoon. “Indeed, the system has a unique solution. Can you find it?”
“Good heavens!” cried the Poor Nut, “I am baffled.” Are you?
Section I. Solving Linear Systems 13
 I.2 Describing the Solution Set
A linear system with a unique solution has a solution set with one element. A linear system with no solution has a solution set that is empty. In these cases the solution set is easy to describe. Solution sets are a challenge to describe only when they contain many elements.
2.1 Example This system has many solutions because in echelon form
2x +z=3
x−y−z=1 −→ −y−(3/2)z=−1/2
2x + z= 3 3x − y = 4 −(3/2)ρ1+ρ3 −y − (3/2)z = −1/2
−(1/2)ρ1 +ρ2
2x + z= 3 −→ −y − (3/2)z = −1/2 0=0
not all of the variables are leading variables. Theorem 1.5 shows that an (x, y, z) satisfies the first system if and only if it satisfies the third. So we can describe the solution set { (x, y, z) | 2x + z = 3 and x − y − z = 1 and 3x − y = 4 } in this way.
{ (x, y, z) | 2x + z = 3 and −y − 3z/2 = −1/2 } (∗)
This description is better because it has two equations instead of three but it is not optimal because it still has some hard to understand interactions among the variables.
To improve it, use the variable that does not lead any equation, z, to describe the variables that do lead, x and y. The second equation gives y = (1/2)−(3/2)z and the first equation gives x = (3/2)−(1/2)z. Thus we can describe the solution set as this set of triples.
{((3/2) − (1/2)z, (1/2) − (3/2)z, z) | z ∈ R} (∗∗)
Compared with (∗), the advantage of (∗∗) is that z can be any real number. This makes the job of deciding which tuples are in the solution set much easier. For instance, taking z = 2 shows that (1/2, −5/2, 2) is a solution.
2.3 Example Reduction of a linear system can end with more than one variable
−ρ2 +ρ3
 2.2 Definition In an echelon form linear system the variables that are not leading are free.
14
free. Gauss’s Method on this system
x+ y+z−w= 1
y− z+ w=−1 −3ρ1+ρ3
Chapter One. Linear Systems
 x+ y+z−w= 1 y− z+ w=−1 3x +6z−6w= 6 −→ −3y+3z−3w= 3
−y+z−w= 1
leaves x and y leading and both z and w free. To get the description that we prefer, we work from the bottom. We first express the leading variable y in terms of z and w, as y = −1 + z − w. Moving up to the top equation, substituting for y gives x + (−1 + z − w) + z − w = 1 and solving for x leaves x = 2 − 2z + 2w. The solution set
{ (2 − 2z + 2w, −1 + z − w, z, w) | z, w ∈ R } (∗∗) has the leading variables expressed in terms of the variables that are free.
2.4 Example The list of leading variables may skip over some columns. After this reduction
2x−2y =0 2x−2y =0
z+3w=2 0=0 2z+6w=4
2x−2y =0 −2ρ2+ρ4 z+3w=2
z+3w=2 −(3/2)ρ1+ρ3 −→
3x−3y =0 −(1/2)ρ1+ρ4 x− y+2z+6w=4
−y+z−w= 1 x+y+z−w= 1
3ρ2+ρ3 y−z+w=−1 −→
ρ2+ρ4 0= 0 0=0
−→
0=0 0=0
x and z are the leading variables, not x and y. The free variables are y and w and so we can describe the solution set as {(y,y,2−3w,w)|y,w∈R}. For instance, (1, 1, 2, 0) satisfies the system — take y = 1 and w = 0. The four-tuple (1, 0, 5, 4) is not a solution since its first coordinate does not equal its second.
A variable that we use to describe a family of solutions is a parameter. We say that the solution set in the prior example is parametrized with y and w.
The terms ‘parameter’ and ‘free variable’ do not mean the same thing. In the prior example y and w are free because in the echelon form system they do not lead. They are parameters because we used them to describe the set of solutions. Had we instead rewritten the second equation as w = 2/3 − (1/3)z then the free variables would still be y and w but the parameters would be y and z.
Section I. Solving Linear Systems 15 In the rest of this book we will solve linear systems by bringing them to
echelon form and then parametrizing with the free variables.
2.5 Example This is another system with infinitely many solutions.
 x+2y =1
2x +z =2 −→ −4y+z =0 3x+2y+z−w=4 −3ρ1+ρ3 −4y+z−w=1
−2ρ1 +ρ2
x+ 2y =1
x+ 2y =1 −→ −4y+z =0 −w=1
The leading variables are x, y, and w. The variable z is free. Notice that, although there are infinitely many solutions, the value of w doesn’t vary but is constant w = −1. To parametrize, write w in terms of z with w = −1 + 0z. Then y = (1/4)z. Substitute for y in the first equation to get x = 1 − (1/2)z. The solution set is {(1 − (1/2)z, (1/4)z, z, −1) | z ∈ R}.
Parametrizing solution sets shows that systems with free variables have infinitely many solutions. For instance, above z takes on all of infinitely many real number values, each associated with a different solution.
We finish this subsection by developing a streamlined notation for linear systems and their solution sets.
We usually denote a matrix with an upper case roman letters. For instance,
 1 2.2 5  A= 3 4 −7
has 2 rows and 3 columns and so is a 2×3 matrix. Read that aloud as “two-by- three”; the number of rows is always stated first. (The matrix has parentheses around it so that when two matrices are adjacent we can tell where one ends and the other begins.) We name matrix entries with the corresponding lower-case letter, so that the entry in the second row and first column of the above array is a2,1 = 3. Note that the order of the subscripts matters: a1,2 ̸= a2,1 since a1,2 = 2.2. We denote the set of all n×m matrices by Mn×m.
We do Gauss’s Method using matrices in essentially the same way that we did it for systems of equations: a matrix row’s leading entry is its first nonzero entry (if it has one) and we perform row operations to arrive at matrix echelon form, where the leading entry in lower rows are to the right of those in the rows
−ρ2 +ρ3
 2.6 Definition An m×n matrix is a rectangular array of numbers with m rows and n columns. Each number in the matrix is an entry.
16 Chapter One. Linear Systems above. We like matrix notation because it lightens the clerical load, the copying
of variables and the writing of +’s and =’s.
2.7 Example We can abbreviate this linear system
 with this matrix.
x+2y =4 y− z=0 x +2z=4
1 2 0 4 0 1 −1 0 1024
   The vertical bar reminds a reader of the difference between the coefficients on the system’s left hand side and the constants on the right. With a bar, this is an augmented matrix.
1 2 
1 2 0 4
  −ρ1+ρ3
0 4
 2ρ2+ρ3
1 2 0 4  
      −1 0 −→
The second row stands for y − z = 0 and the first row stands for x + 2y = 4 so
the solution set is { (4 − 2z, z, z) | z ∈ R }.
Matrix notation also clarifies the descriptions of solution sets. Example 2.3’s {(2−2z+2w,−1+z−w,z,w)|z,w∈R} is hard to read. We will rewrite it to group all of the constants together, all of the coefficients of z together, and all of the coefficients of w together. We write them vertically, in one-column matrices.
2 −2 2
0 1 −1 0 −→ 1 0 2 4
0 1
0 −2 2 0
0 1 −1 0 0 0 0 0
   −1  1  −1
{ + ·z+ ·w|z,w∈R}
0 1 0 001
For instance, the top line says that x = 2 − 2z + 2w and the second line says that y = −1 + z − w. (Our next section gives a geometric interpretation that will help us picture the solution sets.)
Vectors are an exception to the convention of representing matrices with capital roman letters. We use lower-case roman or greek letters overlined with an
 2.8 Definition A column vector, often just called a vector, is a matrix with a single column. A matrix with a single row is a row vector. The entries of a vector are sometimes called components. A column or row vector whose components are all zeros is a zero vector.
Section I. Solving Linear Systems 17 arrow: a⃗, ⃗b, ... or α⃗, β⃗, ... (boldface is also common: a or α). For instance,
this is a column vector with a third component of 7. 1
⃗ v =  3  7
A zero vector is denoted ⃗0. There are many different zero vectors — the one-tall zero vector, the two-tall zero vector, etc. — but nonetheless we will often say “the” zero vector, expecting that the size will be clear from the context.
  2.9 Definition The linear equation a1x1 + a2x2 + · · · + anxn = d with unknowns x1,... ,xn is satisfied by
s1 .
⃗s =  . .  sn
if a1s1 + a2s2 + · · · + ansn = d. A vector satisfies a linear system if it satisfies each equation in the system.
The style of description of solution sets that we use involves adding the vectors, and also multiplying them by real numbers. Before we give the examples showing the style we first need to define these operations.
Note that for the addition to be defined the vectors must have the same number of entries. This entry-by-entry addition works for any pair of matrices, not just vectors, provided that they have the same number of rows and columns.
 2.10 Definition The vector sum of ⃗u and ⃗v is the vector of the sums. u1 v1 u1+v1
.. .  ⃗u + ⃗v =  . .  +  . .  =  . . 
un vn un + vn
 2.11 Definition The scalar multiplication of the real number r and the vector ⃗v is the vector of the multiples.
v1 rv1 . .
r·⃗v=r· . = .  vn rvn
As with the addition operation, the entry-by-entry scalar multiplication operation extends beyond vectors to apply to any matrix.
18 Chapter One. Linear Systems
 We write scalar multiplication either as r · ⃗v or ⃗v · r, and sometimes even omit the ‘·’ symbol: r⃗v. (Do not refer to scalar multiplication as ‘scalar product’ because that name is for a different operation.)
2.12 Example
2  3  2+3 5      
1 7 4 28
3 + −1 = 3 − 1 = 2 141+45
7 ·   =   −1  −7 
−3 −21
Observe that the definitions of addition and scalar multiplication agree where they overlap; for instance, ⃗v + ⃗v = 2⃗v.
With these definitions, we are set to use matrix and vector notation to both solve systems and express the solution.
2.13 Example This system
reduces in this way.
2x+y −w =4 y + w+u=4 x −z+2w =0
2 1 0 −1 0 4 2 1 0 −1 0 4   −(1/2)ρ1+ρ3   0 1 0 1 1 4 −→ 0 1 0 1 1 4
10−1 2 00 0−1/2−15/20−2
2 1 0 −1 0 4 (1/2)ρ2 +ρ3   −→ 0 1 0 1 1 4
00−1 3 1/20
Thesolutionsetis{(w+(1/2)u,4−w−u,3w+(1/2)u,w,u)|w,u∈R}. We write that in vector form.
x 0  1  1/2 y 4 −1 −1
 {z=0+ 3 w+1/2u|w,u∈R}
w 0  1   0  u001
Note how well vector notation sets off the coefficients of each parameter. For instance, the third row of the vector form shows plainly that if u is fixed then z increases three times as fast as w. Another thing shown plainly is that setting
         
Section I. Solving Linear Systems
19
 both w and u to zero gives that
x 0
y 4 
 z  = 0 w 0 u0
is a particular solution of the linear system. 2.14 Example In the same way, the system
reduces
x− y+ z=1 3x +z=3 5x−2y+3z=5
1 −1 1 1 1 −1
  −3ρ1+ρ2 
1   1 
  3 0 1 3 5 −2 3 5
−→ 0 −5ρ1+ρ3 0
−→ 0 3 −2 0 0000
3 −2 0 3 −2 0
  1 −1 1   1 −ρ2+ρ3  
  to give a one-parameter solution set.
1 −1/3 {0+ 2/3 z|z∈R}
01
As in the prior example, the vector not associated with the parameter
1 0
0
Before the exercises, we will consider what we have accomplished and what we will do in the remainder of the chapter. So far we have done the mechanics of Gauss’s Method. We have not stopped to consider any of the questions that arise, except for proving Theorem 1.5 — which justifies the method by showing that it gives the right answers.
For example, can we always describe solution sets as above, with a particular solution vector added to an unrestricted linear combination of some other vectors?
is a particular solution of the system.
20 Chapter One. Linear Systems
 We’ve noted that the solution sets described in this way have infinitely many members so answering this question would tell us about the size of solution sets. The following subsection shows that the answer is “yes.” This chapter’s second section then uses that answer to describe the geometry of solution sets.
Other questions arise from the observation that we can do Gauss’s Method in more than one way (for instance, when swapping rows we may have a choice of rows to swap with). Theorem 1.5 says that we must get the same solution set no matter how we proceed but if we do Gauss’s Method in two ways must we get the same number of free variables in each echelon form system? Must those be the same variables, that is, is it impossible to solve a problem one way to get y and w free and solve it another way to get y and z free? The third section of this chapter answers “yes,” that from any starting linear system, all derived echelon form versions have the same free variables.
Thus, by the end of the chapter we will not only have a solid grounding in the practice of Gauss’s Method but we will also have a solid grounding in the theory. We will know exactly what can and cannot happen in a reduction.
Exercises
  2.15 Find the indicated entry of the matrix, if it is defined.  1 3 1 
A= 2 −1 4 (a) a2,1 (b) a1,2 (c) a2,2 (d) a3,1
  2.16 Give the size of each matrix.
 1 0 4  1 1  5 10 
(a) 2 1 5 (b)−1 1 (c) 10 5 3 −1
  2.17 Do the indicated vector operation, if it is defined. 2 3  4  1 3
(a) 1+0 (b) 5 −1 (c) 5−1 14 11
 1  1 3 2 1
(e) 2 +2 (f) 61−40+21 3135
 2   3  (d) 7 1 +9 5
  2.18 Solve each system using matrix notation. Express the solution using vec- tors.
(a)3x+6y=18 (b)x+y= 1 (c) x1 + x3= 4 x+2y= 6 x−y=−1 x1 −x2 +2x3 = 5 4x1 −x2 +5x3 =17
(d) 2a+b−c=2 (e) x+2y−z =3 (f) x +z+w=4 2a +c=3 2x+ y +w=4 2x+y −w=2 a−b =0 x− y+z+w=1 3x+y+z =7
2.19 Solve each system using matrix notation. Give each solution set in vector notation.
Section I. Solving Linear Systems
21
 (a)
(d)
2x+y−z=1 (b) x − z =1 4x−y =3 y+2z−w=3 x+2y+3z−w=7
a+2b+3c+d−e=1 3a− b+ c+d+e=3
(c) x− y+ z =0 y +w=0 3x− 2y+3z+w=0 −y −w=0
2.20 Solve each system using matrix notation. Express the solution set using vectors.
(a)
3x+2y+z=1 x+ y−2z= 0
x− y+z=2 (b) x− y =−3 (c) 2x−y−z+w= 4
5x+5y+z=0 3x− y−2z=−6 x+y+z =−1 2y−2z= 3
x+y−2z= 0 (d) x−y =−3 3x−y−2z= 0
  2.21 The vector is in the set. What value of the parameters produces that vec- tor?
 5   1 
(a) −5 ,{ −1 k|k∈R}
−1 −2 3
(b)  2 , {  1  i + 0 j | i, j ∈ R }
101 0 1 2
(c) −4, { 1 m + 0 n | m, n ∈ R } 201
2.22 Decide if the vector is in the set.
(a) (b)
(c)
(d)
 3   −6 
−1 ,{ 2 k|k∈R}
 5   5 
4 ,{ −4 j|j∈R}
2 0 1
 1 , {  3  + −1 r | r ∈ R }
−1 −7 3 1 2 −3
0, { 0 j + −1 k | j, k ∈ R } 111
2.23 [Cleary] A farmer with 1200 acres is considering planting three different crops, corn, soybeans, and oats. The farmer wants to use all 1200 acres. Seed corn costs $20 per acre, while soybean and oat seed cost $50 and $12 per acre respectively. The farmer has $40 000 available to buy seed and intends to spend it all.
(a) Use the information above to formulate two linear equations with three unknowns and solve it.
(b) Solutions to the system are choices that the farmer can make. Write down two reasonable solutions.
(c) Suppose that in the fall when the crops mature, the farmer can bring in
22 Chapter One. Linear Systems
 revenue of $100 per acre for corn, $300 per acre for soybeans and $80 per acre for oats. Which of your two solutions in the prior part would have resulted in a larger revenue?
2.24 Parametrize the solution set of this one-equation system. x1 + x2 + · · · + xn = 0
  2.25 (a) Apply Gauss’s Method to the left-hand side to solve x+2y − w=a
2x +z =b x+ y +2w=c
for x, y, z, and w, in terms of the constants a, b, and c. (b) Use your answer from the prior part to solve this.
x+2y −w=3 2x +z =1 x+ y +2w=−2
2.26 Why is the comma needed in the notation ‘ai,j’ for matrix entries?   2.27 Give the 4×4 matrix whose i,j-th entry is
(a) i+j; (b) −1 to the i+j power.
2.28 For any matrix A, the transpose of A, written AT, is the matrix whose columns
are the rows of A. Find the transpose of each of these.  123   2−3   5 10  1
(a) 4 5 6 (b) 1 1 (c) 10 5 (d)1 0
  2.29 (a) Describe all functions f(x) = ax2 + bx + c such that f(1) = 2 and f(−1) = 6. (b) Describe all functions f(x) = ax2 + bx + c such that f(1) = 2.
2.30 Show that any set of five points from the plane R2 lie on a common conic section, that is, they all satisfy some equation of the form ax2 +by2 +cxy+dx+ey+f = 0 where some of a, ... ,f are nonzero.
2.31 Make up a four equations/four unknowns system having (a) a one-parameter solution set;
(b) a two-parameter solution set;
(c) a three-parameter solution set.
? 2.32 [Shepelev] This puzzle is from a Russian web-site http://www.arbuz.uz/ and there are many solutions to it, but mine uses linear algebra and is very naive. There’s a planet inhabited by arbuzoids (watermeloners, to translate from Russian). Those creatures are found in three colors: red, green and blue. There are 13 red arbuzoids, 15 blue ones, and 17 green. When two differently colored arbuzoids meet, they both change to the third color.
The question is, can it ever happen that all of them assume the same color?
? 2.33 [USSR Olympiad no. 174]
(a) Solve the system of equations.
ax+ y=a2
x+ay= 1
For what values of a does the system fail to have solutions, and for what values
of a are there infinitely many solutions?
Section I. Solving Linear Systems 23 (b) Answer the above question for the system.
ax+ y=a3 x+ay= 1
? 2.34 [Math. Mag., Sept. 1952] In air a gold-surfaced sphere weighs 7588 grams. It is known that it may contain one or more of the metals aluminum, copper, silver, or lead. When weighed successively under standard conditions in water, benzene, alcohol, and glycerin its respective weights are 6588, 6688, 6778, and 6328 grams. How much, if any, of the forenamed metals does it contain if the specific gravities of the designated substances are taken to be as follows?
 Aluminum 2.7 Copper 8.9 Gold 19.3 Lead 11.3 Silver 10.8
Alcohol 0.81 Benzene 0.90 Glycerin 1.26 Water 1.00
I.3 General = Particular + Homogeneous
In the prior subsection the descriptions of solution sets all fit a pattern. They have a vector that is a particular solution of the system added to an unre- stricted combination of some other vectors. The solution set from Example 2.13 illustrates.
0 1 1/2 4 −1 −1

{ 0 +w 3 +u1/2|w,u∈R}
 0   1   0  001
particular unrestricted
  solution
combination
The combination is unrestricted in that w and u can be any real numbers— there is no condition like “such that 2w − u = 0” to restrict which pairs w, u we can use.
That example shows an infinite solution set fitting the pattern. The other two kinds of solution sets also fit. A one-element solution set fits because it has a particular solution and the unrestricted combination part is trivial. That is, instead of being a combination of two vectors or of one vector, it is a combination of no vectors. (By convention the sum of an empty set of vectors is the zero vector.) An empty solution set fits the pattern because there is no particular solution and thus there are no sums of that form.
24 Chapter One. Linear Systems
  3.1 Theorem Any linear system’s solution set has the form
{ ⃗p + c 1 β⃗ 1 + · · · + c k β⃗ k | c 1 , . . . , c k ∈ R }
where ⃗p is any particular solution and where the number of vectors β⃗1, ..., β⃗k equals the number of free variables that the system has after a Gaussian reduction.
The solution description has two parts, the particular solution ⃗p and the unrestricted linear combination of the β⃗ ’s. We shall prove the theorem with two corresponding lemmas.
We will focus first on the unrestricted combination. For that we consider systems that have the vector of zeroes as a particular solution so that we can shorten ⃗p+c1β⃗1 +···+ckβ⃗k to c1β⃗1 +···+ckβ⃗k.
3.3 Example With any linear system like
3x + 4y = 3
2x− y=1
we associate a system of homogeneous equations by setting the right side to
 3.2 Definition A linear equation is homogeneous if it has a constant of zero, so thatitcanbewrittenasa1x1+a2x2+···+anxn =0.
zeros.
Compare the reduction of the original system
3x+4y=3 −(2/3)ρ1+ρ2 3x+ 2x− y=1 −→
3x + 4y = 0 2x− y=0
with the reduction of the associated homogeneous system.
3x + 4y = 0 −(2/3)ρ1+ρ2 3x + 4y = 0 2x− y=0 −→ −(11/3)y=0
Obviously the two reductions go in the same way. We can study how to reduce a linear systems by instead studying how to reduce the associated homogeneous system.
Studying the associated homogeneous system has a great advantage over studying the original system. Nonhomogeneous systems can be inconsistent. But a homogeneous system must be consistent since there is always at least one solution, the zero vector.
4y =3 −(11/3)y=−1
Section I. Solving Linear Systems 25 3.4 Example Some homogeneous systems have the zero vector as their only
 solution.
3x+2y+z=0 6x+4y =0 −→
3x+2y+ z=0 −2z=0 −→
y+ z=0
3x+2y+ z=0 y+ z=0 −2z=0
The solution set
−2ρ1 +ρ2 y+z=0
ρ2 ↔ρ3
3.5 Example Some homogeneous systems have many solutions. One is the Chemistry problem from the first page of the first subsection.
7x −7z =0 8x+ y−5z−2w=0 y−3z =0 3y−6z− w=0
7x −7z =0 −(8/7)ρ1+ρ2 y+3z−2w=0 −→ y−3z =0 3y−6z− w=0
−ρ2+ρ3 −→ −3ρ2+ρ4
7x−7z =0 y+ 3z−2w=0 −6z + 2w = 0 −15z+5w=0
7x−7z =0
−(5/2)ρ3+ρ4 −→
1/3
y +
3z − 2w = 0 −6z+2w=0 0=0
 1 
{ w | w ∈ R}
1/3 1
has many vectors besides the zero vector (if we take w to be a number of molecules then solutions make sense only when w is a nonnegative multiple of 3).
 3.6Lemma Foranyhomogeneouslinearsystemthereexistvectorsβ⃗1,...,β⃗k such that the solution set of the system is
{ c 1 β⃗ 1 + · · · + c k β⃗ k | c 1 , . . . , c k ∈ R }
where k is the number of free variables in an echelon form version of the system.
We will make two points before the proof. The first is that the basic idea of the proof is straightforward. Consider this system of homogeneous equations in
26 Chapter One. Linear Systems echelon form.
x+y+2z+u+v=0 y+ z+u−v=0 u+v=0
Start with the bottom equation. Express its leading variable in terms of the free variables with u = −v. For the next row up, substitute for the leading variable u of the row below y+z+(−v)−v = 0 and solve for this row’s leading variable y = −z + 2v. Iterate: on the next row up, substitute expressions found in lower rows x+(−z+2v)+2z+(−v)+v = 0 and solve for the leading variable x = −z − 2v. To finish, write the solution in vector notation
x −1 −2 y −1 2

 z  =  1  z +  0  v for z, v ∈ R u  0  −1
v01
and recognize that the β⃗ 1 and β⃗ 2 of the lemma are the vectors associated with the free variables z and v.
The prior paragraph is an example, not a proof. But it does suggest the second point about the proof, its approach. The example moves row-by-row up the system, using the equations from lower rows to do the next row. This points to doing the proof by mathematical induction.∗
Induction is an important and non-obvious proof technique that we shall use a number of times in this book. We will do proofs by induction in two steps, a base step and an inductive step. In the base step we verify that the statement is true for some first instance, here that for the bottom equation we can write the leading variable in terms of free variables. In the inductive step we must establish an implication, that if the statement is true for all prior cases then it follows for the present case also. Here we will establish that if for the bottom-most t rows we can express the leading variables in terms of the free variables, then for the t + 1-th row from the bottom we can also express the leading variable in terms of those that are free.
Those two steps together prove the statement for all the rows because by the base step it is true for the bottom equation, and by the inductive step the fact that it is true for the bottom equation shows that it is true for the next one up. Then another application of the inductive step implies that it is true for the third equation up, etc.
Proof Apply Gauss’s Method to get to echelon form. There may be some 0 = 0 equations; we ignore these (if the system consists only of 0 = 0 equations then
  ∗ More information on mathematical induction is in the appendix.
Section I. Solving Linear Systems 27
 the lemma is trivially true because there are no leading variables). But because the system is homogeneous there are no contradictory equations.
We will use induction to verify that each leading variable can be expressed in terms of free variables. That will finish the proof because we can use the free variables as parameters and the β⃗ ’s are the vectors of coefficients of those free variables.
For the base step consider the bottom-most equation
am,lm xlm + am,lm+1xlm+1 + · · · + am,nxn = 0 (∗)
where am,lm ̸= 0. (The ‘l’ means “leading” so that xlm is the leading variable in row m.) This is the bottom row so any variables after the leading one must be free. Move these to the right hand side and divide by am,lm
xlm =(−am,lm+1/am,lm)xlm+1 +···+(−am,n/am,lm)xn
to express the leading variable in terms of free variables. (There is a tricky technical point here: if in the bottom equation (∗) there are no variables to the right of xlm then xlm = 0. This satisfies the statement we are verifying because, as alluded to at the start of this subsection, it has xlm written as a sum of a number of the free variables, namely as the sum of zero many, under the convention that a trivial sum totals to 0.)
For the inductive step assume that the statement holds for the bottom-most t rows, with 0   t < m − 1. That is, assume that for the m-th equation, and the (m − 1)-th equation, etc., up to and including the (m − t)-th equation, we can express the leading variable in terms of free ones. We must verify that this then also holds for the next equation up, the (m − (t + 1))-th equation. For that, take each variable that leads in a lower equation xlm , . . . , xlm−t and substitute its expression in terms of free variables. We only need expressions for leading variables from lower equations because the system is in echelon form, so the leading variables in equations above this one do not appear in this equation. The result has a leading term of am−(t+1),lm−(t+1) xlm−(t+1) with am−(t+1),lm−(t+1) ̸= 0, and the rest of the left hand side is a linear combination of free variables. Move the free variables to the right side and divide by am−(t+1),lm−(t+1) to end with this equation’s leading variable xlm−(t+1) in terms of free variables.
We have done both the base step and the inductive step so by the principle of mathematical induction the proposition is true. QED
This shows, as discussed between the lemma and its proof, that we can parametrize solution sets using the free variables. We say that the set of vectors { c1 β⃗ 1 + · · · + ck β⃗ k | c1 , . . . , ck ∈ R } is generated by or spanned by the set {β⃗1,...,β⃗k}.
28 Chapter One. Linear Systems To finish the proof of Theorem 3.1 the next lemma considers the particular
solution part of the solution set’s description.
Proof We will show mutual set inclusion, that any solution to the system is in the above set and that anything in the set is a solution of the system.∗
For set inclusion the first way, that if a vector solves the system then it is in the set described above, assume that ⃗s solves the system. Then ⃗s − ⃗p solves the associated homogeneous system since for each equation index i,
ai,1(s1 −p1)+···+ai,n(sn −pn)
=(ai,1s1 +···+ai,nsn)−(ai,1p1 +···+ai,npn)=di −di =0
where pj and sj are the j-th components of ⃗p and ⃗s. Express ⃗s in the required ⃗p + ⃗h form by writing ⃗s − ⃗p as ⃗h.
For set inclusion the other way, take a vector of the form ⃗p + ⃗h, where ⃗p solves the system and ⃗h solves the associated homogeneous system and note that ⃗p + ⃗h solves the given system since for any equation index i,
ai,1(p1 +h1)+···+ai,n(pn +hn)
=(ai,1p1 +···+ai,npn)+(ai,1h1 +···+ai,nhn)=di +0=di
where as earlier pj and hj are the j-th components of ⃗p and ⃗h. QED The two lemmas together establish Theorem 3.1. Remember that theorem
with the slogan, “General = Particular + Homogeneous”. 3.8 Example This system illustrates Theorem 3.1.
  3.7 Lemma For a linear system and for any particular solution ⃗p, the solution set equals {⃗p + ⃗h | ⃗h satisfies the associated homogeneous system}.
Gauss’s Method
−2ρ1 +ρ2 −→
shows that the general solution is a singleton set.
1 { 0 }
0
x+2y− z=1 2x+4y =2 y−3z=0
x+2y− z=1 y−3z=0 y−3z=0 2z=0
x+2y− z=1
2z=0 −→
∗ More information on set equality is in the appendix.
ρ2 ↔ρ3
 
Section I. Solving Linear Systems 29 That single vector is obviously a particular solution. The associated homogeneous
system reduces via the same row operations
x+2y− z=0 x+2y− z=0 −2ρ1 +ρ2 ρ2 ↔ρ3
 to also give a singleton set.
2x+4y =0 −→ −→ y−3z=0 y−3z=0 2z=0
0 { 0 }
0
So, as discussed at the start of this subsection, in this single-solution case the general solution results from taking the particular solution and adding to it the unique solution of the associated homogeneous system.
3.9 Example The start of this subsection also discusses that the case where the general solution set is empty fits the General = Particular + Homogeneous pattern too. This system illustrates.
x+y+3z+2w= 1
It has no solutions because the final two equations conflict. But the associated homogeneous system does have a solution, as do all homogeneous systems.
x +z+w=0
2x−y + w=0 −→
x + z+ w=−1
2x−y + w= 3 −→ −y−2z−w= 5
−2ρ1 +ρ2 −ρ1 +ρ3
x + z+w=−1 y+2z+w= 2
x +z+w=0 −→ −y−2z−w=0 x+y+3z+2w=0 0=0
In fact, the solution set is infinite.
−1 −1
−2ρ1 +ρ2 −ρ1 +ρ3
ρ2 +ρ3
−2 −1
{ z+ w|z,w∈R}
1 0 01
Nonetheless, because the original system has no particular solution, its general solution set is empty — there are no vectors of the form ⃗p + ⃗h because there are no ⃗p ’s.
 3.10 Corollary Solution sets of linear systems are either empty, have one element, or have infinitely many elements.
30 Chapter One. Linear Systems
 Proof We’ve seen examples of all three happening so we need only prove that there are no other possibilities.
First observe a homogeneous system with at least one non-⃗0 solution ⃗v has infinitely many solutions. This is because any scalar multiple of ⃗v also solves the homogeneous system and there are infinitely many vectors in the set of scalar multiplesof⃗v: ifs,t∈Rareunequalthens⃗v̸=t⃗v,sinces⃗v−t⃗v=(s−t)⃗vis non-⃗0 as any non-0 component of ⃗v, when rescaled by the non-0 factor s − t, will give a non-0 value.
Now apply Lemma 3.7 to conclude that a solution set
{⃗p + ⃗h | ⃗h solves the associated homogeneous system}
is either empty (if there is no particular solution ⃗p), or has one element (if there is a ⃗p and the homogeneous system has the unique solution ⃗0), or is infinite (if there is a ⃗p and the homogeneous system has a non-⃗0 solution, and thus by the prior paragraph has infinitely many solutions). QED
This table summarizes the factors affecting the size of a general solution.
number of solutions of the homogeneous system
  yes solution
one
unique solution
no solutions
infinitely many
infinitely many solutions
no solutions
particular exists?
no
 The dimension on the top of the table is the simpler one. When we perform Gauss’s Method on a linear system, ignoring the constants on the right side and so paying attention only to the coefficients on the left-hand side, we either end with every variable leading some row or else we find some variable that does not lead a row, that is, we find some variable that is free. (We formalize “ignoring the constants on the right” by considering the associated homogeneous system.)
A notable special case is systems having the same number of equations as unknowns. Such a system will have a solution, and that solution will be unique, if and only if it reduces to an echelon form system where every variable leads its row (since there are the same number of variables as rows), which will happen if and only if the associated homogeneous system has a unique solution.
 3.11 Definition A square matrix is nonsingular if it is the matrix of coefficients of a homogeneous system with a unique solution. It is singular otherwise, that is, if it is the matrix of coefficients of a homogeneous system with infinitely many solutions.
Section I. Solving Linear Systems 31 3.12 Example The first of these matrices is nonsingular while the second is
singular
 12   12  34 36
because the first of these homogeneous systems has a unique solution while the second has infinitely many solutions.
x+2y=0 x+2y=0 3x+4y=0 3x+6y=0
We have made the distinction in the definition because a system with the same number of equations as variables behaves in one of two ways, depending on whether its matrix of coefficients is nonsingular or singular. Where the matrix of coefficients is nonsingular the system has a unique solution for any constants on the right side: for instance, Gauss’s Method shows that this system
x+2y=a 3x + 4y = b
has the unique solution x = b−2a and y = (3a−b)/2. On the other hand, where the matrix of coefficients is singular the system never has a unique solution — it has either no solutions or else has infinitely many, as with these.
x+2y=1 x+2y=1 3x+6y=2 3x+6y=3
The definition uses the word ‘singular’ because it means “departing from general expectation.” People often, naively, expect that systems with the same number of variables as equations will have a unique solution. Thus, we can think of the word as connoting “troublesome,” or at least “not ideal.” (That ‘singular’ applies to those systems that never have exactly one solution is ironic but it is the standard term.)
3.13 Example The systems from Example 3.3, Example 3.4, and Example 3.8 each have an associated homogeneous system with a unique solution. Thus these matrices are nonsingular.
   
3 4  6 − 4 0 
3 2 1
1 2 −1  2 4 0  0 1 −3
2 −1 0 1 1
The Chemistry problem from Example 3.5 is a homogeneous system with more
32 Chapter One. Linear Systems than one solution so its matrix is singular.
7 0 −7 0 8 1 −5 −2  0 1 − 3 0  0 3 −6 −1
The table above has two dimensions. We have considered the one on top: we can tell into which column a given linear system goes solely by considering the system’s left-hand side; the constants on the right-hand side play no role in this.
The table’s other dimension, determining whether a particular solution exists, is tougher. Consider these two systems with the same left side but different right sides.
3x+2y=5 3x+2y=5 3x+2y=5 3x+2y=4
The first has a solution while the second does not, so here the constants on the right side decide if the system has a solution. We could conjecture that the left side of a linear system determines the number of solutions while the right side determines if solutions exist but that guess is not correct. Compare these two, with the same right sides but different left sides.
3x+2y=5 3x+2y=5 4x+2y=4 3x+2y=4
The first has a solution but the second does not. Thus the constants on the right side of a system don’t alone determine whether a solution exists. Rather, that depends on some interaction between the left and right.
For some intuition about that interaction, consider this system with one of the coefficients left unspecified, as the variable c.
x+2y+3z=1
x+ y+ z=1 cx+3y+4z=0
If c = 2 then this system has no solution because the left-hand side has the third row as the sum of the first two, while the right-hand does not. If c ̸= 2 then this system has a unique solution (try it with c = 1). For a system to have a solution, if one row of the matrix of coefficients on the left is a linear combination of other rows then on the right the constant from that row must be the same combination of constants from the same rows.
More intuition about the interaction comes from studying linear combinations. That will be our focus in the second chapter, after we finish the study of Gauss’s Method itself in the rest of this chapter.
 
Section I. Solving Linear Systems 33
 Exercises
3.14 Solve this system. Then solve the associated homogeneous system. x+ y−2z= 0
x−y =−3 3x− y−2z=−6 2y−2z= 3
  3.15 Solve each system. Express the solution set using vectors. Identify a particular solution and the solution set of the homogeneous system.
(a)3x+6y=18 (b)x+y= 1 (c) x1 + x3= 4 x+2y= 6 x−y=−1 x1 −x2 +2x3 = 5 4x1 −x2 +5x3 =17
(d) 2a+b−c=2 (e) x+2y−z =3 (f) x +z+w=4 2a +c=3 2x+ y +w=4 2x+y −w=2 a−b =0 x− y+z+w=1 3x+y+z =7
3.16 Solve each system, giving the solution set in vector notation. Identify a particular solution and the solution of the homogeneous system.
(a)
(d)
2x+y−z=1 (b) x − z =1 4x−y =3 y+2z−w=3 x+2y+3z−w=7
a+2b+3c+d−e=1 3a− b+ c+d+e=3
(c) x− y+ z =0 y +w=0 3x− 2y+3z+w=0 −y −w=0
  3.17 For the system
(a) (b)
4 −12
  3.20 Singular or nonsingular?
4 12
2x−y −w=3 y+z+2w= 2 x−2y−z =−1
which of these can be used as the particular solution part of some general solu- tion?
0 2 −1 (a) −3 (b) 1 (c) −4
 5   1   8 
0 0 −1
  3.18 Lemma 3.7 says that we can use any particular solution for ⃗p. Find, if possible,
a general solution to this system
x− y +w=4 2x+3y−z =0 y+z+w=4
that uses the given vector as its particular solution.
0 −5 2 (a) 0 (b) 1 (c) −1
0 −7  1  4 10 1
3.19 One is nonsingular while the other is singular. Which is which?  13   13 
34
Chapter One. Linear Systems
  1 2  (a) 1 3
 1 2  (b) −3 −6
 1 2 1  (c) 1 3 1
1 2 1 (d)1 1 3
2 2 1 (e)1 0 5
−1 1 4
  3.21 Is the given vector in the set generated by the given set?
 2   1   1  (a) 3 ,{ 4 , 5 }
−1 2 1 (b)  0 , {1,0}
101
1 1 2 3 4
(c) 3, {0,1,3,2} 04501
1 2 3 (d) 0 , {1 , 0}
1 0 0
112
3.22 Prove that any linear system with a nonsingular matrix of coefficients has a solution, and that the solution is unique.
3.23 In the proof of Lemma 3.6, what happens if there are no non-0 = 0 equations?   3.24 Prove that if ⃗s and ⃗t satisfy a homogeneous system then so do these vec-
tors.
(a) ⃗s + ⃗t (b) 3⃗s (c) k⃗s + m⃗t for k, m ∈ R
What’s wrong with this argument: “These three show that if a homogeneous system has one solution then it has many solutions — any multiple of a solution is another solution, and any sum of solutions is a solution also — so there are no homogeneous systems with exactly one solution.”?
3.25 Prove that if a system with only rational coefficients and constants has a solution then it has at least one all-rational solution. Must it have infinitely many?
347
Section II. Linear Geometry 35 II Linear Geometry
If you have seen the elements of vectors then this section is an optional review. However, later work will refer to this material so if this is not a review then it is not optional.
In the first section we had to do a bit of work to show that there are only three types of solution sets — singleton, empty, and infinite. But this is easy to see geometrically in the case of systems with two equations and two unknowns. Draw each two-unknowns equation as a line in the plane and then the two lines could have a unique intersection, be parallel, or be the same line.
 Unique solution
3x+2y= 7 x− y=−1
No solutions
3x+2y=7 3x+2y=4
Infinitely many solutions
3x+2y= 7 6x+4y=14
   These pictures aren’t a short way to prove the results from the prior section, because those results apply to linear systems with any number of variables. But they do provide a visual insight, another way of seeing those results.
This section develops what we need to express our results geometrically. In particular, while the two-dimensional case is familiar, to extend to systems with more than two unknowns we shall need some higher-dimensional geometry.
II.1 Vectors in Space
“Higher-dimensional geometry” sounds exotic. It is exotic — interesting and eye-opening. But it isn’t distant or unreachable.
We begin by defining one-dimensional space to be R. To see that the definition is reasonable, picture a one-dimensional space
and pick a point to label 0 and another to label 1.
01
Now, with a scale and a direction, we have a correspondence with R. For instance,
  
36 Chapter One. Linear Systems
 to find the point matching +2.17, start at 0 and head in the direction of 1, and go 2.17 times as far.
The basic idea here, combining magnitude with direction, is the key to extending to higher dimensions.
An object in R, or in any Rn, comprised of a magnitude and a direction is a vector (we use the same word as in the prior section because we shall show below how to describe such an object with a column vector). We can draw a vector as having some length and pointing in some direction.
There is a subtlety involved in the definition of a vector as consisting of a magnitude and a direction — these
are equal, even though they start in different places They are equal because they have equal lengths and equal directions. Again: those vectors are not just alike, they are equal.
How can things that are in different places be equal? Think of a vector as representing a displacement (the word ‘vector’ is Latin for “carrier” or “traveler”). These two squares undergo displacements that are equal despite that they start in different places.
When we want to emphasize this property vectors have of not being anchored we refer to them as free vectors. Thus, these free vectors are equal, as each is a displacement of one over and two up.
More generally, vectors in the plane are the same if and only if they have the same change in first components and the same change in second components: the vector extending from (a1, a2) to (b1, b2) equals the vector from (c1, c2) to (d1,d2)ifandonlyifb1−a1 =d1−c1 andb2−a2 =d2−c2.
Saying ‘the vector that, were it to start at (a1, a2), would extend to (b1, b2)’ would be unwieldy. We instead describe that vector as
 b −a   11
            b2 − a2
Section II. Linear Geometry 37 so that we represent the ‘one over and two up’ arrows shown above in this way.
 1 
2
We often draw the arrow as starting at the origin, and we then say it is in the canonical position (or natural position or standard position). When
 v   ⃗v= 1
is in canonical position then it extends from the origin to the endpoint (v1,v2). We will typically say “the point
 1  2”
rather than “the endpoint of the canonical position of” that vector. Thus, we will call each of these R2.
  x   {(x1,x2)|x1,x2 ∈R} { 1 |x1,x2 ∈R}
 v    rv    v    w    v +w   r·1=1 1+1=11
v2 rv2 v2 w2 v2 + w2
v2
x2
In the prior section we defined vectors and vector operations with an algebraic
motivation;
we can now understand those operations geometrically. For instance, if ⃗v represents a displacement then 3⃗v represents a displacement in the same direction but three times as far and −1⃗v represents a displacement of the same distance as ⃗v but in the opposite direction.
⃗v
3⃗v
−⃗v
And, where ⃗v and w⃗ represent displacements, ⃗v+w⃗ represents those displacements combined.
⃗v + w⃗
w⃗
     ⃗v
38 Chapter One. Linear Systems
 The long arrow is the combined displacement in this sense: imagine that you are walking on a ship’s deck. Suppose that in one minute the ship’s motion gives it a displacement relative to the sea of ⃗v, and in the same minute your walking gives you a displacement relative to the ship’s deck of w⃗ . Then ⃗v + w⃗ is your displacement relative to the sea.
Another way to understand the vector sum is with the parallelogram rule. Draw the parallelogram formed by the vectors ⃗v and w⃗ . Then the sum ⃗v + w⃗ extends along the diagonal to the far corner.
 w⃗
⃗v + w⃗
⃗v
 The above drawings show how vectors and vector operations behave in R2. We can extend to R3, or to even higher-dimensional spaces where we have no pictures, with the obvious generalization: the free vector that, if it starts at (a1, . . . , an), ends at (b1, . . . , bn), is represented by this column.
b1 −a1  . 

bn − an
Vectors are equal if they have the same representation. We aren’t too careful about distinguishing between a point and the vector whose canonical representa- tion ends at that point.
And, we do addition and scalar multiplication component-wise.
Having considered points, we next turn to lines. In R2, the line through
(1,2) and (3,1) is comprised of (the endpoints of) the vectors in this set.  1   2 
{ 2 +t −1 |i∈R}
In the description the vector that is associated with the parameter t
 2   3   1  −1 = 1 − 2
v1 .
Rn ={ . |v1,...,vn ∈R} vn
 
Section II. Linear Geometry 39
 is the one shown in the picture as having its whole body in the line — it is a direction vector for the line. Note that points on the line to the left of x = 1 are described using negative values of t.
In R3, the line through (1, 2, 1) and (2, 3, 2) is the set of (endpoints of) vectors of this form
1 1
{2 + t · 1 | t ∈ R}
11
and lines in even higher-dimensional spaces work in the same way.
In R3, a line uses one parameter so that a particle on that line would be free to move back and forth in one dimension. A plane involves two parameters. For example, the plane through the points (1, 0, 5), (2, 1, −3), and (−2, 4, 0.5)
consists of (endpoints of) the vectors in this set.
1  1   −3  {0+t 1 +s 4 |t,s∈R}
5 −8 −4.5
The column vectors associated with the parameters come from these calculations.
  1   2  1  1 = 1 −0
−8 −3 5
 −3  −2 1  4 = 4 −0
−4.5 0.5 5
As with the line, note that we describe some points in this plane with negative t’s or negative s’s or both.
Calculus books often describe a plane by using a single linear equation.
x
P = {y | 2x + y + z = 4}
z
To translate from this to the vector description, think of this as a one-equation
 
40 Chapter One. Linear Systems linear system and parametrize: x = 2 − y/2 − z/2.
2 −1/2 −1/2 P={0+y· 1 +z· 0 |y,z∈R}
001
Shown in grey are the vectors associated with y and z, offset from the origin by 2 units along the x-axis, so that their entire body lies in the plane. Thus the vector sum of the two, shown in black, has its entire body in the plane along with the rest of the parallelogram.
Generalizing,asetoftheform{⃗p+t1⃗v1 +t2⃗v2 +···+tk⃗vk |t1,...,tk ∈R} where ⃗v1, . . . ,⃗vk ∈ Rn and k   n is a k-dimensional linear surface (or k-flat).
  For example, in R4
is a line,
is a plane, and
2 1
 π  0
{ +t |t∈R}
3 0 −0.5 0
0 1 2
0  1  0
{ +t +s |t,s∈R}
0 0 1 0 −1 0
3 0 1 2
 1   0  0 0
{ +r +s +t |r,s,t∈R}
−2 0 1 1 0.5 −1 0 0
is a three-dimensional linear surface. Again, the intuition is that a line permits motion in one direction, a plane permits motion in combinations of two directions, etc. When the dimension of the linear surface is one less than the dimension of the space, that is, when in Rn we have an (n − 1)-flat, the surface is called a hyperplane.
A description of a linear surface can be misleading about the dimension. For example, this
1 1 2
 0   1   2 
L={ +t +s |t,s∈R}
−1 0 0 −2 −1 −2
Section II. Linear Geometry 41 is a degenerate plane because it is actually a line, since the vectors are multiples
of each other and we can omit one.
1 1
  0   1 
L={ +r |r∈R}
−1 0 −2 −1
We shall see in the Linear Independence section of Chapter Two what relation- ships among vectors causes the linear surface they generate to be degenerate.
We now can restate in geometric terms our conclusions from earlier. First, the solution set of a linear system with n unknowns is a linear surface in Rn. Specifically, it is a k-dimensional linear surface, where k is the number of free variables in an echelon form version of the system. For instance, in the single equation case the solution set is an n − 1-dimensional hyperplane in Rn, where n   1. Second, the solution set of a homogeneous linear system is a linear surface passing through the origin. Finally, we can view the general solution set of any linear system as being the solution set of its associated homogeneous system offset from the origin by a vector, namely by any particular solution.
Exercises
  1.1 Find the canonical name for each vector. (a) the vector from (2, 1) to (4, 2) in R2
(b) the vector from (3, 3) to (2, 5) in R2
(c) the vector from (1,0,6) to (5,0,3) in R3 (d) the vector from (6,8,8) to (6,8,8) in R3
  1.2 Decide if the two vectors are equal.
(a) the vector from (5, 3) to (6, 2) and the vector from (1, −2) to (1, 1)
(b) the vector from (2,1,1) to (3,0,4) and the vector from (5,1,4) to (6,0,7)
  1.3 Does (1,0,2,1) lie on the line through (−2,1,1,0) and (5,10,−1,4)?
  1.4 (a) Describe the plane through (1, 1, 5, −1), (2, 2, 2, 0), and (3, 1, 0, 4).
(b) Is the origin in that plane?
1.5 Give a vector description of each.
(a)theplanesubsetofR3 withequationx−2y+z=4
(b) the plane in R3 with equation 2x+y+4z = −1 (c)thehyperplanesubsetofR4 withequationx+y+z+w=10
1.6 Describe the plane that contains this point and line. 2 −1 1
0 {  0  + 1 t | t ∈ R } 3 −4 2
  1.7 Intersect these planes.
1 0 1 0 2
{ 1 t + 1 s | t, s ∈ R } { 1 + 3 k + 0 m | k, m ∈ R } 13 004
42 Chapter One. Linear Systems   1.8 Intersect each pair, if possible.
1 0 1 0
(a) {1+t1|t∈R}, { 3 +s1|s∈R}
2 1 −2 2 2 1 0 0
(b) {0+t 1 |t∈R}, {s1+w4|s,w∈R} 1 −1 2 1
1.9 When a plane does not pass through the origin, performing operations on vectors whose bodies lie in it is more complicated than when the plane passes through the origin. Consider the picture in this subsection of the plane
2 −0.5 −0.5
{0+ 1 y+ 0 z|y,z∈R}
001
and the three vectors with endpoints (2, 0, 0), (1.5, 1, 0), and (1.5, 0, 1).
(a) Redraw the picture, including the vector in the plane that is twice as long as the one with endpoint (1.5, 1, 0). The endpoint of your vector is not (3, 2, 0);
what is it?
(b) Redraw the picture, including the parallelogram in the plane that shows the
sum of the vectors ending at (1.5, 0, 1) and (1.5, 1, 0). The endpoint of the sum,
on the diagonal, is not (3, 1, 1); what is it?
1.10 Show that the line segments (a1, a2)(b1, b2) and (c1, c2)(d1, d2) have the same
lengths and slopes if b1 − a1 = d1 − c1 and b2 − a2 = d2 − c2. Is that only if? 1.11 How should we define R0?
? 1.12 [Math. Mag., Jan. 1957] A person traveling eastward at a rate of 3 miles per hour finds that the wind appears to blow directly from the north. On doubling his speed it appears to come from the north east. What was the wind’s velocity?
1.13 Euclid describes a plane as “a surface which lies evenly with the straight lines on itself”. Commentators such as Heron have interpreted this to mean, “(A plane surface is) such that, if a straight line pass through two points on it, the line coincides wholly with it at every spot, all ways”. (Translations from [Heath], pp. 171-172.) Do planes, as described in this section, have that property? Does this description adequately define planes?
II.2 Length and Angle Measures
We’ve translated the first section’s results about solution sets into geometric terms, to better understand those sets. But we must be careful not to be misled by our own terms — labeling subsets of Rk of the forms {⃗p + t⃗v | t ∈ R} and {⃗p+t⃗v+sw⃗ |t,s∈R} as ‘lines’ and ‘planes’ doesn’t make them act like the lines and planes of our past experience. Rather, we must ensure that the names suit the sets. While we can’t prove that the sets satisfy our intuition—we
   
Section II. Linear Geometry 43
 can’t prove anything about intuition — in this subsection we’ll observe that a result familiar from R2 and R3, when generalized to arbitrary Rn, supports the idea that a line is straight and a plane is flat. Specifically, we’ll see how to do Euclidean geometry in a ‘plane’ by giving a definition of the angle between two Rn vectors, in the plane that they generate.
2.2 Remark This is a natural generalization of the Pythagorean Theorem. A classic motivating discussion is in [Polya].
For any nonzero ⃗v, the vector ⃗v/|⃗v| has length one. We say that the second normalizes ⃗v to length one.
We can use that to get a formula for the angle between two vectors. Consider two vectors in R3 where neither is a multiple of the other
 2.1 Definition The length of a vector ⃗v ∈ Rn is the square root of the sum of the squares of its components.
| ⃗v | =   v 21 + · · · + v 2n
  ⃗v
⃗u
(the special case of multiples will turn out below not to be an exception). They determine a two-dimensional plane — for instance, put them in canonical position and take the plane formed by the origin and the endpoints. In that plane consider the triangle with sides ⃗u, ⃗v, and ⃗u − ⃗v.
Apply the Law of Cosines: |⃗u − ⃗v |2 = |⃗u |2 + |⃗v |2 − 2 |⃗u | |⃗v | cos θ where θ is the angle between the vectors. The left side gives
(u1 −v1)2 +(u2 −v2)2 +(u3 −v3)2
=(u21 −2u1v1 +v21)+(u2 −2u2v2 +v2)+(u23 −2u3v3 +v23)
while the right side gives this.
(u21 +u2 +u23)+(v21 +v2 +v23)−2|⃗u||⃗v|cosθ
 
44 Chapter One. Linear Systems Canceling squares u21 , . . . , v23 and dividing by 2 gives a formula for the angle.
θ = arccos( u1v1 + u2v2 + u3v3 ) | ⃗u | | ⃗v |
In higher dimensions we cannot draw pictures as above but we can instead make the argument analytically. First, the form of the numerator is clear; it comes from the middle terms of (ui − vi)2.
Note that the dot product of two vectors is a real number, not a vector, and that the dot product is only defined if the two vectors have the same number of components. Note also that dot product is related to length: ⃗u • ⃗u = u 1 u 1 + · · · + u n u n = | ⃗u | 2 .
2.4 Remark Some authors require that the first vector be a row vector and that the second vector be a column vector. We shall not be that strict and will allow the dot product operation between two column vectors.
Still reasoning analytically but guided by the pictures, we use the next theorem to argue that the triangle formed by the line segments making the bodies of ⃗u, ⃗v, and ⃗u + ⃗v in Rn lies in the planar subset of Rn generated by ⃗u and ⃗v (see the figure below).
This is the source of the familiar saying, “The shortest distance between two points is in a straight line.”
   2.3 Definition The dot product (or inner product or scalar product) of two n-component real vectors is the linear combination of their components.
⃗u•⃗v=u1v1 +u2v2 +···+unvn
 2.5 Theorem (Triangle Inequality) For any ⃗u,⃗v ∈ Rn, | ⃗u + ⃗v |   | ⃗u | + | ⃗v |
with equality if and only if one of the vectors is a nonnegative scalar multiple of the other one.
  ⃗u + ⃗v start ⃗u
finish
⃗v
  Proof (We’ll use some algebraic properties of dot product that we have not yet checked, for instance that ⃗u • (a⃗ + ⃗b) = ⃗u • a⃗ + ⃗u • ⃗b and that ⃗u • ⃗v = ⃗v • ⃗u. See
Section II. Linear Geometry 45 Exercise 18.) Since all the numbers are positive, the inequality holds if and only
if its square holds.
| ⃗u + ⃗v | 2   ( | ⃗u | + | ⃗v | ) 2 (⃗u+⃗v)• (⃗u+⃗v) |⃗u|2 +2|⃗u||⃗v|+|⃗v|2
⃗u• ⃗u+⃗u•⃗v+⃗v• ⃗u+⃗v•⃗v ⃗u• ⃗u+2|⃗u||⃗v|+⃗v•⃗v 2 ⃗u • ⃗v   2 | ⃗u | | ⃗v |
That, in turn, holds if and only if the relationship obtained by multiplying both sides by the nonnegative numbers |⃗u | and |⃗v |
 and rewriting
2(|⃗v|⃗u)• (|⃗u|⃗v) 2|⃗u|2|⃗v|2
0   | ⃗u | 2 | ⃗v | 2 − 2 ( | ⃗v | ⃗u ) • ( | ⃗u | ⃗v ) + | ⃗u | 2 | ⃗v | 2
is true. But factoring shows that it is true
0 (|⃗u|⃗v−|⃗v|⃗u)• (|⃗u|⃗v−|⃗v|⃗u)
since it only says that the square of the length of the vector |⃗u | ⃗v − |⃗v | ⃗u is not negative. Asforequality,itholdswhen,andonlywhen,|⃗u|⃗v−|⃗v|⃗uis⃗0. The check that |⃗u|⃗v = |⃗v|⃗u if and only if one vector is a nonnegative real scalar multiple of the other is easy. QED
This result supports the intuition that even in higher-dimensional spaces, lines are straight and planes are flat. We can easily check from the definition that linear surfaces have the property that for any two points in that surface, the line segment between them is contained in that surface. But if the linear surface were not flat then that would allow for a shortcut.
 P
Q
Because the Triangle Inequality says that in any Rn the shortest cut between two endpoints is simply the line segment connecting them, linear surfaces have no bends.
Back to the definition of angle measure. The heart of the Triangle Inequality’s proof is the ⃗u • ⃗v   |⃗u | |⃗v | line. We might wonder if some pairs of vectors satisfy the inequality in this way: while ⃗u • ⃗v is a large number, with absolute value bigger than the right-hand side, it is a negative large number. The next result says that does not happen.
46 Chapter One. Linear Systems
  2.6 Corollary (Cauchy-Schwarz Inequality) For any ⃗u,⃗v ∈ Rn, |⃗u • ⃗v|   |⃗u||⃗v|
with equality if and only if one vector is a scalar multiple of the other.
Proof The Triangle Inequality’s proof shows that ⃗u • ⃗v   |⃗u | |⃗v | so if ⃗u • ⃗v is positive or zero then we are done. If ⃗u • ⃗v is negative then this holds.
|⃗u • ⃗v| = −(⃗u • ⃗v) = (−⃗u) • ⃗v   |−⃗u||⃗v| = |⃗u||⃗v|
The equality condition is Exercise 19. QED
The Cauchy-Schwarz inequality assures us that the next definition makes sense because the fraction has absolute value less than or equal to one.
 2.7 Definition The angle between two nonzero vectors ⃗u,⃗v ∈ Rn is θ=arccos( ⃗u•⃗v )
 | ⃗u | | ⃗v |
(if either is the zero vector then we take the angle to be a right angle).
 2.8 Corollary Vectors from Rn are orthogonal, that is, perpendicular, if and only if their dot product is zero. They are parallel if and only if their dot product equals the product of their lengths.
2.9 Example These vectors are orthogonal.
  1    1 
−1• 1=0
We’ve drawn the arrows away from canonical position but nevertheless the
vectors are orthogonal.
2.10 Example The R3 angle formula given at the start of this subsection is a special case of the definition. Between these two
0 3
2 1
1 0
  
Section II. Linear Geometry 47 the angle is
(1)(0) + (1)(3) + (0)(2) 3 arccos(√12 + 12 + 02√02 + 32 + 22 ) = arccos(√2√13)
approximately 0.94 radians. Notice that these vectors are not orthogonal. Al- though the yz-plane may appear to be perpendicular to the xy-plane, in fact the two planes are that way only in the weak sense that there are vectors in each orthogonal to all vectors in the other. Not every vector in each is orthogonal to all vectors in the other.
Exercises
  2.11 Find the length of each vector.
      4 0 1
(a)3 (b)−1 (c)1 (d)0 (e)−1 1 2   1
100
  2.12 Find the angle between each two, if it is defined.  1   1  1 0  1  1
(a) 2 , 4 (b)2,4 (c) 2 ,4 01 −1
  2.13 [Ohanian] During maneuvers preceding the Battle of Jutland, the British battle cruiser Lion moved as follows (in nautical miles): 1.2 miles north, 6.1 miles 38 degrees east of south, 4.0 miles at 89 degrees east of north, and 6.5 miles at 31 degrees east of north. Find the distance between starting and ending positions. (Ignore the earth’s curvature.)
       Find k so that these two vectors are perpendicular.  k   4 
13
Describe the set of vectors in R3 orthogonal to the one with entries 1, 3, and −1.
2.14
2.15   2.16
(b) Find the angle between the diagonal of the unit cube in R3 and one of the axes.
(c) Find the angle between the diagonal of the unit cube in Rn and one of the axes.
(d) What is the limit, as n goes to ∞, of the angle between the diagonal of the unit cube in Rn and any one of the axes?
2.17 Is any vector perpendicular to itself?
2.18 Describe the algebraic properties of dot product.
(a) Is it right-distributive over addition: (⃗u + ⃗v) • w⃗ = ⃗u • w⃗ + ⃗v • w⃗ ? (b) Is it left-distributive (over addition)?
(c) Does it commute?
(d) Associate?
(e) How does it interact with scalar multiplication?
(a) Find the angle between the diagonal of the unit square in R2 and any one of the axes.
48 Chapter One. Linear Systems
 As always, you must back any assertion with either a proof or an example.
2.19 Verify the equality condition in Corollary 2.6, the Cauchy-Schwarz Inequal-
ity.
(a) Show that if ⃗u is a negative scalar multiple of ⃗v then ⃗u • ⃗v and ⃗v • ⃗u are less
than or equal to zero.
(b) Show that |⃗u • ⃗v| = |⃗u | |⃗v | if and only if one vector is a scalar multiple of the
other.
2.20 Suppose that ⃗u•⃗v=⃗u• w⃗ and ⃗u̸=⃗0. Must⃗v=w⃗?
  2.21 Does any vector have length zero except a zero vector? (If “yes”, produce an example. If “no”, prove it.)
  2.22 Find the midpoint of the line segment connecting (x1,y1) with (x2,y2) in R2. Generalize to Rn.
2.23 Show that if ⃗v ̸= ⃗0 then ⃗v/|⃗v | has length one. What if ⃗v = ⃗0?
2.24 Showthatifr 0thenr⃗visrtimesaslongas⃗v. Whatifr<0?
  2.25 A vector ⃗v ∈ Rn of length one is a unit vector. Show that the dot product of two unit vectors has absolute value less than or equal to one. Can ‘less than’
happen? Can ‘equal to’?
2.26 Prove that |⃗u + ⃗v |2 + |⃗u − ⃗v |2 = 2|⃗u |2 + 2|⃗v |2 .
2.27 Showthatif⃗x•⃗y=0forevery⃗ythen⃗x=⃗0.
2.28 Is |⃗u1 + ··· + ⃗un|   |⃗u1| + ··· + |⃗un|? If it is true then it would generalize the
Triangle Inequality.
2.29 What is the ratio between the sides in the Cauchy-Schwarz inequality?
2.30 Why is the zero vector defined to be perpendicular to every vector?
2.31 Describe the angle between two vectors in R1.
2.32 Give a simple necessary and sufficient condition to determine whether the angle
between two vectors is acute, right, or obtuse.
2.33 Generalize to Rn the converse of the Pythagorean Theorem, that if ⃗u and ⃗v are
perpendicularthen|⃗u+⃗v|2 =|⃗u|2+|⃗v|2.
2.34 Show that |⃗u | = |⃗v | if and only if ⃗u + ⃗v and ⃗u − ⃗v are perpendicular. Give an
example in R2.
2.35 Show that if a vector is perpendicular to each of two others then it is perpen-
dicular to each vector in the plane they generate. (Remark. They could generate a
degenerate plane — a line or a point — but the statement remains true.) 2.36 Prove that, where ⃗u,⃗v ∈ Rn are nonzero vectors, the vector
⃗u + ⃗v | ⃗u | | ⃗v |
bisects the angle between them. Illustrate in R2.
2.37 Verify that the definition of angle is dimensionally correct: (1) if k > 0 then the
cosine of the angle between k⃗u and ⃗v equals the cosine of the angle between ⃗u and ⃗v, and (2) if k < 0 then the cosine of the angle between k⃗u and ⃗v is the negative of the cosine of the angle between ⃗u and ⃗v.
  2.38 Show that the inner product operation is linear: for ⃗u,⃗v,w⃗ ∈ Rn and k,m ∈ R, ⃗u • ( k ⃗v + m w⃗ ) = k ( ⃗u • ⃗v ) + m ( ⃗u • w⃗ ) .
  
Section II. Linear Geometry 49
2.39 The geometric mean of two positive reals x, y is √xy. It is analogous to the arithmetic mean (x + y)/2. Use the Cauchy-Schwarz inequality to show that the geometric mean of any x, y ∈ R is less than or equal to the arithmetic mean.
? 2.40 [Cleary] Astrologers claim to be able to recognize trends in personality and fortune that depend on an individual’s birthday by incorporating where the stars were 2000 years ago. Suppose that instead of star-gazers coming up with stuff, math teachers who like linear algebra (we’ll call them vectologers) had come up with a similar system as follows: Consider your birthday as a row vector (month day). For instance, I was born on July 12 so my vector would be (7 12). Vectologers have made the rule that how well individuals get along with each other depends on the angle between vectors. The smaller the angle, the more harmonious the relationship.
(a) Find the angle between your vector and mine, in radians.
(b) Would you get along better with me, or with a professor born on September 19? (c) For maximum harmony in a relationship, when should the other person be
born?
(d) Is there a person with whom you have a “worst case” relationship, i.e., your
vector and theirs are orthogonal? If so, what are the birthdate(s) for such people?
If not, explain why not.
? 2.41 [Am. Math. Mon., Feb. 1933] A ship is sailing with speed and direction ⃗v1; the
wind blows apparently (judging by the vane on the mast) in the direction of a vector a⃗ ; on changing the direction and speed of the ship from ⃗v1 to ⃗v2 the apparent wind is in the direction of a vector ⃗b.
Find the vector velocity of the wind.
2.42 Verify the Cauchy-Schwarz inequality by first proving Lagrange’s identity:     2           
  a j b j =
a 2j
b 2j − ( a k b j − a j b k ) 2 1 k<j n
1 j n
and then noting that the final term is positive. This result is an improvement over Cauchy-Schwarz because it gives a formula for the difference between the two sides. Interpret that difference in R2.
1 j n
1 j n
50 Chapter One. Linear Systems III Reduced Echelon Form
After developing the mechanics of Gauss’s Method, we observed that it can be done in more than one way. For example, from this matrix
 2 2  43
we could derive any of these three echelon form matrices.
 2 2   1 1   2 0  0 −1 0 −1 0 −1
The first results from −2ρ1 + ρ2. The second comes from doing (1/2)ρ1 and then −4ρ1 +ρ2. The third comes from −2ρ1 +ρ2 followed by 2ρ2 +ρ1 (after the first row combination the matrix is already in echelon form but it is nonetheless a legal row operation).
In this chapter’s first section we noted that this raises questions. Will any two echelon form versions of a linear system have the same number of free variables? If yes, will the two have exactly the same free variables? In this section we will give a way to decide if one linear system can be derived from another by row operations. The answers to both questions, both “yes,” will follow from that.
III.1 Gauss-Jordan Reduction
Here is an extension of Gauss’s Method that has some advantages. 1.1 Example To solve
x+y−2z=−2 y+3z= 7 x −z=−1
we can start as usual by reducing it to echelon form.
 1 1 −2−2 11−2−2 −ρ1+ρ3  ρ2+ρ3  
  −→ 0 1 3   7 −→ 0 1 3   7 0 −1 1 1 0 0 4 8
  We can keep going to a second stage by making the leading entries into 1’s
1 1 −2 −2 (1/4)ρ3   −→ 0 1 3 7
   0012
Section III. Reduced Echelon Form 51
 and then to a third stage that uses the leading entries to eliminate all of the other entries in each column by combining upwards.
1 1 0 2 −3ρ3+ρ2   −→ 0 1 0 1
2ρ3 +ρ1
The answer is x = 1, y = 1, and z = 2.
Using one entry to clear out the rest of a column is pivoting on that entry.
Notice that the row combination operations in the first stage move left to right while the combination operations in the third stage move right to left.
1.2 Example The middle stage operations that turn the leading entries into 1’s don’t interact so we can combine multiple ones into a single step.
    
1 0 0 1 −ρ2+ρ1  
    −→ 0 1 0 1 0012 0012
    2 1 7 −2ρ1+ρ2
4 −2 6
2 1 7
0 −4 −8   
1 1/2 7/2
(−1/4)ρ2 0 1 2   
−(1/2)ρ2 +ρ1 1 0 5/2
012
−→
   (1/2)ρ1 −→
  −→
 The answer is x = 5/2 and y = 2.
This extension of Gauss’s Method is the Gauss-Jordan Method or Gauss-
Jordan reduction.
The cost of using Gauss-Jordan reduction to solve a system is the additional arithmetic. The benefit is that we can just read off the solution set description. In any echelon form system, reduced or not, we can read off when the system
has an empty solution set because there is a contradictory equation. We can read off when the system has a one-element solution set because there is no contradiction and every variable is the leading variable in some row. And, we can read off when the system has an infinite solution set because there is no contradiction and at least one variable is free.
However, in reduced echelon form we can read off not just the size of the solution set but also its description. We have no trouble describing the solution set when it is empty, of course. Example 1.1 and 1.2 show how in a single
 1.3 Definition A matrix or linear system is in reduced echelon form if, in addition to being in echelon form, each leading entry is a 1 and is the only nonzero entry in its column.
52 Chapter One. Linear Systems element solution set case the single element is in the column of constants. The
next example shows how to read the parametrization of an infinite solution set.
 1.4 Example
2 6 1 2 5 2 6 1
 −ρ2+ρ3  
2   5
  0 3 1 4 1 0 3 1 2 5
4   1 1
−→ 0 3 1
0 0 0 −2 4
(1/2)ρ1
−→ −→ −→ 0
   −(4/3)ρ3 +ρ2 −3ρ2 +ρ1 
0 −1/2 0 −9/2
 1 1/3 0 3
 (1/3)ρ2 −(1/2)ρ3
As a linear system this is
−ρ3 +ρ1
− 1/2x3 x2 + 1/3x3
0
= −9/2
= 3 x4= −2
 1/2 
0
0 1 −2
 so a solution set description is this.
x1 −9/2
x1
x2   3 
S={ = + x3 |x3 ∈R}
−1/3 x3 0 1
x4 −2 0
Thus, echelon form isn’t some kind of one best form for systems. Other forms, such as reduced echelon form, have advantages and disadvantages. Instead of picturing linear systems (and the associated matrices) as things we operate on, always directed toward the goal of echelon form, we can think of them as interrelated, where we can get from one to another by row operations. The rest of this subsection develops this thought.
1.5 Lemma Elementary row operations are reversible.
Proof For any matrix A, the effect of swapping rows is reversed by swapping them back, multiplying a row by a nonzero k is undone by multiplying by 1/k, and adding a multiple of row i to row j (with i ̸= j) is undone by subtracting the same multiple of row i from row j.
ρi ↔ρj ρj ↔ρi kρi (1/k)ρi kρi +ρj −kρi +ρj
A −→ −→ A A −→ −→ A A −→ −→ A
(We need the i ̸= j condition; see Exercise 18.) QED
 
Section III. Reduced Echelon Form 53
 Again, the point of view that we are developing, supported now by the lemma, is that the term ‘reduces to’ is misleading: where A −→ B, we shouldn’t think of B as after A or simpler than A. Instead we should think of the two matrices as interrelated. Below is a picture. It shows the matrices from the start of this section and their reduced echelon form version in a cluster, as inter-reducible.
 2 0 
0−1  11 
 0 −1
      2 2  43
 2 2  0 −1
   1 0  01
We say that matrices that reduce to each other are equivalent with respect to the relationship of row reducibility. The next result justifies this, using the definition of an equivalence.∗
1.6 Lemma Between matrices, ‘reduces to’ is an equivalence relation.
Proof We must check the conditions (i) reflexivity, that any matrix reduces to itself, (ii) symmetry, that if A reduces to B then B reduces to A, and (iii) transitivity, that if A reduces to B and B reduces to C then A reduces to C.
Reflexivity is easy; any matrix reduces to itself in zero-many operations.
The relationship is symmetric by the prior lemma — if A reduces to B by some row operations then also B reduces to A by reversing those operations.
For transitivity, suppose that A reduces to B and that B reduces to C. Following the reduction steps from A → ··· → B with those from B → ··· → C gives a reduction from A to C. QED
The diagram below shows the collection of all matrices as a box. Inside that box each matrix lies in a class. Matrices are in the same class if and only if they are interreducible. The classes are disjoint — no matrix is in two distinct classes. We have partitioned the collection of matrices into row equivalence classes.†
∗ More information on equivalence relations is in the appendix.
† More information on partitions and class representatives is in the appendix.
  1.7 Definition Two matrices that are interreducible by elementary row operations are row equivalent.
 A B
...
  
54 Chapter One. Linear Systems
 One of the classes is the cluster of interrelated matrices from the start of this section sketched above (it includes all of the nonsingular 2×2 matrices).
The next subsection proves that the reduced echelon form of a matrix is unique. Rephrased in terms of the row-equivalence relationship, we shall prove that every matrix is row equivalent to one and only one reduced echelon form matrix. In terms of the partition what we shall prove is: every equivalence class contains one and only one reduced echelon form matrix. So each reduced echelon form matrix serves as a representative of its class.
Exercises
  1.8 Use Gauss-Jordan reduction to solve each system.
(a) x+y=2 (b) x −z=4 (c) 3x−2y= 1
x−y=0 2x+2y (d) 2x− y =−1
x+3y− z= 5 y+2z= 5
=1 6x+ y=1/2
1.9 Do Gauss-Jordan reduction.
(a) x+y− z=3 (b) x+y+2z=0
2x−y− z=1 2x−y+ z=1
3x+y+2z=0 4x+y+5z=1
  1.10 Find the reduced echelon form of each matrix.
 2 1  1 3 1 (a) 1 3 (b)2 0 4
1 0 3 1 2 (c)1 4 2 1 5
3 4 8 1 2
0 1 3 2 (d)0 0 5 6
1515
1.11 Get the reduced echelon form of each. 021 131
(a)2 −1 1 (b)2 6 2 −2 −1 0 −1 0 0
−1 −3 −3
  1.12 Find each solution set by using Gauss-Jordan reduction and then reading off the parametrization.
(a) 2x+y−z=1 4x−y =3
(b)x −z =1 y+2z−w=3 x+2y+3z−w=7
(c) x− y+ z =0 y +w=0 3x− 2y+3z+w=0 −y −w=0
(d) a+2b+3c+d−e=1 3a− b+ c+d+e=3
Section III. Reduced Echelon Form 55 1.13 Give two distinct echelon form versions of this matrix.
2 1 1 3 6 4 1 2 1515
  1.14 List the reduced echelon forms possible for each size. (a) 2×2 (b) 2×3 (c) 3×2 (d) 3×3
  1.15 What results from applying Gauss-Jordan reduction to a nonsingular matrix?
1.16 Decide whether each relation is an equivalence on the set of 2×2 matri- ces.
(a) two matrices are related if they have the same entry in the first row and first column
(b) two matrices are related if their four entries sum to the same total
(c) two matrices are related if they have the same entry in the first row and first
column, or the same entry in the second row and second column
1.17 [Cleary] Consider the following relationship on the set of 2×2 matrices: we say that A is sum-what like B if the sum of all of the entries in A is the same as the sum of all the entries in B. For instance, the zero matrix would be sum-what like the matrix whose first row had two sevens, and whose second row had two negative sevens. Prove or disprove that this is an equivalence relation on the set of 2×2 matrices.
1.18 The proof of Lemma 1.5 contains a reference to the i ̸= j condition on the row combination operation.
(a) Write down a 2×2 matrix with nonzero entries, and show that the −1 · ρ1 + ρ1 operation is not reversed by 1 · ρ1 + ρ1.
(b) Expand the proof of that lemma to make explicit exactly where it uses the i ̸= j condition on combining.
1.19 [Cleary] Consider the set of students in a class. Which of the following re- lationships are equivalence relations? Explain each answer in at least a sen- tence.
(a) Two students x, y are related if x has taken at least as many math classes as y. (b) Students x, y are related if they have names that start with the same letter.
1.20 Show that each of these is an equivalence on the set of 2×2 matrices. Describe the equivalence classes.
(a) Two matrices are related if they have the same product down the diagonal, that is, if the product of the entries in the upper left and lower right are equal. (b) Two matrices are related if they both have at least one entry that is a 1, or if
neither does.
1.21 Show that each is not an equivalence on the set of 2×2 matrices.
(a) Two matrices A,B are related if a1,1 = −b1,1.
(b) Two matrices are related if the sum of their entries are within 5, that is, A is
related to B if |(a1,1 + · · · + a2,2) − (b1,1 + · · · + b2,2)| < 5.
 
56 Chapter One. Linear Systems III.2 The Linear Combination Lemma
We will close this chapter by proving that every matrix is row equivalent to one and only one reduced echelon form matrix. The ideas here will reappear, and be further developed, in the next chapter.
The crucial observation concerns how row operations act to transform one matrix into another: the new rows are linear combinations of the old.
2.1 Example Consider this Gauss-Jordan reduction.     
2 1 0 −(1/2)ρ1+ρ2 2 1 0
     1 3 5
−→
0 5/2 5
  
1 1/2 0
 (1/2)ρ1 −→ (2/5)ρ2
0
1 2
  
1 0 −1 012
  −(1/2)ρ2 +ρ1 −→
 Denoting those matrices A → D → G → B and writing the rows of A as α1 and α2, etc., we have this.
    
α1 −(1/2)ρ1+ρ2 δ1 = α1
α −→ δ =−(1/2)α +α
2212
(1/2)ρ1 −→ (2/5)ρ2
  
γ1 = (1/2)α1
γ2 = −(1/5)α1 + (2/5)α2   
β1 = (3/5)α1 − (1/5)α2 β2 = −(1/5)α1 + (2/5)α2
−(1/2)ρ2+ρ1 −→
2.2 Example The fact that Gaussian operations combine rows linearly also holds if there is a row swap. With this A, D, G, and B
        
02 ρ1↔ρ2 11 (1/2)ρ2 11 −ρ2+ρ1 10 11 −→ 02 −→ 01 −→ 01
we get these linear relationships.
   ⃗    α⃗1 ρ1↔ρ2 δ1 = α⃗2 (1/2)ρ2 ⃗γ1 = α⃗2
α⃗ −→ ⃗δ =α⃗ −→ ⃗γ =(1/2)α⃗ 22121
 ⃗  −ρ2+ρ1 β1 =(−1/2)α⃗1 +1·α⃗2
In summary, Gauss’s Method systematically finds a suitable sequence of linear combinations of the rows.
−→ β⃗2 =(1/2)α⃗1
Section III. Reduced Echelon Form 57
  2.3 Lemma (Linear Combination Lemma) A linear combination of linear combina- tions is a linear combination.
Proof Given the set c1,1x1 + · · · + c1,nxn through cm,1x1 + · · · + cm,nxn of linear combinations of the x’s, consider a combination of those
d1(c1,1x1 +···+c1,nxn) +···+ dm(cm,1x1 +···+cm,nxn)
where the d’s are scalars along with the c’s. Distributing those d’s and regrouping
gives
=(d1c1,1 +···+dmcm,1)x1 +···+ (d1c1,n +···+dmcm,n)xn
which is also a linear combination of the x’s. QED
Proof For any two interreducible matrices A and B there is some minimum number of row operations that will take one to the other. We proceed by induction on that number.
In the base step, that we can go from one matrix to another using zero reduction operations, the two are equal. Then each row of B is trivially a combinationofA’srowsβ⃗i =0·α⃗1+···+1·α⃗i+···+0·α⃗m.
For the inductive step assume the inductive hypothesis: with k   0, any matrix that can be derived from A in k or fewer operations has rows that are linear combinations of A’s rows. Consider a matrix B such that reducing A to B requires k + 1 operations. In that reduction there is a next-to-last matrix G, so that A −→ · · · −→ G −→ B. The inductive hypothesis applies to this G because it is only k steps away from A. That is, each row of G is a linear combination of the rows of A.
We will verify that the rows of B are linear combinations of the rows of G. Then the Linear Combination Lemma, Lemma 2.3, applies to show that the rows of B are linear combinations of the rows of A.
If the row operation taking G to B is a swap then the rows of B are just the rows of G reordered and each row of B is a linear combination of the rows of G. If the operation taking G to B is multiplication of a row by a scalar cρi then β⃗i = c⃗γi and the other rows are unchanged. Finally, if the row operation is adding a multiple of one row to another rρi + ρj then only row j of B differs from the matching row of G, and β⃗ j = rγi + γj , which is indeed a linear combinations of the rows of G.
Because we have proved both a base step and an inductive step, the proposi- tion follows by the principle of mathematical induction. QED
 2.4 Corollary Where one matrix reduces to another, each row of the second is a linear combination of the rows of the first.
58 Chapter One. Linear Systems
 We now have the insight that Gauss’s Method builds linear combinations of the rows. But of course its goal is to end in echelon form, since that is a particularly basic version of a linear system, as it has isolated the variables. For instance, in this matrix
2 3 7 8 0 0
0 0 1 5 1 1 R =  0 0 0 3 3 0 
000021
x1 has been removed from x5’s equation. That is, Gauss’s Method has made x5’s row in some way independent of x1’s row.
The following result makes this intuition precise. We sometimes refer to Gauss’s Method as Gaussian elimination. What it eliminates is linear relation- ships among the rows.
Proof Let R be an echelon form matrix and consider its non-⃗0 rows. First observe that if we have a row written as a combination of the others ⃗ρi = c1⃗ρ1+···+ci−1⃗ρi−1+ci+1⃗ρi+1+···+cm⃗ρm thenwecanrewritethatequation as
⃗0=c1⃗ρ1 +···+ci−1⃗ρi−1 +ci⃗ρi +ci+1⃗ρi+1 +···+cm⃗ρm (∗)
where not all the coefficients are zero; specifically, ci = −1. The converse holds also: given equation (∗) where some ci ̸= 0 we could express ⃗ρi as a combination of the other rows by moving ci⃗ρi to the left and dividing by −ci. Therefore we will have proved the theorem if we show that in (∗) all of the coefficients are 0. For that we use induction on the row number i.
The base case is the first row i = 1 (if there is no such nonzero row, so that R is the zero matrix, then the lemma holds vacuously). Let li be the column number of the leading entry in row i. Consider the entry of each row that is in column l1. Equation (∗) gives this.
0=c1r1,l1 +c2r2,l1 +···+cmrm,l1 (∗∗)
The matrix is in echelon form so every row after the first has a zero entry in that column r2,l1 = · · · = rm,l1 = 0. Thus equation (∗∗) shows that c1 = 0, because r1,l1 ̸= 0 as it leads the row.
The inductive step is much the same as the base step. Again consider equation (∗). We will prove that if the coefficient ci is 0 for each row index i ∈ {1,...,k} then ck+1 is also 0. We focus on the entries from column lk+1.
 2.5 Lemma In an echelon form matrix, no nonzero row is a linear combination of the other nonzero rows.
0=c1r1,lk+1 +···+ck+1rk+1,lk+1 +···+cmrm,lk+1
Section III. Reduced Echelon Form 59
 By the inductive hypothesis c1, . . . ck are all 0 so this reduces to the equation 0 = ck+1rk+1,lk+1 + · · · + cmrm,lk+1 . The matrix is in echelon form so the entries rk+2,lk+1 , . . . , rm,lk+1 are all 0. Thus ck+1 = 0, because rk+1,lk+1 ̸= 0 as it is the leading entry. QED
With that, we are ready to show that the end product of Gauss-Jordan reduction is unique.
Proof [Yuster] Fix a number of rows m. We will proceed by induction on the number of columns n.
The base case is that the matrix has n = 1 column. If this is the zero matrix then its echelon form is the zero matrix. If instead it has any nonzero entries then when the matrix is brought to reduced echelon form it must have at least one nonzero entry, which must be a 1 in the first row. Either way, its reduced echelon form is unique.
For the inductive step we assume that n > 1 and that all m row matrices having fewer than n columns have a unique reduced echelon form. Consider an m×n matrix A and suppose that B and C are two reduced echelon form matrices derived from A. We will show that these two must be equal.
Let Aˆ be the matrix consisting of the first n − 1 columns of A. Observe that any sequence of row operations that bring A to reduced echelon form will also bring Aˆ to reduced echelon form. By the inductive hypothesis this reduced echelon form of Aˆ is unique, so if B and C differ then the difference must occur in column n.
We finish the inductive step, and the argument, by showing that the two cannot differ only in that column. Consider a homogeneous system of equations for which A is the matrix of coefficients.
 2.6 Theorem Each matrix is row equivalent to a unique reduced echelon form matrix.
a1,1x1 + a1,2x2 +···+ a1,nxn = 0 a2,1x1 + a2,2x2 +···+ a2,nxn = 0
(∗) By Theorem One.I.1.5 the set of solutions to that system is the same as the set
am,1x1 +am,2x2 +···+am,nxn = 0 of solutions to B’s system
b1,1x1 + b1,2x2 +···+ b1,nxn = 0 b2,1x1 + b2,2x2 +···+ b2,nxn = 0
.
(∗∗)
bm,1x1 +bm,2x2 +···+bm,nxn = 0
.
60
and to C’s.
Chapter One. Linear Systems
 c1,1x1 + c1,2x2 +···+ c1,nxn = 0 c2,1x1 + c2,2x2 +···+ c2,nxn = 0
.
cm,1x1 +cm,2x2 +···+cm,nxn = 0
(∗∗∗)
With B and C different only in column n, suppose that they differ in row i. Subtract row i of (∗∗∗) from row i of (∗∗) to get the equation (bi,n −ci,n)·xn = 0. We’ve assumed that bi,n ̸= ci,n so xn = 0. Thus in (∗∗) and (∗∗∗) the n-th column contains a leading entry, or else the variable xn would be free. That’s a contradiction because with B and C equal on the first n − 1 columns, the leading entries in the n-th column would have to be in the same row, and with both matrices in reduced echelon form, both leading entries would have to be 1, and would have to be the only nonzero entries in that column. So B = C. QED
We have asked whether any two echelon form versions of a linear system have the same number of free variables, and if so are they exactly the same variables? With the prior result we can answer both questions “yes.” There is no linear system such that, say, we could apply Gauss’s Method one way and get y and z free but apply it another way and get y and w free.
Before the proof, recall the distinction between free variables and parameters. This system
x+y =1 y+z=2
has one free variable, z, because it is the only variable not leading a row. We have the habit of parametrizing using the free variable y = 2 − z, x = −1 + z, but we could also parametrize using another variable, such as z = 2 − y, x = 1 − y. So the set of parameters is not unique, it is the set of free variables that is unique.
Proof The prior result says that the reduced echelon form is unique. We get from any echelon form version to the reduced echelon form by eliminating up, so any echelon form version of a system has the same free variables as the reduced echelon form version. QED
We close with a recap. In Gauss’s Method we start with a matrix and then derive a sequence of other matrices. We defined two matrices to be related if we can derive one from the other. That relation is an equivalence relation, called row equivalence, and so partitions the set of all matrices into row equivalence classes.
 2.7 Corollary If from a starting linear systems we derive by Gauss’s Method two different echelon form systems, then the two have the same free variables.
Section III. Reduced Echelon Form 61
   1 3  27
 1 3  01
...
 (There are infinitely many matrices in the pictured class, but we’ve only got room to show two.) We have proved there is one and only one reduced echelon form matrix in each row equivalence class. So the reduced echelon form is a canonical form∗ for row equivalence: the reduced echelon form matrices are representatives of the classes.
The idea here is that one way to understand a mathematical situation is by being able to classify the cases that can happen. This is a theme in this book and we have seen this several times already. We classified solution sets of linear systems into the no-elements, one-element, and infinitely-many elements cases. We also classified linear systems with the same number of equations as unknowns into the nonsingular and singular cases.
Here, where we are investigating row equivalence, we know that the set of all matrices breaks into the row equivalence classes and we now have a way to put our finger on each of those classes — we can think of the matrices in a class as derived by row operations from the unique reduced echelon form matrix in that class.
Put in more operational terms, uniqueness of reduced echelon form lets us answer questions about the classes by translating them into questions about the representatives. For instance, as promised in this section’s opening, we now can decide whether one matrix can be derived from another by row reduction. We apply the Gauss-Jordan procedure to both and see if they yield the same reduced echelon form.
2.8 Example These matrices are not row equivalent  1 −3   1 −3 
−26 −25
∗ More information on canonical representatives is in the appendix.
 ⋆
⋆
 1 0  01
⋆
⋆
...
 
62 Chapter One. Linear Systems
 because their reduced echelon forms are not equal.
 1−3   10  0001
2.9 Example Any nonsingular 3×3 matrix Gauss-Jordan reduces to this.
1 0 0 0 1 0 001
2.10 Example We can describe all the classes by listing all possible reduced echelon form matrices. Any 2×2 matrix lies in one of these: the class of matrices
row equivalent to this,
the infinitely many classes of matrices row equivalent to one of this type
 1 a  00
where a ∈ R (including a = 0), the class of matrices row equivalent to this,  0 1 
00
and the class of matrices row equivalent to this
 1 0  01
(this is the class of nonsingular 2×2 matrices). Exercises
  2.11 Decide if the matrices are row equivalent.
 1 2  0 1  1 0 21 0 2
(a)48,12 (b)3−11,0210 5 −1 5 2 0 4
2 1 −1 1 0 2   1 1 1  0 3 −1  (c)1 1 0, 0 2 10 (d) −1 2 2 , 2 2 5
4 3 −1
 1 1 1   0 1 2 
(e) 0 0 3 , 1 −1 1
2.12 Which of these matrices are row equivalent to each other?
 0 0  00
Section III. Reduced Echelon Form
63
  13  2 4
 1 5  2 10
(c)
 1−1  3 0
(d)
 2 6  4 10
(a)
(e) (f)
(b)
 01   33 
−10 22
2.13 Produce three other matrices row equivalent to the given one.
 13  012 (a)4−1 (b)111
234
  2.14 Perform Gauss’s Method on this matrix. Express each row of the final matrix as a linear combination of the rows of the starting matrix.
1 2 1 3 −1 0 040
2.15 Describe the matrices in each of the classes represented in Example 2.10. 2.16 Describe all matrices in the row equivalence class of these.
 1 0   1 2   1 1  (a) (b) (c)
002413
2.17 How many row equivalence classes are there?
2.18 Can row equivalence classes contain different-sized matrices? 2.19 How big are the row equivalence classes?
(a) Show that for any matrix of all zeros, the class is finite.
(b) Do any other classes contain only finitely many members?
  2.20 Give two reduced echelon form matrices that have their leading entries in the
same columns, but that are not row equivalent.
  2.21 Show that any two n×n nonsingular matrices are row equivalent. Are any two
singular matrices row equivalent?
  2.22 Describe all of the row equivalence classes containing these.
(a) 2×2 matrices (b) 2×3 matrices (c) 3×2 matrices
(d) 3×3 matrices
2.23 (a) Show that a vector β⃗0 is a linear combination of members of the set
{β⃗1,...,β⃗n } if and only if there is a linear relationship ⃗0 = c0β⃗0 + ··· + cnβ⃗n
where c0 is not zero. (Hint. Watch out for the β⃗ 0 = ⃗0 case.) (b) Use that to simplify the proof of Lemma 2.5.
  2.24 [Trono] Three truck drivers went into a roadside cafe. One truck driver pur- chased four sandwiches, a cup of coffee, and ten doughnuts for $8.45. Another driver purchased three sandwiches, a cup of coffee, and seven doughnuts for $6.30. What did the third truck driver pay for a sandwich, a cup of coffee, and a doughnut?
2.25 The Linear Combination Lemma says which equations can be gotten from Gaussian reduction of a given linear system.
(1) Produce an equation not implied by this system.
3x+4y=8 2x+ y=3
(2) Can any equation be derived from an inconsistent system?
64
Chapter One. Linear Systems
 2.26 [Hoffman & Kunze] Extend the definition of row equivalence to linear systems. Under your definition, do equivalent systems have the same solution set?
2.27 In this matrix
1 2 3 3 0 3 145
the first and second columns add to the third.
(a) Show that remains true under any row operation. (b) Make a conjecture.
(c) Prove that it holds.
Topic
Computer Algebra Systems
The linear systems in this chapter are small enough that their solution by hand is easy. For large systems, including those involving thousands of equations, we need a computer. There are special purpose programs such as LINPACK for this. Also popular are general purpose computer algebra systems including Maple, Mathematica, or MATLAB, and Sage.
For example, in the Topic on Networks, we need to solve this.
i0 − i1 − i2 = 0 i1 −i3 −i5 =0 i2 −i4+i5 =0 i3+ i4 −i6= 0 5i1 +10i3 =10 2i2 +4i4 =10 5i1−2i2 +50i5 =0
Doing this by hand would take time and be error-prone. A computer is better. Here is that system solved with Sage. (There are many ways to do this; the
one here has the advantage of simplicity.)
sage: var('i0,i1,i2,i3,i4,i5,i6')
(i0, i1, i2, i3, i4, i5, i6)
sage: network_system=[i0-i1-i2==0, i1-i3-i5==0,
....: i2-i4+i5==0, i3+i4-i6==0, 5*i1+10*i3==10,
....: 2*i2+4*i4==10, 5*i1-2*i2+50*i5==0]
sage: solve(network_system, i0,i1,i2,i3,i4,i5,i6) [[i0 == (7/3), i1 == (2/3), i2 == (5/3), i3 == (2/3),
i4 == (5/3), i5 == 0, i6 == (7/3)]]
Magic.
Here is the same system solved under Maple. We enter the array of coefficients
and the vector of constants, and then we get the solution.
> A:=array( [[1,-1,-1,0,0,0,0], [0,1,0,-1,0,-1,0],
[0,0,1,0,-1,1,0], [0,0,0,1,1,0,-1], [0,5,0,10,0,0,0],
              
66
Chapter One. Linear Systems
  [0,0,2,0,4,0,0],
[0,5,-2,0,0,50,0]] ); > u:=array( [0,0,0,0,10,10,0] );
> linsolve(A,u); 72525 7
[ -, -, -, -, -, 0, - ] 33333 3
If a system has infinitely many solutions then the program will return a parametrization.
Exercises
1 Use the computer to solve the two problems that opened this chapter. (a) This is the Statics problem.
40h + 15c = 100
25c = 50 + 50h
(b) This is the Chemistry problem.
7h = 7j
8h + 1i = 5j + 2k 1i = 3j
3i = 6j + 1k
2 Use the computer to solve these systems from the first subsection, or conclude ‘many solutions’ or ‘no solutions’.
      (a) 2x+2y=5 (b) −x+y=1 (c) x−3y+ z= 1 x−4y=0 x+y=2 x+ y+2z=14
(e) 4y+z=20(f)2x+z+w=5 2x−2y+z= 0 y −w=−1 x +z=5 3x −z−w=0 x+ y−z=10 4x+y+2z+w= 9
(d) −x− y=1 −3x−3y=2
3 Use the computer to solve these systems from the second subsection. (a)3x+6y=18 (b)x+y= 1 (c) x1 + x3= 4 x+2y= 6 x−y=−1 x1 −x2 +2x3 = 5
4x1 −x2 +5x3 =17
(d) 2a+b−c=2 (e) x+2y−z =3 (f) x +z+w=4 2a +c=3 2x+ y +w=4 2x+y −w=2 a−b =0 x− y+z+w=1 3x+y+z =7
4 What does the computer give for the solution of the general 2×2 system?
ax+ cy=p bx+dy=q
Topic
Accuracy of Computations
Gauss’s Method lends itself to computerization. The code below illustrates. It operates on an n×n matrix named a, doing row combinations using the first row, then the second row, etc.
for(row=1; row<=n-1; row++){
for(row_below=row+1; row_below<=n; row_below++){
multiplier=a[row_below,row]/a[row,row]; for(col=row; col<=n; col++){
a[row_below,col]-=multiplier*a[row,col]; }
} }
This is in the C language. The for(row=1; row<=n-1; row++){ .. } loop initial- izes row at 1 and then iterates while row is less than or equal to n − 1, each time through incrementing row by one with the ++ operation. The other non- obvious language construct is that the -= in the innermost loop has the effect of a[row_below,col]=-1*multiplier*a[row,col]+a[row_below,col].
While that code is a first take on mechanizing Gauss’s Method, it is naive. For one thing, it assumes that the entry in the row,row position is nonzero. So one way that it needs to be extended is to cover the case where finding a zero in that location leads to a row swap or to the conclusion that the matrix is singular.
We could add some if statements to cover those cases but we will instead consider another way in which this code is naive. It is prone to pitfalls arising from the computer’s reliance on floating point arithmetic.
For example, above we have seen that we must handle a singular system as a separate case. But systems that are nearly singular also require care. Consider this one (the extra digits are in the ninth significant place).
         x+2y=3
1.000 000 01x + 2y = 3.000 000 01
(∗)
By eye we easily spot the solution x = 1, y = 1. A computer has more trouble. If it represents real numbers to eight significant places, called single precision, then
68 Chapter One. Linear Systems
 it will represent the second equation internally as 1.000 000 0x + 2y = 3.000 000 0, losing the digits in the ninth place. Instead of reporting the correct solution, this computer will think that the two equations are equal and it will report that the system is singular.
For some intuition about how the computer could come up with something that far off, consider this graph of the system.
(1, 1)
We cannot tell the two lines apart; this system is nearly singular in the sense that the two lines are nearly the same line. This gives the system (∗) the property that a small change in an equation can cause a large change in the solution. For instance, changing the 3.000 000 01 to 3.000 000 03 changes the intersection point from (1, 1) to (3, 0). The solution changes radically depending on the ninth digit, which explains why an eight-place computer has trouble. A problem that is very sensitive to inaccuracy or uncertainties in the input values is ill-conditioned.
The above example gives one way in which a system can be difficult to solve on a computer. It has the advantage that the picture of nearly-equal lines gives a memorable insight into one way for numerical difficulties to happen. Unfortunately this insight isn’t useful when we wish to solve some large system. We typically will not understand the geometry of an arbitrary large system.
There are other ways that a computer’s results may be unreliable, besides that the angle between some of the linear surfaces is small. For example, consider this system (from [Hamming]).
 0.001x+y=1 x−y=0
(∗∗)
The second equation gives x = y, so x = y = 1/1.001 and thus both variables have values that are just less than 1. A computer using two digits represents the system internally in this way (we will do this example in two-digit floating point arithmetic for clarity but inventing a similar one with eight or more digits is easy).
(1.0×10−3)·x+(1.0×100)·y=1.0×100 (1.0×100)·x−(1.0×100)·y=0.0×100
The row reduction step −1000ρ1 + ρ2 produces a second equation −1001y = −1000, which this computer rounds to two places as (−1.0×103)y = −1.0×103.
Topic: Accuracy of Computations 69
 The computer decides from the second equation that y = 1 and with that it concludes from the first equation that x = 0. The y value is close but the x is bad — the ratio of the actual answer to the computer’s answer is infinite. In short, another cause of unreliable output is the computer’s reliance on floating point arithmetic when the system-solving code leads to using leading entries that are small.
An experienced programmer may respond by using double precision, which retains sixteen significant digits, or perhaps using some even larger size. This will indeed solve many problems. However, double precision has greater memory requirements and besides we can obviously tweak the above to give the same trouble in the seventeenth digit, so double precision isn’t a panacea. We need a strategy to minimize numerical trouble as well as some guidance about how far we can trust the reported solutions.
A basic improvement on the naive code above is to not determine the factor to use for row combinations by simply taking the entry in the row,row position, but rather to look at all of the entries in the row column below the row,row entry and take one that is likely to give reliable results because it is not too small. This is partial pivoting.
For example, to solve the troublesome system (∗∗) above we start by looking at both equations for a best entry to use, and take the 1 in the second equation as more likely to give good results. The combination step of −.001ρ2 + ρ1 gives a first equation of 1.001y = 1, which the computer will represent as (1.0 × 100)y = 1.0 × 100, leading to the conclusion that y = 1 and, after back- substitution, that x = 1, both of which are close to right. We can adapt the code from above to do this.
for(row=1; row<=n-1; row++){
/* find the largest entry in this column (in row max) */
max=row;
for(row_below=row+1; row_below<=n; row_below++){
if (abs(a[row_below,row]) > abs(a[max,row])); max = row_below;
}
/* swap rows to move that best entry up */
for(col=row; col<=n; col++){ temp=a[row,col]; a[row,col]=a[max,col]; a[max,col]=temp;
}
/* proceed as before */
for(row_below=row+1; row_below<=n; row_below++){ multiplier=a[row_below,row]/a[row,row];
for(col=row; col<=n; col++){ a[row_below,col]-=multiplier*a[row,col];
} }
}
A full analysis of the best way to implement Gauss’s Method is beyond the scope of this book (see [Wilkinson 1965]), but the method recommended by
                     
70 Chapter One. Linear Systems
 most experts first finds the best entry among the candidates and then scales it to a number that is less likely to give trouble. This is scaled partial pivoting.
In addition to returning a result that is likely to be reliable, most well-done code will return a conditioning number that describes the factor by which uncertainties in the input numbers could be magnified to become inaccuracies in the results returned (see [Rice]).
The lesson is that just because Gauss’s Method always works in theory, and just because computer code correctly implements that method, doesn’t mean that the answer is reliable. In practice, always use a package where experts have worked hard to counter what can go wrong.
Exercises
1 Using two decimal places, add 253 and 2/3.
2 This intersect-the-lines problem contrasts with the example discussed above.
   (1, 1)
x+2y=3 3x−2y=1
Illustrate that in this system some small change in the numbers will produce only a small change in the solution by changing the constant in the bottom equation to 1.008 and solving. Compare it to the solution of the unchanged system.
3 Consider this system ([Rice]).
0.000 3x + 1.556y = 1.559
0.345 4x − 2.346y = 1.108
(a) Solve it. (b) Solve it by rounding at each step to four digits.
4 Rounding inside the computer often has an effect on the result. Assume that your
machine has eight significant digits.
(a) Show that the machine will compute (2/3) + ((2/3) − (1/3)) as unequal to
((2/3) + (2/3)) − (1/3). Thus, computer arithmetic is not associative.
(b) Compare the computer’s version of (1/3)x + y = 0 and (2/3)x + 2y = 0. Is
twice the first equation the same as the second?
5 Ill-conditioning is not only dependent on the matrix of coefficients. This example
[Hamming] shows that it can arise from an interaction between the left and right sides of the system. Let ε be a small real.
3x+2y+z= 6 2x+2εy+2εz=2+4ε x+2εy− εz= 1+ε
(a) Solve the system by hand. Notice that the ε’s divide out only because there is an exact cancellation of the integer parts on the right side as well as on the left. (b) Solve the system by hand, rounding to two decimal places, and with ε = 0.001.
 
Topic
Analyzing Networks
The diagram below shows some of a car’s electrical network. The battery is on the left, drawn as stacked line segments. The wires are lines, shown straight and with sharp right angles for neatness. Each light is a circle enclosing a loop.
       Brake Actuated Switch
Light Switch
Dimmer Lo Hi
Dome Light
Door Actuated Switch
  Off
         12V
            LRLRLRLRLR
Brake Parking Rear Headlights Lights Lights Lights
 The designer of such a network needs to answer questions such as: how much electricity flows when both the hi-beam headlights and the brake lights are on? We will use linear systems to analyze simple electrical networks.
For the analysis we need two facts about electricity and two facts about electrical networks.
The first fact is that a battery is like a pump, providing a force impelling the electricity to flow, if there is a path. We say that the battery provides a potential. For instance, when the driver steps on the brake then the switch makes contact and so makes a circuit on the left side of the diagram, which includes the brake lights. Once the circuit exists, the battery’s force creates a current flowing through that circuit, lighting the lights.
The second electrical fact is that in some kinds of network components the amount of flow is proportional to the force provided by the battery. That is, for each such component there is a number, it’s resistance, such that the potential
72 Chapter One. Linear Systems
 is equal to the flow times the resistance. Potential is measured in volts, the rate of flow is in amperes, and resistance to the flow is in ohms; these units are defined so that volts = amperes · ohms.
Components with this property, that the voltage-amperage response curve is a line through the origin, are resistors. For example, if a resistor measures 2 ohms then wiring it to a 12 volt battery results in a flow of 6 amperes. Conversely, if electrical current of 2 amperes flows through that resistor then there must be a 4 volt potential difference between it’s ends. This is the voltage drop across the resistor. One way to think of the electrical circuits that we consider here is that the battery provides a voltage rise while the other components are voltage drops.
The facts that we need about networks are Kirchoff’s Current Law, that for any point in a network the flow in equals the flow out and Kirchoff’s Voltage Law, that around any circuit the total drop equals the total rise.
We start with the network below. It has a battery that provides the potential to flow and three resistors, shown as zig-zags. When components are wired one after another, as here, they are in series.
   20 volt potential
2 ohm resistance
3 ohm resistance
5 ohm resistance
    By Kirchoff’s Voltage Law, because the voltage rise is 20 volts, the total voltage drop must also be 20 volts. Since the resistance from start to finish is 10 ohms (the resistance of the wire connecting the components is negligible), the current is (20/10) = 2 amperes. Now, by Kirchhoff’s Current Law, there are 2 amperes through each resistor. Therefore the voltage drops are: 4 volts across the 2 ohm resistor, 10 volts across the 5 ohm resistor, and 6 volts across the 3 ohm resistor.
The prior network is simple enough that we didn’t use a linear system but the next one is more complicated. Here the resistors are in parallel.
20 volt 12 ohm 8 ohm
We begin by labeling the branches as below. Let the current through the left branch of the parallel portion be i1 and that through the right branch be i2,
     
Topic: Analyzing Networks 73
 and also let the current through the battery be i0. Note that we don’t need to know the actual direction of flow — if current flows in the direction opposite to our arrow then we will get a negative number in the solution.
↑ i0 i1 ↓ ↓ i2
The Current Law, applied to the split point in the upper right, gives that i0 = i1 + i2. Applied to the split point lower right it gives i1 + i2 = i0. In the circuit that loops out of the top of the battery, down the left branch of the parallel portion, and back into the bottom of the battery, the voltage rise is 20 while the voltage drop is i1 · 12, so the Voltage Law gives that 12i1 = 20. Similarly, the circuit from the battery to the right branch and back to the battery gives that 8i2 = 20. And, in the circuit that simply loops around in the left and right branches of the parallel portion (we arbitrarily take the direction of clockwise), there is a voltage rise of 0 and a voltage drop of 8i2 − 12i1 so 8i2 − 12i1 = 0.
 i0 − −i0 +
i1 − i2 = 0
i1 + i2 = 0 12i1 = 20 8i2 = 20 −12i1+8i2= 0
The solution is i0 = 25/6, i1 = 5/3, and i2 = 5/2, all in amperes. (Incidentally, this illustrates that redundant equations can arise in practice.)
Kirchhoff’s laws can establish the electrical properties of very complex net- works. The next diagram shows five resistors, whose values are in ohms, wired in series-parallel.
   10 volt
52 50
10 4
       This is a Wheatstone bridge (see Exercise 3). To analyze it, we can place the arrows in this way.
74
Chapter One. Linear Systems
  ↑ i0
i5 →
i3 ↘
Kirchhoff’s Current Law, applied to the top node, the left node, the right node,
and the bottom node gives these.
i0 = i1 + i2
i1 = i3 + i5 i2 + i5 = i4
i3 + i4 = i0
Kirchhoff’s Voltage Law, applied to the inside loop (the i0 to i1 to i3 to i0 loop),
the outside loop, and the upper loop not involving the battery, gives these.
5i1 + 10i3 = 10 2i2 + 4i4 = 10
5i1 +50i5 −2i2 =0
Those suffice to determine the solution i0 = 7/3, i1 = 2/3, i2 = 5/3, i3 = 2/3, i4 =5/3,andi5 =0.
We can understand many kinds of networks in this way. For instance, the exercises analyze some networks of streets.
Exercises
1 Calculate the amperages in each part of each network. (a) This is a simple network.
i1 ↙
↘ i2
↙ i4
   9 volt
2 ohm
3 ohm
   2 ohm
(b) Compare this one with the parallel case discussed above.
3 ohm
2 ohm
      9 volt 2 ohm
2 ohm
     
Topic: Analyzing Networks
75
 (c) This is a reasonably complicated network. 3 ohm
3 ohm
2 ohm
       9 volt 3 ohm
2 ohm
2 ohm
4 ohm
      2 In the first network that we analyzed, with the three resistors in series, we just added to get that they acted together like a single resistor of 10 ohms. We can do a similar thing for parallel circuits. In the second circuit analyzed,
20 volt 12 ohm 8 ohm
the electric current through the battery is 25/6 amperes. Thus, the parallel portion is equivalent to a single resistor of 20/(25/6) = 4.8 ohms.
(a) What is the equivalent resistance if we change the 12 ohm resistor to 5 ohms? (b) What is the equivalent resistance if the two are each 8 ohms?
(c) Find the formula for the equivalent resistance if the two resistors in parallel
      are r1 ohms and r2 ohms.
3 A Wheatstone bridge is used to measure resistance.
r1
r2
rg
r3
r4
          Show that in this circuit if the current flowing through rg is zero then r4 = r2r3/r1. (To operate the device, put the unknown resistance at r4. At rg is a meter that shows the current. We vary the three resistances r1, r2, and r3 —typically they each have a calibrated knob — until the current in the middle reads 0. Then the equation gives the value of r4.)
4 Consider this traffic circle.
Main Street
North Avenue
Pier Boulevard
   
76
Chapter One. Linear Systems
 This is the traffic volume, in units of cars per ten minutes.
North Pier Main into 100 150 25 out of 75 150 50
We can set up equations to model how the traffic flows.
(a) Adapt Kirchhoff’s Current Law to this circumstance. Is it a reasonable
modeling assumption?
(b) Label the three between-road arcs in the circle with a variable. Using the
(adapted) Current Law, for each of the three in-out intersections state an equation
describing the traffic flow at that node.
(c) Solve that system.
(d) Interpret your solution.
(e) Restate the Voltage Law for this circumstance. How reasonable is it?
  5 This is a network of streets.
Willow
west
Shelburne St
Winooski Ave
east
 Jay Ln
We can observe the hourly flow of cars into this network’s entrances, and out of its exits.
east Winooski west Winooski Willow Jay Shelburne into 80 50 65 – 40 outof 30 5 70 55 75
(Note that to reach Jay a car must enter the network via some other road first, which is why there is no ‘into Jay’ entry in the table. Note also that over a long period of time, the total in must approximately equal the total out, which is why both rows add to 235 cars.) Once inside the network, the traffic may flow in different ways, perhaps filling Willow and leaving Jay mostly empty, or perhaps flowing in some other way. Kirchhoff’s Laws give the limits on that freedom.
(a) Determine the restrictions on the flow inside this network of streets by setting up a variable for each block, establishing the equations, and solving them. Notice that some streets are one-way only. (Hint: this will not yield a unique solution, since traffic can flow through this network in various ways; you should get at least one free variable.)
(b) Suppose that someone proposes construction for Winooski Avenue East be- tween Willow and Jay, and traffic on that block will be reduced. What is the least amount of traffic flow that can we can allow on that block without disrupting the hourly flow into and out of the network?
  
Chapter Two
Vector Spaces
The first chapter finished with a fair understanding of how Gauss’s Method solves a linear system. It systematically takes linear combinations of the rows. Here we move to a general study of linear combinations.
We need a setting. At times in the first chapter we’ve combined vectors from R2, at other times vectors from R3, and at other times vectors from higher- dimensional spaces. So our first impulse might be to work in Rn, leaving n unspecified. This would have the advantage that any of the results would hold for R2 and for R3 and for many other spaces, simultaneously.
But if having the results apply to many spaces at once is advantageous then sticking only to Rn’s is restrictive. We’d like our results to apply to combinations of row vectors, as in the final section of the first chapter. We’ve even seen some spaces that are not simply a collection of all of the same-sized column vectors or row vectors. For instance, we’ve seen a homogeneous system’s solution set that is a plane inside of R3. This set is a closed system in that a linear combination of these solutions is also a solution. But it does not contain all of the three-tall column vectors, only some of them.
We want the results about linear combinations to apply anywhere that linear combinations make sense. We shall call any such set a vector space. Our results, instead of being phrased as “Whenever we have a collection in which we can sensibly take linear combinations . . . ”, will be stated “In any vector space . . . ”
Such a statement describes at once what happens in many spaces. To understand the advantages of moving from studying a single space to studying a class of spaces, consider this analogy. Imagine that the government made laws one person at a time: “Leslie Jones can’t jay walk.” That would be bad; statements have the virtue of economy when they apply to many cases at once. Or suppose that they said, “Kim Ke must stop when passing an accident.” Contrast that with, “Any doctor must stop when passing an accident.” More general statements, in some ways, are clearer.
 
78 Chapter Two. Vector Spaces I Definition of Vector Space
We shall study structures with two operations, an addition and a scalar multi- plication, that are subject to some simple conditions. We will reflect more on the conditions later but on first reading notice how reasonable they are. For instance, surely any operation that can be called an addition (e.g., column vector addition, row vector addition, or real number addition) will satisfy conditions (1) through (5) below.
I.1 Definition and Examples
  1.1 Definition A vector space (over R) consists of a set V along with two operations‘+’and‘·’subjecttotheconditionsthatforallvectors⃗v,w⃗,⃗u∈V and all scalars r, s ∈ R:
(1) the set V is closed under vector addition, that is, ⃗v + w⃗ ∈ V
(2) vector addition is commutative, ⃗v + w⃗ = w⃗ + ⃗v
(3) vector addition is associative, (⃗v + w⃗ ) + ⃗u = ⃗v + (w⃗ + ⃗u)
(4) thereisazerovector⃗0∈Vsuchthat⃗v+⃗0=⃗v forall⃗v∈V
(5) each ⃗v ∈ V has an additive inverse w⃗ ∈ V such that w⃗ + ⃗v = ⃗0
(6) the set V is closed under scalar multiplication, that is, r · ⃗v ∈ V
(7) addition of scalars distributes over scalar multiplication, (r+s)·⃗v = r·⃗v+s·⃗v
(8) scalar multiplication distributes over vector addition, r·(⃗v+w⃗ ) = r·⃗v+r·w⃗
(9) ordinary multiplication of scalars associates with scalar multiplication,
(rs) · ⃗v = r · (s · ⃗v)
(10) multiplication by the scalar 1 is the identity operation, 1 · ⃗v = ⃗v.
1.2 Remark The definition involves two kinds of addition and two kinds of multiplication, and so may at first seem confused. For instance, in condition (7) the ‘+’ on the left is addition of two real numbers while the ‘+’ on the right is addition of two vectors in V. These expressions aren’t ambiguous because of context; for example, r and s are real numbers so ‘r + s’ can only mean real number addition. In the same way, item (9)’s left side ‘rs’ is ordinary real number multiplication, while its right side ‘s · ⃗v’ is the scalar multiplication defined for this vector space.
The best way to understand the definition is to go through the examples below and for each, check all ten conditions. The first example includes that check, written out at length. Use it as a model for the others. Especially important are the closure conditions, (1) and (6). They specify that the addition and scalar
Section I. Definition of Vector Space 79
 multiplication operations are always sensible — they are defined for every pair of vectors and every scalar and vector, and the result of the operation is a member of the set.
1.3 Example This subset of R2 is a line through the origin.  x 
L={ y |y=3x}
We shall verify that it is a a vector space, under the usual meaning of ‘+’ and ‘·’.
 x   x   x+x   x   rx  1+2=12r·=
y1y2y1+y2 yry
These operations are just the ones of R2, reused on its subset L. We say that L inherits these operations from R3.
We shall check all ten conditions. The paragraph having to do with addition
has five conditions. For condition (1), closure under addition, suppose that we
start with two vectors from the line L,
 x    x  
⃗v1=1 ⃗v2=2 y1 y2
so that they satisfy the restrictions that y1 = 3x1 and y2 = 3x2. Their sum
 x +x   ⃗v1+⃗v2= 1 2 y1 + y2
is also a member of the line L because the fact that its second component is three times its first y1 + y2 = 3(x1 + x2) follows from the restrictions on ⃗v1 and ⃗v2. For (2), that addition of vectors commutes, just compare
 x +x   ⃗v 1 + ⃗v 2 = 1 2 y1 + y2
⃗v 2 + ⃗v 1 =
 x +x   2 1
y2 + y1
and note that they are equal since their entries are real numbers and real numbers commute. (That the vectors satisfy the restriction of lying in the line is not relevant for this condition; they commute just because all vectors in the plane commute.) Condition (3), associativity of vector addition, is similar.
 x   x   x   (x+x)+x  (1+2)+3=123
y1 y2 y3
(y1 +y2)+y3  x1 +(x2 +x3) 
= y1+(y2+y3)
 x   x   x 
=1+(2+3) y1 y2 y3
80 Chapter Two. Vector Spaces For the fourth condition we must produce a vector that acts as the zero element.
The vector of zeroes will do.
 x   0   x 
y+0=y
Note that ⃗0 ∈ L as its second component is triple its first. For (5), that given any ⃗v ∈ L we can produce an additive inverse, we have
 −x   x   0 
−y + y = 0
and so the vector −⃗v is the desired inverse. As for the prior condition, observe here that if ⃗v ∈ L, so that y = 3x, then −⃗v ∈ L also, since −y = 3(−x).
The checks for the five conditions having to do with scalar multiplication are similar. For (6), closure under scalar multiplication, suppose that r ∈ R and
 ⃗v ∈ L
 x  ⃗v= y
so that it satisfies the restriction y = 3x. Then  x   rx 
r·⃗v=r· y = ry
is also a member of L: the fact that its second component is three times its first
ry = 3(rx) follows from the restriction on ⃗v. Next, this checks (7).  x   (r + s)x   rx + sx   x   x 
(r+s)· y = (r+s)y = ry+sy =r· y +s· y For (8) we have this.
 x    x    r(x +x )   rx +rx    x    x   r·(1+2)=12=12=r·1+r·2 y1 y2 r(y1 +y2) ry1 +ry2 y1 y2
The ninth
 x   (rs)x   r(sx)   x  (rs)· y = (rs)y = r(sy) =r·(s· y )
and tenth conditions are also straightforward.
 x   1x   x  1· y = 1y = y
Section I. Definition of Vector Space 81
 1.4 Example The whole plane, the set R2, is a vector space if the operations ‘+’ and ‘·’ have their usual meaning.
 x   x   x+x   x   rx  1+2=12r·=
y1y2y1+y2 yry
We shall check just two of the conditions, the closure conditions. For (1) observe that the result of the vector sum
 x   x   x+x  1+2=12 y1 y2 y1 + y2
is a column array with two real entries, and so is a member of the plane R2. In contrast with the prior example, here there is no restriction on the vectors that we must check.
Condition (6) is similar. The vector
 x   rx  r· y = ry
has two real entries, and so is a member of R2.
In a similar way, each Rn is a vector space with the usual operations of vector addition and scalar multiplication. (In R1, we usually do not write the members as column vectors, i.e., we usually do not write ‘(π)’. Instead we just write ‘π’.)
1.5 Example Example 1.3 gives a subset of R2 that is a vector space. For contrast, consider the set of two-tall columns with entries that are integers, under the same operations of component-wise addition and scalar multiplication. This is a subset of R2 but it is not a vector space: it is not closed under scalar multiplication, that is, it does not satisfy condition (6). For instance, on the left below is a vector with integer entries, and a scalar.
 4   2  0.5· 3 = 1.5
On the right is a column vector that is not a member of the set, since its entries are not all integers.
1.6 Example The one-element set
0
0 { } 0
0
82
is a vector space under the operations
0 0 0
Chapter Two. Vector Spaces
0 0
 0 0 += r·= 0 0 0 0 0
0 0 0 00000
that it inherits from R4.
A vector space must have at least one element, its zero vector. Thus a
one-element vector space is the smallest possible.
1.7 Definition A one-element vector space is a trivial space.
The examples so far involve sets of column vectors with the usual operations. But vector spaces need not be collections of column vectors, or even of row vectors. Below are some other types of vector spaces. The term ‘vector space’ does not mean ‘collection of columns of reals’. It means something more like ‘collection in which any linear combination is sensible’.
1.8Example ConsiderP3={a0+a1x+a2x2+a3x3|a0,...,a3∈R},theset of polynomials of degree three or less (in this book, we’ll take constant polyno- mials, including the zero polynomial, to be of degree zero). It is a vector space under the operations
(a0 +a1x+a2x2 +a3x3)+(b0 +b1x+b2x2 +b3x3)
=(a0 +b0)+(a1 +b1)x+(a2 +b2)x2 +(a3 +b3)x3
and
(the verification is easy). This vector space is worthy of attention because these are the polynomial operations familiar from high school algebra. For instance, 3·(1−2x+3x2 −4x3)−2·(2−3x+x2 −(1/2)x3)=−1+7x2 −11x3.
Although this space is not a subset of any Rn, there is a sense in which we can think of P3 as “the same” as R4. If we identify these two space’s elements in this way
 r · (a0 + a1x + a2x2 + a3x3) = (ra0) + (ra1)x + (ra2)x2 + (ra3)x3
a0 + a1x + a2x2 + a3x3
corresponds to
a0  a1 
  a2 
a3
Section I. Definition of Vector Space 83 then the operations also correspond. Here is an example of corresponding
 additions.
1−2x+0x2 +1x3 + 2+3x+7x2−4x3 3+1x+7x2 −3x3
1 2 3
−2 3 1 correspondsto  + =  0 7 7
1 −4 −3
 Things we are thinking of as “the same” add to “the same” sum. Chapter Three makes precise this idea of vector space correspondence. For now we shall just leave it as an intuition.
In general we write Pn for the vector space of polynomials of degree n or less {a0 +a1x+a2x2 +···+anxn |a0,...,an ∈R}, under the operations of the usual polynomial addition and scalar multiplication. We will often use these spaces as examples.
1.9 Example The set M2×2 of 2×2 matrices with real number entries is a vector space under the natural entry-by-entry operations.
 a b   w x   a+w b+x   a b   ra rb  c d + y z = c+y d+z r· c d = rc rd
As in the prior example, we can think of this space as “the same” as R4.
We write Mn×m for the vector space of n×m matrices under the natural operations of matrix addition and scalar multiplication. As with the polynomial spaces, we will often use these as examples.
1.10 Example The set {f | f: N → R} of all real-valued functions of one natural number variable is a vector space under the operations
(f1 +f2)(n)=f1(n)+f2(n) (r·f)(n)=rf(n)
so that if, for example, f1(n) = n2 + 2 sin(n) and f2(n) = − sin(n) + 0.5 then (f1 +2f2)(n)=n2 +1.
We can view this space as a generalization of Example 1.4 — instead of 2-tall vectors, these functions are like infinitely-tall vectors.
 n f(n)=n2+1 01 12 25
3 10
. .
1 2
 corresponds to 5 10
. .
      
84 Chapter Two. Vector Spaces
 Addition and scalar multiplication are component-wise, as in Example 1.4. (We can formalize “infinitely-tall” by saying that it means an infinite sequence, or that it means a function from N to R.)
1.11 Example The set of polynomials with real coefficients {a0+a1x+···+anxn |n∈Nanda0,...,an ∈R}
makes a vector space when given the natural ‘+’
(a0 +a1x+···+anxn)+(b0 +b1x+···+bnxn)
=(a0 +b0)+(a1 +b1)x+···+(an +bn)xn r·(a0 +a1x+...anxn)=(ra0)+(ra1)x+...(ran)xn
This space differs from the space P3 of Example 1.8. This space contains not just degree three polynomials, but degree thirty polynomials and degree three hundred polynomials, too. Each individual polynomial of course is of a finite degree, but the set has no single bound on the degree of all of its members.
We can think of this example, like the prior one, in terms of infinite-tuples. For instance, we can think of 1 + 3x + 5x2 as corresponding to (1, 3, 5, 0, 0, . . .). However, this space differs from the one in Example 1.10. Here, each member of the set has a finite degree, that is, under the correspondence there is no element from this space matching (1, 2, 5, 10, . . . ). Vectors in this space correspond to infinite-tuples that end in zeroes.
1.12 Example The set {f | f: R → R} of all real-valued functions of one real variable is a vector space under these.
(f1 +f2)(x)=f1(x)+f2(x) (r·f)(x)=rf(x)
The difference between this and Example 1.10 is the domain of the functions.
1.13Example ThesetF={acosθ+bsinθ|a,b∈R}ofreal-valuedfunctionsof the real variable θ is a vector space under the operations
(a1cosθ+b1sinθ)+(a2cosθ+b2sinθ)=(a1 +a2)cosθ+(b1 +b2)sinθ and
r · (a cos θ + b sin θ) = (ra) cos θ + (rb) sin θ
inherited from the space in the prior example. (We can think of F as “the same” as R2 in that a cos θ + b sin θ corresponds to the vector with components a and b.)
and ‘·’.
Section I. Definition of Vector Space 85
 1.14 Example The set
{f: R → R | d2f + f = 0} dx2
 is a vector space under the, by now natural, interpretation.
(f + g) (x) = f(x) + g(x) (r · f) (x) = r f(x) In particular, notice that closure is a consequence
   and
d2 (f + g) + (f + g) = ( d2 f + f) + ( d2 g + g) dx2 dx2 dx2
d2(rf) + (rf) = r( d2f + f) dx2 dx2
  of basic Calculus. This turns out to equal the space from the prior example — functions satisfying this differential equation have the form a cos θ + b sin θ — but this description suggests an extension to solutions sets of other differential equations.
1.15 Example The set of solutions of a homogeneous linear system in n variables is a vector space under the operations inherited from Rn. For example, for closure under addition consider a typical equation in that system c1 x1 + · · · + cn xn = 0 and suppose that both these vectors
v1 w1 . .
⃗v =  . .  w⃗ =  . .  vn wn
satisfy the equation. Then their sum ⃗v + w⃗ also satisfies that equation: c1(v1 + w1)+···+cn(vn +wn) = (c1v1 +···+cnvn)+(c1w1 +···+cnwn) = 0. The checks of the other vector space conditions are just as routine.
We often omit the multiplication symbol ‘·’ between the scalar and the vector. We distinguish the multiplication in c1v1 from that in r⃗v by context, since if both multiplicands are real numbers then it must be real-real multiplication while if one is a vector then it must be scalar-vector multiplication.
Example 1.15 has brought us full circle since it is one of our motivating examples. Now, with some feel for the kinds of structures that satisfy the definition of a vector space, we can reflect on that definition. For example, why specify in the definition the condition that 1 · ⃗v = ⃗v but not a condition that 0 · ⃗v = ⃗0 ?
One answer is that this is just a definition — it gives the rules and you need to follow those rules to continue.
86 Chapter Two. Vector Spaces
 Another answer is perhaps more satisfying. People in this area have worked to develop the right balance of power and generality. This definition is shaped so that it contains the conditions needed to prove all of the interesting and important properties of spaces of linear combinations. As we proceed, we shall derive all of the properties natural to collections of linear combinations from the conditions given in the definition.
The next result is an example. We do not need to include these properties in the definition of vector space because they follow from the properties already listed there.
Proof For(1)notethat⃗v=(1+0)·⃗v=⃗v+(0·⃗v). Addtobothsidesthe additive inverse of ⃗v, the vector w⃗ such that w⃗ + ⃗v = ⃗0.
w⃗ +⃗v=w⃗ +⃗v+0·⃗v ⃗0 = ⃗0 + 0 · ⃗v
⃗0 = 0 · ⃗v
Item (2) is easy: (−1·⃗v)+⃗v = (−1+1)·⃗v = 0·⃗v =⃗0. For (3), r·⃗0 = r·(0·⃗0) =
(r · 0) · ⃗0 = ⃗0 will do. QED The second item shows that we can write the additive inverse of ⃗v as ‘−⃗v ’
without worrying about any confusion with (−1) · ⃗v.
A recap: our study in Chapter One of Gaussian reduction led us to consider collections of linear combinations. So in this chapter we have defined a vector space to be a structure in which we can form such combinations, subject to simple conditions on the addition and scalar multiplication operations. In a phrase: vector spaces are the right context in which to study linearity.
From the fact that it forms a whole chapter, and especially because that chapter is the first one, a reader could suppose that our purpose in this book is the study of linear systems. The truth is that we will not so much use vector spaces in the study of linear systems as we instead have linear systems start us on the study of vector spaces. The wide variety of examples from this subsection shows that the study of vector spaces is interesting and important in its own right. Linear systems won’t go away. But from now on our primary objects of study will be vector spaces.
Exercises
1.17 Name the zero vector for each of these vector spaces.
(a) The space of degree three polynomials under the natural operations.
 1.16 Lemma In any vector space V, for any⃗v ∈ V and r ∈ R, we have (1) 0·⃗v =⃗0, (2) (−1 · ⃗v) + ⃗v = ⃗0, and (3) r · ⃗0 = ⃗0.
Section I. Definition of Vector Space 87
 (b) The space of 2×4 matrices.
(c) The space {f: [0..1] → R | f is continuous}.
(d) The space of real-valued functions of one natural number variable.
  1.18 Find the additive inverse, in the vector space, of the vector. (a) In P3, the vector −3−2x+x2.
(b) In the space 2×2,
 1 −1  03.
(c) In { aex + be−x | a, b ∈ R }, the space of functions of the real variable x under the natural operations, the vector 3ex − 2e−x.
  1.19 For each, list three elements and then show it is a vector space.
(a) The set of linear polynomials P1 = {a0 +a1x|a0,a1 ∈R} under the usual
polynomial addition and scalar multiplication operations.
(b) The set of linear polynomials {a0 + a1x | a0 − 2a1 = 0}, under the usual poly-
nomial addition and scalar multiplication operations.
Hint. Use Example 1.3 as a guide. Most of the ten conditions are just verifications.
1.20 For each, list three elements and then show it is a vector space.
(a) The set of 2×2 matrices with real entries under the usual matrix operations. (b) The set of 2×2 matrices with real entries where the 2,1 entry is zero, under
the usual matrix operations.
  1.21 For each, list three elements and then show it is a vector space.
(a) The set of three-component row vectors with their usual operations. (b) The set
x y
{z∈R4 |x+y−z+w=0} 
w
under the operations inherited from R4.
  1.22 Show that each of these is not a vector space. (Hint. Check closure by listing
two members of each set and trying some operations on them.) (a) Under the operations inherited from R3, this set
x
{y∈R3 |x+y+z=1} z
(b) Under the operations inherited from R3, this set x
{y∈R3 |x2 +y2 +z2 =1} z
(c) Under the usual matrix operations,  a 1 
(d) Under the usual polynomial operations,
{a0 +a1x+a2x2 |a0,a1,a2 ∈R+}
where R+ is the set of reals greater than zero
{ b c |a,b,c∈R}
88 Chapter Two. Vector Spaces (e) Under the inherited operations,
 x  2
{ y ∈R |x+3y=4and2x−y=3and6x+4y=10}
1.23 Define addition and scalar multiplication operations to make the complex numbers a vector space over R.
1.24 Is the set of rational numbers a vector space over R under the usual addition and scalar multiplication operations?
1.25 Show that the set of linear combinations of the variables x, y, z is a vector space under the natural addition and scalar multiplication operations.
1.26 Prove that this is not a vector space: the set of two-tall column vectors with real entries subject to these operations.
  x1   x2   x1 −x2   x   rx  y + y = y −y r· y = ry
1212
1.27 Prove or disprove that R3 is a vector space under these operations.
x1  x2  0 x rx (a) y1+y2=0 and ry=ry
z1 z2 0 z rz x1  x2  0 x 0
(b) y1+y2=0 and ry=0 z1 z2 0 z 0
  1.28 For each, decide if it is a vector space; the intended operations are the natural ones.
(a) The diagonal 2×2 matrices
{ 0
0 
b |a,b∈R}
(b) This set of 2×2 matrices
 x x+y 
| x,y ∈ R}
(c) This set
{ x+y y x
 a
y
{z∈R4 |x+y+w=1}

w
(d) The set of functions {f: R → R | df/dx + 2f = 0}
(e) The set of functions {f: R → R | df/dx + 2f = 1}
  1.29 Prove or disprove that this is a vector space: the real-valued functions f of one
real variable such that f(7) = 0.
  1.30 Show that the set R+ of positive reals is a vector space when we interpret ‘x+y’
to mean the product of x and y (so that 2+3 is 6), and we interpret ‘r·x’ as the
r-th power of x.
1.31 Is { (x, y) | x, y ∈ R } a vector space under these operations?
(a) (x1 , y1 ) + (x2 , y2 ) = (x1 + x2 , y1 + y2 ) and r · (x, y) = (rx, y) (b) (x1 , y1 ) + (x2 , y2 ) = (x1 + x2 , y1 + y2 ) and r · (x, y) = (rx, 0)
Section I. Definition of Vector Space 89
 1.32 Prove or disprove that this is a vector space: the set of polynomials of degree greater than or equal to two, along with the zero polynomial.
1.33 At this point “the same” is only an intuition, but nonetheless for each vector space identify the k for which the space is “the same” as Rk.
(a) The 2×3 matrices under the usual operations
(b) The n×m matrices (under their usual operations) (c) This set of 2×2 matrices
 a 0 
{ b c |a,b,c∈R}
(d) This set of 2×2 matrices
{ b c |a+b+c=0}
1.34 Using +⃗ to represent vector addition and ⃗· for scalar multiplication, restate the definition of vector space.
1.35 Prove these.
(a) For any ⃗v ∈ V, if w⃗ ∈ V is an additive inverse of ⃗v, then ⃗v is an additive
inverse of w⃗ . So a vector is an additive inverse of any additive inverse of itself. (b) Vector addition left-cancels: if⃗v,⃗s,⃗t∈V then⃗v+⃗s=⃗v+⃗t implies that⃗s=⃗t. 1.36 The definition of vector spaces does not explicitly say that ⃗0 + ⃗v = ⃗v (it instead
says that ⃗v + ⃗0 = ⃗v). Show that it must nonetheless hold in any vector space.
  1.37 Prove or disprove that this is a vector space: the set of all matrices, under the
usual operations.
1.38 In a vector space every element has an additive inverse. Can some elements
have two or more?
1.39 (a) Prove that every point, line, or plane thru the origin in R3 is a vector
space under the inherited operations. (b) What if it doesn’t contain the origin?
1.40 Using the idea of a vector space we can easily reprove that the solution set of a homogeneous linear system has either one element or infinitely many elements. Assume that ⃗v ∈ V is not ⃗0.
(a) Prove that r · ⃗v = ⃗0 if and only if r = 0.
(b) Prove that r1 ·⃗v=r2 ·⃗v if and only if r1 =r2.
(c) Prove that any nontrivial vector space is infinite.
(d) Use the fact that a nonempty solution set of a homogeneous linear system is
a vector space to draw the conclusion.
1.41 Is this a vector space under the natural operations: the real-valued functions of
one real variable that are differentiable?
1.42 A vector space over the complex numbers C has the same definition as a vector
space over the reals except that scalars are drawn from C instead of from R. Show that each of these is a vector space over the complex numbers. (Recall how complex numbers add and multiply: (a0 + a1i) + (b0 + b1i) = (a0 + b0) + (a1 + b1)i and (a0 + a1i)(b0 + b1i) = (a0b0 − a1b1) + (a0b1 + a1b0)i.)
 a 0 
(a) The set of degree two polynomials with complex coefficients
90
Chapter Two. Vector Spaces
 (b) This set
 0 a 
{ b 0 |a,b∈Canda+b=0+0i}
1.43 Name a property shared by all of the Rn’s but not listed as a requirement for a vector space.
1.44 (a) Prove that for any four vectors ⃗v1 , . . . , ⃗v4 ∈ V we can associate their sum in any way without changing the result.
((⃗v1 +⃗v2)+⃗v3)+⃗v4 =(⃗v1 +(⃗v2 +⃗v3))+⃗v4 =(⃗v1 +⃗v2)+(⃗v3 +⃗v4)
=⃗v1 +((⃗v2 +⃗v3)+⃗v4)=⃗v1 +(⃗v2 +(⃗v3 +⃗v4))
This allows us to write ‘⃗v1 +⃗v2 +⃗v3 +⃗v4’ without ambiguity.
(b) Prove that any two ways of associating a sum of any number of vectors give
the same sum. (Hint. Use induction on the number of vectors.)
1.45 Example 1.5 gives a subset of R2 that is not a vector space, under the obvious operations, because while it is closed under addition, it is not closed under scalar multiplication. Consider the set of vectors in the plane whose components have the same sign or are 0. Show that this set is closed under scalar multiplication but not addition.
I.2 Subspaces and Spanning Sets
In Example 1.3 we saw a vector space that is a subset of R2, a line through the origin. There, the vector space R2 contains inside it another vector space, the line.
2.2 Example This plane through the origin x
P = {  y  | x + y + z = 0 } z
is a subspace of R3. As required by the definition the plane’s operations are inherited from the larger space, that is, vectors add in P as they add in R3
x x x+x 1212
y1 + y2 = y1 + y2 z1 z2 z1 + z2
 2.1 Definition For any vector space, a subspace is a subset that is itself a vector space, under the inherited operations.
Section I. Definition of Vector Space 91
 and scalar multiplication is also the same as in R3. To show that P is a subspace we need only note that it is a subset and then verify that it is a space. We won’t check all ten conditions, just the two closure ones. For closure under addition, note that if the summands satisfy that x1 + y1 + z1 = 0 and x2 +y2 +z2 = 0 then the sum satisfies that (x1 +x2)+(y1 +y2)+(z1 +z2) = (x1 + y1 + z1) + (x2 + y2 + z2) = 0. For closure under scalar multiplication, if x + y + z = 0 then the scalar multiple has rx + ry + rz = r(x + y + z) = 0.
2.3 Example The x-axis in R2 is a subspace, where the addition and scalar multiplication operations are the inherited ones.
 x   x   x+x   x   rx  1+2=12r·=
00000
As in the prior example, to verify directly from the definition that this is a subspace we simply note that it is a subset and then check that it satisfies the conditions in definition of a vector space. For instance the two closure conditions are satisfied: adding two vectors with a second component of zero results in a vector with a second component of zero and multiplying a scalar times a vector with a second component of zero results in a vector with a second component of zero.
2.4 Example Another subspace of R2 is its trivial subspace.  0 
{0}
Any vector space has a trivial subspace {⃗0 }. At the opposite extreme, any vector space has itself for a subspace. A subspace that is not the entire space is a proper subspace.
2.5 Example Vector spaces that are not Rn’s also have subspaces. The space of cubic polynomials {a + bx + cx2 + dx3 | a, b, c, d ∈ R} has a subspace comprised of all linear polynomials {m + nx | m, n ∈ R}.
2.6 Example Another example of a subspace that is not a subset of an Rn followed the definition of a vector space. The space in Example 1.12 of all real-valued functions of one real variable {f | f: R → R} has the subspace in Example 1.14 of functions satisfying the restriction (d2 f/dx2) + f = 0.
2.7 Example The definition requires that the addition and scalar multiplication operations must be the ones inherited from the larger space. The set S = {1} is a subset of R1. And, under the operations 1+1 = 1 and r·1 = 1 the set S is a vector space, specifically, a trivial space. However, S is not a subspace of R1 because those aren’t the inherited operations, since of course R1 has 1 + 1 = 2.
92 Chapter Two. Vector Spaces
 2.8 Example Being vector spaces themselves, subspaces must satisfy the closure conditions. The set R+ is not a subspace of the vector space R1 because with the inherited operations it is not closed under scalar multiplication: if ⃗v = 1 then −1 ·⃗v ̸∈ R+.
The next result says that Example 2.8 is prototypical. The only way that a subset can fail to be a subspace, if it is nonempty and uses the inherited operations, is if it isn’t closed.
 2.9 Lemma For a nonempty subset S of a vector space, under the inherited operations the following are equivalent statements.∗
(1) S is a subspace of that vector space
(2) S is closed under linear combinations of pairs of vectors: for any vectors
⃗s1,⃗s2 ∈ S and scalars r1, r2 the vector r1⃗s1 + r2⃗s2 is in S
(3) S is closed under linear combinations of any number of vectors: for any vectors⃗s1,...,⃗sn ∈Sandscalarsr1,...,rn thevectorr1⃗s1+···+rn⃗sn is
an element of S.
Briefly, a subset is a subspace if and only if it is closed under linear combinations.
Proof ‘The following are equivalent’ means that each pair of statements are equivalent.
(1) ⇐⇒ (2) (2) ⇐⇒ (3) (3) ⇐⇒ (1)
Wewillprovetheequivalencebyestablishingthat(1) =⇒ (3) =⇒ (2) =⇒ (1). This strategy is suggested by the observation that the implications (1) =⇒ (3) and (3) =⇒ (2) are easy and so we need only argue that (2) =⇒ (1).
Assume that S is a nonempty subset of a vector space V that is closed under combinations of pairs of vectors. We will show that S is a vector space by checking the conditions.
The vector space definition has five conditions on addition. First, for closure under addition, if ⃗s1,⃗s2 ∈ S then ⃗s1 +⃗s2 ∈ S, as it is a combination of a pair of vectors and we are assuming that S is closed under those. Second, for any ⃗s1,⃗s2 ∈ S, because addition is inherited from V, the sum ⃗s1 +⃗s2 in S equals the sum ⃗s1 +⃗s2 in V, and that equals the sum ⃗s2 +⃗s1 in V (because V is a vector space, its addition is commutative), and that in turn equals the sum ⃗s2 + ⃗s1 in S. The argument for the third condition is similar to that for the second. For the fourth, consider the zero vector of V and note that closure of S under linear combinations of pairs of vectors gives that 0 · ⃗s + 0 · ⃗s = ⃗0 is an element of S (where ⃗s is any member of the nonempty set S); checking that ⃗0 acts under the inherited operations as the additive identity of S is easy. The fifth condition
 ∗More information on equivalence of statements is in the appendix.
Section I. Definition of Vector Space 93
 is satisfied because for any ⃗s ∈ S, closure under linear combinations of pairs of vectors shows that 0 · ⃗0 + (−1) · ⃗s is an element of S, and it is obviously the additive inverse of ⃗s under the inherited operations. The verifications for the scalar multiplication conditions are similar; see Exercise 34. QED
We will usually verify that a subset is a subspace by checking that it satisfies statement (2).
2.10Remark Atthestartofthischapterweintroducedvectorspacesascollections in which linear combinations “make sense.” Theorem 2.9’s statements (1)-(3) say that we can always make sense of an expression like r1⃗s1 + r2⃗s2 in that the vector described is in the set S.
As a contrast, consider the set T of two-tall vectors whose entries add to a number greater than or equal to zero. Here we cannot just write any linear combination such as 2⃗t1 − 3⃗t2 and be confident the result is an element of T .
Lemma 2.9 suggests that a good way to think of a vector space is as a collection of unrestricted linear combinations. The next two examples take some spaces and recasts their descriptions to be in that form.
2.11 Example We can show that this plane through the origin subset of R3 x
S = {y | x − 2y + z = 0} z
is a subspace under the usual addition and scalar multiplication operations of column vectors by checking that it is nonempty and closed under linear combinations of two vectors. But there is another way. Think of x − 2y + z = 0 as a one-equation linear system and parametrize it by expressing the leading variable in terms of the free variables x = 2y − z.
2y − z 2 −1
S={ y |y,z∈R}={y1+z 0 |y,z∈R} (∗)
z01
Now, to show that this is a subspace consider r1⃗s1 + r2⃗s2. Each ⃗si is a linear combination of the two vectors in (∗) so this is a linear combination of linear combinations.
2 −1 2 −1 r1 ·(y1 1+z1  0 )+r2 ·(y2 1+z2  0 )
0101
The Linear Combination Lemma, Lemma One.III.2.3, shows that the total is a linear combination of the two vectors and so Theorem 2.9’s statement (2) is satisfied.
94 Chapter Two. Vector Spaces 2.12 Example This is a subspace of the 2×2 matrices M2×2.
  a 0 
L={ b c |a+b+c=0}
To parametrize, express the condition as a = −b − c.
 −b−c 0   −1 0   −1 0 
| b, c ∈ R }
As above, we’ve described the subspace as a collection of unrestricted linear combinations. To show it is a subspace, note that a linear combination of vectors from L is a linear combination of linear combinations and so statement (2) is true.
No notation for the span is completely standard. The square brackets used here are common but so are ‘span(S)’ and ‘sp(S)’.
2.14 Remark In Chapter One, after we showed that we can write the solution setofahomogeneouslinearsystemas{c1β⃗1+···+ckβ⃗k |c1,...,ck ∈R},we described that as the set ‘generated’ by the β⃗ ’s. We now call that the span of {β⃗1,...,β⃗k}.
Recall also from that proof that the span of the empty set is defined to be the set {⃗0} because of the convention that a trivial linear combination, a combination of zero-many vectors, adds to ⃗0. Besides, defining the empty set’s span to be the trivial subspace is convenient because it keeps results like the next one from needing exceptions for the empty set.
2.15 Lemma In a vector space, the span of any subset is a subspace.
Proof IfthesubsetSisemptythenbydefinitionitsspanisthetrivialsubspace. If S is not empty then by Lemma 2.9 we need only check that the span [S] is closed under linear combinations of pairs of elements. For a pair of vectors from that span, ⃗v = c1⃗s1 + ··· + cn⃗sn and w⃗ = cn+1⃗sn+1 + ··· + cm⃗sm, a linear combination
p·(c1⃗s1 +···+cn⃗sn)+r·(cn+1⃗sn+1 +···+cm⃗sm)
=pc1⃗s1 +···+pcn⃗sn +rcn+1⃗sn+1 +···+rcm⃗sm
L = { b c | b, c ∈ R } = { b 1 0 + c 0 1
 2.13 Definition The span (or linear closure) of a nonempty subset S of a vector space is the set of all linear combinations of vectors from S.
[S]={c1⃗s1+···+cn⃗sn |c1,...,cn ∈Rand⃗s1,...,⃗sn ∈S} The span of the empty subset of a vector space is its trivial subspace.
 
Section I. Definition of Vector Space 95
 is a linear combination of elements of S and so is an element of [S] (possibly some of the ⃗si’s from ⃗v equal some of the ⃗sj’s from w⃗ but that does not matter). QED
The converse of the lemma holds: any subspace is the span of some set, because a subspace is obviously the span of itself, the set of all of its members. Thus a subset of a vector space is a subspace if and only if it is a span. This fits the intuition that a good way to think of a vector space is as a collection in which linear combinations are sensible.
Taken together, Lemma 2.9 and Lemma 2.15 show that the span of a subset S of a vector space is the smallest subspace containing all of the members of S.
2.16 Example In any vector space V, for any vector ⃗v ∈ V, the set {r ·⃗v | r ∈ R} is a subspace of V. For instance, for any vector ⃗v ∈ R3 the line through the origin containing that vector {k⃗v | k ∈ R} is a subspace of R3. This is true even if ⃗v is the zero vector, in which case it is the degenerate line, the trivial subspace.
2.17 Example The span of this set is all of R2.  1    1  
{ 1 , −1 }
We know that the span is some subspace of R2. To check that it is all of R2 we must show that any member of R2 is a linear combination of these two vectors. So we ask: for which vectors with real components x and y are there scalars c1 and c2 such that this holds?
Gauss’s Method
c1 1 +c2 c1 + c2 = x −ρ1+ρ2
c1 − c2 = y −→
 1   1   x 
−1 = y (∗) c1 + c2 = x
−2c2 = −x + y
with back substitution gives c2 = (x − y)/2 and c1 = (x + y)/2. This shows that for any x, y there are appropriate coefficients c1, c2 making (∗) true — we can write any element of R2 as a linear combination of the two given ones. For instance,forx=1andy=2thecoefficientsc2 =−1/2andc1 =3/2willdo.
Since spans are subspaces, and we know that a good way to understand a subspace is to parametrize its description, we can try to understand a set’s span in that way.
2.18 Example Consider, in the vector space of quadratic polynomials P2, the span of the set S = {3x−x2,2x}. By the definition of span, it is the set of
96 Chapter Two. Vector Spaces
 unrestricted linear combinations of the two { c1 (3x − x2 ) + c2 (2x) | c1 , c2 ∈ R }. Clearly polynomials in this span must have a constant term of zero. Is that necessary condition also sufficient?
We are asking: for which members a2x2 + a1x + a0 of P2 are there c1 and c2 such that a2x2 + a1x + a0 = c1(3x − x2) + c2(2x)? Polynomials are equal when their coefficients are equal so we want conditions on a2, a1, and a0 making that triple a solution of this system.
−c1 =a2 3c1 +2c2 =a1 0=a0
Gauss’s Method and back-substitution gives c1 = −a2, and c2 = (3/2)a2 + (1/2)a1, and 0 = a0. Thus as long as there is no constant term a0 = 0 we can give coefficients c1 and c2 to describe that polynomial as an element of the span. For instance, for the polynomial 0 − 4x + 3x2, the coefficients c1 = −3 and c2 =5/2willdo. Sothespanofthegivensetis[S]={a1x+a2x2 |a1,a2 ∈R}.
Incidentally, this shows that the set { x, x2 } spans the same subspace. A space can have more than one spanning set. Two other sets spanning this subspace are {x,x2,−x+2x2} and {x,x+x2,x+2x2,...}.
2.19 Example The picture below shows the subspaces of R3 that we now know of: the trivial subspace, lines through the origin, planes through the origin, and the whole space. (Of course, the picture shows only a few of the infinitely many cases. Line segments connect subsets with their supersets.) In the next section we will prove that R3 has no other kind of subspace, so in fact this lists them all.
This describes each subspace as the span of a set with a minimal number of members. With this, the subspaces fall naturally into levels — planes on one level, lines on another, etc.
   
 
    
      
     1 0    1 0
1 0
{x0+y1} {x0+z0} {x1+z0} ···
000101
H  
 A H  
     A H
1 0 H 2 1
{x0} {y1} {y1} {y1} ···
0001
XP
XX P H XP
XPH
XX P H A 
XXPPH 0 XXP { 0 }
X 0
1 0 0 {x0+y1+z0}
001
Section I. Definition of Vector Space 97
 So far in this chapter we have seen that to study the properties of linear combinations, the right setting is a collection that is closed under these combi- nations. In the first subsection we introduced such collections, vector spaces, and we saw a great variety of examples. In this subsection we saw still more spaces, ones that are subspaces of others. In all of the variety there is a com- monality. Example 2.19 above brings it out: vector spaces and subspaces are best understood as a span, and especially as a span of a small number of vectors. The next section studies spanning sets that are minimal.
Exercises
  2.20 Which of these subsets of the vector space of 2×2 matrices are subspaces under the inherited operations? For each one that is a subspace, parametrize its description. For each that is not, give a condition that fails.
 a 0 
(a){ 0 b |a,b∈R}
 a 0 
(b){ 0 b |a+b=0}
 a 0 
(c){ 0 b |a+b=5}
 a c 
(d){ 0 b |a+b=0,c∈R}
  2.21 Is this a subspace of P2: {a0 + a1x + a2x2 | a0 + 2a1 + a2 = 4}? If it is then parametrize its description.
2.22 Is the vector in the span of the set?
1 21
0 { 1 ,−1} 3 −1 1
  2.23 Decide if the vector lies in the span of the set, inside of the space. 2 1 0
(a) 0, {0 , 0}, in R3 101
(b) x−x3, {x2,2x+x2,x+x3}, in P3  0 1   1 0   2 0 
(c) 4 2,{1 1,2 3},inM2×2
2.24 Which of these are members of the span [{cos2 x,sin2 x}] in the vector space of real-valued functions of one real variable?
(a) f(x)=1 (b) f(x)=3+x2 (c) f(x)=sinx (d) f(x)=cos(2x)
  2.25 Which of these sets spans R3? That is, which of these sets has the property that any three-tall vector can be expressed as a suitable linear combination of the set’s elements?
98
Chapter Two. Vector Spaces
 1 0 0 (a) {0,2,0}
2 1 0 1 3 (b) {0,1,0} (c) {1,0}
003 101 00 1 3 −1 2 2 3 5 6
(d) {0,1, 0 ,1} (e) {1,0,1,0} 1005 1122
  2.26 Parametrize each subspace’s description. Then express each subspace as a span.
(a) The subset {(a b c) | a − c = 0} of the three-wide row vectors (b) This subset of M2×2
 a b 
{ c d |a+d=0}
(c) This subset of M2×2
 a b 
(d)Thesubset{a+bx+cx3 |a−2b+c=0}ofP3
(e) The subset of P2 of quadratic polynomials p such that p(7) = 0
  2.27 Find a set to span the given subspace of the given space. (Hint. Parametrize
each.)
(a) the xz-plane in R3
x
(b) {y | 3x + 2y + z = 0} in R3
z x
y
(c) {  z  | 2x + y + w = 0 and y + 2z = 0 } in R4

w
(d) {a0 + a1x + a2x2 + a3x3 | a0 + a1 = 0 and a2 − a3 = 0} in P3 (e) The set P4 in the space P4
(f) M2×2 in M2×2
2.28 Is R2 a subspace of R3?
  2.29 Decide if each is a subspace of the vector space of real-valued functions of one
real variable.
(a) The even functions {f: R → R | f(−x) = f(x) for all x}. For example, two mem-
bers of this set are f1(x) = x2 and f2(x) = cos(x).
(b) The odd functions {f: R → R | f(−x) = −f(x) for all x}. Two members are
f3(x) = x3 and f4(x) = sin(x).
2.30 Example 2.16 says that for any vector ⃗v that is an element of a vector space V,
the set {r ·⃗v | r ∈ R} is a subspace of V. (This is simply the span of the singleton
set {⃗v}.) Must any such subspace be a proper subspace?
2.31 An example following the definition of a vector space shows that the solution
set of a homogeneous linear system is a vector space. In the terminology of this subsection, it is a subspace of Rn where the system has n variables. What about a non-homogeneous linear system; do its solutions form a subspace (under the inherited operations)?
{ c d |2a−c−d=0anda+3b=0}
Section I. Definition of Vector Space 99
 2.32 [Cleary] Give an example of each or explain why it would be impossible to do so.
(a) A nonempty subset of M2×2 that is not a subspace.
(b) A set of two vectors in R2 that does not span the space.
2.33 Example 2.19 shows that R3 has infinitely many subspaces. Does every non- trivial space have infinitely many subspaces?
2.34 Finish the proof of Lemma 2.9.
2.35 Show that each vector space has only one trivial subspace.
2.36 Show that for any subset S of a vector space, the span of the span equals the span [[S]] = [S]. (Hint. Members of [S] are linear combinations of members of S. Members of [[S]] are linear combinations of linear combinations of members of S.)
2.37 All of the subspaces that we’ve seen in some way use zero in their description. For example, the subspace in Example 2.3 consists of all the vectors from R2 with a second component of zero. In contrast, the collection of vectors from R2 with a second component of one does not form a subspace (it is not closed under scalar multiplication). Another example is Example 2.2, where the condition on the vectors is that the three components add to zero. If the condition there were that the three components add to one then it would not be a subspace (again, it would fail to be closed). However, a reliance on zero is not strictly necessary. Consider the set
under these operations.
x
{y | x + y + z = 1}
z
x1 x2
y1+y2= y1 +y2  ry= ry 
x1 +x2 −1 x rx−r+1 z1 z2 z1+z2 z rz
(a) Show that it is not a subspace of R3. (Hint. See Example 2.7).
(b) Show that it is a vector space. Note that by the prior item, Lemma 2.9 can
not apply.
(c) Show that any subspace of R3 must pass through the origin, and so any
subspace of R3 must involve zero in its description. Does the converse hold? Does any subset of R3 that contains the origin become a subspace when given the inherited operations?
2.38 We can give a justification for the convention that the sum of zero-many vectors equals the zero vector. Consider this sum of three vectors ⃗v1 +⃗v2 +⃗v3.
(a) What is the difference between this sum of three vectors and the sum of the first two of these three?
(b) What is the difference between the prior sum and the sum of just the first one vector?
(c) What should be the difference between the prior sum of one vector and the sum of no vectors?
(d) So what should be the definition of the sum of no vectors?
100 Chapter Two. Vector Spaces
 2.39 Is a space determined by its subspaces? That is, if two vector spaces have the same subspaces, must the two be equal?
2.40 (a) Give a set that is closed under scalar multiplication but not addition. (b) Give a set closed under addition but not scalar multiplication.
(c) Give a set closed under neither.
2.41 Show that the span of a set of vectors does not depend on the order in which the vectors are listed in that set.
2.42 Which trivial subspace is the span of the empty set? Is it 0
{0}⊆R3, or {0+0x}⊆P1, 0
or some other subspace?
2.43 Show that if a vector is in the span of a set then adding that vector to the set
won’t make the span any bigger. Is that also ‘only if’?
  2.44 Subspaces are subsets and so we naturally consider how ‘is a subspace of’
interacts with the usual set operations.
(a) If A, B are subspaces of a vector space, must their intersection A ∩ B be a
subspace? Always? Sometimes? Never?
(b) Must the union A ∪ B be a subspace?
(c) If A is a subspace, must its complement be a subspace?
(Hint. Try some test subspaces from Example 2.19.)
2.45 Does the span of a set depend on the enclosing space? That is, if W is a
subspace of V and S is a subset of W (and so also a subset of V), might the span
of S in W differ from the span of S in V?
2.46 Is the relation ‘is a subspace of’ transitive? That is, if V is a subspace of W
and W is a subspace of X, must V be a subspace of X?
2.47 Because ‘span of’ is an operation on sets we naturally consider how it interacts
with the usual set operations.
(a) If S ⊆ T are subsets of a vector space, is [S] ⊆ [T]? Always? Sometimes?
Never?
(b) If S, T are subsets of a vector space, is [S ∪ T ] = [S] ∪ [T ]?
(c) If S, T are subsets of a vector space, is [S ∩ T ] = [S] ∩ [T ]?
(d) Is the span of the complement equal to the complement of the span?
2.48 Reprove Lemma 2.15 without doing the empty set separately.
2.49 Find a structure that is closed under linear combinations, and yet is not a vector space.
Section II. Linear Independence 101
 II Linear Independence
The prior section shows how to understand a vector space as a span, as an unrestricted linear combination of some of its elements. For example, the space of linear polynomials {a + bx | a, b ∈ R} is spanned by the set {1, x}. The prior section also showed that a space can have many sets that span it. Two more sets that span the space of linear polynomials are {1,2x} and {1,x,2x}.
At the end of that section we described some spanning sets as ‘minimal’ but we never precisely defined that word. We could mean that a spanning set is minimal if it contains the smallest number of members of any set with the same span, so that {1,x,2x} is not minimal because it has three members while we can give two-element sets spanning the same space. Or we could mean that a spanning set is minimal when it has no elements that we can remove without changing the span. Under this meaning { 1, x, 2x } is not minimal because removing the 2x to get {1,x} leaves the span unchanged.
The first sense of minimality appears to be a global requirement, in that to check if a spanning set is minimal we seemingly must look at all the sets that span and find one with the least number of elements. The second sense of minimality is local since we need to look only at the set and consider the span with and without various elements. For instance, using the second sense we could compare the span of {1, x, 2x} with the span of {1, x} and note that 2x is a “repeat” in that its removal doesn’t shrink the span.
In this section we will use the second sense of ‘minimal spanning set’ because of this technical convenience. However, the most important result of this book is that the two senses coincide. We will prove that in the next section.
II.1 Definition and Examples
We saw “repeats” in the first chapter. There, Gauss’s Method turned them into 0 = 0 equations.
1.1 Example Recall the Statics example from Chapter One’s opening. We got two balances with the pair of unknown-mass objects, one at 40 cm and 15 cm and another at −50 cm and 25 cm, and we then computed the value of those masses. Had we instead gotten the second balance at 20 cm and 7.5 cm then Gauss’s Method on the resulting two-equations, two-unknowns system would not have yielded a solution, it would have yielded a 0 = 0 equation along with an equation containing a free variable. Intuitively, the problem is that (20 7.5) is half of (40 15), that is, (20 7.5) is in the span of the set {(40 15)} and so is
102 Chapter Two. Vector Spaces repeated data. We would have been trying to solve a two-unknowns problem
with essentially only one piece of information.
We take⃗v to be a “repeat” of the vectors in a set S if⃗v ∈ [S] so that it depends
on, that is, is expressible in terms of, elements of the set ⃗v = c1⃗s1 + · · · + cn⃗sn .
Proof Half of the if and only if is immediate: if ⃗v ∈/ [S] then the sets are not equal because ⃗v ∈ [S ∪ {⃗v}].
Fortheotherhalfassumethat⃗v∈[S]sothat⃗v=c1⃗s1+···+cn⃗sn forsome scalars ci and vectors ⃗si ∈ S. We will use mutual containment to show that the sets [S ∪ {⃗v}] and [S] are equal. The containment [S ∪ {⃗v}] ⊇ [S] is clear.
To show containment in the other direction let w⃗ be an element of [S ∪ {⃗v}]. Then w⃗ is a linear combination of elements of S ∪ {⃗v}, which we can write as w⃗ = cn+1⃗sn+1 + · · · + cn+k⃗sn+k + cn+k+1⃗v. (Possibly some of the ⃗si’s from w⃗ ’s equation are the same as some of those from ⃗v’s equation but that does not matter.) Expand ⃗v.
w⃗ =cn+1⃗sn+1 +···+cn+k⃗sn+k +cn+k+1 ·(c1⃗s1 +···+cn⃗sn) Recognize the right hand side as a linear combination of linear combinations of
vectors from S. Thus w⃗ ∈ [S]. QED The discussion at the section’s opening involved removing vectors instead of
adding them.
The corollary says that to know whether removing a vector will decrease the span, we need to know whether the vector is a linear combination of others in the set.
Thus the set {⃗s0, . . . ,⃗sn } is independent if there is no equality ⃗si = c0⃗s0 + . . . + ci−1⃗si−1 + ci+1⃗si+1 + . . . + cn⃗sn . The definition’s use of the word ‘others’ means that writing ⃗si as a linear combination via ⃗si = 1 · ⃗si does not count.
  1.2 Lemma Where V is a vector space, S is a subset of that space, and ⃗v is an element of that space, [S ∪ {⃗v}] = [S] if and only if ⃗v ∈ [S].
 1.3 Corollary For ⃗v ∈ S, omitting that vector does not shrink the span [S] = [S − {⃗v}] if and only if it is dependent on other vectors in the set ⃗v ∈ [S].
 1.4 Definition In any vector space, a set of vectors is linearly independent if none of its elements is a linear combination of the others from the set.∗ Otherwise the set is linearly dependent.
 ∗See also Remark 1.12.
Section II. Linear Independence 103 Observe that, although this way of writing one vector as a combination of
the others
⃗s0 =c1⃗s1 +c2⃗s2 +···+cn⃗sn
visually sets off ⃗s0, algebraically there is nothing special about that vector in that equation. For any ⃗si with a coefficient ci that is non-0 we can rewrite to isolate ⃗si.
⃗si = (1/ci)⃗s0 + · · · + (−ci−1/ci)⃗si−1 + (−ci+1/ci)⃗si+1 + · · · + (−cn/ci)⃗sn
When we don’t want to single out any vector we will instead say that ⃗s0 , ⃗s1 , . . . , ⃗sn are in a linear relationship and put all of the vectors on the same side. The next result rephrases the linear independence definition in this style. It is how we usually compute whether a finite set is dependent or independent.
Proof If S is linearly independent then no vector ⃗si is a linear combination of other vectors from S so there is no linear relationship where some of the ⃗s ’s have nonzero coefficients.
If S is not linearly independent then some ⃗si is a linear combination ⃗si = c1⃗s1+···+ci−1⃗si−1+ci+1⃗si+1+···+cn⃗sn ofothervectorsfromS. Subtracting ⃗si from both sides gives a relationship involving a nonzero coefficient, the −1 in front of ⃗si. QED
1.6 Example In the vector space of two-wide row vectors, the two-element set {(40 15),(−50 25)} is linearly independent. To check this, take
c1 ·(40 15)+c2 ·(−50 25)=(0 0) and solve the resulting system.
40c1 − 50c2 = 0 −(15/40)ρ1 +ρ2 40c1 − 50c2 = 0 15c1 + 25c2 = 0 −→ (175/4)c2 = 0
Both c1 and c2 are zero. So the only linear relationship between the two given row vectors is the trivial relationship.
In the same vector space, the set {(40 15),(20 7.5)} is linearly dependent since we can satisfy c1 ·(40 15)+c2 ·(20 7.5) = (0 0) with c1 = 1 and c2 = −2.
1.7 Example The set {1 + x, 1 − x} is linearly independent in P2, the space of quadratic polynomials with real coefficients, because
0+0x+0x2 =c1(1+x)+c2(1−x)=(c1 +c2)+(c1 −c2)x+0x2
  1.5 Lemma A subset S of a vector space is linearly independent if and only if among its elements the only linear relationship c1⃗s1 + · · · + cn⃗sn = ⃗0 (with ⃗si ̸=⃗sj foralli̸=j)isthetrivialonec1 =0,...,cn =0.
104 gives
Chapter Two. Vector Spaces
c1 + c2 =0 2c2 = 0
 c1 +c2 =0 −ρ1+ρ2 c1 − c2 = 0 −→
since polynomials are equal only if their coefficients are equal. Thus, the only linear relationship between these two members of P2 is the trivial one.
1.8 Example The rows of this matrix
2 3 1 0 A =  0 − 1 0 − 2 
0001
form a linearly independent set. This is easy to check for this case but also recall that Lemma One.III.2.5 shows that the rows of any echelon form matrix form a linearly independent set.
1.9 Example In R3, where
⃗v1 = 4 ⃗v2 = 9 ⃗v3 = 18
524
the set S = {⃗v1,⃗v2,⃗v3 } is linearly dependent because this is a relationship
0·⃗v1 +2·⃗v2 −1·⃗v3 =⃗0
where not all of the scalars are zero (the fact that some of the scalars are zero
doesn’t matter).
That example illustrates why, although Definition 1.4 is a clearer statement of what independence means, Lemma 1.5 is better for computations. Working straight from the definition, someone trying to compute whether S is linearly independent would start by setting ⃗v1 = c2⃗v2 + c3⃗v3 and concluding that there are no such c2 and c3. But knowing that the first vector is not dependent on the other two is not enough. This person would have to go on to try ⃗v2 = c1⃗v1 +c3⃗v3, in order to find the dependence c1 = 0, c3 = 1/2. Lemma 1.5 gets the same conclusion with only one computation.
1.10 Example The empty subset of a vector space is linearly independent. There is no nontrivial linear relationship among its members as it has no members.
1.11 Example In any vector space, any subset containing the zero vector is linearly dependent. One example is, in the space P2 of quadratic polynomials, the subset {1+x,x+x2,0}. It is linearly dependent because 0·⃗v1 +0·⃗v2 +1·⃗0 =⃗0 is a nontrivial relationship, since not all of the coefficients are zero.
A subtler way to see that this subset is dependent is to remember that the zero vector is equal to the trivial sum, the sum of the empty set. So any set
3 2 4
Section II. Linear Independence 105
 containing the zero vector has an element that is a combination of a subset of other vectors from the set, specifically, the zero vector is a combination of the empty subset.
1.12 Remark Definition 1.4 describes when a collection of vectors is linearly inde- pendent. In this book we will follow the practice of referring to an independent ‘set’, because sets are the most familiar collections and because that practice is universal. But to be complete we will note that sets will not do.
Recall that a set is a collection with two properties: order does not matter, so that the set {1,2} equals the set {2,1}, and repeats collapse, so that the set {1, 1, 2} equals the set {1, 2}.
Now consider this matrix reduction.
1 1 1 1 1 1  (1/2)ρ2  2 2 2 −→ 1 1 1
123 123
On the left the set of matrix rows {(1 1 1),(2 2 2),(1 2 3)} is linearly depen- dent. On the right the set {(1 1 1),(1 1 1),(1 2 3)}, which equals the set {(1 1 1),(1 2 3)}, is linearly independent. That’s not what we want; we want to say that the collection of the rows of the second matrix is also dependent.
Thus, for a proper definition of dependence and independence, we need a type of collection where for the second matrix the row (1 1 1) appears twice. A collection where order does not matter and where repeats do not collapse is a multiset. So, strictly speaking, we should in the definition replace ‘set’ with ‘multiset’, and adjust all later discussions also.
However, most of the time the distinction won’t matter and departing from standard terminology has its own pitfalls, so we will follow the practice of referring to a linearly independent or dependent ‘set.’ Later in this book we shall occasionally need to take combinations without letting repeated parts collapse, and we shall do that without comment.
Proof This follows from Corollary 1.3. If S is linearly independent then none of its vectors is dependent on the other elements, so removal of any vector will shrink the span. If S is not linearly independent then it contains a vector that is dependent on other elements of the set, and removal of that vector will not shrink the span. QED
So a spanning set is minimal if and only if it is linearly independent.
 1.13 Corollary A set S is linearly independent if and only if for any ⃗v ∈ S, its removal shrinks the span [S − {v}]   [S].
106 Chapter Two. Vector Spaces The prior result addresses removing elements from a linearly independent
set. The next one adds elements.
Proof We will show that S ∪ {⃗v} is not linearly independent if and only if ⃗v ∈ [S].
Suppose first that v ∈ [S]. Express ⃗v as a combination ⃗v = c1⃗s1 + · · · + cn⃗sn. Rewrite that ⃗0 = c1⃗s1 + · · · + cn⃗sn − 1 · ⃗v. Since v ∈/ S, it does not equal any of the ⃗si so this is a nontrivial linear dependence among the elements of S ∪ {⃗v}. Thus that set is not linearly independent.
Now suppose that S∪{⃗v} is not linearly independent and consider a nontrivial dependence among its members ⃗0 = c1⃗s1 + · · · + cn⃗sn + cn+1 · ⃗v. If cn+1 = 0 then that is a dependence among the elements of S, but we are assuming that S is independent, so cn+1 ̸= 0. Rewrite the equation as ⃗v = (c1/cn+1)⃗s1 + · · · + (cn/cn+1)⃗sn to get ⃗v ∈ [S] QED
1.15 Example This subset of R3 is linearly independent. 1
S = {0} 0
The span of S is the x-axis. Here are two supersets, one that is linearly dependent and the other independent.
1 −3 1 0 dependent: {0, 0 } independent: {0,1}
00 00
We got the dependent superset by adding a vector from the x-axis and so the span did not grow. We got the independent superset by adding a vector that isn’t in [S], because it has a nonzero y component, causing the span to grow.
For the independent set
1 0 S = {0,1}
00
the span [S] is the xy-plane. Here are two supersets.
1 0 3 1 0 0
dependent: {0,1,−2} independent: {0,1,0} 000 001
  1.14 Lemma Suppose that S is linearly independent and that ⃗v ∈/ S. Then the set S ∪ {⃗v} is linearly independent if and only if ⃗v ∈/ [S].
Section II. Linear Independence 107
 As above, the additional member of the dependent superset comes from [S], the xy-plane, while the added member of the independent superset comes from outside of that span.
Finally, consider this independent set
1 0 0 S = {0 , 1 , 0}
001
with [S] = R3. We can get a linearly dependent superset.
1 0 0 2 dependent: {0,1,0,−1}
0013
But there is no linearly independent superset of S. One way to see that is to note that for any vector that we would add to S, the equation
x 1 0 0 y = c1 0 + c2 1 + c3 0
z001
hasasolutionc1 =x,c2 =y,andc3 =z. Anotherwaytoseeitisthatwe
cannot add any vectors from outside of the span [S] because that span is R3.
Proof If S = {⃗s1, . . . ,⃗sn } is linearly independent then S itself satisfies the statement, so assume that it is linearly dependent.
By the definition of dependent, S contains a vector ⃗v1 that is a linear combination of the others. Define the set S1 = S − {⃗v1 }. By Corollary 1.3 the span does not shrink [S1] = [S].
If S1 is linearly independent then we are done. Otherwise iterate: take a vector ⃗v2 that is a linear combination of other members of S1 and discard it to derive S2 = S1 − {⃗v2 } such that [S2] = [S1]. Repeat this until a linearly independent set Sj appears; one must appear eventually because S is finite and the empty set is linearly independent. (Formally, this argument uses induction on the number of elements in S. Exercise 41 asks for the details.) QED
Thus if we have a set that is linearly dependent then we can, without changing the span, pare down by discarding what we have called “repeat” vectors.
 1.16 Corollary In a vector space, any finite set has a linearly independent subset with the same span.
108 Chapter Two. Vector Spaces 1.17 Example This set spans R3 (the check is routine) but is not linearly inde-
 pendent.
1 0 1 0 3 S = {0 , 2 , 2 , −1 , 3}
00010
We will calculate which vectors to drop in order to get a subset that is independent but has the same span. This linear relationship
1 0 1 0 3 0
c1 0 + c2 2 + c3 2 + c4 −1 + c5 3 = 0 (∗)
000100
gives a system
c1 +c3+ +3c5=0 2c2 +2c3 −c4 +3c5 =0 c4 =0
whose solution set has this parametrization.
c1 −1  −3 
c2  −1 −3/2 {c3=c3 1 +c5 0 |c3,c5 ∈R}
 c 4   0   0  c5 0 1
Setc5 =1andc3 =0togetaninstanceof(∗).
1 0 1 0 3 0
− 3 ·  0  − 3 ·  2  + 0 ·  2  + 0 ·  − 1  + 1 ·  3  =  0  0200100
This shows that the vector from S that we’ve associated with c5 is in the span of the set of c1’s vector and c2’s vector. We can discard S’s fifth vector without shrinking the span.
Similarly, set c3 = 1, and c5 = 0 to get an instance of (∗) that shows we can discard S’s third vector without shrinking the span. Thus this set has the same span as S.
1 0 0 { 0 , 2 , −1 }
001
The check that it is linearly independent is routine.
 
Section II. Linear Independence 109
  1.18Corollary AsubsetS={⃗s1,...,⃗sn}ofavectorspaceislinearlydependent
if and only if some s⃗ is a linear combination of the vectors ⃗s , . . . , ⃗s listed i 1 i−1
before it.
Proof ConsiderS ={},S ={s⃗},S ={⃗s,⃗s },etc.Someindexi 1isthe 011212
first one with Si−1 ∪ {⃗si } linearly dependent, and there ⃗si ∈ [Si−1]. QED
The proof of Corollary 1.16 describes producing a linearly independent set by shrinking, by taking subsets. And the proof of Corollary 1.18 describes finding a linearly dependent set by taking supersets. We finish this subsection by considering how linear independence and dependence interact with the subset relation between sets.
Proof Both are clear. QED
Restated, subset preserves independence and superset preserves dependence.
Those are two of the four possible cases. The third case, whether subset preserves linear dependence, is covered by Example 1.17, which gives a linearly dependent set S with one subset that is linearly dependent and another that is independent. The fourth case, whether superset preserves linear independence, is covered by Example 1.15, which gives cases where a linearly independent set has both an independent and a dependent superset. This table summarizes.
 1.19 Lemma Any subset of a linearly independent set is also linearly independent. Any superset of a linearly dependent set is also linearly dependent.
Sˆ ⊂ S
S independent Sˆ must be independent
S dependent Sˆ may be either
Sˆ ⊃ S
Sˆ may be either
Sˆ must be dependent
   Example 1.15 has something else to say about the interaction between linear independence and superset. It names a linearly independent set that is maximal in that it has no supersets that are linearly independent. By Lemma 1.14 a linearly independent set is maximal if and only if it spans the entire space, because that is when all the vectors in the space are already in the span. This nicely complements Lemma 1.13, that a spanning set is minimal if and only if it is linearly independent.
Exercises
  1.20 Decide whether each subset of R3 is linearly dependent or linearly indepen- dent.
110
Chapter Two. Vector Spaces
 1 2 4 (a) {−3,2,−4}
1 2 3 (b) {7,7,7}
0 1 (c) { 0 ,0}
−1 4
5 4 14
9 2 3 12
(d) {9,0, 5 ,12} 0 1 −4 −1
7 7 7
  1.21 Which of these subsets of P3 are linearly dependent and which are indepen- dent?
(a) {3−x+9x2,5−6x+3x2,1+1x−5x2}
(b) {−x2,1+4x2}
(c) {2+x+7x2,3−x+2x2,4−3x2}
(d) {8+3x+3x2,x+2x2,2+2x+2x2,8−2x+5x2}
1.22 Determine if each set is linearly independent in the natural space. 1 −1
(a) {2, 1 } (b) {(1 3 1),(−1 4 3),(−1 11 7)} 00
 5 4  0 0  1 0  (c){1 2,0 0,−1 4}
  1.23 Prove that each set {f,g} is linearly independent in the vector space of all functions from R+ to R.
(a) f(x) = x and g(x) = 1/x
(b) f(x) = cos(x) and g(x) = sin(x) (c) f(x) = ex and g(x) = ln(x)
  1.24 Which of these subsets of the space of real-valued functions of one real variable is linearly dependent and which is linearly independent? (We have abbreviated some constant functions; e.g., in the first item, the ‘2’ stands for the constant function f(x) = 2.)
(a) {2,4sin2(x),cos2(x)} (b) {1,sin(x),sin(2x)} (c) {x,cos(x)}
(d) {(1+x)2,x2 +2x,3} (e) {cos(2x),sin2(x),cos2(x)} (f) {0,x,x2}
1.25 Does the equation sin2(x)/cos2(x) = tan2(x) show that this set of functions {sin2(x),cos2(x),tan2(x)} is a linearly dependent subset of the set of all real-valued functions with domain the interval (−π/2..π/2) of real numbers between −π/2 and π/2)?
1.26 Is the xy-plane subset of the vector space R3 linearly independent?
  1.27 Show that the nonzero rows of an echelon form matrix form a linearly indepen-
dent set.
1.28 (a) Show that if the set {⃗u,⃗v,w⃗ } is linearly independent then so is the set {⃗u,⃗u+⃗v,⃗u+⃗v+w⃗ }.
(b) What is the relationship between the linear independence or dependence of {⃗u,⃗v,w⃗ } and the independence or dependence of {⃗u−⃗v,⃗v−w⃗,w⃗ −⃗u}?
1.29 Example 1.10 shows that the empty set is linearly independent. (a) When is a one-element set linearly independent?
(b) How about a set with two elements?
Section II. Linear Independence 111 1.30 In any vector space V, the empty set is linearly independent. What about all
of V?
1.31 Show that if {⃗x,⃗y,⃗z} is linearly independent then so are all of its proper subsets: {⃗x,⃗y}, {⃗x,⃗z}, {⃗y,⃗z}, {⃗x},{⃗y}, {⃗z}, and {}. Is that ‘only if’ also?
 1.32 (a) Show that this
1 −1 S = {1, 2 }
00
is a linearly independent subset of R3. (b) Show that
3
2 0
is in the span of S by finding c1 and c2 giving a linear relationship. 1 −1 3
c1 1 + c2  2  = 2 000
Show that the pair c1,c2 is unique.
(c) Assume that S is a subset of a vector space and that ⃗v is in [S], so that ⃗v is a
linear combination of vectors from S. Prove that if S is linearly independent then a linear combination of vectors from S adding to ⃗v is unique (that is, unique up to reordering and adding or taking away terms of the form 0 · ⃗s). Thus S as a spanning set is minimal in this strong sense: each vector in [S] is a combination of elements of S a minimum number of times — only once.
(d) Prove that it can happen when S is not linearly independent that distinct linear combinations sum to the same vector.
1.33 Prove that a polynomial gives rise to the zero function if and only if it is the zero polynomial. (Comment. This question is not a Linear Algebra matter but we often use the result. A polynomial gives rise to a function in the natural way: x  → cn xn + · · · + c1 x + c0 .)
1.34 Return to Section 1.2 and redefine point, line, plane, and other linear surfaces to avoid degenerate cases.
1.35 (a) Show that any set of four vectors in R2 is linearly dependent.
(b) Is this true for any set of five? Any set of three?
(c) What is the most number of elements that a linearly independent subset of
R2 can have?
1.36 Is there a set of four vectors in R3 such that any three form a linearly independent
set?
1.37 Must every linearly dependent set have a subset that is dependent and a subset that is independent?
1.38 In R4 what is the biggest linearly independent set you can find? The smallest? The biggest linearly dependent set? The smallest? (‘Biggest’ and ‘smallest’ mean that there are no supersets or subsets with the same property.)
112 Chapter Two. Vector Spaces
   1.39 Linear independence and linear dependence are properties of sets. We can thus naturally ask how the properties of linear independence and dependence act with respect to the familiar elementary set relations and operations. In this body of this subsection we have covered the subset and superset relations. We can also consider the operations of intersection, complementation, and union.
(a) How does linear independence relate to intersection: can an intersection of linearly independent sets be independent? Must it be?
(b) How does linear independence relate to complementation?
(c) Show that the union of two linearly independent sets can be linearly indepen-
dent.
(d) Show that the union of two linearly independent sets need not be linearly
independent.
1.40 Continued from prior exercise. What is the interaction between the property of linear independence and the operation of union?
(a) We might conjecture that the union S∪T of linearly independent sets is linearly independent if and only if their spans have a trivial intersection [S] ∩ [T] = {⃗0}. What is wrong with this argument for the ‘if’ direction of that conjecture? “If the union S ∪ T is linearly independent then the only solution to c1⃗s1 + · · · + cn⃗sn +d1⃗t1 +···+dm⃗tm =⃗0 is the trivial one c1 = 0, ..., dm = 0. So any member of the intersection of the spans must be the zero vector because in c1⃗s1 + · · · + cn⃗sn = d1⃗t1 + · · · + dm⃗tm each scalar is zero.”
(b) Give an example showing that the conjecture is false.
(c) Find linearly independent sets S and T so that the union of S − (S ∩ T ) and
T − (S ∩ T ) is linearly independent, but the union S ∪ T is not linearly independent. (d) Characterize when the union of two linearly independent sets is linearly
independent, in terms of the intersection of spans.
1.41 For Corollary 1.16,
(a) fill in the induction for the proof;
(b) give an alternate proof that starts with the empty set and builds a sequence
of linearly independent subsets of the given finite set until one appears with the same span as the given set.
1.42 With a some calculation we can get formulas to determine whether or not a set of vectors is linearly independent.
(a) Show that this subset of R2
{c,d}
 a   b 
is linearly independent if and only if ad − bc ̸= 0.
(b) Show that this subset of R3 {d,e,f}
ghi
is linearly independent iff aei + bfg + cdh − hfa − idb − gec ̸= 0.
a b c
Section II. Linear Independence
113
 (c) When is this subset of R3
a b {d,e}
gh
linearly independent?
(d) This is an opinion question: for a set of four vectors from R4, must there be a
formula involving the sixteen entries that determines independence of the set?
(You needn’t produce such a formula, just decide if one exists.)
  1.43 (a) Prove that a set of two perpendicular nonzero vectors from Rn is linearly
independent when n > 1.
(b) What if n=1? n=0?
(c) Generalize to more than two vectors.
1.44 Consider the set of functions from the interval (−1 . . . 1) ⊆ R to R.
(a) Show that this set is a vector space under the usual operations.
(b) Recall the formula for the sum of an infinite geometric series: 1 + x + x2 + · · · =
1/(1 − x) for all x ∈ (−1..1). Why does this not express a dependence inside of the set { g(x) = 1/(1 − x), f0 (x) = 1, f1 (x) = x, f2 (x) = x2 , . . . } (in the vector space that we are considering)? (Hint. Review the definition of linear combination.)
(c) Show that the set in the prior item is linearly independent.
This shows that some vector spaces exist with linearly independent subsets that are infinite.
1.45 Show that, where S is a subspace of V, if a subset T of S is linearly independent in S then T is also linearly independent in V. Is that ‘only if’?
114 Chapter Two. Vector Spaces III Basis and Dimension
The prior section ends with the observation that a spanning set is minimal when it is linearly independent and a linearly independent set is maximal when it spans the space. So the notions of minimal spanning set and maximal independent set coincide. In this section we will name this idea and study its properties.
III.1 Basis
Because a basis is a sequence, meaning that bases are different if they contain the same elements but in different orders, we denote it with angle brackets ⟨β⃗ 1, β⃗ 2, . . .⟩.∗ (A sequence is linearly independent if the multiset consisting of the elements of the sequence is independent. Similarly, a sequence spans the space if the set of elements of the sequence spans the space.)
1.2 Example This is a basis for R2.
 2   1 
  1.1 Definition A basis for a vector space is a sequence of vectors that is linearly independent and that spans the space.
⟨4,1⟩
 2   1   0  2c1 + 1c2 = 0
It is linearly independent
c1 4 + c2 1 = 0 =⇒ 4c1 + 1c2 = 0 =⇒ c1 = c2 = 0 and it spans R2.
2c1 + 1c2 = x 4c1 +1c2 =y
c2 =2x−yandc1 =(y−x)/2  1   2 
=⇒
1.3 Example This basis for R2 differs from the prior one
⟨1,4⟩
because it is in a different order. The verification that it is a basis is just as in
the prior example.
 ∗ More information on sequences is in the appendix.
Section III. Basis and Dimension 115 1.4 Example The space R2 has many bases. Another one is this.
 1   0  ⟨0,1⟩
The verification is easy.
  1.5 Definition For any Rn
En = ⟨.,.,..., .⟩
. . . 001
is the standard (or natural) basis. We denote these vectors ⃗e1,...,⃗en.
1 0 0 0 1 0
Calculus books denote R2’s standard basis vectors as ⃗ı and ⃗ instead of ⃗e1 and ⃗e2 and they denote to R3’s standard basis vectors as ⃗ı, ⃗, and ⃗k instead of ⃗e1, ⃗e2, and ⃗e3. Note that ⃗e1 means something different in a discussion of R3 than it means in a discussion of R2.
1.6Example Considerthespace{a·cosθ+b·sinθ|a,b∈R}offunctionsofthe real variable θ. This is a natural basis ⟨cos θ, sin θ⟩ = ⟨1·cos θ+0·sin θ, 0·cos θ+ 1 · sinθ⟩. A more generic basis for this space is ⟨cosθ − sinθ,2cosθ + 3sinθ⟩. Verification that these two are bases is Exercise 27.
1.7 Example A natural basis for the vector space of cubic polynomials P3 is ⟨1, x, x2, x3⟩. Two other bases for this space are ⟨x3, 3x2, 6x, 6⟩ and ⟨1, 1 + x, 1 + x + x2, 1 + x + x2 + x3⟩. Checking that each is linearly independent and spans the space is easy.
1.8 Example The trivial space {⃗0} has only one basis, the empty one ⟨⟩.
1.9 Example The space of finite-degree polynomials has a basis with infinitely
many elements ⟨1, x, x2 , . . .⟩.
1.10 Example We have seen bases before. In the first chapter we described the
solution set of homogeneous systems such as this one
by parametrizing.
x+y −w=0 z+w=0
−1 1
 1   0 
{ y+ w|y,w∈R}
0 −1 01
116 Chapter Two. Vector Spaces
 Thus the vector space of solutions is the span of a two-element set. This two- vector set is also linearly independent, which is easy to check. Therefore the solution set is a subspace of R4 with a basis comprised of these two vectors.
1.11 Example Parametrization finds bases for other vector spaces, not just for solution sets of homogeneous systems. To find a basis for this subspace of M2×2
 a b 
{ c 0 |a+b−2c=0}
we rewrite the condition as a = −b + 2c.
 −b+2c b   −1 1   2 0 
{ c 0 |b,c∈R}={b 0 0 +c 1 0 |b,c∈R} Thus, this is a natural candidate for a basis.
 −1 1   2 0  ⟨00,10⟩
The above work shows that it spans the space. Linear independence is also easy. Consider again Example 1.2. To verify that the set spans the space we looked
at linear combinations that total to a member of the space c1β⃗ 1 + c2β⃗ 2 =  x . y
We only noted in that example that such a combination exists, that for each x, y there exists a c1,c2, but in fact the calculation also shows that the combination is unique: c1 must be (y−x)/2 and c2 must be 2x−y.
We consider linear combinations to be the same if they have the same summands but in a different order, or if they differ only in the addition or deletion of terms of the form ‘0 · β⃗’.
Proof A sequence is a basis if and only if its vectors form a set that spans and that is linearly independent. A subset is a spanning set if and only if each vector in the space is a linear combination of elements of that subset in at least one way. Thus we need only show that a spanning subset is linearly independent if and only if every vector in the space is a linear combination of elements from the subset in at most one way.
Consider two expressions of a vector as a linear combination of the members of the subset. Rearrange the two sums, and if necessary add some 0 · β⃗ i
 1.12 Theorem In any vector space, a subset is a basis if and only if each vector in the space can be expressed as a linear combination of elements of the subset in one and only one way.
Section III. Basis and Dimension 117 terms, so that the two sums combine the same β⃗’s in the same order: ⃗v =
c1β⃗1 +c2β⃗2 +···+cnβ⃗n and⃗v=d1β⃗1 +d2β⃗2 +···+dnβ⃗n. Now c1β⃗1 +c2β⃗2 +···+cnβ⃗n =d1β⃗1 +d2β⃗2 +···+dnβ⃗n
holds if and only if
(c1 −d1)β⃗1 +···+(cn −dn)β⃗n =⃗0
holds. So, asserting that each coefficient in the lower equation is zero is the same thing as asserting that ci = di for each i, that is, that every vector is expressible as a linear combination of the β⃗ ’s in a unique way. QED
  1.13 Definition In a vector space with basis B the representation of ⃗v with respect to B is the column vector of the coefficients used to express ⃗v as a linear combination of the basis vectors:
Rep (⃗v)=
B  . 
where B = ⟨β⃗1,...,β⃗n⟩ and ⃗v = c1β⃗1 + c2β⃗2 + ··· + cnβ⃗n. The c’s are the coordinates of ⃗v with respect to B.
c1 c2
. cn
1.14 Example In P3, with respect to the basis B = ⟨1, 2x, 2x2, 2x3⟩, the represen-
tation of x + x2 is
0
1/2 RepB(x + x2) =   1/2
0
B
because x+x2 = 0·1+(1/2)·2x+(1/2)·2x2 +0·2x3. With respect to a different basis D = ⟨1 + x, 1 − x, x + x2, x + x3⟩, the representation is different.
0
0 RepD(x+x2)=  1
0D
(When there is more than one basis around, to help keep straight which repre- sentation is with respect to which basis we often write it as a subscript on the column vector.)
118 Chapter Two. Vector Spaces 1.15 Remark Definition 1.1 requires that a basis be a sequence because without
that we couldn’t write these coordinates in a fixed order.
1.16 Example In R2, where ⃗v =  3 , to find the coordinates of that vector with
 respect to the basis
we solve
2
 1   0  B=⟨1,2⟩
 1   0   3  c1 1 +c2 2 = 2
andgetthatc1 =3andc2 =−1/2.
 3  RepB(⃗v) = −1/2
1.17 Remark This use of column notation and the term ‘coordinate’ has both a
disadvantage and an advantage. The disadvantage is that representations look
like vectors from Rn, which can be confusing when the vector space is Rn, as in
the prior example. We must infer the intent from the context. For example, the
phrase ‘in R2, where ⃗v =  3 ’ refers to the plane vector that, when in canonical 2
position, ends at (3, 2). And in the end of that example, although we’ve omitted a subscript B from the column, that the right side is a representation is clear from the context.
The advantage of the notation and the term is that they generalize the familiar case: in Rn and with respect to the standard basis En, the vector starting at the origin and ending at (v1 , . . . , vn ) has this representation.
v1 v1 . .
RepEn( . )= .  vn vn En
Our main use of representations will come later but the definition appears here because the fact that every vector is a linear combination of basis vectors in a unique way is a crucial property of bases, and also to help make a point. For calculation of coordinates among other things, we shall restrict our attention to spaces with bases having only finitely many elements. That will start in the next subsection.
Exercises
  1.18 Decide if each is a basis for P2.
(a) ⟨x2 −x+1,2x+1,2x−1⟩ (b) ⟨x+x2,x−x2⟩
  1.19 Decide if each is a basis for R3.
Section III. Basis and Dimension
119
 1 3 0 (a) ⟨2,2,0⟩
3 1 1 0 1 1
(d) ⟨ 2 ,1,3⟩ −1 1 0
1 3 (b) ⟨2,2⟩
3 1
0 1 2 (c) ⟨ 2 ,1,5⟩
−1 1 0
  1.20 Represent the vector with respect to the basis.  1   1   −1  2
(a) 2 ,B=⟨ 1 , 1 ⟩⊆R
(b) x2 + x3 , D = ⟨1, 1 + x, 1 + x + x2 , 1 + x + x2 + x3 ⟩ ⊆ P3
0 −1
(c)  0 , E4 ⊆ R4 
1
1.21 Represent the vector with respect to each of the two bases.  3   1   1   1   1 
⃗v = − 1 B 1 = ⟨ − 1 , 1 ⟩ , B 2 = ⟨ 2 , 3 ⟩
1.22 Find a basis for P2, the space of all quadratic polynomials. Must any such
basis contain a polynomial of each degree: degree zero, degree one, and degree two? 1.23 Find a basis for the solution set of this system.
x1−4x2+3x3− x4=0 2x1 −8x2 +6x3 −2x4 =0
  1.24 Find a basis for M2×2, the space of 2×2 matrices.
  1.25 Find a basis for each.
(a) The subspace {a2x2 +a1x+a0 |a2 −2a1 =a0} of P2
(b) The space of three-wide row vectors whose first and second components add
to zero
(c) This subspace of the 2×2 matrices
 a b 
{ 0 c |c−2b=0}
1.26 Find a basis for each space, and verify that it is a basis.
(a) The subspace M = {a + bx + cx2 + dx3 | a − 2b + c − d = 0} of P3. (b) This subspace of M2×2.
 a b 
W={ c d |a−c=0}
1.27 Check Example 1.6.
  1.28 Find the span of each set and then find a basis for that span.
(a) {1+x,1+2x} in P2 (b) {2−2x,3+4x2} in P2
  1.29 Find a basis for each of these subspaces of the space P3 of cubic polynomi- als.
(a) The subspace of cubic polynomials p(x) such that p(7) = 0
(b) The subspace of polynomials p(x) such that p(7) = 0 and p(5) = 0
(c) The subspace of polynomials p(x) such that p(7) = 0, p(5) = 0, and p(3) = 0
120 Chapter Two. Vector Spaces (d) The space of polynomials p(x) such that p(7) = 0, p(5) = 0, p(3) = 0,
and p(1) = 0
1.30 We’ve seen that the result of reordering a basis can be another basis. Must it
be?
1.31 Can a basis contain a zero vector?
  1.32 Let ⟨β⃗1,β⃗2,β⃗3⟩ be a basis for a vector space.
(a) Show that ⟨c1β⃗1,c2β⃗2,c3β⃗3⟩ is a basis when c1,c2,c3 ̸= 0. What happens
when at least one ci is 0?
(b) Prove that ⟨α⃗ 1 , α⃗ 2 , α⃗ 3 ⟩ is a basis where α⃗ i = β⃗ 1 + β⃗ i .
1.33 Find one vector ⃗v that will make each into a basis for the space.  1  1 0
(a) ⟨ 1 ,⃗v⟩ in R2 (b) ⟨1,1,⃗v⟩ in R3 (c) ⟨x,1+x2,⃗v⟩ in P2 00
  1.34 Where ⟨β⃗1,...,β⃗n⟩ is a basis, show that in this equation c1β⃗1 +···+ckβ⃗k =ck+1β⃗k+1 +···+cnβ⃗n
each of the ci’s is zero. Generalize.
1.35 A basis contains some of the vectors from a vector space; can it contain them
all?
1.36 Theorem 1.12 shows that, with respect to a basis, every linear combination is unique. If a subset is not a basis, can linear combinations be not unique? If so, must they be?
  1.37 A square matrix is symmetric if for all indices i and j, entry i, j equals entry j, i.
(a) Find a basis for the vector space of symmetric 2×2 matrices. (b) Find a basis for the space of symmetric 3×3 matrices.
(c) Find a basis for the space of symmetric n×n matrices.
1.38 We can show that every basis for R3 contains the same number of vec- tors.
(a) Show that no linearly independent subset of R3 contains more than three vectors.
(b) Show that no spanning subset of R3 contains fewer than three vectors. Hint: recall how to calculate the span of a set and show that this method cannot yield all of R3 when we apply it to fewer than three vectors.
1.39 One of the exercises in the Subspaces subsection shows that the set x
{y | x + y + z = 1} z
is a vector space under these operations.
x1 x2 x1 +x2 −1 x rx−r+1 y1+y2= y1 +y2  ry= ry 
z1 z2 z1+z2 z rz Find a basis.
 
Section III. Basis and Dimension 121 III.2 Dimension
The previous subsection defines a basis of a vector space and shows that a space can have many different bases. So we cannot talk about “the” basis for a vector space. True, some vector spaces have bases that strike us as more natural than others, for instance, R2’s basis E2 or P2’s basis ⟨1,x,x2⟩. But for the vector space{a2x2+a1x+a0 |2a2−a0 =a1},noparticularbasisleapsoutatusas the natural one. We cannot, in general, associate with a space any single basis that best describes it.
We can however find something about the bases that is uniquely associated with the space. This subsection shows that any two bases for a space have the same number of elements. So with each space we can associate a number, the number of vectors in any of its bases.
Before we start, we first limit our attention to spaces where at least one basis has only finitely many members.
One space that is not finite-dimensional is the set of polynomials with real coefficients Example 1.11; this space is not spanned by any finite subset since that would contain a polynomial of largest degree but this space has polynomials of all degrees. Such spaces are interesting and important but we will focus in a different direction. From now on we will study only finite-dimensional vector spaces. In the rest of this book we shall take ‘vector space’ to mean ‘finite-dimensional vector space’.
2.2 Remark One reason for sticking to finite-dimensional spaces is so that the representation of a vector with respect to a basis is a finitely-tall vector and we can easily write it. Another reason is that the statement ‘any infinite-dimensional vector space has a basis’ is equivalent to a statement called the Axiom of Choice [Blass 1984] and so covering this would move us far past this book’s scope. (A discussion of the Axiom of Choice is in the Frequently Asked Questions list for sci.math, and another accessible one is [Rucker].)
To prove the main theorem we shall use a technical result, the Exchange Lemma. We first illustrate it with an example.
2.3 Example Here is a basis for R3 and a vector given as a linear combination of members of that basis.
1 1 0 1 1 1 0
B = ⟨0 , 1 , 0⟩ 2 = (−1) · 0 + 2 1 + 0 · 0 0020002
  2.1 Definition A vector space is finite-dimensional if it has a basis with only finitely many vectors.
122 Chapter Two. Vector Spaces
 Two of the basis vectors have non-zero coefficients. Pick one, for instance the first. Replace it with the vector that we’ve expressed as the combination
1 1 0 Bˆ = ⟨2 , 1 , 0⟩
002
and the result is another basis for R3.
Proof Call the outcome of the exchange Bˆ = ⟨β⃗1,...,β⃗i−1,⃗v,β⃗i+1,...,β⃗n⟩. We first show that Bˆ is linearly independent. Any relationship d1β⃗1 +···+
di⃗v + · · · + dnβ⃗ n = ⃗0 among the members of Bˆ, after substitution for ⃗v,
d1β⃗1 +···+di ·(c1β⃗1 +···+ciβ⃗i +···+cnβ⃗n)+···+dnβ⃗n =⃗0 (∗)
gives a linear relationship among the members of B. The basis B is linearly independent so the coefficient dici of β⃗i is zero. Because we assumed that ci is nonzero, di = 0. Using this in equation (∗) gives that all of the other d’s are also zero. Therefore Bˆ is linearly independent.
We finish by showing that Bˆ has the same span as B. Half of this argument, that [Bˆ] ⊆ [B], is easy; we can write any member d1β⃗ 1+· · ·+di⃗v+· · ·+dnβ⃗ n of [Bˆ] as d1β⃗ 1 +· · ·+di ·(c1β⃗ 1 +· · ·+cnβ⃗ n)+· · ·+dnβ⃗ n, which is a linear combination of linear combinations of members of B, and hence is in [B]. For the [B] ⊆ [Bˆ] half of the argument, recall that if ⃗v = c1β⃗ 1 +· · ·+cnβ⃗ n with ci ̸= 0 then we can rearrange the equation to β⃗ i = (−c1/ci)β⃗ 1 + · · · + (1/ci)⃗v + · · · + (−cn/ci)β⃗ n. Now, consider any member d1β⃗1 +···+diβ⃗i +···+dnβ⃗n of [B], substitute for β⃗i its expression as a linear combination of the members of Bˆ, and recognize, as in the first half of this argument, that the result is a linear combination of linear combinations of members of Bˆ, and hence is in [Bˆ]. QED
Proof Fix a vector space with at least one finite basis. Choose, from among all of this space’s bases, one B = ⟨β⃗1,...,β⃗n⟩ of minimal size. We will show that any other basis D = ⟨⃗δ1,⃗δ2, . . .⟩ also has the same number of members, n. Because B has minimal size, D has no fewer than n vectors. We will argue that it cannot have more than n vectors.
 2.4 Lemma (Exchange Lemma) Assume that B = ⟨β⃗1,...,β⃗n⟩ is a basis for a vector space, and that for the vector ⃗v the relationship ⃗v = c1β⃗ 1 + c2β⃗ 2 + · · · + cn β⃗ n has ci ̸= 0. Then exchanging β⃗ i for ⃗v yields another basis for the space.
 2.5 Theorem In any finite-dimensional vector space, all bases have the same number of elements.
Section III. Basis and Dimension 123
 The basis B spans the space and ⃗δ1 is in the space, so ⃗δ1 is a nontrivial linear combination of elements of B. By the Exchange Lemma, we can swap ⃗δ1 for a vector from B, resulting in a basis B1, where one element is ⃗δ1 and all of the n − 1 other elements are β⃗ ’s.
The prior paragraph forms the basis step for an induction argument. The inductive step starts with a basis Bk (for 1   k < n) containing k members of D and n−k members of B. We know that D has at least n members so there is a ⃗δk+1. Represent it as a linear combination of elements of Bk. The key point: in that representation, at least one of the nonzero scalars must be associated with a β⃗ i or else that representation would be a nontrivial linear relationship among elements of the linearly independent set D. Exchange ⃗δk+1 for β⃗ i to get a new basis Bk+1 with one ⃗δ more and one β⃗ fewer than the previous basis Bk.
Repeat that until no β⃗ ’s remain, so that Bn contains ⃗δ1, . . . ,⃗δn. Now, D cannot have more than these n vectors because any ⃗δn+1 that remains would be in the span of Bn (since it is a basis) and hence would be a linear combination of the other ⃗δ’s, contradicting that D is linearly independent. QED
2.7 Example Any basis for Rn has n vectors since the standard basis En has n vectors. Thus, this definition of ‘dimension’ generalizes the most familiar use of term, that Rn is n-dimensional.
2.8 Example The space Pn of polynomials of degree at most n has dimension n+1. We can show this by exhibiting any basis—⟨1,x,...,xn⟩ comes to mind— and counting its members.
2.9 Example The space of functions {a·cosθ+b·sinθ|a,b∈R} of the real variable θ has dimension 2 since this space has the basis ⟨cos θ, sin θ⟩.
2.10 Example A trivial space is zero-dimensional since its basis is empty.
Again, although we sometimes say ‘finite-dimensional’ for emphasis, from now on we take all vector spaces to be finite-dimensional. So in the next result the word ‘space’ means ‘finite-dimensional vector space’.
Proof The proof of Theorem 2.5 never uses that D spans the space, only that it is linearly independent. QED
2.12 Example Recall the diagram from Example I.2.19 showing the subspaces of R3. Each subspace is described with a minimal spanning set, a basis. The
 2.6 Definition The dimension of a vector space is the number of vectors in any of its bases.
 2.11 Corollary No linearly independent set can have a size greater than the dimension of the enclosing space.
124 Chapter Two. Vector Spaces
 whole space has a basis with three members, the plane subspaces have bases with two members, the line subspaces have bases with one member, and the trivial subspace has a basis with zero members.
In that section we could not show that these are R3’s only subspaces. We can show it now. The prior corollary proves that There are no, say, five-dimensional subspaces of three-space. Further, by Definition 2.6 the dimension of every space is a whole number so there are no subspaces of R3 that are somehow 1.5-dimensional, between lines and planes. Thus the list of subspaces that we gave is exhaustive; the only subspaces of R3 are either three-, two-, one-, or zero-dimensional.
2.13 Corollary Any linearly independent set can be expanded to make a basis.
Proof If a linearly independent set is not already a basis then it must not span the space. Adding to the set a vector that is not in the span will preserve linear independence by Lemma II.1.14. Keep adding until the resulting set does span the space, which the prior corollary shows will happen after only a finite number of steps. QED
2.14 Corollary Any spanning set can be shrunk to a basis.
Proof Call the spanning set S. If S is empty then it is already a basis (the space must be a trivial space). If S = {⃗0} then it can be shrunk to the empty basis, thereby making it linearly independent, without changing its span.
Otherwise, S contains a vector ⃗s1 with ⃗s1 ̸= ⃗0 and we can form a basis
B1 =⟨⃗s1⟩. If[B1]=[S]thenwearedone. Ifnotthenthereisa⃗s2 ∈[S]such
that ⃗s ̸∈ [B ]. Let B = ⟨⃗s , s⃗ ⟩; by Lemma II.1.14 this is linearly independent 21212
so if [B2] = [S] then we are done.
We can repeat this process until the spans are equal, which must happen in
at most finitely many steps. QED
Proof First we will show that a subset with n vectors is linearly independent if and only if it is a basis. The ‘if’ is trivially true — bases are linearly independent. ‘Only if’ holds because a linearly independent set can be expanded to a basis, but a basis has n elements, so this expansion is actually the set that we began with.
To finish, we will show that any subset with n vectors spans the space if and only if it is a basis. Again, ‘if’ is trivial. ‘Only if’ holds because any spanning
   2.15 Corollary In an n-dimensional space, a set composed of n vectors is linearly independent if and only if it spans the space.
Section III. Basis and Dimension 125 set can be shrunk to a basis, but a basis has n elements and so this shrunken
set is just the one we started with. QED
The main result of this subsection, that all of the bases in a finite-dimensional vector space have the same number of elements, is the single most important result in this book. As Example 2.12 shows, it describes what vector spaces and subspaces there can be.
One immediate consequence brings us back to when we considered the two things that could be meant by the term ‘minimal spanning set’. At that point we defined ‘minimal’ as linearly independent but we noted that another reasonable interpretation of the term is that a spanning set is ‘minimal’ when it has the fewest number of elements of any set with the same span. Now that we have shown that all bases have the same number of elements, we know that the two senses of ‘minimal’ are equivalent.
Exercises
Assume that all spaces are finite-dimensional unless otherwise stated.
  2.16 Find a basis for, and the dimension of, P2.
2.17 Find a basis for, and the dimension of, the solution set of this system.
x1−4x2+3x3− x4=0 2x1 −8x2 +6x3 −2x4 =0
  2.18 Find a basis for, and the dimension of, each space. x
y
(a) {z∈R4 |x−w+z=0}

w
(b) the set of 5×5 matrices whose only nonzero entries are on the diagonal (e.g.,
in entry 1, 1 and 2, 2, etc.)
(c) {a0 + a1x + a2x2 + a3x3 | a0 + a1 = 0 and a2 − 2a3 = 0} ⊆ P3
2.19 Find a basis for, and the dimension of, M2×2, the vector space of 2×2 matrices. 2.20 Find the dimension of the vector space of matrices
 a b  cd
subject to each condition.
(a) a,b,c,d ∈ R
(b) a − b + 2c = 0 and d ∈ R
(c) a + b + c = 0, a + b − c = 0, and d ∈ R
   2.21 Find the dimension of this subspace of R2.  a+b 
S={ a+c |a,b,c∈R}
  2.22 Find the dimension of each.
(a) The space of cubic polynomials p(x) such that p(7) = 0
(b) The space of cubic polynomials p(x) such that p(7) = 0 and p(5) = 0
126 Chapter Two. Vector Spaces
 (c) The space of cubic polynomials p(x) such that p(7) = 0, p(5) = 0, and p(3) = 0 (d) The space of cubic polynomials p(x) such that p(7) = 0, p(5) = 0, p(3) = 0,
and p(1) = 0
2.23 What is the dimension of the span of the set { cos2 θ, sin2 θ, cos 2θ, sin 2θ }? This
span is a subspace of the space of all real-valued functions of one real variable. 2.24 Find the dimension of C47, the vector space of 47-tuples of complex numbers. 2.25 What is the dimension of the vector space M3×5 of 3×5 matrices?
  2.26 Show that this is a basis for R4.
1 1 1 1
⟨0,1,1,1⟩ 0 0 1 1
0001
(We can use the results of this subsection to simplify this job.) 2.27 Decide if each is a basis for P2.
(a) {1,x2,x2 −x} (b) {x2 +x,x2 −x} (c) {2x2 +x+1,2x+1,2}
(d) {3x2,−1,3x,x2 −x} 2.28 Refer to Example 2.12.
(a) Sketch a similar subspace diagram for P2.
(b) Sketch one for M2×2.
  2.29 Where S is a set, the functions f: S → R form a vector space under the natural
operations: the sum f + g is the function given by f + g (s) = f(s) + g(s) and the scalar product is r · f (s) = r · f(s). What is the dimension of the space resulting for each domain?
(a) S={1} (b) S={1,2} (c) S={1,...,n}
2.30 (See Exercise 29.) Prove that this is an infinite-dimensional space: the set of
all functions f: R → R under the natural operations.
2.31 (See Exercise 29.) What is the dimension of the vector space of functions
f: S → R, under the natural operations, where the domain S is the empty set? 2.32 Show that any set of four vectors in R2 is linearly dependent.
2.33 Show that ⟨α⃗ 1, α⃗ 2, α⃗ 3⟩ ⊂ R3 is a basis if and only if there is no plane through
the origin containing all three vectors.
2.34 Prove that any subspace of a finite dimensional space is finite dimensional. 2.35 Where is the finiteness of B used in Theorem 2.5?
2.36 Prove that if U and W are both three-dimensional subspaces of R5 then U ∩ W
is non-trivial. Generalize.
2.37 A basis for a space consists of elements of that space. So we are naturally led to
how the property ‘is a basis’ interacts with operations ⊆ and ∩ and ∪. (Of course, a basis is actually a sequence in that it is ordered, but there is a natural extension of these operations.)
(a) Consider first how bases might be related by ⊆. Assume that U,W are subspaces of some vector space and that U ⊆ W. Can there exist bases BU for U and BW for W such that BU ⊆ BW ? Must such bases exist?
For any basis BU for U, must there be a basis BW for W such that BU ⊆ BW?
Section III. Basis and Dimension 127
 For any basis BW for W, must there be a basis BU for U such that BU ⊆ BW?
For any bases BU,BW for U and W, must BU be a subset of BW? (b) Is the ∩ of bases a basis? For what space?
(c) Is the ∪ of bases a basis? For what space?
(d) What about the complement operation?
(Hint. Test any conjectures against some subspaces of R3.)
  2.38 Consider how ‘dimension’ interacts with ‘subset’. Assume U and W are both
subspaces of some vector space, and that U ⊆ W.
(a) Prove that dim(U)   dim(W).
(b) Prove that equality of dimension holds if and only if U = W.
(c) Show that the prior item does not hold if they are infinite-dimensional.
? 2.39 [Wohascum no. 47] For any vector ⃗v in Rn and any permutation σ of the numbers 1, 2, . . . , n (that is, σ is a rearrangement of those numbers into a new order), define σ(⃗v) to be the vector whose components are vσ(1), vσ(2), . . . , and vσ(n) (where σ(1) is the first number in the rearrangement, etc.). Now fix ⃗v and let V be the span of {σ(⃗v) | σ permutes 1, . . . , n}. What are the possibilities for the dimension of V?
III.3 Vector Spaces and Linear Systems
We will now reconsider linear systems and Gauss’s Method, aided by the tools and terms of this chapter. We will make three points.
For the first, recall the insight from the Chapter One that Gauss’s Method works by taking linear combinations of rows — if two matrices are related by row operations A −→ · · · −→ B then each row of B is a linear combination of the rows of A. Therefore, the right setting in which to study row operations in general, and Gauss’s Method in particular, is the following vector space.
 3.1 Definition The row space of a matrix is the span of the set of its rows. The row rank is the dimension of this space, the number of linearly independent rows.
3.2 Example If
 2 3  A=46
then Rowspace(A) is this subspace of the space of two-component row vectors.
{c1 ·(2 3)+c2 ·(4 6)|c1,c2 ∈R}
The second row vector is linearly dependent on the first and so we can simplify the above description to {c · (2 3) | c ∈ R}.
128 Chapter Two. Vector Spaces
  3.3 Lemma If two matrices A and B are related by a row operation ρi ↔ρj kρ kρi +ρj
A −→ B or A −→ B or A −→ B
(for i ̸= j and k ̸= 0) then their row spaces are equal. Hence, row-equivalent matrices have the same row space and therefore the same row rank.
i
Proof Corollary One.III.2.4 shows that when A −→ B then each row of B is a linear combination of the rows of A. That is, in the above terminology, each row of B is an element of the row space of A. Then Rowspace(B) ⊆ Rowspace(A) follows because a member of the set Rowspace(B) is a linear combination of the rows of B, so it is a combination of combinations of the rows of A, and by the Linear Combination Lemma is also a member of Rowspace(A).
For the other set containment, recall Lemma One.III.1.5, that row opera- tions are reversible so A −→ B if and only if B −→ A. Then Rowspace(A) ⊆ Rowspace(B) follows as in the previous paragraph. QED
Of course, Gauss’s Method performs the row operations systematically, with the goal of echelon form.
Proof Lemma One.III.2.5 says that no nonzero row of an echelon form matrix is a linear combination of the other rows. This result restates that using this chapter’s terminology. QED
Thus, in the language of this chapter, Gaussian reduction works by eliminating linear dependences among rows, leaving the span unchanged, until no nontrivial linear relationships remain among the nonzero rows. In short, Gauss’s Method produces a basis for the row space.
3.5 Example From any matrix, we can produce a basis for the row space by performing Gauss’s Method and taking the nonzero rows of the resulting echelon form matrix. For instance,
 3.4 Lemma The nonzero rows of an echelon form matrix make up a linearly independent set.
1 3 1
1 3 1
  −ρ1+ρ2 6ρ2+ρ3   1 4 1 −→ −→ 0 1 0 205 −2ρ1+ρ3 003
produces the basis ⟨(1 3 1), (0 1 0), (0 0 3)⟩ for the row space. This is a basis for the row space of both the starting and ending matrices, since the two row spaces are equal.
Section III. Basis and Dimension 129 Using this technique, we can also find bases for spans not directly involving
row vectors.
Our interest in column spaces stems from our study of linear systems. An example is that this system
c1 +3c2 +7c3 =d1 2c1 +3c2 +8c3 =d2 c2 +2c3 =d3 4c1 + 4c3 = d4
has a solution if and only if the vector of d’s is a linear combination of the other
  3.6 Definition The column space of a matrix is the span of the set of its columns. The column rank is the dimension of the column space, the number of linearly independent columns.
column vectors,
1 3 7 d1 
2 3 8 d2  c1 +c2 +c3 =  0 1 2 d3 
4 0 4 d4
meaning that the vector of d’s is in the column space of the matrix of coefficients.
3.7 Example Given this matrix,
1 3 7 2 3 8 0 1 2 404
to get a basis for the column space, temporarily turn the columns into rows and reduce.
1 2 0 4  
−3ρ1+ρ2 −2ρ2+ρ3 −→ −→
1 2 0 4   
3 3 1 0
7 8 2 4 −7ρ1+ρ3
0 −3 1 −12 0 0 0 0
Now turn the rows back to columns.
1  0 
2 −3 ⟨ , ⟩ 0  1 
4 −12
The result is a basis for the column space of the given matrix.
130 Chapter Two. Vector Spaces
  3.8 Definition The transpose of a matrix is the result of interchanging its rows and columns, so that column j of the matrix A is row j of AT and vice versa.
So we can summarize the prior example as “transpose, reduce, and transpose back.”
We can even, at the price of tolerating the as-yet-vague idea of vector spaces being “the same,” use Gauss’s Method to find bases for spans in other types of vector spaces.
3.9 Example To get a basis for the span of {x2 +x4,2x2 +3x4,−x2 −3x4} in the space P4, think of these three polynomials as “the same” as the row vectors (0 0 1 0 1),(0 0 2 0 3),and(0 0 −1 0 −3),applyGauss’sMethod
ρ1 +ρ3
make the phrase “the same” precise at the start of the next chapter.)
Thus, the first point for this subsection is that the tools of this chapter give us a more conceptual understanding of Gaussian reduction.
For the second point observe that row operations on a matrix can change its column space.
     
1 2 −2ρ1+ρ2 1 2 24 −→ 00
The column space of the left-hand matrix contains vectors with a second compo- nent that is nonzero but the column space of the right-hand matrix contains only vectors whose second component is zero, so the two spaces are different. This observation makes next result surprising.
3.10 Lemma Row operations do not change the column rank.
Proof Restated,ifAreducestoBthenthecolumnrankofBequalsthecolumn rank of A.
This proof will be finished if we show that row operations do not affect linear relationships among columns, because the column rank is the size of the largest set of unrelated columns. That is, we will show that a relationship exists among columns (such as that the fifth column is twice the second plus the fourth) if and only if that relationship exists after the row operation. But this is exactly the
0 0 1 0 1
  −2ρ1+ρ2 2ρ2+ρ3   0 0 2 0 3 −→ −→ 0 0 0 0 1
0 0 −1 0 −3
and translate back to get the basis ⟨x2 + x4 , x4 ⟩. (As mentioned earlier, we will
0 0 1 0 1 0 0 0 0 0
 
Section III. Basis and Dimension 131 first theorem of this book, Theorem One.I.1.5: in a relationship among columns,
a1,1 a1,n 0
 a2,1   a2,n  0 c1 · . +···+cn · . =.  .   .   . 
am,1 am,n 0
row operations leave unchanged the set of solutions (c1, . . . , cn). QED
Another way to make the point that Gauss’s Method has something to say about the column space as well as about the row space is with Gauss-Jordan reduction. It ends with the reduced echelon form of a matrix, as here.
1 3 1 6 1 3 0 2 2 6 3 16 −→ ··· −→ 0 0 1 4 1316 0000
Consider the row space and the column space of this result.
The first point made earlier in this subsection says that to get a basis for the
row space we can just collect the rows with leading entries. However, because this is in reduced echelon form, a basis for the column space is just as easy: collect the columns containing the leading entries, ⟨⃗e1,⃗e2⟩. Thus, for a reduced echelon form matrix we can find bases for the row and column spaces in essentially the same way, by taking the parts of the matrix, the rows or columns, containing the leading entries.
3.11 Theorem For any matrix, the row rank and column rank are equal.
Proof Bring the matrix to reduced echelon form. Then the row rank equals the number of leading entries since that equals the number of nonzero rows. Then also, the number of leading entries equals the column rank because the set of columns containing leading entries consists of some of the ⃗ei’s from a standard basis, and that set is linearly independent and spans the set of columns. Hence, in the reduced echelon form matrix, the row rank equals the column rank, because each equals the number of leading entries.
But Lemma 3.3 and Lemma 3.10 show that the row rank and column rank are not changed by using row operations to get to reduced echelon form. Thus the row rank and the column rank of the original matrix are also equal. QED
3.12 Definition The rank of a matrix is its row rank or column rank.
So the second point that we have made in this subsection is that the column space and row space of a matrix have the same dimension.
   
132 Chapter Two. Vector Spaces
 Our final point is that the concepts that we’ve seen arising naturally in the study of vector spaces are exactly the ones that we have studied with linear systems.
 3.13 Theorem For linear systems with n unknowns and with matrix of coefficients A, the statements
(1) therankofAisr
(2) the vector space of solutions of the associated homogeneous system has
dimension n − r are equivalent.
So if the system has at least one particular solution then for the set of solutions, the number of parameters equals n − r, the number of variables minus the rank of the matrix of coefficients.
Proof The rank of A is r if and only if Gaussian reduction on A ends with r nonzero rows. That’s true if and only if echelon form matrices row equivalent to A have r-many leading variables. That in turn holds if and only if there are n − r free variables. QED
 3.14 Corollary Where the matrix A is n×n, these statements (1) therankofAisn
(2) A is nonsingular
(3) the rows of A form a linearly independent set
(4) the columns of A form a linearly independent set
(5) any linear system whose matrix of coefficients is A has one and only one
solution are equivalent.
Proof Clearly (1) ⇐⇒ (2) ⇐⇒ (3) ⇐⇒ (4). The last, (4) ⇐⇒ (5), holds because a set of n column vectors is linearly independent if and only if it is a basis for Rn, but the system
a1,1  a1,n  d1 
a2,1  a2,n  d2  c1 . +···+cn . = .   .   .   . 
am,1 am,n dm
has a unique solution for all choices of d1, . . . , dn ∈ R if and only if the vectors
of a’s on the left form a basis. QED
3.15 Remark [Munkres] Sometimes the results of this subsection are mistakenly remembered to say that the general solution of an m equations, n unknowns
Section III. Basis and Dimension 133
 system uses n − m parameters. The number of equations is not the relevant number; rather, what matters is the number of independent equations, the num- ber of equations in a maximal independent set. Where there are r independent equations, the general solution involves n − r parameters.
Exercises
3.16 Transpose each.
 2 1   2 1 
 1 4 3  0 (c) 6 7 8 (d)0
(a) 3 1 (b) 1 3 (e) (−1 −2)
0
  3.17 Decide if the vector is in the row space of the matrix.  2 1  0 1 3
(a) 3 1,(10) (b)−1 0 1,(111) −1 2 7
  3.18 Decide if the vector is in the column space.  1 1  1  1 3 11 (a)11,3 (b)204,0
1−3−3 0
  3.19 Decide if the vector is in the column space of the matrix.
 2 1  1   4 −8  0  1 −1 12 (a) 2 5 , −3 (b) 2 −4 , 1 (c)  1 1 −1, 0
  3.20 Find a basis for the row space of this matrix.
2 0 3 4 0 1 1 −1  3 1 0 2  1 0 −4 −1
  3.21 Find the rank of each matrix.
2 1 3 1 −1 2 1 3 2
(a)1 −1 2 (b)3 −3 6 (c)5 1 1 1 0 3 −2 2 −4 6 4 3
0 0 0 (d)0 0 0
000
3.22 Give a basis for the column space of this matrix. Give the matrix’s rank.
1 3 −1 2 2 1 1 0 0114
  3.23 Find a basis for the span of each set. (a){(1 3),(−1 3),(1 4),(2 1)}⊆M1×2
1 3 1
(b) {2, 1 ,−3} ⊆ R3 1 −1 −3
−1−11 0
134
Chapter Two. Vector Spaces
 (c) {1+x,1−x2,3+2x−x2}⊆P3
 1 0 1  1 0 3  −1 0 −5 
1 −1 0
(a) {1, 2 ,12} 306
(b) {x+x2,2−2x,7,4+3x+2x2}
3.25 Which matrices have rank zero? Rank one?
  3.26 Given a, b, c ∈ R, what choice of d will cause this matrix to have the rank of one?
 a b  cd
3.27 Find the column rank of this matrix.
 1 3 −1 5 0 4 
201041
3.28 Show that a linear system with at least one solution has at most one solution if and only if the matrix of coefficients has rank equal to the number of its columns.
  3.29 If a matrix is 5×9, which set must be dependent, its set of rows or its set of
columns?
3.30 Give an example to show that, despite that they have the same dimension, the
row space and column space of a matrix need not be equal. Are they ever equal? 3.31 Show that the set {(1,−1,2,−3),(1,1,2,0),(3,−1,6,−6)} does not have the
same span as {(1,0,1,0),(0,2,0,3)}. What, by the way, is the vector space?
  3.32 Show that this set of column vectors
d1 3x+2y+4z=d1 {d2|there are x, y, and z such that: x − z=d2}
d3 2x+2y+5z=d3 is a subspace of R3. Find a basis.
3.33 Show that the transpose operation is linear: (rA+sB)T =rAT +sBT
for r,s ∈ R and A,B ∈ Mm×n.
  3.34 In this subsection we have shown that Gaussian reduction finds a basis for the
row space.
(a) Show that this basis is not unique — different reductions may yield different
bases.
(b) Produce matrices with equal row spaces but unequal numbers of rows.
(c) Prove that two matrices have equal row spaces if and only if after Gauss-Jordan
reduction they have the same nonzero rows.
3.35 Why is there not a problem with Remark 3.15 in the case that r is bigger than
n?
3.36 Show that the row rank of an m×n matrix is at most m. Is there a better bound?
(d){3 1 −1,2 1 4,−1 −1 −9}⊆M2×3
3.24 Give a basis for the span of each set, in the natural vector space.
Section III. Basis and Dimension 135
 3.37 Show that the rank of a matrix equals the rank of its transpose.
3.38 True or false: the column space of a matrix equals the row space of its transpose.   3.39 We have seen that a row operation may change the column space. Must it?
3.40 Prove that a linear system has a solution if and only if that system’s matrix of coefficients has the same rank as its augmented matrix.
3.41 An m×n matrix has full row rank if its row rank is m, and it has full column rank if its column rank is n.
(a) Show that a matrix can have both full row rank and full column rank only if it is square.
(b) Prove that the linear system with matrix of coefficients A has a solution for any d1, ..., dn’s on the right side if and only if A has full row rank.
(c) Prove that a homogeneous system has a unique solution if and only if its matrix of coefficients A has full column rank.
(d) Prove that the statement “if a system with matrix of coefficients A has any solution then it has a unique solution” holds if and only if A has full column rank.
3.42 How would the conclusion of Lemma 3.3 change if Gauss’s Method were changed to allow multiplying a row by zero?
3.43 What is the relationship between rank(A) and rank(−A)? Between rank(A) and rank(kA)? What, if any, is the relationship between rank(A), rank(B), and rank(A + B)?
III.4 Combining Subspaces
This subsection is optional. It is required only for the last sections of Chapter Three and Chapter Five and for occasional exercises. You can pass it over without loss of continuity.
One way to understand something is to see how to build it from component parts. For instance, we sometimes think of R3 put together from the x-axis, the y-axis, and z-axis. In this subsection we will describe how to decompose a vector space into a combination of some of its subspaces. In developing this idea of subspace combination, we will keep the R3 example in mind as a prototype.
Subspaces are subsets and sets combine via union. But taking the combination operation for subspaces to be the simple set union operation isn’t what we want. For instance, the union of the x-axis, the y-axis, and z-axis is not all of R3. In fact this union is not a subspace because it is not closed under addition: this vector
1 0 0 1
0 + 1 + 0 = 1 0011
136 Chapter Two. Vector Spaces
 is in none of the three axes and hence is not in the union. Therefore to combine subspaces, in addition to the members of those subspaces, we must at least also include all of their linear combinations.
Writing ‘+’ fits with the conventional practice of using this symbol for a natural accumulation operation.
4.2 Example Our R3 prototype works with this. Any vector w⃗ ∈ R3 is a linear combination c1⃗v1 + c2⃗v2 + c3⃗v3 where ⃗v1 is a member of the x-axis, etc., in this
 4.1 Definition Where W1, . . . , Wk are subspaces of a vector space, their sum is thespanoftheirunionW1+W2+···+Wk =[W1∪W2∪···Wk].
way
w  w   0   0  11
w2=1· 0 +1·w2+1· 0  w3 0 0 w3
and so x-axis + y-axis + z-axis = R3.
4.3 Example A sum of subspaces can be less than the entire space. Inside of P4, let L be the subspace of linear polynomials { a + bx | a, b ∈ R } and let C be the subspace of purely-cubic polynomials {cx3 | c ∈ R}. Then L + C is not all of P4. Instead,L+C={a+bx+cx3 |a,b,c∈R}.
4.4 Example A space can be described as a combination of subspaces in more than one way. Besides the decomposition R3 = x-axis + y-axis + z-axis, we can also write R3 = xy-plane + yz-plane. To check this, note that any w⃗ ∈ R3 can be written as a linear combination of a member of the xy-plane and a member
of the yz-plane; here are two such combinations.
w  w   0  w   w   0 
11 11 w2=1·w2+1· 0  w2=1·w2/2+1·w2/2
w3 0 w3 w3 0 w3
The above definition gives one way in which we can think of a space as a
combination of some of its parts. However, the prior example shows that there is
at least one interesting property of our benchmark model that is not captured by
the definition of the sum of subspaces. In the familiar decomposition of R3, we
often speak of a vector’s ‘x part’ or ‘y part’ or ‘z part’. That is, in our prototype
each vector has a unique decomposition into pieces from the parts making up
the whole space. But in the decomposition used in Example 4.4, we cannot refer
to the “xy part” of a vector—these three sums
1 1 0 1 0 1 0
2 = 2 + 0 = 0 + 2 = 1 + 1 3030303
Section III. Basis and Dimension 137
 all describe the vector as comprised of something from the first plane plus something from the second plane, but the “xy part” is different in each.
That is, when we consider how R3 is put together from the three axes we might mean “in such a way that every vector has at least one decomposition,” which gives the definition above. But if we take it to mean “in such a way that every vector has one and only one decomposition” then we need another condition on combinations. To see what this condition is, recall that vectors are uniquely represented in terms of a basis. We can use this to break a space into a sum of subspaces such that any vector in the space breaks uniquely into a sum of members of those subspaces.
4.5 Example Consider R3 with its standard basis E3 = ⟨⃗e1,⃗e2,⃗e3⟩. The subspace
with the basis B1 = ⟨⃗e1⟩ is the x-axis, the subspace with the basis B2 = ⟨⃗e2⟩ is
the y-axis, and the subspace with the basis B3 = ⟨⃗e3⟩ is the z-axis. The fact
that any member of R3 is expressible as a sum of vectors from these subspaces x x 0 0
y = 0 + y + 0 z00z
reflects the fact that E3 spans the space — this equation x 1 0 0
y = c1 0 + c2 1 + c3 0 z001
has a solution for any x, y, z ∈ R. And the fact that each such expression is unique reflects that fact that E3 is linearly independent, so any equation like the one above has a unique solution.
4.6 Example We don’t have to take the basis vectors one at a time, we can conglomerate them into larger sequences. Consider again the space R3 and the vectors from the standard basis E3. The subspace with the basis B1 = ⟨⃗e1,⃗e3⟩ is the xz-plane. The subspace with the basis B2 = ⟨⃗e2⟩ is the y-axis. As in the prior example, the fact that any member of the space is a sum of members of the two subspaces in one and only one way
x x 0 y = 0 + y
zz0
is a reflection of the fact that these vectors form a basis — this equation
x 1 0 0
y = (c1 0 + c3 0) + c2 1 z010
138 Chapter Two. Vector Spaces has one and only one solution for any x, y, z ∈ R.
  4.7 Definition The concatenation of the sequences B1 = ⟨β⃗ 1,1 , . . . , β⃗ 1,n1 ⟩, . . . , Bk = ⟨β⃗ k,1 , . . . , β⃗ k,nk ⟩ adjoins them into a single sequence.
B ⌢B ⌢···⌢B = ⟨β⃗ ,...,β⃗ ,β⃗ ,...,β⃗
1 2 k 1,1 1,n1 2,1 k,nk
⟩
 4.8 Lemma Let V be a vector space that is the sum of some of its subspaces V = W1 +···+Wk. Let B1, ..., Bk be bases for these subspaces. The following are equivalent.
(1) The expression of any ⃗v ∈ V as a combination ⃗v = w⃗ 1 + · · · + w⃗ k with w⃗ i ∈ Wi is unique.
(2) The concatenation B ⌢ · · ·⌢ B is a basis for V. 1k
(3) Among nonzero vectors from different Wi’s every linear relationship is trivial.
Proof We will show that (1) =⇒ (2), that (2) =⇒ (3), and finally that (3) =⇒ (1). For these arguments, observe that we can pass from a combination of w⃗ ’s to a combination of β⃗ ’s
d1w⃗1 +···+dkw⃗k =d1(c1,1β⃗1,1 +···+c1,n1β⃗1,n1) +···+dk(ck,1β⃗k,1 +···+ck,nkβ⃗k,nk)
=d1c1,1 ·β⃗1,1 +···+dkck,nk ·β⃗k,nk (∗)
and vice versa (we can move from the bottom to the top by taking each di to be 1).
For (1) =⇒ (2), assume that all decompositions are unique. We will show that B ⌢···⌢B spans the space and is linearly independent. It spans the
1k
space because the assumption that V = W1 + · · · + Wk means that every ⃗v can be expressed as ⃗v = w⃗ 1 + · · · + w⃗ k, which translates by equation (∗) to an expression of ⃗v as a linear combination of the β⃗ ’s from the concatenation. For linear independence, consider this linear relationship.
⃗0=c1,1β⃗1,1 +···+ck,nkβ⃗k,nk
Regroup as in (∗) (that is, move from bottom to top) to get the decomposition ⃗0 = w⃗ 1 + · · · + w⃗ k. Because the zero vector obviously has the decomposition ⃗0 = ⃗0 + · · · + ⃗0, the assumption that decompositions are unique shows that each w⃗ i is the zero vector. This means that ci,1β⃗ i,1 + · · · + ci,ni β⃗ i,ni = ⃗0, and since each Bi is a basis we have the desired conclusion that all of the c’s are zero.
For (2) =⇒ (3) assume that the concatenation of the bases is a basis for the entire space. Consider a linear relationship among nonzero vectors from different
Section III. Basis and Dimension 139 Wi’s. This might or might not involve a vector from W1, or one from W2, etc.,
so we write it ⃗0 = · · · + diw⃗ i + · · · . As in equation (∗) expand the vector. ⃗0= ···+di(ci,1β⃗i,1 +···+ci,niβ⃗i,ni)+···
 = ···+dici,1 ·β⃗i,1 +···+dici,ni ·β⃗i,ni +···
The linear independence of B ⌢ · · ·⌢ B gives that each coefficient d c is zero.
1k ii,j
Since w⃗ i is nonzero vector, at least one of the ci,j’s is not zero, and thus di is
zero. This holds for each di, and therefore the linear relationship is trivial. Finally, for (3) =⇒ (1), assume that among nonzero vectors from different Wi’s any linear relationship is trivial. Consider two decompositions of a vector ⃗v= ···+w⃗i+··· and⃗v= ···+⃗uj+··· wherew⃗i ∈Wi and⃗uj ∈Wj. Subtract one from the other to get a linear relationship, something like this (if there is no
⃗ui or w⃗ j then leave those out).
⃗0= ···+(w⃗i −⃗ui)+···+(w⃗j −⃗uj)+···
The case assumption that statement (3) holds implies that the terms each equal the zero vector w⃗ i − ⃗ui = ⃗0. Hence decompositions are unique. QED
 4.9 Definition A collection of subspaces { W1 , . . . , Wk } is independent if no nonzero vector from any Wi is a linear combination of vectors from the other subspaces W1,...,Wi−1,Wi+1,...,Wk.
 4.10 Definition A vector space V is the direct sum (or internal direct sum) of its subspaces W1,...,Wk if V = W1 + W2 + ··· + Wk and the collection {W1, . . . , Wk } is independent. We write V = W1 ⊕ W2 ⊕ · · · ⊕ Wk.
4.11 Example Our prototype works: R3 = x-axis ⊕ y-axis ⊕ z-axis. 4.12 Example The space of 2×2 matrices is this direct sum.
 a 0   0 b   0 0 
{ 0 d |a,d∈R}⊕{ 0 0 |b∈R}⊕{ c 0 |c∈R}
It is the direct sum of subspaces in many other ways as well; direct sum decompositions are not unique.
Proof In Lemma 4.8, the number of basis vectors in the concatenation equals the sum of the number of vectors in the sub-bases. QED
 4.13 Corollary The dimension of a direct sum is the sum of the dimensions of its summands.
140 Chapter Two. Vector Spaces The special case of two subspaces is worth its own mention.
  4.14 Definition When a vector space is the direct sum of two of its subspaces then they are complements.
 4.15 Lemma A vector space V is the direct sum of two of its subspaces W1 and W2 if and only if it is the sum of the two V = W1 + W2 and their intersection is trivial W1 ∩ W2 = {⃗0 }.
Proof Suppose first that V = W1 ⊕ W2. By definition, V is the sum of the two V = W1 + W2. To show that their intersection is trivial let ⃗v be a vector from W1 ∩ W2 and consider the equation ⃗v = ⃗v. On that equation’s left side is a member of W1 and on the right is a member of W2, which we can think of as a linear combination of members of W2. But the two spaces are independent so the only way that a member of W1 can be a linear combination of vectors from W2 is if that member is the zero vector ⃗v = ⃗0.
For the other direction, suppose that V is the sum of two spaces with a trivial intersection. To show that V is a direct sum of the two we need only show that the spaces are independent — that no nonzero member of the first is expressible as a linear combination of members of the second, and vice versa. This holds because any relationship w⃗ 1 = c1w⃗ 2,1 + · · · + ckw⃗ 2,k (with w⃗ 1 ∈ W1 and w⃗ 2,j ∈ W2 for all j) shows that the vector on the left is also in W2, since the right side is a combination of members of W2. The intersection of these two spaces is trivial, so w⃗ 1 = ⃗0. The same argument works for any w⃗ 2. QED
4.16 Example In R2 the x-axis and the y-axis are complements, that is, R2 = x-axis⊕y-axis. A space can have more than one pair of complementary subspaces; another pair for R2 are the subspaces consisting of the lines y = x and y = 2x.
4.17Example InthespaceF={acosθ+bsinθ|a,b∈R},thesubspacesW1 = {acosθ | a ∈ R} and W2 = {bsinθ | b ∈ R} are complements. The prior exam- ple noted that a space can be decomposed into more than one pair of comple- ments. In addition note that F can has more than one pair of complementary subspaces where the first in the pair is W1 — another complement of W1 is W3 ={bsinθ+bcosθ|b∈R}.
4.18 Example In R3, the xy-plane and the yz-planes are not complements, which is the point of the discussion following Example 4.4. One complement of the xy-plane is the z-axis.
Here is a natural question that arises from Lemma 4.15: for k > 2 is the simplesumV=W1+···+Wk alsoadirectsumifandonlyiftheintersection of the subspaces is trivial?
Section III. Basis and Dimension 141
 4.19 Example If there are more than two subspaces then having a trivial inter- section is not enough to guarantee unique decomposition (i.e., is not enough to ensure that the spaces are independent). In R3, let W1 be the x-axis, let W2 be the y-axis, and let W3 be this.
q
W3 ={q|q,r∈R}
r
The check that R3 = W1 +W2 +W3 is easy. The intersection W1 ∩W2 ∩W3 is
trivial, but decompositions aren’t unique.
x 0 0 x x−y 0 y  y  =  0  +  y − x  +  x  =  0  +  0  +  y 
z00z00z
(This example also shows that this requirement is also not enough: that all pairwise intersections of the subspaces be trivial. See Exercise 30.)
In this subsection we have seen two ways to regard a space as built up from component parts. Both are useful; in particular we will use the direct sum definition at the end of the Chapter Five.
Exercises
  4.20 Decide if R2 is the direct sum of each W1 and W2.  x   x 
(a)W1 ={ 0 |x∈R},W2 ={ x |x∈R}  s   s 
(b)W1 ={ s |s∈R},W2 ={ 1.1s |s∈R} (c) W1 = R2, W2 = {⃗0}
 t  (d)W1=W2={ t |t∈R}
 1   x   −1   0  (e)W1={ 0 + 0 |x∈R},W2={ 0 + y |y∈R}
  4.21 Show that R3 is the direct sum of the xy-plane with each of these. (a) the z-axis
(b) the line
4.22 IsP2 thedirectsumof{a+bx2 |a,b∈R}and{cx|c∈R}?   4.23 In Pn, the even polynomials are the members of this set
E={p∈Pn |p(−x)=p(x) for all x} and the odd polynomials are the members of this set.
O={p∈Pn |p(−x)=−p(x) for all x} Show that these are complementary subspaces.
z
{z | z ∈ R} z
142 Chapter Two. Vector Spaces
 4.24 Which of these subspaces of R3
W1: the x-axis, W2: the y-axis, W3: the z-axis,
W4:theplanex+y+z=0, W5:theyz-plane can be combined to
(a) sum to R3? (b) direct sum to R3?
 4.25 ShowthatPn ={a0 |a0 ∈R}⊕...⊕{anxn |an ∈R}.
4.26 What is W1 + W2 if W1 ⊆ W2?
4.27 Does Example 4.5 generalize? That is, is this true or false: if a vector space V has a basis ⟨β⃗ 1, . . . , β⃗ n⟩ then it is the direct sum of the spans of the one-dimensional subspaces V = [{ β⃗ 1 }] ⊕ . . . ⊕ [{ β⃗ n }]?
4.28 Can R4 be decomposed as a direct sum in two different ways? Can R1?
4.29 This exercise makes the notation of writing ‘+’ between sets more natural.
Prove that, where W1,...,Wk are subspaces of a vector space,
W1 +···+Wk ={w⃗1 +w⃗2 +···+w⃗k |w⃗1 ∈W1,...,w⃗k ∈Wk},
and so the sum of subspaces is the subspace of all sums.
4.30 (Refer to Example 4.19. This exercise shows that the requirement that pairwise intersections be trivial is genuinely stronger than the requirement only that the intersection of all of the subspaces be trivial.) Give a vector space and three subspaces W1, W2, and W3 such that the space is the sum of the subspaces, the intersection of all three subspaces W1 ∩ W2 ∩ W3 is trivial, but the pairwise intersections W1 ∩ W2, W1 ∩ W3, and W2 ∩ W3 are nontrivial.
4.31 Prove that if V = W1 ⊕ . . . ⊕ Wk then Wi ∩ Wj is trivial whenever i ̸= j. This shows that the first half of the proof of Lemma 4.15 extends to the case of more than two subspaces. (Example 4.19 shows that this implication does not reverse; the other half does not extend.)
4.32 Recall that no linearly independent set contains the zero vector. Can an independent set of subspaces contain the trivial subspace?
  4.33 Does every subspace have a complement?
  4.34 Let W1,W2 be subspaces of a vector space.
(a) Assume that the set S1 spans W1, and that the set S2 spans W2. Can S1 ∪ S2
span W1 + W2? Must it?
(b) Assume that S1 is a linearly independent subset of W1 and that S2 is a linearly
independent subset of W2. Can S1 ∪ S2 be a linearly independent subset of W1 + W2? Must it?
4.35 When we decompose a vector space as a direct sum, the dimensions of the subspaces add to the dimension of the space. The situation with a space that is given as the sum of its subspaces is not as simple. This exercise considers the two-subspace special case.
(a) For these subspaces of M2×2 find W1 ∩ W2, dim(W1 ∩ W2), W1 + W2, and dim(W1 + W2).
 0 0   0 b 
W1={ c d |c,d∈R} W2={ c 0 |b,c∈R}
Section III. Basis and Dimension 143
 (b) Suppose that U and W are subspaces of a vector space. Suppose that the sequence ⟨β⃗ 1, . . . , β⃗ k⟩ is a basis for U ∩ W. Finally, suppose that the prior sequence has been expanded to give a sequence ⟨⃗μ1,...,⃗μj,β⃗1,...,β⃗k⟩ that is a basis for U, and a sequence ⟨β⃗1,...,β⃗k,ω⃗1,...,ω⃗p⟩ that is a basis for W. Prove that this sequence
⟨⃗μ1,...,⃗μj,β⃗1,...,β⃗k,ω⃗ 1,...,ω⃗ p⟩
is a basis for the sum U+W.
(c) Conclude that dim(U + W) = dim(U) + dim(W) − dim(U ∩ W).
(d) Let W1 and W2 be eight-dimensional subspaces of a ten-dimensional space.
List all values possible for dim(W1 ∩ W2).
4.36 Let V = W1 ⊕ ··· ⊕ Wk and for each index i suppose that Si is a linearly
independent subset of Wi. Prove that the union of the Si’s is linearly independent. 4.37 A matrix is symmetric if for each pair of indices i and j, the i, j entry equals the j, i entry. A matrix is antisymmetric if each i, j entry is the negative of the j, i
entry.
(a) Give a symmetric 2×2 matrix and an antisymmetric 2×2 matrix. (Remark.
For the second one, be careful about the entries on the diagonal.)
(b) What is the relationship between a square symmetric matrix and its transpose?
Between a square antisymmetric matrix and its transpose?
(c) Show that Mn×n is the direct sum of the space of symmetric matrices and the
space of antisymmetric matrices.
4.38 Let W1, W2, W3 be subspaces of a vector space. Prove that (W1 ∩ W2) + (W1 ∩
W3) ⊆ W1 ∩ (W2 + W3). Does the inclusion reverse?
4.39 The example of the x-axis and the y-axis in R2 shows that W1 ⊕ W2 = V does
not imply that W1 ∪ W2 = V. Can W1 ⊕ W2 = V and W1 ∪ W2 = V happen?
4.40 Consider Corollary 4.13. Does it work both ways — that is, supposing that V = W1+···+Wk, is V = W1⊕···⊕Wk if and only if dim(V) = dim(W1)+···+dim(Wk)? 4.41 We know that if V = W1 ⊕W2 then there is a basis for V that splits into a basis for W1 and a basis for W2. Can we make the stronger statement that every
basis for V splits into a basis for W1 and a basis for W2? 4.42 We can ask about the algebra of the ‘+’ operation.
(a) Is it commutative; is W1 + W2 = W2 + W1?
(b) Is it associative; is (W1 + W2) + W3 = W1 + (W2 + W3)?
(c) Let W be a subspace of some vector space. Show that W + W = W.
(d) Must there be an identity element, a subspace I such that I+W = W +I = W
for all subspaces W?
(e) Does left-cancellation hold: if W1 + W2 = W1 + W3 then W2 = W3? Right
cancellation?
Topic
Fields
Computations involving only integers or only rational numbers are much easier than those with real numbers. Could other algebraic structures, such as the integers or the rationals, work in the place of R in the definition of a vector space?
If we take “work” to mean that the results of this chapter remain true then there is a natural list of conditions that a structure (that is, number system) must have in order to work in the place of R. A field is a set F with operations ‘+’ and ‘·’ such that
(1) foranya,b∈Ftheresultofa+bisinF,anda+b=b+a,andifc∈F then a + (b + c) = (a + b) + c
(2) foranya,b∈Ftheresultofa·bisinF,anda·b=b·a,andifc∈Fthen a · (b · c) = (a · b) · c
(3) ifa,b,c∈Fthena·(b+c)=a·b+a·c
(4) thereisanelement0∈Fsuchthatifa∈Fthena+0=a,andforeach
a ∈ F there is an element −a ∈ F such that (−a) + a = 0
(5) thereisanelement1∈Fsuchthatifa∈Fthena·1=a,andforeach
elementa̸=0ofFthereisanelementa−1 ∈Fsuchthata−1·a=1.
For example, the algebraic structure consisting of the set of real numbers
along with its usual addition and multiplication operation is a field. Another field is the set of rational numbers with its usual addition and multiplication operations. An example of an algebraic structure that is not a field is the integers, because it fails the final condition.
Some examples are more surprising. The set B = {0,1} under these opera- tions:
   +01 ·01 001 000 110 101
      is a field; see Exercise 4.
Topic: Fields 145
 We could in this book develop Linear Algebra as the theory of vector spaces with scalars from an arbitrary field. In that case, almost all of the statements here would carry over by replacing ‘R’ with ‘F’, that is, by taking coefficients, vector entries, and matrix entries to be elements of F (the exceptions are statements involving distances or angles, which would need additional development). Here are some examples; each applies to a vector space V over a field F.
∗ Forany⃗v∈V anda∈F,(i)0·⃗v=⃗0,(ii)−1·⃗v+⃗v=⃗0,and(iii)a·⃗0=⃗0. ∗ The span, the set of linear combinations, of a subset of V is a subspace of
V.
∗ Any subset of a linearly independent set is also linearly independent.
∗ In a finite-dimensional vector space, any two bases have the same number of elements.
(Even statements that don’t explicitly mention F use field properties in their proof.)
We will not develop vector spaces in this more general setting because the additional abstraction can be a distraction. The ideas we want to bring out already appear when we stick to the reals.
The exception is Chapter Five. There we must factor polynomials, so we will switch to considering vector spaces over the field of complex numbers.
Exercises
1 Check that the real numbers form a field. 2 Prove that these are fields.
(a) The rational numbers Q (b) The complex numbers C
3 Give an example that shows that the integer number system is not a field. 4 Check that the set B = {0,1} is a field under the operations listed above,
5 Give suitable operations to make the set {0,1,2} a field.
Topic
Crystals
Everyone has noticed that table salt comes in little cubes.
This orderly outside arises from an orderly inside — the way the atoms lie is also cubical, these cubes stack in neat rows and columns, and the salt faces tend to be just an outer layer of cubes. One cube of atoms is shown below. Salt is sodium chloride and the small spheres shown are sodium while the big ones are chloride. To simplify the view, it only shows the sodiums and chlorides on the front, top, and right.
   The specks of salt that we see above have many repetitions of this fundamental unit. A solid, such as table salt, with a regular internal structure is a crystal.
We can restrict our attention to the front face. There we have a square repeated many times giving a lattice of atoms.
Topic: Crystals 147
                                 The distance along the sides of each square cell is about 3.34 Ångstroms (an Ångstrom is 10−10 meters). When we want to refer to atoms in the lattice that number is unwieldy, and so we take the square’s side length as a unit. That is, we naturally adopt this basis.
 3.34    0   ⟨ 0 , 3.34 ⟩
Now we can describe, say, the atom in the upper right of the lattice picture above as 3β⃗ 1 + 2β⃗ 2, instead of 10.02 Ångstroms over and 6.68 up.
Another crystal from everyday experience is pencil lead. It is graphite, formed from carbon atoms arranged in this shape.
 This is a single plane of graphite, called graphene. A piece of graphite consists of many of these planes, layered. The chemical bonds between the planes are much weaker than the bonds inside the planes, which explains why pencils write — the graphite can be sheared so that the planes slide off and are left on the paper.
We can get a convenient unit of length by decomposing the hexagonal ring into three regions that are rotations of this unit cell.
The vectors that form the sides of that unit cell make a convenient basis. The distance along the bottom and slant is 1.42 Ångstroms, so this
 1.42   0.71  ⟨ 0 , 1.23 ⟩
  
148 Chapter Two. Vector Spaces
 is a good basis.
Another familiar crystal formed from carbon is diamond. Like table salt it
is built from cubes but the structure inside each cube is more complicated. In addition to carbons at each corner,
there are carbons in the middle of each face.
(To show the new face carbons clearly, the corner carbons are reduced to dots.) There are also four more carbons inside the cube, two that are a quarter of the way up from the bottom and two that are a quarter of the way down from the top.
(As before, carbons shown earlier are reduced here to dots.) The distance along any edge of the cube is 2.18 Ångstroms. Thus, a natural basis for describing the locations of the carbons and the bonds between them, is this.
2.18  0   0  ⟨ 0 ,2.18, 0 ⟩
0 0 2.18
The examples here show that the structures of crystals is complicated enough to need some organized system to give the locations of the atoms and how they are chemically bound. One tool for that organization is a convenient basis. This application of bases is simple but it shows a science context where the idea arises naturally.
Exercises
1 How many fundamental regions are there in one face of a speck of salt? (With a ruler, we can estimate that face is a square that is 0.1 cm on a side.)
                 
Topic: Crystals 149
 2 In the graphite picture, imagine that we are interested in a point 5.67 Ångstroms over and 3.14 Ångstroms up from the origin.
(a) Express that point in terms of the basis given for graphite.
(b) How many hexagonal shapes away is this point from the origin?
(c) Express that point in terms of a second basis, where the first basis vector is
the same, but the second is perpendicular to the first (going up the plane) and
of the same length.
3 Give the locations of the atoms in the diamond cube both in terms of the basis,
and in Ångstroms.
4 This illustrates how we could compute the dimensions of a unit cell from the
shape in which a substance crystallizes ([Ebbing], p. 462).
(a) Recall that there are 6.022 × 1023 atoms in a mole (this is Avogadro’s number).
From that, and the fact that platinum has a mass of 195.08 grams per mole,
calculate the mass of each atom.
(b) Platinum crystallizes in a face-centered cubic lattice with atoms at each lattice
point, that is, it looks like the middle picture given above for the diamond crystal. Find the number of platinum’s per unit cell (hint: sum the fractions of platinum’s that are inside of a single cell).
(c) From that, find the mass of a unit cell.
(d) Platinum crystal has a density of 21.45 grams per cubic centimeter. From
this, and the mass of a unit cell, calculate the volume of a unit cell. (e) Find the length of each edge.
(f) Describe a natural three-dimensional basis.
Topic
Voting Paradoxes
Imagine that a Political Science class studying the American presidential process holds a mock election. The 29 class members rank the Democratic Party, Republican Party, and Third Party nominees, from most preferred to least preferred (> means ‘is preferred to’).
 preference order
Democrat > Republican > Third Democrat > Third > Republican Republican > Democrat > Third Republican > Third > Democrat Third > Democrat > Republican Third > Republican > Democrat
number with that preference
5 4 2 8 8 2
 What is the preference of the group as a whole?
Overall, the group prefers the Democrat to the Republican by five votes;
seventeen voters ranked the Democrat above the Republican versus twelve the other way. And the group prefers the Republican to the Third’s nominee, fifteen to fourteen. But, strangely enough, the group also prefers the Third to the Democrat, eighteen to eleven.
Democrat
7 voters 5 voters
Third Republican
1 voter
This is a voting paradox, specifically, a majority cycle.
Mathematicians study voting paradoxes in part because of their implications
for practical politics. For instance, the instructor can manipulate this class into
   
Topic: Voting Paradoxes 151
 choosing the Democrat as the overall winner by first asking for a vote between the Republican and the Third, and then asking for a vote between the winner of that contest, who will be the Republican, and the Democrat. By similar manipulations the instructor can make any of the other two candidates come out as the winner. (We will stick to three-candidate elections but the same thing happens in larger elections.)
Mathematicians also study voting paradoxes simply because they are inter- esting. One interesting aspect is that the group’s overall majority cycle occurs despite that each single voter’s preference list is rational, in a straight-line order. That is, the majority cycle seems to arise in the aggregate without being present in the components of that aggregate, the preference lists. However we can use linear algebra to argue that a tendency toward cyclic preference is actually present in each voter’s list and that it surfaces when there is more adding of the tendency than canceling.
For this, abbreviating the choices as D, R, and T, we can describe how a voter with preference order D > R > T contributes to the above cycle.
D
  −1 voter TR
1 voter
(The negative sign is here because the arrow describes T as preferred to D, but this voter likes them the other way.) The descriptions for the other preference lists are in the table on page 153.
Now, to conduct the election we linearly combine these descriptions; for instance, the Political Science mock election
DDD
−1 1 −1 1 1 −1 5·T R +4·T R +···+2·T R
1 −1 −1 yields the circular group preference shown earlier.
Of course, taking linear combinations is linear algebra. The graphical cycle notation is suggestive but inconvenient so we use column vectors by starting at the D and taking the numbers from the cycle in counterclockwise order. Thus, we represent the mock election and a single D > R > T vote in this way.
7 −1 1 and  1 
51
We will decompose vote vectors into two parts, one cyclic and the other acyclic. For the first part, we say that a vector is purely cyclic if it is in this
1 voter
          
152 Chapter Two. Vector Spaces subspace of R3.
k 1
C = {  k  | k ∈ R } = { k ·  1  | k ∈ R }
k1
For the second part, consider the set of vectors that are perpendicular to all of the vectors in C. Exercise 6 shows that this is a subspace
c c k 11
C⊥ = {c2 | c2 • k = 0 for all k ∈ R} c3 c3 k
c  −1 −1 1
={c2|c1 +c2 +c3 =0}={c2 1 +c3 0 |c2,c3 ∈R} c3 01
(read that aloud as “C perp”). So we are led to this basis for R3. 1 −1 −1
⟨1, 1 , 0 ⟩ 101
We can represent votes with respect to this basis, and thereby decompose them into a cyclic part and an acyclic part. (Note for readers who have covered the optional section in this chapter: that is, the space is the direct sum of C and C⊥.)
For example, consider the D > R > T voter discussed above. We represent that voter with respect to the basis
 c1−c2−c3=−1 c1− c2− c3=−1 −ρ1 +ρ2 (−1/2)ρ2 +ρ3
c1+c2=1−→−→ 2c2+c3=2
−ρ1 +ρ3
(3/2)c3= 1 using the coordinates c1 = 1/3, c2 = 2/3, and c3 = 2/3. Then
−1 1 −1 −1 1/3 −4/3  1  = 1 · 1 + 2 ·  1  + 2 ·  0  = 1/3 +  2/3  1 3 1 3 0 3 1 1/3 2/3
gives the desired decomposition into a cyclic part and an acyclic part.
DDD
−1 1 1/3 1/3 −4/3 2/3
TR=TR+TR
c1 +c3= 1
            1
1/3 2/3
Topic: Voting Paradoxes 153
 Thus we can see that this D > R > T voter’s rational preference list does have a cyclic part.
The T > R > D voter is opposite to the one just considered in that the ‘>’ symbols are reversed. This voter’s decomposition
DDD
1 −1 −1/3 −1/3 4/3 −2/3 TR=TR+TR
−1 −1/3 −2/3
shows that these opposite preferences have decompositions that are opposite. We say that the first voter has positive spin since the cycle part is with the direction that we have chosen for the arrows, while the second voter’s spin is negative.
The fact that these opposite voters cancel each other is reflected in the fact that their vote vectors add to zero. This suggests an alternate way to tally an election. We could first cancel as many opposite preference lists as possible and then determine the outcome by adding the remaining lists.
The table below contains the three pairs of opposite preference lists. For instance, the top line contains the voters discussed above.
positive spin negative spin
Democrat > Republican > Third Third > Republican > Democrat DDDDDD
−1 1 1/3 1/3 −4/3 2/3 1 −1 −1/3 −1/3 4/3 −2/3 TR=TR+TR TR=TR+TR
1/3 2/3 −1 −1/3 −2/3 Republican > Third > Democrat Democrat > Third > Republican
DDDDDD
1 −1 1/3 1/3 2/3 −4/3 −1 1 −1/3 −1/3 −2/3 4/3 TR=TR+TR TR=TR+TR
1/3 2/3 −1 −1/3 −2/3 Third > Democrat > Republican Republican > Democrat > Third
DDDDDD
1 1 1/3 1/3 2/3 2/3 −1 −1 −1/3 −1/3 −2/3 −2/3 TR=TR+TR TR=TR+TR
−1 1/3 −4/3 1 −1/3 4/3
If we conduct the election as just described then after the cancellation of as many opposite pairs of voters as possible there will remain three sets of preference lists: one set from the first row, one from the second row, and one from the third row. We will finish by proving that a voting paradox can happen only if the spins of these three sets are in the same direction. That is, for a voting paradox to occur the three remaining sets must all come from the left of the table or all come from the right (see Exercise 3). This shows that there is some connection
                            1
                  1
                  
154 Chapter Two. Vector Spaces
 between the majority cycle and the decomposition that we are using—a voting paradox can happen only when the tendencies toward cyclic preference reinforce each other.
For the proof, assume that we have canceled opposite preference orders and are left with one set of preference lists for each of the three rows. Consider the first row’s remaining preference lists. They could be from the first row’s left or right (or between, since the lists could have canceled exactly). We shall write
D
−a a TR
a
where a is an integer that is positive if the remaining lists are on the left, where a is negative if the lists are on the right, and zero if the cancellation was perfect. Similarly we have integers b and c for the second and third rows, which can each be positive, negative, or zero.
Then the election is determined by this sum.
DDDD
−a a b −b c c −a+b+c a−b+c TR+TR+TR=TR
a b −c a+b−c
A voting paradox occurs when the three numbers in the total cycle on the right, −a+b+c and a−b+c and a+b−c, are all nonnegative or all nonpositive. We will prove this occurs only when either all three of a, b, and c are nonnegative or all three are nonpositive.
Let the total cycle numbers be nonnegative; the other case is similar.
−a+b+c 0 a−b+c 0 a+b−c 0
Addthefirsttworowstoseethatc 0. Addthefirstandthirdrowsforb 0. And, the second and third rows together give a   0. Thus if the total cycle is nonnegative then in each row the remaining preference lists are from the table’s left. That ends the proof.
This result says only that having all three spin in the same direction is a necessary condition for a majority cycle. It is not sufficient; see Exercise 4.
Voting theory and associated topics are the subject of current research. There are many intriguing results, notably those produced by K Arrow [Arrow] who won the Nobel Prize in part for this work, showing that no voting system is entirely fair (for a reasonable definition of “fair”). Some good introductory arti- cles are [Gardner, 1970], [Gardner, 1974], [Gardner, 1980], and [Neimi & Riker]. [Taylor] is a readable text. The long list of cases from recent American political
               
Topic: Voting Paradoxes 155
 history in [Poundstone] shows these paradoxes are routinely manipulated in practice. (On the other hand, quite recent research shows that computing how to manipulate elections can in general be unfeasible, but this is beyond our scope.) This Topic is drawn from [Zwicker]. (Author’s Note: I would like to thank Professor Zwicker for his kind and illuminating discussions.)
Exercises
1 Here is a reasonable way in which a voter could have a cyclic preference. Suppose that this voter ranks each candidate on each of three criteria.
(a) Draw up a table with the rows labeled ‘Democrat’, ‘Republican’, and ‘Third’, and the columns labeled ‘character’, ‘experience’, and ‘policies’. Inside each column, rank some candidate as most preferred, rank another as in the middle, and rank the remaining one as least preferred.
(b) In this ranking, is the Democrat preferred to the Republican in (at least) two out of three criteria, or vice versa? Is the Republican preferred to the Third? (c) Does the table that was just constructed have a cyclic preference order? If
not, make one that does.
So it is possible for a voter to have a cyclic preference among candidates. The paradox described above, however, is that even if each voter has a straight-line preference list, a cyclic preference can still arise for the entire group.
2 Compute the values in the table of decompositions.
3 Perform the cancellations of opposite preference orders for the Political Science
class’s mock election. Are all the remaining preferences from the left three rows of
the table or from the right?
4 The necessary condition that a voting paradox can happen only if all three
preference lists remaining after cancellation have the same spin is not also suffi- cient.
(a) Give an example of a vote where there is a majority cycle and addition of one more voter with the same spin causes the cycle to go away.
(b) Can the opposite happen; can addition of one voter with a “wrong” spin cause a cycle to appear?
(c) Give a condition that is both necessary and sufficient to get a majority cycle. 5 A one-voter election cannot have a majority cycle because of the requirement that
we’ve imposed that the voter’s list must be rational.
(a) Show that a two-voter election may have a majority cycle. (We consider the
group preference a majority cycle if all three group totals are nonnegative or if
all three are nonpositive—that is, we allow some zero’s in the group preference.) (b) Show that for any number of voters greater than one, there is an election
involving that many voters that results in a majority cycle.
6 LetUbeasubspaceofR3. ProvethatthesetU⊥ ={⃗v|⃗v•⃗u=0forall⃗u∈U} of vectors that are perpendicular to each vector in U is also subspace of R3. Does
this hold if U is not a subspace?
Topic
Dimensional Analysis
 “You can’t add apples and oranges,” the old saying goes. It reflects our experience that in applications the quantities have units and keeping track of those units can help. Everyone has done calculations such as this one that use the units as a check.
60 sec · 60 min · 24 hr · 365 day = 31 536 000 sec min hr day year year
     We can take the idea of including the units beyond bookkeeping. We can use units to draw conclusions about what relationships are possible among the physical quantities.
To start, consider the falling body equation distance = 16 · (time)2. If the distance is in feet and the time is in seconds then this is a true statement. However it is not correct in other unit systems, such as meters and seconds, because 16 isn’t the right constant in those systems. We can fix that by attaching units to the 16, making it a dimensional constant.
dist = 16 ft · (time)2 sec2
Now the equation holds also in the meter-second system because when we align the units (a foot is approximately 0.30 meters),
distance in meters = 16 0.30 m · (time in sec)2 = 4.8 m · (time in sec)2 sec2 sec2
the constant gets adjusted. So in order to have equations that are correct across unit systems, we restrict our attention to those that use dimensional constants. Such an equation is complete.
Moving away from a particular unit system allows us to just measure quan- tities in combinations of some units of length L, mass M, and time T. These three are our physical dimensions. For instance, we could measure velocity in feet/second or fathoms/hour but at all events it involves a unit of length divided by a unit of time so the dimensional formula of velocity is L/T. Similarly, density’s dimensional formula is M/L3.
   
Topic: Dimensional Analysis 157
 To write the dimensional formula we shall use negative exponents instead of fractions and we shall include the dimensions with a zero exponent. Thus we will write the dimensional formula of velocity as L1M0T−1 and that of density as L−3M1T0.
With that, “you can’t add apples and oranges” becomes the advice to check that all of an equation’s terms have the same dimensional formula. An example is this version of the falling body equation d − gt2 = 0. The dimensional formula of the d term is L1M0T0. For the other term, the dimensional formula of g is L1M0T−2 (g is given above as 16ft/sec2) and the dimensional formula of t is L0M0T1 so that of the entire gt2 term is L1M0T−2(L0M0T1)2 = L1M0T0. Thus the two terms have the same dimensional formula. An equation with this property is dimensionally homogeneous.
Quantities with dimensional formula L0M0T0 are dimensionless. For ex- ample, we measure an angle by taking the ratio of the subtended arc to the radius
arc
r
which is the ratio of a length to a length (L1M0T0)(L1M0T0)−1 and thus angles have the dimensional formula L0M0T0.
The classic example of using the units for more than bookkeeping, using them to draw conclusions, considers the formula for the period of a pendulum.
p = –some expression involving the length of the string, etc.–
The period is in units of time L0M0T1. So the quantities on the other side of the equation must have dimensional formulas that combine in such a way that their L’s and M’s cancel and only a single T remains. The table on page 158 has the quantities that an experienced investigator would consider possibly relevant to the period of a pendulum. The only dimensional formulas involving L are for the length of the string and the acceleration due to gravity. For the L’s of these two to cancel when they appear in the equation they must be in ratio, e.g., as (l/g)2, or as cos(l/g), or as (l/g)−1. Therefore the period is a function of l/g.
This is a remarkable result: with a pencil and paper analysis, before we ever took out the pendulum and made measurements, we have determined something about what makes up its period.
To do dimensional analysis systematically, we need two facts (arguments for these are in [Bridgman], Chapter II and IV). The first is that each equation relating physical quantities that we shall see involves a sum of terms, where each term has the form
mp1mp2 ···mpk 12k
 
158 Chapter Two. Vector Spaces
 for numbers m1, . . . , mk that measure the quantities.
For the second fact, observe that an easy way to construct a dimensionally
homogeneous expression is by taking a product of dimensionless quantities or by adding such dimensionless terms. Buckingham’s Theorem states that any complete relationship among quantities with dimensional formulas can be algebraically manipulated into a form where there is some function f such that
f(Π1,...,Πn) = 0
for a complete set {Π1, . . . , Πn } of dimensionless products. (The first example below describes what makes a set of dimensionless products ‘complete’.) We usually want to express one of the quantities, m1 for instance, in terms of the others. For that we will assume that the above equality can be rewritten
−p2 −pk ˆ
m1 =m2 ···mk ·f(Π2,...,Πn)
where Π1 = m1mp2 · · · mpk is dimensionless and the products Π2, . . . , Πn 2kˆ
don’t involve m1 (as with f, here f is an arbitrary function, this time of n − 1 arguments). Thus, to do dimensional analysis we should find which dimensionless products are possible.
For example, again consider the formula for a pendulum’s period.
quantity
dimensional
formula
  period p   L0M0T1 length of string l   L1M0T0 mass of bob m   L0M1T0
acceleration due to gravity g   L1M0T−2 arc of swing θ   L0M0T0
By the first fact cited above, we expect the formula to have (possibly sums of terms of) the form pp1 lp2 mp3 gp4 θp5 . To use the second fact, to find which combinations of the powers p1, . . . , p5 yield dimensionless products, consider this equation.
(L0M0T1)p1 (L1M0T0)p2 (L0M1T0)p3 (L1M0T−2)p4 (L0M0T0)p5 = L0M0T0 It gives three conditions on the powers.
p2 +p4=0 p3 =0 p1 −2p4 =0
Topic: Dimensional Analysis 159 Note that p3 = 0 so the mass of the bob does not affect the period. Gaussian
reduction and parametrization of that system gives this
p1  1  0
p2  −1/2 0
{p3= 0 p1 +0p5 |p1,p5 ∈R}
p4  1/2  0 p5 0 1
(we’ve taken p1 as one of the parameters in order to express the period in terms of the other quantities).
The set of dimensionless products contains all terms pp1lp2mp3ap4θp5 subject to the conditions above. This set forms a vector space under the ‘+’ operation of multiplying two such products and the ‘·’ operation of raising such a product to the power of the scalar (see Exercise 5). The term ‘complete set of dimensionless products’ in Buckingham’s Theorem means a basis for this vector space.
Wecangetabasisbyfirsttakingp1 =1,p5 =0,andthentakingp1 =0, p5 = 1. The associated dimensionless products are Π1 = pl−1/2g1/2 and Π2 = θ. Because the set {Π1,Π2} is complete, Buckingham’s Theorem says that
1/2 −1/2 ˆ   ˆ
p = l g · f(θ) = l/g · f(θ)
where fˆ is a function that we cannot determine from this analysis (a first year physics text will show by other means that for small angles it is approximately
ˆ
the constant function f(θ) = 2π).
Thus, analysis of the relationships that are possible between the quantities with the given dimensional formulas has given us a fair amount of information: a pendulum’s period does not depend on the mass of the bob, and it rises with the square root of the length of the string.
For the next example we try to determine the period of revolution of two bodies in space orbiting each other under mutual gravitational attraction. An experienced investigator could expect that these are the relevant quantities.
  quantity
dimensional
formula
 period p   L0M0T1 mean separation r   L1M0T0 first mass m1   L0M1T0 second mass m2   L0M1T0
gravitational constant G   L3M−1T−2 To get the complete set of dimensionless products we consider the equation
(L0M0T1)p1 (L1M0T0)p2 (L0M1T0)p3 (L0M1T0)p4 (L3M−1T−2)p5 = L0M0T0
   
160
which results in a system
with this solution.
p2
Chapter Two. Vector Spaces
+ 3p5 = 0 p3+p4− p5=0 − 2p5 = 0
 p1
1 0
−3/2  0  
{ 1/2 p1 +−1p4 |p1,p4 ∈R}  0   1 
1/2 0
As earlier, the set of dimensionless products of these quantities forms a
vector space and we want to produce a basis for that space, a ‘complete’ set of
dimensionless products. One such set, gotten from setting p1 = 1 and p4 = 0
and also setting p1 = 0 and p4 = 1 is {Π1 = pr−3/2m1/2G1/2, Π2 = m−1m2 }. 11
With that, Buckingham’s Theorem says that any complete relationship among these quantities is stateable this form.
3/2 −1/2 −1/2 ˆ −1 r3/2 ˆ
p=r m1 G ·f(m1 m2)= √Gm ·f(m2/m1)
1
Remark. An important application of the prior formula is when m1 is the mass of the sun and m2 is the mass of a planet. Because m1 is very much greater than m2, the argument to fˆ is approximately 0, and we can wonder whether this part of the formula remains approximately constant as m2 varies. One way to see that it does is this. The sun is so much larger than the planet that the mutual rotation is approximately about the sun’s center. If we vary the planet’s mass m2 by a factor of x (e.g., Venus’s mass is x = 0.815 times Earth’s mass), then the force of attraction is multiplied by x, and x times the force acting on x times the mass gives, since F = ma, the same acceleration, about the same center (approximately). Hence, the orbit will be the same and so its period will be the same, and thus the right side of the above equation also remains
ˆ
unchanged (approximately). Therefore, f(m2/m1) is approximately constant as
m2 varies. This is Kepler’s Third Law: the square of the period of a planet is proportional to the cube of the mean radius of its orbit about the sun.
The final example was one of the first explicit applications of dimensional analysis. Lord Raleigh considered the speed of a wave in deep water and suggested these as the relevant quantities.
  
Topic: Dimensional Analysis
161
 quantity
dimensional
formula
 velocity of the wave v   L1M0T−1 density of the water d   L−3M1T0 acceleration due to gravity g   L1M0T−2
wavelength λ   L1M0T0
(L1M0T−1)p1 (L−3M1T0)p2 (L1M0T−2)p3 (L1M0T0)p4 = L0M0T0
The equation gives this system
with this solution space.
There is one dimensionless product, Π1 = vg−1/2λ−1/2, and so v is √λg times a constant; fˆ is constant since it is a function of no arguments. The quantity d is not involved in the relationship.
The three examples above show that dimensional analysis can bring us far toward expressing the relationship among the quantities. For further reading, the classic reference is [Bridgman] — this brief book is delightful. Another source is [Giordano, Wells, Wilde]. A description of dimensional analysis’s place in modeling is in [Giordano, Jaye, Weir].
Exercises
1 [de Mestre] Consider a projectile, launched with initial velocity v0, at an angle θ.
To study its motion we may guess that these are the relevant quantities.
p1−3p2+ p3+p4=0 p2 =0 −p1 −2p3 =0
1
 0 
{ p1 |p1 ∈R}
−1/2 −1/2
 quantity
dimensional
formula
 horizontal position x   L1M0T0 vertical position y   L1M0T0
acceleration due to gravity g   L1M0T−2 time t   L0M0T1
(a) Show that {gt/v0,gx/v20,gy/v20,θ} is a complete set of dimensionless products. (Hint. One way to go is to find the appropriate free variables in the linear system that arises but there is a shortcut that uses the properties of a basis.)
 initial speed v0
angle of launch θ   L0M0T0
L1M0T−1
162
Chapter Two. Vector Spaces
 2
(b) These two equations of motion for projectiles are familiar: x = v0 cos(θ)t and y = v0 sin(θ)t − (g/2)t2. Manipulate each to rewrite it as a relationship among the dimensionless products of the prior item.
[Einstein] conjectured that the infrared characteristic frequencies of a solid might be determined by the same forces between atoms as determine the solid’s ordinary elastic behavior. The relevant quantities are these.
quantity
characteristic frequency ν compressibility k number of atoms per cubic cm N mass of an atom m
dimensional formula L0M0T−1
L1M−1T2 L−3M0T0
     3
4
5
relationship among quantities with these dimensional formulas, k is a constant times ν−2N−1/3m−1. This conclusion played an important role in the early study of quantum phenomena.
[Giordano, Wells, Wilde] The torque produced by an engine has dimensional formula L2M1T−2. We may first guess that it depends on the engine’s rotation rate (with dimensional formula L0M0T−1), and the volume of air displaced (with dimensional formula L3M0T0).
(a) Try to find a complete set of dimensionless products. What goes wrong?
(b) Adjust the guess by adding the density of the air (with dimensional formula
L−3M1T0). Now find a complete set of dimensionless products.
[Tilley] Dominoes falling make a wave. We may conjecture that the wave speed v depends on the spacing d between the dominoes, the height h of each domino, and
the acceleration due to gravity g.
(a) Find the dimensional formula for each of the four quantities.
(b) Show that { Π1 = h/d, Π2 = dg/v2 } is a complete set of dimensionless products. (c) Show that if h/d is fixed then the propagation speed is proportional to the
square root of d.
Prove that the dimensionless products form a vector space under the +⃗ operation
of multiplying two such products and the⃗· operation of raising such the product to the power of the scalar. (The vector arrows are a precaution against confusion.) That is, prove that, for any particular homogeneous system, this set of products of powers of m1, ..., mk
{mp1 ...mpk | p , ..., p satisfy the system} 1k1k
is a vector space under:
L0M1T0
Show that there is one dimensionless product. Conclude that, in any complete
and
mp1 ...mpk+⃗mq1 ...mqk = mp1+q1 ...mpk+qk 1k1k1k
r⃗·(mp1 ...mpk) = mrp1 ...mrpk 1k1k
(assume that all variables represent real numbers).
6 The advice about apples and oranges is not right. Consider the familiar equations
for a circle C=2πr and A=πr2.
(a) Check that C and A have different dimensional formulas.
Topic: Dimensional Analysis 163
 (b) Produce an equation that is not dimensionally homogeneous (i.e., it adds apples and oranges) but is nonetheless true of any circle.
(c) The prior item asks for an equation that is complete but not dimensionally homogeneous. Produce an equation that is dimensionally homogeneous but not complete.
(Just because the old saying isn’t strictly right doesn’t keep it from being a useful strategy. Dimensional homogeneity is often used to check the plausibility of equations used in models. For an argument that any complete equation can easily be made dimensionally homogeneous, see [Bridgman], Chapter I, especially page 15.)
Chapter Three
Maps Between Spaces I Isomorphisms
In the examples following the definition of a vector space we expressed the intuition that some spaces are “the same” as others. For instance, the space of two-tall column vectors and the space of two-wide row vectors are not equal because their elements — column vectors and row vectors — are not equal, but we feel that these spaces differ only in how their elements appear. We will now make this precise.
This section illustrates a common phase of a mathematical investigation. With the help of some examples we’ve gotten an idea. We will next give a formal definition and then we will produce some results backing our contention that the definition captures the idea. We’ve seen this happen already, for instance in the first section of the Vector Space chapter. There, the study of linear systems led us to consider collections closed under linear combinations. We defined such a collection as a vector space and we followed it with some supporting results.
That wasn’t an end point, instead it led to new insights such as the idea of a basis. Here also, after producing a definition and supporting it, we will get two surprises (pleasant ones). First, we will find that the definition applies to some unforeseen, and interesting, cases. Second, the study of the definition will lead to new ideas. In this way, our investigation will build momentum.
I.1 Definition and Examples
We start with two examples that suggest the right definition.
 
166 Chapter Three. Maps Between Spaces
 1.1 Example The space of two-wide row vectors and the space of two-tall column vectors are “the same” in that if we associate the vectors that have the same components, e.g.,
 1  2
(read the double arrow as “corresponds to”) then this association respects the operations. For instance these corresponding vectors add to corresponding totals
 1   3   4  (12)+(34)=(46) ←→ 2 + 4 = 6
and here is an example of the correspondence respecting scalar multiplication.
 1   5  5·(12)=(510) ←→ 5· 2 = 10
Stated generally, under the correspondence
 a   0
a1
 a   b   a+b 
(a0 a1) ←→ both operations are preserved:
(a0 a1)+(b0 b1)=(a0 +b0 and
a1 +b1)←→
0 + 0 = a1 b1
 a   ra  0 = 0
0 0 a1 + b1
r · (a0 a1) = (ra0 ra1) ←→ (all of the variables are scalars).
r ·
a1
ra1
(12) ←→
1.2 Example Another two spaces that we can think of as “the same” are P2, the space of quadratic polynomials, and R3. A natural correspondence is this.
a  1 0
a0 +a1x+a2x2 ←→ a1 (e.g., 1+2x+3x2 ←→ 2) a2 3
This preserves structure: corresponding elements add in a corresponding way
a+ax+ax2 a b a+b 0120000
+ b 0 + b 1 x + b 2 x 2 ← →  a 1  +  b 1  =  a 1 + b 1  (a0 +b0)+(a1 +b1)x+(a2 +b2)x2 a2 b2 a2 +b2
 
Section I. Isomorphisms 167 and scalar multiplication corresponds also.
 r·(a0 +a1x+a2x2)=(ra0)+(ra1)x+(ra2)x2
a ra 00
←→ r·a1=ra1 a2 ra2
 1.3 Definition An isomorphism between two vector spaces V and W is a map f: V → W that
(1) is a correspondence: f is one-to-one and onto;∗ (2) preserves structure: if ⃗v1,⃗v2 ∈ V then
f ( ⃗v 1 + ⃗v 2 ) = f ( ⃗v 1 ) + f ( ⃗v 2 ) andif⃗v∈V andr∈Rthen
f(r⃗v) = rf(⃗v)
(we write V =∼ W, read “V is isomorphic to W”, when such a map exists).
“Morphism” means map, so “isomorphism” means a map expressing sameness. 1.4Example ThevectorspaceG={c1cosθ+c2sinθ|c1,c2 ∈R}offunctions
of θ is isomorphic to R2 under this map.
c1 cos θ + c2 sin θ  −→
We will check this by going through the conditions in the definition. We will first verify condition (1), that the map is a correspondence between the sets underlying the spaces.
To establish that f is one-to-one we must prove that f(a⃗) = f(⃗b) only when a⃗ = ⃗b. If
f(a1 cos θ + a2 sin θ) = f(b1 cos θ + b2 sin θ)
then by the definition of f
 a   b  1=1 a2 b2
from which we conclude that a1 = b1 and a2 = b2, because column vectors are equal only when they have equal components. Thus a1 cos θ + a2 sin θ = b1 cos θ + b2 sin θ, and as required we’ve verified that f(a⃗ ) = f(⃗b) implies that a⃗ = ⃗b .
  
f c1 c2
 ∗More information on correspondences is in the appendix.
168 Chapter Three. Maps Between Spaces
 To prove that f is onto we must check that any member of the codomain R2 is the image of some member of the domain G. So, consider a member of the
codomain
 x  y
and note that it is the image under f of xcosθ+ysinθ.
Next we will verify condition (2), that f preserves structure. This computation
shows that f preserves addition.
f (a1 cosθ+a2 sinθ)+(b1 cosθ+b2 sinθ) 
=f (a1 +b1)cosθ+(a2 +b2)sinθ 
 a +b   =11
a2 + b2  a   b 
=1+1 a2 b2
= f(a1 cos θ + a2 sin θ) + f(b1 cos θ + b2 sin θ) The computation showing that f preserves scalar multiplication is similar.
f  r · (a1 cos θ + a2 sin θ)   = f( ra1 cos θ + ra2 sin θ )  ra  
With both (1) and (2) verified, we know that f is an isomorphism and we can say that the spaces are isomorphic G =∼ R2.
1.5 Example Let V be the space {c1x + c2y + c3z | c1, c2, c3 ∈ R} of linear combi- nations of the three variables under the natural addition and scalar multiplication operations. Then V is isomorphic to P2, the space of quadratic polynomials.
To show this we must produce an isomorphism map. There is more than one possibility; for instance, here are four to choose among.
c1x+c2y+c3z
f1 2  −→ c1+c2x+c3x
f2 2  −→ c2+c3x+c1x
f3 2  −→ −c1−c2x−c3x
f4 2  −→ c1 +(c1 +c2)x+(c1 +c3)x
=1 ra2
 a   =r· 1
a2
=r· f(a1cosθ+a2sinθ)
Section I. Isomorphisms 169
 The first map is the more natural correspondence in that it just carries the coefficients over. However we shall do f2 to underline that there are isomorphisms other than the obvious one. (Checking that f1 is an isomorphism is Exercise 14.)
To show that f2 is one-to-one we will prove that if f2(c1x + c2y + c3z) = f2(d1x + d2y + d3z) then c1x + c2y + c3z = d1x + d2y + d3z. The assumption that f2(c1x+c2y+c3z) = f2(d1x+d2y+d3z) gives, by the definition of f2, that c2 + c3x + c1x2 = d2 + d3x + d1x2. Equal polynomials have equal coefficients so c2 = d2, c3 = d3, and c1 = d1. Hence f2(c1x+c2y+c3z) = f2(d1x+d2y+d3z) implies that c1x + c2y + c3z = d1x + d2y + d3z, and f2 is one-to-one.
The map f2 is onto because a member a + bx + cx2 of the codomain is the image of a member of the domain, namely it is f2(cx + ay + bz). For instance, 2+3x−4x2 is f2(−4x+2y+3z).
The computations for structure preservation are like those in the prior example. The map f2 preserves addition
f2 (c1x + c2y + c3z) + (d1x + d2y + d3z) 
=f2 (c1 +d1)x+(c2 +d2)y+(c3 +d3)z 
=(c2 +d2)+(c3 +d3)x+(c1 +d1)x2
=(c2 +c3x+c1x2)+(d2 +d3x+d1x2)
= f2(c1x + c2y + c3z) + f2(d1x + d2y + d3z)
and scalar multiplication.
f2 r · (c1x + c2y + c3z)  = f2(rc1x + rc2y + rc3z) = rc2 + rc3x + rc1x2
=r·(c2 +c3x+c1x2) =r· f2(c1x+c2y+c3z)
Thus f2 is an isomorphism. We write V =∼ P2.
1.6 Example Every space is isomorphic to itself under the identity map. The
check is easy.
1.7 Definition An automorphism is an isomorphism of a space with itself.
1.8 Example A dilation map ds : R2 → R2 that multiplies all vectors by a nonzero
 scalar s is an automorphism of R2. ⃗u
⃗v
d1.5 −→
d1.5 (⃗u) d1.5 (⃗v)
      
170 Chapter Three. Maps Between Spaces Another automorphism is a rotation or turning map, tθ : R2 → R2 that rotates
 all vectors through an angle θ.
⃗u
tπ/6 −→
tπ/6 (⃗u)
      A third type of automorphism of R2 is a map fl : R2 → R2 that flips or reflects all vectors over a line l through the origin.
 f l ( ⃗u )
1.9 Example Consider the space P5 of polynomials of degree 5 or less and the map f that sends a polynomial p(x) to p(x − 1). For instance, under this map x2  → (x−1)2 = x2 −2x+1 and x3 +2x  → (x−1)3 +2(x−1) = x3 −3x2 +5x−3. This map is an automorphism of this space; the check is Exercise 25.
This isomorphism of P5 with itself does more than just tell us that the space is “the same” as itself. It gives us some insight into the space’s structure. Below is a family of parabolas, graphs of members of P5. Each has a vertex at y = −1, and the left-most one has zeroes at −2.25 and −1.75, the next one has zeroes at −1.25 and −0.75, etc.
p0 p1
Substitution of x − 1 for x in any function’s argument shifts its graph to the right by one. Thus, f(p0) = p1, and f’s action is to shift all of the parabolas to the right by one. Notice that the picture before f is applied is the same as the picture after f is applied because while each parabola moves to the right, another one comes in from the left to take its place. This also holds true for cubics, etc. So the automorphism f expresses the idea that P5 has a certain horizontal-homogeneity: if we draw two pictures showing all members of P5, one
     ⃗u fl l −→
   Checking that these are automorphisms is Exercise 33.
 
Section I. Isomorphisms 171 picture centered at x = 0 and the other centered at x = 1, then the two pictures
would be indistinguishable.
As described in the opening to this section, having given the definition of isomorphism, we next look to support the thesis that it captures our intuition of vector spaces being the same. First, the definition itself is persuasive: a vector space consists of a set and some structure and the definition simply requires that the sets correspond and that the structures correspond also. Also persuasive are the examples above, such as Example 1.1, which dramatize that isomorphic spaces are the same in all relevant respects. Sometimes people say, where V =∼ W, that “ W is just V painted green” — differences are merely cosmetic.
The results below further support our contention that under an isomorphism all the things of interest in the two vector spaces correspond. Because we introduced vector spaces to study linear combinations, “of interest” means “pertaining to linear combinations.” Not of interest is the way that the vectors are presented typographically (or their color!).
1.10 Lemma An isomorphism maps a zero vector to a zero vector.
Proof Where f: V → W is an isomorphism, fix some ⃗v ∈ V. Then f(⃗0V) =
f(0 · ⃗v) = 0 · f(⃗v) = ⃗0W . QED
   1.11 Lemma For any map f: V → W between vector spaces these statements are equivalent.
(1) f preserves structure
f(⃗v1 + ⃗v2) = f(⃗v1) + f(⃗v2) and f(c⃗v) = c f(⃗v)
(2) f preserves linear combinations of two vectors f(c1⃗v1 + c2⃗v2) = c1f(⃗v1) + c2f(⃗v2)
(3) f preserves linear combinations of any finite number of vectors f(c1⃗v1 +···+cn⃗vn)=c1f(⃗v1)+···+cnf(⃗vn)
Proof Since the implications (3) =⇒ (2) and (2) =⇒ (1) are clear, we need only show that (1) =⇒ (3). So assume statement (1). We will prove (3) by induction on the number of summands n.
The one-summand base case, that f(c⃗v1 ) = c f(⃗v1 ), is covered by the second clause of statement (1).
For the inductive step assume that statement (3) holds whenever there are k or fewer summands. Consider the k + 1-summand case. Use the first half of (1)
172 Chapter Three. Maps Between Spaces
 to break the sum along the final ‘+’.
f(c1⃗v1 + · · · + ck⃗vk + ck+1⃗vk+1) = f(c1⃗v1 + · · · + ck⃗vk) + f(ck+1⃗vk+1)
Use the inductive hypothesis to break up the k-term sum on the left. = f(c1⃗v1) + · · · + f(ck⃗vk) + f(ck+1⃗vk+1)
Now the second half of (1) gives
= c 1 f ( ⃗v 1 ) + · · · + c k f ( ⃗v k ) + c k + 1 f ( ⃗v k + 1 )
when applied k + 1 times. QED
We often use item (2) to simplify the verification that a map preserves structure. Finally, a summary. In the prior chapter, after giving the definition of a vector space, we looked at examples and noted that some spaces seemed to be essentially the same as others. Here we have defined the relation ‘=∼’ and have argued that it is the right way to precisely say what we mean by “the same” because it preserves the features of interest in a vector space — in particular, it preserves linear combinations. In the next section we will show that isomorphism
is an equivalence relation and so partitions the collection of vector spaces.
Exercises
  1.12 Verify, using Example 1.4 as a model, that the two correspondences given before the definition are isomorphisms.
(a) Example 1.1 (b) Example 1.2  1.13 Forthemapf:P1 →R2 givenby
a + bx  −→
b
Find the image of each of these elements of the domain.
(a) 3−2x (b) 2+2x (c) x Show that this map is an isomorphism.
1.14 Show that the natural map f1 from Example 1.5 is an isomorphism.
1.15 Show that the map t: P2 → P2 given by t(ax2 +bx+c) = bx2 −(a+c)x+a is
an isomorphism.
  1.16 Verify that this map is an isomorphism: h: R4 → M2×2 given by
f  a−b 
a
b  c a+d 
c →b d 
d
  1.17 Decide whether each map is an isomorphism. If it is an isomorphism then prove it and if it isn’t then state a condition that it fails to satisfy.
(a) f: M2×2 → R given by
 a b 
c d  →ad−bc
Section I. Isomorphisms
173
 (b) f: M2×2 → R4 given by
 a b   a+b+c 
a+b+c+d c d   →  a + b 
a
 ab  23
(c) f: M2×2 → P3 given by
c d  →c+(d+c)x+(b+a)x +ax
(d) f: M2×2 → P3 given by
 ab  23
c d  →c+(d+c)x+(b+a+1)x +ax
1.18 Show that the map f: R1 → R1 given by f(x) = x3 is one-to-one and onto. Is it an isomorphism?
  1.19 Refer to Example 1.1. Produce two more isomorphisms (of course, you must also verify that they satisfy the conditions in the definition of isomorphism).
1.20 Refer to Example 1.2. Produce two more isomorphisms (and verify that they satisfy the conditions).
  1.21 Show that, although R2 is not itself a subspace of R3, it is isomorphic to the xy-plane subspace of R3.
1.22 Find two isomorphisms between R16 and M4×4.
  1.23 For what k is Mm×n isomorphic to Rk?
1.24 For what k is Pk isomorphic to Rn?
1.25 Prove that the map in Example 1.9, from P5 to P5 given by p(x)  → p(x − 1),
is a vector space isomorphism.
1.26 Why, in Lemma 1.10, must there be a ⃗v ∈ V? That is, why must V be
nonempty?
1.27 Are any two trivial spaces isomorphic?
1.28 In the proof of Lemma 1.11, what about the zero-summands case (that is, if n
is zero)?
1.29 Show that any isomorphism f: P0 → R1 has the form a  → ka for some nonzero
real number k.
1.30 These prove that isomorphism is an equivalence relation.
(a) Show that the identity map id: V → V is an isomorphism. Thus, any vector space is isomorphic to itself.
(b) Show that if f: V → W is an isomorphism then so is its inverse f−1 : W → V. Thus, if V is isomorphic to W then also W is isomorphic to V.
(c) Show that a composition of isomorphisms is an isomorphism: if f: V → W is an isomorphism and g: W → U is an isomorphism then so also is g ◦ f: V → U. Thus, if V is isomorphic to W and W is isomorphic to U, then also V is isomorphic to U.
1.31 Suppose that f: V → W preserves structure. Show that f is one-to-one if and only if the unique member of V mapped by f to ⃗0W is ⃗0V .
174 Chapter Three. Maps Between Spaces
 1.32 Suppose that f: V → W is an isomorphism. Prove that the set {⃗v1, . . . ,⃗vk } ⊆ V is linearly dependent if and only if the set of images {f(⃗v1),...,f(⃗vk)} ⊆ W is linearly dependent.
  1.33 Show that each type of map from Example 1.8 is an automorphism. (a) Dilation ds by a nonzero scalar s.
(b) Rotation tθ through an angle θ.
(c) Reflection fl over a line through the origin.
Hint. For the second and third items, polar coordinates are useful.
1.34 Produce an automorphism of P2 other than the identity map, and other than a
shift map p(x)  → p(x − k).
1.35 (a) Show that a function f: R1 → R1 is an automorphism if and only if it has
the form x →kx for some k̸=0.
(b) Let f be an automorphism of R1 such that f(3) = 7. Find f(−2).
(c) Show that a function f: R2 → R2 is an automorphism if and only if it has the
form
 x   ax + by  y  → cx+dy
for some a, b, c, d ∈ R with ad − bc ̸= 0. Hint. Exercises in prior subsections have shown that
 b   a  d is not a multiple of c
if and only if ad − bc ̸= 0.
(d) Let f be an automorphism of R2 with
Find
 1   2   1   0  f(3)= −1 and f(4)= 1 .
 0  f( −1 ).
1.36 Refer to Lemma 1.10 and Lemma 1.11. Find two more things preserved by isomorphism.
1.37 We show that isomorphisms can be tailored to fit in that, sometimes, given vectors in the domain and in the range we can produce an isomorphism associating those vectors.
(a) Let B = ⟨β⃗1,β⃗2,β⃗3⟩ be a basis for P2 so that any ⃗p ∈ P2 has a unique representation as ⃗p = c1 β⃗ 1 + c2 β⃗ 2 + c3 β⃗ 3 , which we denote in this way.
c1  RepB(⃗p) = c2
c3
Show that the RepB(·) operation is a function from P2 to R3 (this entails showing that with every domain vector ⃗v ∈ P2 there is an associated image vector in R3, and further, that with every domain vector ⃗v ∈ P2 there is at most one associated image vector).
(b) Show that this RepB(·) function is one-to-one and onto. (c) Show that it preserves structure.
Section I. Isomorphisms 175 (d) Produce an isomorphism from P2 to R3 that fits these specifications.
1 0 x+x2  →0 and 1−x →1
00
1.38 Prove that a space is n-dimensional if and only if it is isomorphic to Rn. Hint. Fix a basis B for the space and consider the map sending a vector over to its representation with respect to B.
1.39 (Requires the subsection on Combining Subspaces, which is optional.) Let U and W be vector spaces. Define a new vector space, consisting of the set U×W={(⃗u,w⃗)|⃗u∈Uandw⃗ ∈W}alongwiththeseoperations.
(⃗u1,w⃗1)+(⃗u2,w⃗2)=(⃗u1 +⃗u2,w⃗1 +w⃗2) and r·(⃗u,w⃗)=(r⃗u,rw⃗)
This is a vector space, the external direct sum of U and W.
(a) Check that it is a vector space.
(b) Find a basis for, and the dimension of, the external direct sum P2 × R2.
(c) What is the relationship among dim(U), dim(W), and dim(U × W)?
(d) Suppose that U and W are subspaces of a vector space V such that V = U ⊕ W
 I.2
(in this case we say that V is the internal direct sum of U and W). Show that themapf:U×W→V givenby
f
(⃗u, w⃗ )  −→ ⃗u + w⃗
is an isomorphism. Thus if the internal direct sum is defined then the internal
and external direct sums are isomorphic.
Dimension Characterizes Isomorphism
In the prior subsection, after stating the definition of isomorphism, we gave some results supporting our sense that such a map describes spaces as “the same.” Here we will develop this intuition. When two (unequal) spaces are isomorphic we think of them as almost equal, as equivalent. We shall make that precise by proving that the relationship ‘is isomorphic to’ is an equivalence relation.
2.1 Lemma The inverse of an isomorphism is also an isomorphism.
Proof Suppose that V is isomorphic to W via f: V → W. An isomorphism is a correspondence between the sets so f has an inverse function f−1 : W → V that is also a correspondence.∗
We will show that because f preserves linear combinations, so also does f−1. Suppose that w⃗ 1, w⃗ 2 ∈ W. Because it is an isomorphism, f is onto and there
  ∗ More information on inverse functions is in the appendix.
176 Chapter Three. Maps Between Spaces are ⃗v1,⃗v2 ∈ V such that w⃗ 1 = f(⃗v1) and w⃗ 2 = f(⃗v2). Then
f−1(c1 ·w⃗1 +c2 ·w⃗2)=f−1 c1 ·f(⃗v1)+c2 ·f(⃗v2) 
=f−1(f c1⃗v1 +c2⃗v2) =c1⃗v1 +c2⃗v2 =c1 ·f−1(w⃗1)+c2 ·f−1(w⃗2)
since f−1(w⃗ 1) = ⃗v1 and f−1(w⃗ 2) = ⃗v2. With that, by Lemma 1.11’s second statement, this map preserves structure. QED
2.2 Theorem Isomorphism is an equivalence relation between vector spaces.
Proof We must prove that the relation is symmetric, reflexive, and transitive. To check reflexivity, that any space is isomorphic to itself, consider the identity map. It is clearly one-to-one and onto. This shows that it preserves
linear combinations.
id(c1 ·⃗v1 + c2 ·⃗v2) = c1⃗v1 + c2⃗v2 = c1 · id(⃗v1) + c2 · id(⃗v2)
Symmetry, that if V is isomorphic to W then also W is isomorphic to V, holds by Lemma 2.1 since each isomorphism map from V to W is paired with an isomorphism from W to V.
To finish we must check transitivity, that if V is isomorphic to W and W is isomorphic to U then V is isomorphic to U. Let f: V → W and g: W → U be isomorphisms. Consider their composition g ◦ f : V → U. Because the com- position of correspondences is a correspondence, we need only check that the composition preserves linear combinations.
g◦f c1 ·⃗v1 +c2 ·⃗v2 =g f(c1 ·⃗v1 +c2 ·⃗v2)  =g c1 ·f(⃗v1)+c2 ·f(⃗v2) 
= c1 · g f(⃗v1)) + c2 · g(f(⃗v2)  =c1 ·(g◦f)(⃗v1)+c2 ·(g◦f)(⃗v2)
Thus the composition is an isomorphism. QED Since it is an equivalence, isomorphism partitions the universe of vector
spaces into classes: each space is in one and only one isomorphism class.
   All finite dimensional vector spaces:
V =∼ W
The next result characterizes these classes by dimension. That is, we can describe each class simply by giving the number that is the dimension of all of the spaces in that class.
V
W ...
 
Section I. Isomorphisms 177
  2.3 Theorem Vector spaces are isomorphic if and only if they have the same dimension.
In this double implication statement the proof of each half involves a signifi- cant idea so we will do the two separately.
2.4 Lemma If spaces are isomorphic then they have the same dimension.
Proof We shall show that an isomorphism of two spaces gives a correspon- dence between their bases. That is, we shall show that if f: V → W is an isomorphism and a basis for the domain V is B = ⟨β⃗1,...,β⃗n⟩ then its image D = ⟨f(β⃗1),...,f(β⃗n)⟩ is a basis for the codomain W. (The other half of the correspondence, that for any basis of W the inverse image is a basis for V, follows from the fact that f−1 is also an isomorphism and so we can apply the prior sentence to f−1.)
To see that D spans W, fix any w⃗ ∈ W. Because f is an isomorphism it is onto and so there is a ⃗v ∈ V with w⃗ = f(⃗v). Expand ⃗v as a combination of basis vectors.
w⃗ =f(⃗v)=f(v1β⃗1 +···+vnβ⃗n)=v1 ·f(β⃗1)+···+vn ·f(β⃗n) For linear independence of D, if
⃗0W =c1f(β⃗1)+···+cnf(β⃗n)=f(c1β⃗1 +···+cnβ⃗n)
then, since f is one-to-one and so the only vector sent to ⃗0W is ⃗0V , we have that
⃗0V =c1β⃗1+···+cnβ⃗n,whichimpliesthatallofthec’sarezero. QED
2.5 Lemma If spaces have the same dimension then they are isomorphic.
Proof We will prove that any space of dimension n is isomorphic to Rn. Then we will have that all such spaces are isomorphic to each other by transitivity, which was shown in Theorem 2.2.
Let V be n-dimensional. Fix a basis B = ⟨β⃗1,...,β⃗n⟩ for the domain V. Consider the operation of representing the members of V with respect to B as a function from V to Rn.
  ⃗v=v1β⃗1+···+vnβ⃗n  −→. vn
It is well-defined∗ since every ⃗v has one and only one such representation (see Remark 2.6 following this proof).
v1 RepB .
 ∗ More information on well-defined is in the appendix.
178 Chapter Three. Maps Between Spaces This function is one-to-one because if
 then
RepB(u1β⃗1 +···+unβ⃗n)=RepB(v1β⃗1 +···+vnβ⃗n) u1 v1
 .  =  .  
un vn
and so u1 = v1, . . . , un = vn, implying that the original arguments u1β⃗ 1 + ···+unβ⃗n and v1β⃗1 +···+vnβ⃗n are equal.
This function is onto; any member of Rn w1
. w⃗ =  . . 
wn istheimageofsome⃗v∈V,namelyw⃗ =RepB(w1β⃗1+···+wnβ⃗n).
Finally, this function preserves structure. RepB(r·⃗u+s·⃗v)=RepB((ru1 +sv1)β⃗1 +···+(run +svn)β⃗n)
ru1 +sv1 .
= .  run + svn
u1 v1 . .
=r· . +s· .  un vn
= r · RepB(⃗u) + s · RepB(⃗v)
Therefore RepB is an isomorphism. Consequently any n-dimensional space
is isomorphic to Rn. QED
2.6 Remark The proof has a sentence about ‘well-defined.’ Its point is that to be an isomorphism RepB must be a function, and the definition of function requires that for all inputs the associated output exists and is determined by the input. So we must check that every ⃗v is associated with at least one RepB(⃗v), and no more than one.
In the proof we express elements ⃗v of the domain space as combinations of members of the basis B and then associate ⃗v with the column vector of coefficients. That there is at least one expansion of each ⃗v holds because B is a basis and so spans the space.
Section I. Isomorphisms 179
 The worry that there is no more than one associated member of the codomain is subtler. A contrasting example, where an association fails this unique output requirement, illuminates the issue. Let the domain be P2 and consider a set that is not a basis (it is not linearly independent, although it does span the space).
A = { 1 + 0x + 0x2 , 0 + 1x + 0x2 , 0 + 0x + 1x2 , 1 + 1x + 2x2 }
Call those polynomials α⃗ 1, . . . , α⃗ 4. In contrast to the situation when the set is a basis, here there can be more than one expression of a domain vector in terms of members of the set. For instance, consider ⃗v = 1 + x + x2. Here are two different expansions.
⃗v=1α⃗1 +1α⃗2 +1α⃗3 +0α⃗4 ⃗v=0α⃗1 +0α⃗2 −1α⃗3 +1α⃗4 So this input vector ⃗v is associated with more than one column.
1 0
1 0 1 −1 01
Thus, with A the association is not well-defined. (The issue is that A is not linearly independent; to show uniqueness Theorem Two.III.1.12’s proof uses only linear independence.)
In general, any time that we define a function we must check that output values are well-defined. Most of the time that condition is perfectly obvious but in the above proof it needs verification. See Exercise 21.
This gives us a collection of representatives of the isomorphism classes.
All finite dimensional
vector spaces: One representative
per class
The proofs above pack many ideas into a small space. Through the rest of this chapter we’ll consider these ideas again, and fill them out. As a taste of this we will expand here on the proof of Lemma 2.5.
 2.7 Corollary Each finite-dimensional vector space is isomorphic to one and only one of the Rn.
 ⋆ R0
⋆ R2 ⋆ R1
⋆ R3 ...
180 Chapter Three. Maps Between Spaces 2.8 Example The space M2×2 of 2×2 matrices is isomorphic to R4. With this
basis for the domain
 1 0  0 1  0 0  0 0  B=⟨00,00,10,01⟩
the isomorphism given in the lemma, the representation map f1 = RepB, carries
 the entries over.
a   
a b f1 b c d  −→c
d
One way to think of the map f1 is: fix the basis B for the domain, use the standard basis E4 for the codomain, and associate β⃗1 with ⃗e1, β⃗2 with ⃗e2, etc. Then extend this association to all of the members of two spaces.
a
  
a b ⃗ ⃗ ⃗ ⃗ f1 b =aβ1 +bβ2 +cβ3 +dβ4  −→ a⃗e1 +b⃗e2 +c⃗e3 +d⃗e4 =  cd c
d
We can do the same thing with different bases, for instance, taking this basis for the domain.
 2 0  0 2  0 0  0 0  A=⟨00,00,20,02⟩
Associating corresponding members of A and E4 gives this.
 a b  c d
= (a/2)α⃗1 + (b/2)α⃗2 + (c/2)α⃗3 + (d/2)α⃗4
f2 b/2  −→ (a/2)⃗e1 + (b/2)⃗e2 + (c/2)⃗e3 + (d/2)⃗e4 =   c/2
d/2
gives rise to an isomorphism that is different than f1.
The prior map arose by changing the basis for the domain. We can also
change the basis for the codomain. Go back to the basis B above and use this basis for the codomain.
1 0 0 0
0 1 0 0 D=⟨ , , , ⟩
0 0 0 1 0010
a/2
Section I. Isomorphisms 181 Associate β⃗ 1 with ⃗δ1, etc. Extending that gives another isomorphism.
 a a b ⃗ ⃗ ⃗ ⃗ f3 ⃗ ⃗ ⃗ ⃗ b
  
cd d
=aβ1 +bβ2 +cβ3 +dβ4  −→ aδ1 +bδ2 +cδ3 +dδ4 =  c
We close with a recap. Recall that the first chapter defines two matrices to be row equivalent if they can be derived from each other by row operations. There we showed that relation is an equivalence and so the collection of matrices is partitioned into classes, where all the matrices that are row equivalent together fall into a single class. Then for insight into which matrices are in each class we gave representatives for the classes, the reduced echelon form matrices.
In this section we have followed that pattern except that the notion here of “the same” is vector space isomorphism. We defined it and established some properties, including that it is an equivalence. Then, as before, we developed a list of class representatives to help us understand the partition — it classifies vector spaces by dimension.
In Chapter Two, with the definition of vector spaces, we seemed to have opened up our studies to many examples of new structures besides the familiar Rn’s. We now know that isn’t the case. Any finite-dimensional vector space is actually “the same” as a real space.
Exercises
  2.9 Decide if the spaces are isomorphic.
(a) R2, R4 (b) P5, R5 (c) M2×3, R6 (d) P5, M2×3 (e) M2×k, Mk×2
2.10 Which of these spaces are isomorphic to each other? (a) R3 (b) M2×2 (c) P3 (d) R4 (e) P2
  2.11 Consider the isomorphism RepB (·) : P1 → R2 where B = ⟨1, 1 + x⟩. Find the image of each of these elements of the domain.
(a) 3−2x; (b) 2+2x; (c) x
2.12 For which n is the space isomorphic to Rn? (a) P4
(b) P1
(c) M2×3
(d) the plane 2x − y + z = 0 subset of R3
(e) the vector space of linear combinations of three letters { ax + by + cz | a, b, c ∈ R }
  2.13 Show that if m̸=n then Rm ̸=∼ Rn.
  2.14 Is Mm×n =∼ Mn×m?
  2.15 Are any two planes through the origin in R3 isomorphic?
2.16 Find a set of equivalence class representatives other than the set of Rn’s.
182 Chapter Three. Maps Between Spaces
 2.17 True or false: between any n-dimensional space and Rn there is exactly one isomorphism.
2.18 Can a vector space be isomorphic to one of its proper subspaces?
  2.19 This subsection shows that for any isomorphism, the inverse map is also an isomorphism. This subsection also shows that for a fixed basis B of an n-dimensional vector space V, the map RepB : V → Rn is an isomorphism. Find the inverse of
this map.
  2.20 Prove these facts about matrices.
(a) The row space of a matrix is isomorphic to the column space of its transpose.
(b) The row space of a matrix is isomorphic to its column space. 2.21 Show that the function from Theorem 2.3 is well-defined.
2.22 Is the proof of Theorem 2.3 valid when n = 0?
2.23 For each, decide if it is a set of isomorphism class representatives.
(a) {Ck | k ∈ N}
(b) {Pk | k ∈ {−1,0,1,...}} (c) {Mm×n |m,n∈N}
2.24 Let f be a correspondence between vector spaces V and W (that is, a map that is one-to-one and onto). Show that the spaces V and W are isomorphic via f if and only if there are bases B ⊂ V and D ⊂ W such that corresponding vectors have the same coordinates: RepB(⃗v) = RepD(f(⃗v)).
2.25 Consider the isomorphism RepB : P3 → R4.
(a) Vectors in a real space are orthogonal if and only if their dot product is zero.
Give a definition of orthogonality for polynomials.
(b) The derivative of a member of P3 is in P3. Give a definition of the derivative
of a vector in R4.
  2.26 Does every correspondence between bases, when extended to the spaces, give an
isomorphism? That is, suppose that V is a vector space with basis B = ⟨β⃗ 1, . . . , β⃗ n⟩ and that f: B → W is a correspondence such that D = ⟨f(β⃗1),...,f(β⃗n)⟩ is basis
ˆ⃗⃗ˆˆ⃗ˆ⃗ forW. Mustf:V→Wsending⃗v=c1β1+···+cnβn tof(⃗v)=c1f(β1)+···+cnf(βn)
be an isomorphism?
2.27 (Requires the subsection on Combining Subspaces, which is optional.) Sup-
pose that V = V1 ⊕ V2 and that V is isomorphic to the space U under the map f.
Show that U = f(V1) ⊕ f(U2).
2.28 Show that this is not a well-defined function from the rational numbers to the integers: with each fraction, associate the value of its numerator.
Section II. Homomorphisms 183 II Homomorphisms
The definition of isomorphism has two conditions. In this section we will consider the second one. We will study maps that are required only to preserve structure, maps that are not also required to be correspondences.
Experience shows that these maps are tremendously useful. For one thing we shall see in the second subsection below that while isomorphisms describe how spaces are the same, we can think of these maps as describing how spaces are alike.
II.1 Definition
  1.1 Definition A function between vector spaces h : V → W that preserves addition if ⃗v1,⃗v2 ∈ V then h(⃗v1 +⃗v2) = h(⃗v1) + h(⃗v2)
and scalar multiplication
if ⃗v ∈ V and r ∈ R then h(r · ⃗v) = r · h(⃗v)
is a homomorphism or linear map.
1.2 Example The projection map π: R3 → R2
x
  
is a homomorphism. It preserves addition
πx
y  −→ z
y
xx x+x     x x 121212
π(y1+y2) = π(y1 + y2) = x1 + x2 = π(y1) + π(y2) z z z+z y1+y2 z z
121212
and scalar multiplication.
x rx   x 111
π(r · y1) = π(ry1) = rx1 = r · π(y1) z rz ry1 z
111
This is not an isomorphism since it is not one-to-one. For instance, both ⃗0 and ⃗e3 in R3 map to the zero vector in R2.
184 Chapter Three. Maps Between Spaces 1.3 Example The domain and codomain can be other than spaces of column
vectors. Both of these are homomorphisms; the verifications are straightforward. (1) f1:P2 →P3 givenby
a0 + a1x + a2x2  → a0x + (a1/2)x2 + (a2/3)x3 (2) f2:M2×2 →Rgivenby
 a b 
c d  →a+d
1.4 Example Between any two spaces there is a zero homomorphism, mapping every vector in the domain to the zero vector in the codomain.
1.5 Example These two suggest why we use the term ‘linear map’. (1) Themapg:R3 →Rgivenby
 not.
x     t1
  x     t2
yz  −→
5x−2y x + y
yz  −→
 
5x−2y xy
x g
y  −→ 3x + 2y − 4.5z z
is linear, that is, is a homomorphism. The check is easy. In contrast, the map gˆ: R3 → R given by
x
  gˆ
y  −→ 3x + 2y − 4.5z + 1 z
is not linear. To show this we need only produce a single linear combination that the map does not preserve. Here is one.
0 1 0 1 gˆ(0 + 0) = 4 gˆ(0) + gˆ(0) = 5
0000
(2) The first of these two maps t1, t2 : R3 → R2 is linear while the second is
Finding a linear combination that the second map does not preserve is easy.
Section II. Homomorphisms 185
 So one way to think of ‘homomorphism’ is that we are generalizing ‘isomor- phism’ (by dropping the condition that the map is a correspondence), motivated by the observation that many of the properties of isomorphisms have only to do with the map’s structure-preservation property. The next two results are examples of this motivation. In the prior section we saw a proof for each that only uses preservation of addition and preservation of scalar multiplication, and therefore applies to homomorphisms.
1.6 Lemma A homomorphism sends the zero vector to the zero vector.
  1.7 Lemma The following are equivalent for any map f: V → W between vector spaces.
(1) f is a homomorphism
(2) f(c1·⃗v1+c2·⃗v2)=c1·f(⃗v1)+c2·f(⃗v2)foranyc1,c2 ∈Rand⃗v1,⃗v2 ∈V (3) f(c1 ·⃗v1 +···+cn ·⃗vn) = c1 ·f(⃗v1)+···+cn ·f(⃗vn) for any c1,...,cn ∈ R
and⃗v1,...,⃗vn ∈V
1.8 Example The function f: R2 → R4 given by  x/2 
  
x f  0  y  −→x+y
3y
is linear since it satisfies item (2).
 r1(x1/2) + r2(x2/2)   x1/2 
However, some things that hold for isomorphisms fail to hold for homo- morphisms. One example is in the proof of Lemma I.2.4, which shows that an isomorphism between spaces gives a correspondence between their bases. Homomorphisms do not give any such correspondence; Example 1.2 shows this and another example is the zero map between two nontrivial spaces. Instead, for homomorphisms we have a weaker but still very useful result.
  0 
r1(3y1) + r2(3y2) 3y1 3y2
 0
  = r1   + r2   r1(x1 +y1)+r2(x2 +y2) x1 +y1 x2 +y2
 x2/2 
 0 
 1.9 Theorem A homomorphism is determined by its action on a basis: if V is a vec- tor space with basis ⟨β⃗1,...,β⃗n⟩, if W is a vector space, and if w⃗ 1,...,w⃗ n ∈ W (these codomain elements need not be distinct) then there exists a homomor- phism from V to W sending each β⃗ i to w⃗ i , and that homomorphism is unique.
186 Chapter Three. Maps Between Spaces
 Proof For any input ⃗v ∈ V let its expression with respect to the basis be ⃗v = c1β⃗ 1+· · ·+cnβ⃗ n. Define the associated output by using the same coordinates h(⃗v) = c1w⃗ 1 + · · · + cnw⃗ n. This is well defined because, with respect to the basis, the representation of each domain vector ⃗v is unique.
This map is a homomorphism because it preserves linear combinations: where v⃗ =c β⃗ +···+c β⃗ andv⃗ =d β⃗ +···+d β⃗ ,hereisthecalculation.
111nn211nn
h(r1⃗v1 +r2⃗v2)=h((r1c1 +r2d1)β⃗1 +···+(r1cn +r2dn)β⃗n) =(r1c1 +r2d1)w⃗1 +···+(r1cn +r2dn)w⃗n
= r1h(⃗v1) + r2h(⃗v2)
This map is unique because if hˆ : V → W is another homomorphism satisfying that hˆ(β⃗i) = w⃗i for each i then h and hˆ have the same effect on all of the vectors in the domain.
hˆ ( ⃗v ) = hˆ ( c 1 β⃗ 1 + · · · + c n β⃗ n ) = c 1 hˆ ( β⃗ 1 ) + · · · + c n hˆ ( β⃗ n )
= c 1 w⃗ 1 + · · · + c n w⃗ n = h ( ⃗v )
They have the same action so they are the same function. QED
1.11 Example If we specify a map h: R2 → R2 that acts on the standard basis E2 in this way
 1   −1   0   −4  h( 0 )= 1 h( 1 )= 4
then we have also specified the action of h on any other member of the domain. For instance, the value of h on this argument
 3   1   0   1   0   5  h( −2 )=h(3· 0 −2· 1 )=3·h( 0 )−2·h( 1 )= −5
is a direct consequence of the value of h on the basis vectors.
Later in this chapter we shall develop a convenient scheme for computations like this one, using matrices.
 1.10 Definition Let V and W be vector spaces and let B = ⟨β⃗1,...,β⃗n⟩ be a
basis for V. A function defined on that basis f: B → W is extended linearly
to a function fˆ: V → W if for all ⃗v ∈ V such that ⃗v = c1β⃗ 1 + · · · + cnβ⃗ n, the
ˆ⃗⃗ action of the map is f(⃗v) = c1 · f(β1) + · · · + cn · f(βn).
Section II. Homomorphisms 187
  1.12 Definition A linear map from a space into itself t: V → V is a linear trans- formation .
1.13 Remark In this book we use ‘linear transformation’ only in the case where the codomain equals the domain. However, be aware that other sources may instead use it as a synonym for ‘homomorphism’.
1.14 Example The map on R2 that projects all vectors down to the x-axis is a
linear transformation.
1.15 Example The derivative map d/dx: Pn → Pn
n d/dx 2 n−1 a0 +a1x+···+anx  −→ a1 +2a2x+3a3x +···+nanx
is a linear transformation as this result from calculus shows: d(c1f + c2g)/dx = c1 (df/dx) + c2 (dg/dx).
1.16 Example The matrix transpose operation  ab   ac 
c d  → b d
is a linear transformation of M2×2. (Transpose is one-to-one and onto and so is
in fact an automorphism.)
We finish this subsection about maps by recalling that we can linearly combine maps. For instance, for these maps from R2 to itself
        
xf2x xg0 y −→3x−2y and y −→5x
the linear combination 5f − 2g is also a transformation of R2.     
x 5f−2g 10x
y  −→ 5x−10y
We denote the space of linear maps from V to W by L(V,W).
Proof This set is non-empty because it contains the zero homomorphism. So to show that it is a subspace we need only check that it is closed under the
 x   x  y  → 0
 1.17 Lemma For vector spaces V and W, the set of linear functions from V to W is itself a vector space, a subspace of the space of all functions from V to W.
188 Chapter Three. Maps Between Spaces operations. Let f, g : V → W be linear. Then the operation of function addition
 is preserved
(f + g)(c1⃗v1 + c2⃗v2) = f(c1⃗v1 + c2⃗v2) + g(c1⃗v1 + c2⃗v2)
= c1f(⃗v1) + c2f(⃗v2) + c1g(⃗v1) + c2g(⃗v2)
= c1 f + g (⃗v1) + c2 f + g (⃗v2) as is the operation of scalar multiplication of a function.
(r · f)(c1⃗v1 + c2⃗v2) = r(c1f(⃗v1) + c2f(⃗v2))
= c1(r · f)(⃗v1) + c2(r · f)(⃗v2)
Hence L(V, W) is a subspace.
QED
We started this section by defining ‘homomorphism’ as a generalization of ‘isomorphism’, by isolating the structure preservation property. Some of the points about isomorphisms carried over unchanged, while we adapted others.
Note, however, that the idea of ‘homomorphism’ is in no way somehow secondary to that of ‘isomorphism’. In the rest of this chapter we shall work mostly with homomorphisms. This is partly because any statement made about homomorphisms is automatically true about isomorphisms but more because, while the isomorphism concept is more natural, our experience will show that the homomorphism concept is more fruitful and more central to progress.
Exercises
  1.18 Decide if each h: R3 → R2 is linear.
x   x   x  0  x  1 
(a) h(y) = x+y+z (b) h(y) = 0 (c) h(y) = 1 zzz
x  2x+y  (d) h(y) = 3y − 4z
z
  1.19 Decide if each map h: M2×2 → R is linear.
 a b  (a)h(c d)=a+d
 a b 
(b)h(c d)=ad−bc
 a b 
(c) h( c d )=2a+3b+c−d
 ab  2 2 (d)h(c d)=a+b
  1.20 Show that these are homomorphisms. Are they inverse to each other?
(a) d/dx: P3 → P2 given by a0 + a1x + a2x2 + a3x3 maps to a1 + 2a2x + 3a3x2 (b)  : P2 → P3 given by b0 + b1x + b2x2 maps to b0x + (b1/2)x2 + (b2/3)x3
Section II. Homomorphisms 189
 1.21 Is (perpendicular) projection from R3 to the xz-plane a homomorphism? Pro- jection to the yz-plane? To the x-axis? The y-axis? The z-axis? Projection to the origin?
1.22 Verify that each map is a homomorphism. (a) h: P3 → R2 given by
(b) f: R2 → R3 given by
2  a+b  ax +bx+c → a+c
 x   0  y  →x−y
3y
1.23 Show that, while the maps from Example 1.3 preserve linear operations, they are not isomorphisms.
1.24 Is an identity map a linear transformation?
  1.25 Stating that a function is ‘linear’ is different than stating that its graph is a
line.
(a) The function f1 : R → R given by f1(x) = 2x − 1 has a graph that is a line.
Show that it is not a linear function. (b) The function f2 : R2 → R given by
 x 
y  →x+2y
does not have a graph that is a line. Show that it is a linear function.
  1.26 Part of the definition of a linear function is that it respects addition. Does a
linear function respect subtraction?
1.27 Assume that h is a linear transformation of V and that ⟨β⃗ 1, . . . , β⃗ n⟩ is a basis
of V. Prove each statement.
(a) If h(β⃗ i ) = ⃗0 for each basis vector then h is the zero map.
(b) If h(β⃗ i ) = β⃗ i for each basis vector then h is the identity map.
(c) If there is a scalar r such that h(β⃗ i) = r·β⃗ i for each basis vector then h(⃗v) = r·⃗v
for all vectors in V.
1.28 Consider the vector space R+ where vector addition and scalar multiplication
are not the ones inherited from R but rather are these: a + b is the product of a and b, and r·a is the r-th power of a. (This was shown to be a vector space in an earlier exercise.) Verify that the natural logarithm map ln: R+ → R is a homomorphism between these two spaces. Is it an isomorphism?
1.29 Consider this transformation of the plane R2.
 x   x/2 
y  → y/3
Find the image under this map of this ellipse.
 x 2 2
{ y |(x/4)+(y/9)=1}
  1.30 Imagine a rope wound around the earth’s equator so that it fits snugly (suppose that the earth is a sphere). How much extra rope must we add to raise the circle to a constant six feet off the ground?
190 Chapter Three. Maps Between Spaces   1.31 Verify that this map h: R3 → R
 is linear. Generalize.
x x 3
y  → y• −1=3x−y−z
z z −1
1.32 Show that every homomorphism from R1 to R1 acts via multiplication by a scalar. Conclude that every nontrivial linear transformation of R1 is an isomorphism. Is that true for transformations of R2? Rn?
1.33 (a) Show that for any scalars a1,1,...,am,n this map h: Rn → Rm is a homo- morphism.
x ax+···+ax
1 1,1 1
1,n n
 .   →  .  .   .
 
xn
am,1x1 +···+am,nxn
(b) Show that for each i, the i-th derivative operator di/dxi is a linear trans- formation of Pn. Conclude that for any scalars ck,...,c0 this map is a linear transformation of that space.
dk dk−1 d
f  → ck dxk f + ck−1 dxk−1 f + · · · + c1 dx f + c0 f
1.34 Lemma 1.17 shows that a sum of linear functions is linear and that a scalar multiple of a linear function is linear. Show also that a composition of linear functions is linear.
1.35 Wheref:V→Wislinear,supposethatf(⃗v1)=w⃗1,...,f(⃗vn)=w⃗n forsome vectors w⃗1, ..., w⃗n from W.
(a) If the set of w⃗ ’s is independent, must the set of ⃗v ’s also be independent? (b) If the set of ⃗v ’s is independent, must the set of w⃗ ’s also be independent? (c) If the set of w⃗ ’s spans W, must the set of ⃗v’s span V?
(d) If the set of ⃗v’s spans V, must the set of w⃗ ’s span W?
1.36 Generalize Example 1.16 by proving that for every appropriate domain and codomain the matrix transpose map is linear. What are the appropriate domains and codomains?
1.37 (a) Where ⃗u,⃗v ∈ Rn, by definition the line segment connecting them is the set l={t·⃗u+(1−t)·⃗v|t∈[0..1]}. Showthattheimage,underahomomorphism h, of the segment between ⃗u and ⃗v is the segment between h(⃗u) and h(⃗v).
(b) A subset of Rn is convex if, for any two points in that set, the line segment joining them lies entirely in that set. (The inside of a sphere is convex while the skin of a sphere is not.) Prove that linear maps from Rn to Rm preserve the property of set convexity.
  1.38 Let h: Rn → Rm be a homomorphism.
(a) Show that the image under h of a line in Rn is a (possibly degenerate) line in
Rm.
(b) What happens to a k-dimensional linear surface?
1.39 Prove that the restriction of a homomorphism to a subspace of its domain is another homomorphism.
   
Section II. Homomorphisms 191
 1.40 Assume that h: V → W is linear.
(a) Show that the range space of this map {h(⃗v) | ⃗v ∈ V } is a subspace of the
codomain W.
(b) Show that the null space of this map {⃗v ∈ V | h(⃗v) =⃗0W } is a subspace of the
domain V.
(c) Show that if U is a subspace of the domain V then its image {h(⃗u) | ⃗u ∈ U} is
a subspace of the codomain W. This generalizes the first item. (d) Generalize the second item.
1.41 Consider the set of isomorphisms from a vector space to itself. Is this a subspace of the space L(V,V) of homomorphisms from the space to itself?
1.42 Does Theorem 1.9 need that ⟨β⃗1,...,β⃗n⟩ is a basis? That is, can we still get a well-defined and unique homomorphism if we drop either the condition that the set of β⃗ ’s be linearly independent, or the condition that it span the domain?
1.43 Let V be a vector space and assume that the maps f1, f2 : V → R1 are lin- ear.
(a) Define a map F: V → R2 whose component functions are the given linear ones.   f 1 ( ⃗v )  
Show that F is linear.
(b) Does the converse hold—is any linear map from V to R2 made up of two
linear component maps to R1? (c) Generalize.
II.2 Range space and Null space
Isomorphisms and homomorphisms both preserve structure. The difference is that homomorphisms have fewer restrictions, since they needn’t be onto and needn’t be one-to-one. We will examine what can happen with homomorphisms that cannot happen with isomorphisms.
First consider the fact that homomorphisms need not be onto. Of course, each function is onto some set, namely its range. For example, the injection
map ι: R2 → R3
x   
yx  →y 0
is a homomorphism, and is not onto R3. But it is onto the xy-plane.
⃗v → f2(⃗v)
 2.1 Lemma Under a homomorphism, the image of any subspace of the domain is a subspace of the codomain. In particular, the image of the entire space, the range of the homomorphism, is a subspace of the codomain.
192 Chapter Three. Maps Between Spaces
  Proof Let h: V → W be linear and let S be a subspace of the domain V. The image h(S) is a subset of the codomain W, which is nonempty because S is nonempty. Thus, to show that h(S) is a subspace of W we need only show that it is closed under linear combinations of two vectors. If h(⃗s1) and h(⃗s2) are members of h(S) then c1·h(⃗s1)+c2·h(⃗s2) = h(c1·⃗s1)+h(c2·⃗s2) = h(c1·⃗s1+c2·⃗s2) is also a member of h(S) because it is the image of c1 ·⃗s1 +c2 ·⃗s2 from S. QED
 2.2 Definition The range space of a homomorphism h: V → W is R ( h ) = { h ( ⃗v ) | ⃗v ∈ V }
sometimes denoted h(V). The dimension of the range space is the map’s rank.
We shall soon see the connection between the rank of a map and the rank of a matrix.
2.3 Example For the derivative map d/dx: P3 → P3 given by a0 + a1x + a2x2 + a3x3  → a1 + 2a2x + 3a3x2 the range space R(d/dx) is the set of quadratic polynomials {r + sx + tx2 | r, s, t ∈ R}. Thus, this map’s rank is 3.
2.4 Example With this homomorphism h: M2×2 → P3  a b 
c d  →(a+b+2d)+cx2 +cx3
an image vector in the range can have any constant term, must have an x coefficient of zero, and must have the same coefficient of x2 as of x3. That is, therangespaceisR(h)={r+sx2+sx3 |r,s∈R}andsotherankis2.
The prior result shows that, in passing from the definition of isomorphism to the more general definition of homomorphism, omitting the onto requirement doesn’t make an essential difference. Any homomorphism is onto some space, namely its range.
However, omitting the one-to-one condition does make a difference. A homomorphism may have many elements of the domain that map to one element of the codomain. Below is a bean sketch of a many-to-one map between sets.∗ It shows three elements of the codomain that are each the image of many members of the domain. (Rather than picture lots of individual  → arrows, each association of many inputs with one output shows only one such arrow.)
 ∗ More information on many-to-one maps is in the appendix.
Section II. Homomorphisms 193
            Recall that for any function h: V → W, the set of elements of V that map to w⃗ ∈ W is the inverse image h−1(w⃗ ) = {⃗v ∈ V | h(⃗v) = w⃗ }. Above, the left side shows three inverse image sets.
2.5 Example Consider the projection π: R3 → R2
x
  
which is a homomorphism that is many-to-one. An inverse image set is a vertical line of vectors in the domain.
πx
y  −→ z
y
     R3 One example is this.
R2 w⃗
π − 1 (
2.6 Example This homomorphism h: R2 → R1
1   
13 ) = {  3  | z ∈ R } z
  
xh
 −→ x + y
is also many-to-one. For a fixed w ∈ R1 the inverse image h−1(w)
R2
R1
y
    is the set of plane vectors whose components add to w.
w
194 Chapter Three. Maps Between Spaces
 In generalizing from isomorphisms to homomorphisms by dropping the one- to-one condition we lose the property that, intuitively, the domain is “the same” as the range. We lose, that is, that the domain corresponds perfectly to the range. The examples below illustrate that what we retain is that a homomorphism describes how the domain is “analogous to” or “like” the range.
2.7 Example We think of R3 as like R2 except that vectors have an extra component. That is, we think of the vector with components x, y, and z as like the vector with components x and y. Defining the projection map π makes precise which members of the domain we are thinking of as related to which members of the codomain.
To understanding how the preservation conditions in the definition of homo- morphism show that the domain elements are like the codomain elements, start by picturing R2 as the xy-plane inside of R3 (the xy plane inside of R3 is a set of three-tall vectors with a third component of zero and so does not precisely equal the set of two-tall vectors R2, but this embedding makes the picture much clearer). The preservation of addition property says that vectors in R3 act like their shadows in the plane.
x1  x1  x2  x2  x1 +y1  x1 +x2  y1 above y plus y2 above y equals y1 + y2 above y + y
z1z2z+z12 1212
Thinking of π(⃗v) as the “shadow” of ⃗v in the plane gives this restatement: the sum of the shadows π(⃗v1) + π(⃗v2) equals the shadow of the sum π(⃗v1 + ⃗v2). Preservation of scalar multiplication is similar.
Drawing the codomain R2 on the right gives a picture that is uglier but is more faithful to the bean sketch above.
w⃗ 2
w⃗1 +w⃗2
w⃗ 1
Again, the domain vectors that map to w⃗ 1 lie in a vertical line; one is drawn, in gray. Call any member of this inverse image π−1(w⃗ 1) a “w⃗ 1 vector.” Similarly, there is a vertical line of “w⃗ 2 vectors” and a vertical line of “w⃗ 1 + w⃗ 2 vectors.”
         
Section II. Homomorphisms 195
 Now, saying that π is a homomorphism is recognizing that if π(⃗v1) = w⃗1 and π(⃗v2) = w⃗2 then π(⃗v1 +⃗v2) = π(⃗v1)+π(⃗v2) = w⃗1 +w⃗2. That is, the classes add: any w⃗ 1 vector plus any w⃗ 2 vector equals a w⃗ 1 + w⃗ 2 vector. Scalar multiplication is similar.
So although R3 and R2 are not isomorphic π describes a way in which they are alike: vectors in R3 add as do the associated vectors in R2 — vectors add as their shadows add.
2.8 Example A homomorphism can express an analogy between spaces that is more subtle than the prior one. For the map from Example 2.6
  
xh
 −→ x + y
y
fix two numbers in the range w1, w2 ∈ R. A ⃗v1 that maps to w1 has components that add to w1, so the inverse image h−1(w1) is the set of vectors with endpoint on the diagonal line x + y = w1. Think of these as “w1 vectors.” Similarly we have “w2 vectors” and “w1 + w2 vectors.” The addition preservation property says this.
   map h: R3 → R2
x
  
 y    → x x z
⃗v1
⃗v2
a “w1 vector” plus a “w2 vector” equals a “w1 + w2 vector”
Restated, if we add a w1 vector to a w2 vector then h maps the result to a w1 + w2 vector. Briefly, the sum of the images is the image of the sum. Even more briefly, h(⃗v1) + h(⃗v2) = h(⃗v1 +⃗v2).
2.9 Example The inverse images can be structures other than lines. For the linear
the inverse image sets are planes x = 0, x = 1, etc., perpendicular to the x-axis.
⃗v 1 + ⃗v 2
  
196 Chapter Three. Maps Between Spaces
 We won’t describe how every homomorphism that we will use is an analogy because the formal sense that we make of “alike in that . . . ” is ‘a homomorphism exists such that . . . ’. Nonetheless, the idea that a homomorphism between two spaces expresses how the domain’s vectors fall into classes that act like the range’s vectors is a good way to view homomorphisms.
Another reason that we won’t treat all of the homomorphisms that we see as above is that many vector spaces are hard to draw, e.g., a space of polynomials. But there is nothing wrong with leveraging spaces that we can draw: from the three examples 2.7, 2.8, and 2.9 we draw two insights.
The first insight is that in all three examples the inverse image of the range’s zero vector is a line or plane through the origin. It is therefore a subspace of the domain.
(The examples above consider inverse images of single vectors but this result is about inverse images of sets h−1(S) = {⃗v ∈ V | h(⃗v) ∈ S}. We use the same term for both by taking the inverse image of a single element h−1(w⃗ ) to be the inverse image of the one-element set h−1({w⃗ }).)
Proof Let h: V → W be a homomorphism and let S be a subspace of the range space of h. Consider the inverse image of S. It is nonempty because it contains ⃗0V, since h(⃗0V) =⃗0W and⃗0W is an element of S as S is a subspace. To finish we show that h−1(S) is closed under linear combinations. Let ⃗v1 and ⃗v2 be two of its elements, so that h(⃗v1) and h(⃗v2) are elements of S. Then c1⃗v1 + c2⃗v2 is an element of the inverse image h−1(S) because h(c1⃗v1 +c2⃗v2) = c1h(⃗v1)+c2h(⃗v2) is a member of S. QED
 2.10 Lemma For any homomorphism the inverse image of a subspace of the range is a subspace of the domain. In particular, the inverse image of the trivial subspace of the range is a subspace of the domain.
 2.11 Definition The null space or kernel of a linear map h: V → W is the inverse image of ⃗0W .
N (h)=h−1(⃗0W)={⃗v∈V |h(⃗v)=⃗0W} The dimension of the null space is the map’s nullity.
    0V 0W
 
Section II. Homomorphisms 197 2.12 Example The map from Example 2.3 has this null space N (d/dx) =
{a0+0x+0x2+0x3 |a0 ∈R}soitsnullityis1.
2.13 Example The map from Example 2.4 has this null space, and nullity 2.
 a b  
N(h)={ 0 −(a+b)/2 |a,b∈R}
Now for the second insight from the above examples. In Example 2.7 each of the vertical lines squashes down to a single point — in passing from the domain to the range, π takes all of these one-dimensional vertical lines and maps them to a point, leaving the range smaller than the domain by one dimension. Similarly, in Example 2.8 the two-dimensional domain compresses to a one-dimensional range by breaking the domain into the diagonal lines and maps each of those to a single member of the range. Finally, in Example 2.9 the domain breaks into planes which get squashed to a point and so the map starts with a three-dimensional domain but ends two smaller, with a one-dimensional range. (The codomain is two-dimensional but the range is one-dimensional and the dimension of the range is what matters.)
Proof Let h: V → W be linear and let BN = ⟨β⃗1,...,β⃗k⟩ be a basis for the null space. Expand that to a basis BV = ⟨β⃗1,...,β⃗k,β⃗k+1,...,β⃗n⟩ for the entire domain, using Corollary Two.III.2.13. We shall show that BR = ⟨h(β⃗ k+1 ), . . . , h(β⃗ n )⟩ is a basis for the range space. Then counting the size of the bases gives the result.
To see that BR is linearly independent, consider ⃗0W = ck+1h(β⃗ k+1) + · · · + cnh(β⃗n). Wehave⃗0W =h(ck+1β⃗k+1+···+cnβ⃗n)andsock+1β⃗k+1+···+cnβ⃗n is in the null space of h. As BN is a basis for the null space there are scalars c1,...,ck satisfyingthisrelationship.
c1β⃗1 +···+ckβ⃗k =ck+1β⃗k+1 +···+cnβ⃗n
But this is an equation among members of BV, which is a basis for V, so each ci equals 0. Therefore BR is linearly independent.
To show that BR spans the range space consider a member of the range space h(⃗v). Express ⃗v as a linear combination ⃗v = c1β⃗ 1 + · · · + cnβ⃗ n of members of BV. This gives h(⃗v) = h(c1β⃗1 + ··· + cnβ⃗n) = c1h(β⃗1) + ··· + ckh(β⃗k) + ck+1h(β⃗k+1) + ··· + cnh(β⃗n) and since β⃗1, ..., β⃗k are in the null space, we havethath(⃗v)=⃗0+···+⃗0+ck+1h(β⃗k+1)+···+cnh(β⃗n). Thus,h(⃗v)isa linear combination of members of BR, and so BR spans the range space. QED
  2.14 Theorem A linear map’s rank plus its nullity equals the dimension of its domain.
198 Chapter Three. Maps Between Spaces
 2.15 Example Where h: R3 → R4 is x
the range space and null space are
a
0
x   h 0
 yz    − →  y  0
0   R(h)={ |a,b∈R} and N (h)={0|z∈R}
z
and so the rank of h is 2 while the nullity is 1.
2.16 Example If t: R → R is the linear transformation x  → −4x, then the range
is R(t) = R. The rank is 1 and the nullity is 0.
We know that an isomorphism exists between two spaces if and only if the dimension of the range equals the dimension of the domain. We have now seen that for a homomorphism to exist a necessary condition is that the dimension of the range must be less than or equal to the dimension of the domain. For instance, there is no homomorphism from R2 onto R3. There are many homomorphisms from R2 into R3, but none onto.
The range space of a linear map can be of dimension strictly less than the dimension of the domain and so linearly independent sets in the domain may map to linearly dependent sets in the range. (Example 2.3’s derivative transfor- mation on P3 has a domain of dimension 4 but a range of dimension 3 and the derivative sends { 1, x, x2 , x3 } to { 0, 1, 2x, 3x2 }). That is, under a homomorphism independence may be lost. In contrast, dependence stays.
Proof Suppose that c1⃗v1 + · · · + cn⃗vn = ⃗0V with some ci nonzero. Apply h to both sides: h(c1⃗v1 + ··· + cn⃗vn) = c1h(⃗v1) + ··· + cnh(⃗vn) and h(⃗0V) = ⃗0W. Thus we have c1h(⃗v1) + · · · + cnh(⃗vn) = ⃗0W with some ci nonzero. QED
When is independence not lost? The obvious sufficient condition is when the homomorphism is an isomorphism. This condition is also necessary; see
b 0
 2.17 Corollary The rank of a linear map is less than or equal to the dimension of the domain. Equality holds if and only if the nullity of the map is 0.
 2.18 Lemma Under a linear map, the image of a linearly dependent set is linearly dependent.
Section II. Homomorphisms 199
 Exercise 37. We will finish this subsection comparing homomorphisms with isomorphisms by observing that a one-to-one homomorphism is an isomorphism from its domain onto its range.
2.19 Example This one-to-one homomorphism ι: R2 → R3
x   
y
gives a correspondence between R2 and the xy-plane subset of R3.
xι
 −→ y 0
 2.20 Theorem Where V is an n-dimensional vector space, these are equivalent statements about a linear map h: V → W.
(1) h is one-to-one
(2) h has an inverse from its range to its domain that is a linear map
(3) N (h) = {⃗0 }, that is, nullity(h) = 0
(4) rank(h) = n
(5) if ⟨β⃗1,...,β⃗n⟩ is a basis for V then ⟨h(β⃗1),...,h(β⃗n)⟩ is a basis for R(h)
Proof We will first show that (1) ⇐⇒ (2). We will then show that (1) =⇒ (3) =⇒ (4) =⇒ (5) =⇒ (2).
For (1) =⇒ (2), suppose that the linear map h is one-to-one, and therefore has an inverse h−1 : R(h) → V. The domain of that inverse is the range of h and thus a linear combination of two members of it has the form c1h(⃗v1) + c2h(⃗v2). On that combination, the inverse h−1 gives this.
h−1(c1h(⃗v1) + c2h(⃗v2)) = h−1(h(c1⃗v1 + c2⃗v2)) = h−1 ◦ h (c1⃗v1 + c2⃗v2)
= c1⃗v1 + c2⃗v2
= c1 · h−1(h(⃗v1)) + c2 · h−1(h(⃗v2))
Thus if a linear map has an inverse then the inverse must be linear. But this also gives the (2) =⇒ (1) implication, because the inverse itself must be one-to-one. Oftheremainingimplications,(1) =⇒ (3)holdsbecauseanyhomomorphism
maps ⃗0V to ⃗0W , but a one-to-one map sends at most one member of V to ⃗0W . Next, (3) =⇒ (4) is true since rank plus nullity equals the dimension of the
domain.
For (4) =⇒ (5), to show that ⟨h(β⃗1),...,h(β⃗n)⟩ is a basis for the range
space we need only show that it is a spanning set, because by assumption the range has dimension n. Consider h(⃗v) ∈ R(h). Expressing ⃗v as a linear
200 Chapter Three. Maps Between Spaces
 combination of basis elements produces h(⃗v) = h(c1β⃗ 1 + c2β⃗ 2 + · · · + cnβ⃗ n), which gives that h(⃗v) = c1h(β⃗ 1) + · · · + cnh(β⃗ n), as desired.
Finally,forthe(5) =⇒ (2)implication,assumethat⟨β⃗1,...,β⃗n⟩isabasis for V so that ⟨h(β⃗1),...,h(β⃗n)⟩ is a basis for R(h). Then every w⃗ ∈ R(h) has the unique representation w⃗ = c1h(β⃗ 1) + · · · + cnh(β⃗ n). Define a map from R(h) to V by
w⃗  → c1β⃗1 +c2β⃗2 +···+cnβ⃗n
(uniqueness of the representation makes this well-defined). Checking that it is
linear and that it is the inverse of h are easy. QED
We have seen that a linear map expresses how the structure of the domain is like that of the range. We can think of such a map as organizing the domain space into inverse images of points in the range. In the special case that the map is one-to-one, each inverse image is a single point and the map is an isomorphism between the domain and the range.
Exercises
 2.21 Leth:P3 →P4 begivenbyp(x) →x·p(x). Whichoftheseareinthenull space? Which are in the range space?
(a) x3 (b) 0 (c) 7 (d) 12x−0.5x3 (e) 1+3x2 −x3 2.22 Find the range space and the rank of each homomorphism.
(a) h: P3 → R2 given by
(b) f: R2 → R3 given by
2  a+b  ax +bx+c → a+c
 x   0  y  →x−y
3y
  2.23 Find the range space and rank of each map. (a) h: R2 → P3 given by
(b) h: M2×2 → R given by
(c) h: M2×2 → P2 given by
 a  2 b  →a+ax+ax
 a b 
c d  →a+d
 a b  2 c d  →a+b+c+dx
(d) the zero map Z: R3 → R4
  2.24 For each linear map in the prior exercise, find the null space and nullity.   2.25 Find the nullity of each map below.
Section II. Homomorphisms
201
 (a) h:R5 →R8 ofrankfive
(b) h:P3 →P3 ofrankone (d) h: M3×3 → M3×3, onto
(c) h: R6 → R3, an onto map
  2.26 What is the null space of the differentiation transformation d/dx: Pn → Pn?
What is the null space of the second derivative, as a transformation of Pn? The
k-th derivative?
2.27 For the map h: R3 → R2 given by
x  x + y  y → x+z
z
find the range space, rank, null space, and nullity.
2.28 Example 2.7 restates the first condition in the definition of homomorphism as
‘the shadow of a sum is the sum of the shadows’. Restate the second condition in
the same style.
2.29 For the homomorphism h: P3 → P3 given by h(a0 + a1x + a2x2 + a3x3) =
a0+(a0+a1)x+(a2+a3)x3 findthese.
(a) N (h) (b) h−1(2 − x3) (c) h−1(1 + x2)
 2.30 Forthemapf:R2 →Rgivenby  x 
f( y )=2x+y
sketch these inverse image sets: f−1(−3), f−1(0), and f−1(1).
  2.31 Each of these transformations of P3 is one-to-one. For each, find the in-
verse.
(a) a0 +a1x+a2x2 +a3x3  → a0 +a1x+2a2x2 +3a3x3
(b) a0 +a1x+a2x2 +a3x3  → a0 +a2x+a1x2 +a3x3
(c) a0 +a1x+a2x2 +a3x3  → a1 +a2x+a3x2 +a0x3
(d) a0 +a1x+a2x2 +a3x3  → a0 +(a0 +a1)x+(a0 +a1 +a2)x2 +(a0 +a1 +a2 +a3)x3
2.32 Describe the null space and range space of a transformation given by ⃗v  → 2⃗v. 2.33 List all pairs (rank(h), nullity(h)) that are possible for linear maps from R5 to
R3. 2.34   2.35
Does the differentiation map d/dx: Pn → Pn have an inverse? Find the nullity of this map h: Pn → R.
a0 +a1x+···+anxn  →
  x=1 x=0
a0 +a1x+···+anxn dx
(a) Prove that a homomorphism is onto if and only if its rank equals the dimension of its codomain.
(b) Conclude that a homomorphism between vector spaces with the same dimen- sion is one-to-one if and only if it is onto.
2.37 Show that a linear map is one-to-one if and only if it preserves linear indepen- dence.
2.38 Corollary 2.17 says that for there to be an onto homomorphism from a vector space V to a vector space W, it is necessary that the dimension of W be less than or equal to the dimension of V. Prove that this condition is also sufficient; use Theorem 1.9 to show that if the dimension of W is less than or equal to the dimension of V, then there is a homomorphism from V to W that is onto.
2.36
202 Chapter Three. Maps Between Spaces
   2.39 Recall that the null space is a subset of the domain and the range space is a subset of the codomain. Are they necessarily distinct? Is there a homomorphism that has a nontrivial intersection of its null space and its range space?
2.40 Prove that the image of a span equals the span of the images. That is, where h: V → W is linear, prove that if S is a subset of V then h([S]) equals [h(S)]. This generalizes Lemma 2.1 since it shows that if U is any subspace of V then its image {h(⃗u)|⃗u∈U} is a subspace of W, because the span of the set U is U.
2.41 (a)Provethatforanylinearmaph:V→Wandanyw⃗ ∈W,theseth−1(w⃗) has the form
h−1(w⃗)={⃗v+n⃗ |⃗v,n⃗ ∈V and n⃗ ∈N (h) and h(⃗v)=w⃗ }
(if h is not onto and w⃗ is not in the range of h then this set is empty since its third condition cannot be satisfied). Such a set is a coset of N (h) and we denote it as⃗v+N (h).
(b) Consider the map t: R2 → R2 given by
 x  t  ax+by 
y  −→ cx+dy
for some scalars a, b, c, and d. Prove that t is linear.
(c) Conclude from the prior two items that for any linear system of the form ax + by = e
cx+dy= f
we can write the solution set (the vectors are members of R2)
{⃗p + ⃗h | ⃗h satisfies the associated homogeneous system}
where ⃗p is a particular solution of that linear system (if there is no particular solution then the above set is empty).
(d) Show that this map h: Rn → Rm is linear
x ax+···+ax
1 1,1 1
1,n n
 
 .   →  .  .   .
am,1x1 +···+am,nxn
for any scalars a1,1, ..., am,n. Extend the conclusion made in the prior item.
xn
(e) Show that the k-th derivative map is a linear transformation of Pn for each k.
Prove that this map is a linear transformation of the space
dk dk−1 d
f → dxkf+ck−1dxk−1f+···+c1dxf+c0f
for any scalars ck, . . . , c0. Draw a conclusion as above.
2.42 Prove that for any transformation t: V → V that is rank one, the map given by
composing the operator with itself t ◦ t: V → V satisfies t ◦ t = r · t for some real
number r.
2.43 Let h: V → R be a homomorphism, but not the zero homomorphism. Prove
that if ⟨β⃗1,...,β⃗n⟩ is a basis for the null space and if ⃗v ∈ V is not in the null space
then ⟨⃗v,β⃗1,...,β⃗n⟩ is a basis for the entire domain V.
2.44 Show that for any space V of dimension n, the dual space
L(V,R) = {h: V → R | h is linear}
is isomorphic to Rn. It is often denoted V∗. Conclude that V∗ =∼ V.
   
Section II. Homomorphisms 203
 2.45 Show that any linear map is the sum of maps of rank one.
2.46 Is ‘is homomorphic to’ an equivalence relation? (Hint: the difficulty is to decide
on an appropriate meaning for the quoted phrase.)
2.47 Show that the range spaces and null spaces of powers of linear maps t: V → V
form descending
and ascending
V ⊇R(t)⊇R(t2)⊇...
{ ⃗0 } ⊆ N ( t ) ⊆ N ( t 2 ) ⊆ . . .
chains. Also show that if k is such that R(tk) = R(tk+1) then all following range
spaces are equal: R(tk) = R(tk+1) = R(tk+2) . . . . Similarly, if N (tk) = N (tk+1) then N (tk) = N (tk+1) = N (tk+2) = ....
204 Chapter Three. Maps Between Spaces III Computing Linear Maps
The prior section shows that a linear map is determined by its action on a basis. The equation
h(⃗v)=h(c1 ·β⃗1 +···+cn ·β⃗n)=c1 ·h(β⃗1)+···+cn ·h(β⃗n)
describes how we get the value of the map on any vector ⃗v by starting from the value of the map on the vectors β⃗ i in a basis and extending linearly.
This section gives a convenient scheme based on matrices to use the represen- tations of h(β⃗ 1 ), . . . , h(β⃗ n ) to compute, from the representation of a vector in the domain RepB(⃗v), the representation of that vector’s image in the codomain RepD (h(⃗v)).
III.1 Representing Linear Maps with Matrices
1.1 Example For the spaces R2 and R3 fix these bases.
 1 0 1 B = ⟨ 20 , 14 ⟩ D = ⟨0,−2,0⟩
001 Consider the map h: R2 → R3 that is determined by this association.
    
   1    1 2h1h
0  −→ 1 4  −→ 20
To compute the action of this map on any vector at all from the domain we first represent the vector h(β⃗ 1)
1 1 0 1 0 1 = 0 0 − 1 −2 + 1 0 RepD (h(β⃗ 1 )) = −1/2
 10201 1D and the vector h(β⃗ 2).
1 1 0 1 1 2 = 1 0 − 1 −2 + 0 0 RepD (h(β⃗ 2 )) = −1
0001 0
D
Section III. Computing Linear Maps 205 With these, for any member ⃗v of the domain we can compute h(⃗v).
 2   1  h(⃗v)=h(c1· 0 +c2· 4 )
 2   1  =c1·h( 0 )+c2·h( 4 )
 1 0 1 1 0 1 = c1 · (0 0− 1 −2+ 1 0) + c2 · (1 0− 1 −2+ 0 0)
 0201 001 1 0 1
=(0c1 +1c2)·0+(−1c1 −1c2)·−2+(1c1 +0c2)·0 0201
Thus,
if RepB(⃗v) = For instance,
   
c1 then RepD( h(⃗v) ) = −(1/2)c1 − 1c2.
c2 1c1 + 0c2
2 since RepB( 48 ) = 12 we have RepD( h( 48 ) ) = −5/2.
B1
We express computations like the one above with a matrix notation.
       
0 1    0c+1c  12
  c1   −1/2 −1 c = (−1/2)c1 − 1c2
10 2B 1c1+0c2
B,D D
In the middle is the argument ⃗v to the map, represented with respect to the domain’s basis B by the column vector with components c1 and c2. On the right is the value of the map on that argument h(⃗v), represented with respect to the codomain’s basis D. The matrix on the left is the new thing. We will use it to represent the map and we will think of the above equation as representing an application of the map to the matrix.
That matrix consists of the coefficients from the vector on the right, 0 and 1 from the first row, −1/2 and −1 from the second row, and 1 and 0 from the third row. That is, we make it by adjoining the vectors representing the h(β⃗i)’s.
. .  RepD( h(β⃗ 1) ) RepD( h(β⃗ 2) ) 
. . ..
 0c+1c  12
   
206 Chapter Three. Maps Between Spaces
  1.2 Definition Suppose that V and W are vector spaces of dimensions n and m with bases B and D, and that h: V → W is a linear map. If
Rep (h(β⃗ ))=  D 1 .
... Rep (h(β⃗ ))=  D n .
h1,1   h2,1 
h1,n   h2,n 
. hm,1 D
. hm,n D
then
hm,1 hm,2 . . . hm,n is the matrix representation of h with respect to B, D.
 h1,1 h1,2 ... h1,n   h2,1 h2,2 ... h2,n 
Rep (h) =
B,D  . 
.
B,D
In that matrix the number of columns n is the dimension of the map’s domain while the number of rows m is the dimension of the codomain.
We use lower case letters for a map, upper case for the matrix, and lower case again for the entries of the matrix. Thus for the map h, the matrix representing it is H, with entries hi,j.
1.3 Example If h: R3 → P1 is a 
then where
1 h
a2  −→ (2a1 + a2) + (−a3)x a3
0 0 2 B = ⟨0 , 2 , 0⟩
D = ⟨1 + x, −1 + x⟩
100 the action of h on B is this.
0 0
0  −→ −x 2  −→ 2 0  −→ 4
100
A simple calculation
 −1/2   1   2  RepD(−x) = −1/2 RepD(2) = −1 RepD(4) = −2
2 h h h
DDD
Section III. Computing Linear Maps 207 shows that this is the matrix representing h with respect to the bases.
  −1/2 1 2  RepB,D(h) = −1/2 −1 −2
B,D
 1.4 Theorem Assume that V and W are vector spaces of dimensions n and m with bases B and D, and that h: V → W is a linear map. If h is represented by
 h1,1 h1,2 ... h1,n   h2,1 h2,2 ... h2,n 
Rep
and ⃗v ∈ V is represented by
(h) =
B,D  . 
.
hm,1 hm,2 . . . hm,n
B,D
then the representation of the image of ⃗v is this.
 h1,1c1 +h1,2c2 +···+h1,ncn 
Rep (⃗v)=
B  . 
 h2,1c1 +h2,2c2 +···+h2,ncn  D  . 
Rep (h(⃗v))=
.
c1 c2
. cn B
hm,1c1 +hm,2c2 +···+hm,ncn D
Proof This formalizes Example 1.1. See Exercise 32.
QED
 1.5 Definition The matrix-vector product of a m×n matrix and a n×1 vector
is this.

a1,1 a1,2 ... a1,n c  a1,1c1 +···+a1,ncn 1  a2,1 a2,2 ... a2,n .=a2,1c1+···+a2,ncn 
 . .  .
am,1 am,2 ... am,n am,1c1 +···+am,ncn
  . cn  . 
Briefly, application of a linear map is represented by the matrix-vector product of the map’s representative and the vector’s representative.
1.6 Remark Theorem 1.4 is not surprising, because we chose the matrix repre- sentative in Definition 1.2 precisely to make the theorem true — if the theorem
208 Chapter Three. Maps Between Spaces were not true then we would adjust the definition to make it so. Nonetheless,
we need the verification.
1.7 Example For the matrix from Example 1.3 we can calculate where that map
 sends this vector.
4 ⃗ v =  1 
0
With respect to the domain basis B the representation of this vector is
0 RepB(⃗v) = 1/2
2
B
and so the matrix-vector product gives the representation of the value h(⃗v) with respect to the codomain basis D.
0   
RepD(h(⃗v)) = −1/2 1 2 1/2 −1/2 −1 −2 B,D 2 B
 (−1/2)·0+1·(1/2)+2·2 
= (−1/2)·0−1·(1/2)−2·2 = −9/2
DD
To find h(⃗v) itself, not its representation, take (9/2)(1 + x) − (9/2)(−1 + x) = 9.
1.8 Example Let π: R3 → R2 be projection onto the xy-plane. To give a matrix representing this map, we first fix some bases.
1 1 −1
B = ⟨  0  ,  1  ,  0  ⟩ D = ⟨ 21 , 1 1 ⟩
001
For each vector in the domain’s basis, find its image under the map.
1    1    −1     π 1 π 1 π −1
 0 0    − → 0  10    − → 1  01    − → 0
Then find the representation of each image with respect to the codomain’s basis.
 1    1    1   0   −1   −1  RepD( 0 )= −1 RepD( 1 )= 1 RepD( 0 )= 1
Finally, adjoining these representations gives the matrix representing π with
respect to B, D.
 1 0 −1  RepB,D(π) = −1 1 1
B,D
    
  9/2  
Section III. Computing Linear Maps 209
 We can illustrate Theorem 1.4 by computing the matrix-vector product repre- senting this action by the projection map.
2
  
π (  2  ) = 2 2 1
Represent the domain vector with respect to the domain’s basis
2 1 RepB(2) = 2
11B
  1   
to get this matrix-vector product.
2   RepD(π(2)) =
1 0 −1 2 =
0
2 D
−1 1 1 B,D 1 Expanding this into a linear combination of vectors from D
1
B
 2   1   2  0·1+2·1=2
checks that the map’s action is indeed reflected in the operation of the matrix. We will sometimes compress these three displayed equations into one.
2 1 h02
      2=2  −→ =
11H2D2 B
We now have two ways to compute the effect of projection, the straightforward formula that drops each three-tall vector’s third component to make a two-tall vector, and the above formula that uses representations and matrix-vector multiplication. The second way may seem complicated compared to the first, but it has advantages. The next example shows that for some maps this new scheme simplifies the formula.
1.9 Example To represent a rotation map tθ : R2 → R2 that turns all vectors in the plane counterclockwise through an angle θ
   ⃗u
tπ/6 −→
tπ/6 (⃗u)
   
210 Chapter Three. Maps Between Spaces
 we start by fixing the standard bases E2 for both the domain and codomain
basis, Now find the image under the map of each vector in the domain’s basis.
         
1t cosθ 0t −sinθ θθ
 −→  −→ (∗) 0 sinθ 1 cosθ
Represent these images with respect to the codomain’s basis. Because this basis is E2, vectors represent themselves. Adjoin the representations to get the matrix representing the map.
 cos θ − sin θ  RepE2,E2 (tθ) = sin θ cos θ
The advantage of this scheme is that we get a formula for the image of any vector at all just by knowing in (∗) how to represent the image of the two basis vectors. For instance, here we rotate a vector by θ = π/6.
     √        
 3 3 −2 = −2
tπ/6  −→
3/2 1/2
−1/2 √3/2
3 −2
≈
3.598 −0.232 =
3.598 −0.232
 E2
E2    √     √  
More generally, we have a formula for rotation by θ = π/6.
x tπ/6 3/2 −1/2 x ( 3/2)x − (1/2)y
  y  −→ 1/2 √3/2 y = (1/2)x + (√3/2)y
1.10 Example In the definition of matrix-vector product the width of the matrix
  equals the height of the vector. Hence, this product is not defined.
 1 0 0   1  4310
It is undefined for a reason: the three-wide matrix represents a map with a three-dimensional domain while the two-tall vector represents a member of a two-dimensional space. So the vector cannot be in the domain of the map.
Nothing in Definition 1.5 forces us to view matrix-vector product in terms of representations. We can get some insights by focusing on how the entries combine.
A good way to view matrix-vector product is that it is formed from the dot products of the rows of the matrix with the column vector.
. c1 .  ..
 c2   ai,1 ai,2 ... ai,n . =ai,1c1 +ai,2c2 +···+ai,ncn . . . 
. cn .
Section III. Computing Linear Maps 211 Looked at in this row-by-row way, this new operation generalizes dot product.
We can also view the operation column-by-column.
 h1,1 h1,2 ... h1,n c1  h1,1c1 +h1,2c2 +···+h1,ncn   h2,1 h2,2 ... h2,n c2  h2,1c1 +h2,2c2 +···+h2,ncn 
  .  .
hm,1 hm,2 ...
  .  =  .    .   . 
hm,n cn
hm,1c1 +hm,2c2 +···+hm,ncn h1,1  h1,n 
 h2,1   h2,n  =c1 . +···+cn .   .   . 
hm,1 hm,n
The result is the columns of the matrix weighted by the entries of the vector.
1.11 Example
   2            1 0 − 1  − 1  = 2 1 − 1 0 + 1 − 1 = 1
20312037
This way of looking at matrix-vector product brings us back to the objective stated at the start of this section, to compute h(c1β⃗1 +···+cnβ⃗n) as c1h(β⃗1)+ ···+cnh(β⃗n).
We began this section by noting that the equality of these two enables us to compute the action of h on any argument knowing only h(β⃗ 1 ), . . . , h(β⃗ n ). We have developed this into a scheme to compute the action of the map by taking the matrix-vector product of the matrix representing the map with the vector representing the argument. In this way, with respect to any bases, for any linear map there is a matrix representation. The next subsection will show the converse, that if we fix bases then for any matrix there is an associated linear map.
Exercises
  1.12 Multiply the matrix
by each vector, or state “not defined.”
2  −2  0 (a) 1 (b) −2 (c) 0
00
1.13 Multiply this matrix by each vector or state “note defined.”  3 1 
24
1 3 1 0 −1 2 110
212
Chapter Three. Maps Between Spaces
 1  0  0 (a) 2 (b) −1 (c) 0
10
1.14 Perform, if possible, each matrix-vector multiplication.
 2 1   4   1 1 0 1  1 1 1
(a) 3 −1/2 2 (b) −2 1 0 3 (c) −2 1 3 11
1.15 This matrix equation expresses a linear system. Solve it. 2 1 1x 8
0 1 3y = 4 1−12z 4
  1.16 For a homomorphism from P2 to P3 that sends
1 →1+x, x →1+2x, and x2  →x−x3
wheredoes1−3x+2x2 go?
1.17 Let h: R2 → M2×2 be the linear transformation with this action.
 1   1 2   0   0 −1 
0  → 0 1 1  → 1 0 What is its effect on the general vector with entries x and y?
  1.18 Assume that h: R2 → R3 is determined by this action.  1  2  0  0 0  →2 1  →1
0 −1
Using the standard bases, find
(a) the matrix representing this map; (b) a general formula for h(⃗v).
1.19 Represent the homomorphism h: R3 → R2 given by this formula and with respect to these bases.
x  x+y  1 1 1  1   0  y → x+z B=⟨1,1,0⟩ D=⟨ 0 , 2 ⟩
z 100
  1.20 Let d/dx: P3 → P3 be the derivative transformation.
(a) Represent d/dx with respect to B, B where B = ⟨1, x, x2 , x3 ⟩.
(b) Represent d/dx with respect to B, D where D = ⟨1, 2x, 3x2 , 4x3 ⟩.
  1.21 Represent each linear map with respect to each pair of bases.
(a) d/dx: Pn → Pn with respect to B,B where B = ⟨1,x,...,xn⟩, given by
a0 +a1x+a2x2 +···+anxn  →a1 +2a2x+···+nanxn−1
(b)  : Pn → Pn+1 with respect to Bn,Bn+1 where Bi = ⟨1,x,...,xi⟩, given by
  (c)
by
a0+a1x+a2x2+···+anxn →a0x+a1x2+···+ an xn+1  2n+1
01 : Pn → R with respect to B,E1 where B = ⟨1,x,...,xn⟩ and E1 = ⟨1⟩, given
a0 +a1x+a2x2 +···+anxn  →a0 + a1 +···+ an 2 n+1
  
Section III. Computing Linear Maps 213 (d) eval3 : Pn → R with respect to B,E1 where B = ⟨1,x,...,xn⟩ and E1 = ⟨1⟩,
given by
a0 +a1x+a2x2 +···+anxn  →a0 +a1 ·3+a2 ·32 +···+an ·3n (e) slide−1 : Pn → Pn with respect to B,B where B = ⟨1,x,...,xn⟩, given by
a0 +a1x+a2x2 +···+anxn  →a0 +a1 ·(x+1)+···+an ·(x+1)n
1.22 Represent the identity map on any nontrivial space with respect to B, B, where B is any basis.
1.23 Represent, with respect to the natural basis, the transpose transformation on the space M2×2 of 2×2 matrices.
1.24 Assume that B = ⟨β⃗ 1, β⃗ 2, β⃗ 3, β⃗ 4⟩ is a basis for a vector space. Represent with respect to B, B the transformation that is determined by each.
(a) β⃗1  → β⃗2, β⃗2  → β⃗3, β⃗3  → β⃗4, β⃗4  →⃗0 (b) β⃗1  → β⃗2, β⃗2  →⃗0, β⃗3  → β⃗4, β⃗4  →⃗0 (c) β⃗1  → β⃗2, β⃗2  → β⃗3, β⃗3  →⃗0, β⃗4  →⃗0
1.25 Example 1.9 shows how to represent the rotation transformation of the plane with respect to the standard basis. Express these other transformations also with respect to the standard basis.
(a) the dilation map ds, which multiplies all vectors by the same scalar s
(b) the reflection map fl, which reflects all all vectors across a line l through the
origin
  1.26 Consider a linear transformation of R2 determined by these two.
 1   2   1   −1  1  → 0 0  → 0
(a) Represent this transformation with respect to the standard bases. (b) Where does the transformation send this vector?
 0  5
(c) Represent this transformation with respect to these bases.  1   1   2   −1 
B=⟨−1,1⟩ D=⟨2,1⟩
(d) Using B from the prior item, represent the transformation with respect to B, B.
1.27 Suppose that h : V → W is one-to-one so that by Theorem 2.20, for any basis B = ⟨β⃗1,...,β⃗n⟩ ⊂ V the image h(B) = ⟨h(β⃗1),...,h(β⃗n)⟩ is a basis for W.
(a) Represent the map h with respect to B, h(B).
(b) For a member ⃗v of the domain, where the representation of ⃗v has components
c1, ..., cn, represent the image vector h(⃗v) with respect to the image basis h(B). 1.28 Give a formula for the product of a matrix and ⃗ei, the column vector that is
all zeroes except for a single one in the i-th position.
  1.29 For each vector space of functions of one real variable, represent the derivative
transformation with respect to B, B.
(a) { a cos x + b sin x | a, b ∈ R }, B = ⟨cos x, sin x⟩
 
214 Chapter Three. Maps Between Spaces (b){aex+be2x |a,b∈R},B=⟨ex,e2x⟩
(c){a+bx+cex+dxex |a,b,c,d∈R},B=⟨1,x,ex,xex⟩
1.30 Find the range of the linear transformation of R2 represented with respect to the standard bases by each matrix.
 1 0   0 0   a b  (a) 0 0 (b) 3 2 (c) amatrixoftheform 2a 2b
  1.31 Can one matrix represent two different linear maps? That is, can RepB,D(h) = RepBˆ,Dˆ (hˆ)?
1.32 Prove Theorem 1.4.
  1.33 Example 1.9 shows how to represent rotation of all vectors in the plane through an angle θ about the origin, with respect to the standard bases.
(a) Rotation of all vectors in three-space through an angle θ about the x-axis is a transformation of R3. Represent it with respect to the standard bases. Arrange the rotation so that to someone whose feet are at the origin and whose head is at (1, 0, 0), the movement appears clockwise.
(b) Repeat the prior item, only rotate about the y-axis instead. (Put the person’s head at ⃗e2.)
(c) Repeat, about the z-axis.
(d) Extend the prior item to R4. (Hint: we can restate ‘rotate about the z-axis’
as ‘rotate parallel to the xy-plane’.)
1.34 (Schur’s Triangularization Lemma)
(a) Let U be a subspace of V and fix bases BU ⊆ BV . What is the relationship
between the representation of a vector from U with respect to BU and the
representation of that vector (viewed as a member of V) with respect to BV? (b) What about maps?
(c) Fix a basis B = ⟨β⃗1,...,β⃗n⟩ for V and observe that the spans
[∅]={⃗0}⊂[{β⃗1}]⊂[{β⃗1,β⃗2}]⊂ ··· ⊂[B]=V
form a strictly increasing chain of subspaces. Show that for any linear map h:V→WthereisachainW0 ={⃗0}⊆W1 ⊆···⊆Wm =WofsubspacesofW such that
h([{β⃗1,...,β⃗i }]) ⊆ Wi
for each i.
(d) Conclude that for every linear map h : V → W there are bases B, D so the
matrix representing h with respect to B,D is upper-triangular (that is, each
entry hi,j with i > j is zero).
(e) Is an upper-triangular representation unique?
 
Section III. Computing Linear Maps 215 III.2 Any Matrix Represents a Linear Map
The prior subsection shows that the action of a linear map h is described by a matrix H, with respect to appropriate bases, in this way.
v1  h1,1v1 +···+h1,nvn 
.h.
⃗v =  .   −→ h(⃗v) =  .  (∗)
H
vn B hm,1v1 +···+hm,nvn D
Here we will show the converse, that each matrix represents a linear map. So we start with a matrix
 h1,1 h1,2 ... h1,n 
 h2,1 h2,2 ... h2,n  H= .   . 
hm,1 hm,2 . . . hm,n
and we will describe how it defines a map h. We require that the map be represented by the matrix so first note that in (∗) the dimension of the map’s domain is the number of columns n of the matrix and the dimension of the codomain is the number of rows m. Thus, for h’s domain fix an n-dimensional vector space V and for the codomain fix an m-dimensional space W. Also fix bases B = ⟨β⃗1,...,β⃗n⟩ and D = ⟨⃗δ1,...,⃗δm⟩ for those spaces.
Now let h: V → W be: where ⃗v in the domain has the representation v1
. RepB(⃗v) =  . 
vn B
then its image h(⃗v) is the member of the codomain with this representation.
 h1,1v1+···+h1,nvn  .
RepD(h(⃗v)) =  .  hm,1v1 +···+hm,nvn D
That is, to compute the action of h on any ⃗v ∈ V, first express ⃗v with respect to thebasis⃗v=v1β⃗1+···+vnβ⃗n andthenh(⃗v)=(h1,1v1+···+h1,nvn)·⃗δ1+ ···+(hm,1v1 +···+hm,nvn)·⃗δm.
Above we have made some choices; for instance V can be any n-dimensional space and B could be any basis for V, so H does not define a unique function. However, note once we have fixed V, B, W, and D then h is well-defined since ⃗v has a unique representation with respect to the basis B and the calculation of w⃗ from its representation is also uniquely determined.
216 Chapter Three. Maps Between Spaces 2.1 Example Consider this matrix.
1 2 H =  3 4 
56
It is 3×2 so any map that it defines must carry a dimension 2 domain to a dimension 3 codomain. We can choose the domain and codomain to be R2 and P2, with these bases.
 1    1  
B=⟨ 1 , −1 ⟩ D=⟨x2,x2 +x,x2 +x+1⟩
Then let h: R2 → P2 be the function defined by H. We will compute the image under h of this member of the domain.
 −3  ⃗v= 2
 The computation is straightforward.
RepD(h(⃗v)) = H · RepB(⃗v) = 3 4 −5/2 = −23/2
5 6 −35/2
From its representation, computation of w⃗ is routine (−11/2)(x2) − (23/2)(x2 +
x) − (35/2)(x2 + x + 1) = (−69/2)x2 − (58/2)x − (35/2).
Proof We must check that for any matrix H and any domain and codomain bases B, D, the defined map h is linear. If ⃗v, ⃗u ∈ V are such that
1 2    −11/2  −1/2  
 2.2 Theorem Any matrix represents a homomorphism between vector spaces of appropriate dimensions, with respect to any pair of bases.
v1 u1 . .
RepB(⃗v)= .  RepB(⃗u)= .  vn un
and c, d ∈ R then the calculation
h(c⃗v + d⃗u) =  h1,1(cv1 + du1) + · · · + h1,n(cvn + dun)  · ⃗δ1+ ···+ hm,1(cv1 +du1)+···+hm,n(cvn +dun) ·⃗δm
= c · h ( ⃗v ) + d · h ( ⃗u )
supplies that check. QED
Section III. Computing Linear Maps 217
 2.3 Example Even if the domain and codomain are the same, the map that the matrix represents depends on the bases that we choose. If
 1 0   1   0   0   1  H= 0 0 , B1=D1=⟨0 , 1⟩, and B2=D2=⟨1 , 0⟩,
then h1 : R2 → R2 represented by H with respect to B1, D1 maps  c    c    c    c  
1 = 1  → 1 = 1 c2c2 00
B1 D1
while h2 : R2 → R2 represented by H with respect to B2, D2 is this map.  c    c    c    0 
1 = 2  → 2 =
c2 c1 0 c2
B2 D2
These are different functions. The first is projection onto the x-axis while the
second is projection onto the y-axis.
This result means that when convenient we can work solely with matrices, just doing the computations without having to worry whether a matrix of interest represents a linear map on some pair of spaces.
When we are working with a matrix but we do not have particular spaces or bases in mind then we can take the domain and codomain to be Rn and Rm, with the standard bases. This is convenient because with the standard bases vector representation is transparent — the representation of ⃗v is ⃗v. (In this case the column space of the matrix equals the range of the map and consequently the column space of H is often denoted by R(H).)
Given a matrix, to come up with an associated map we can choose among many domain and codomain spaces, and many bases for those. So a matrix can represent many maps. We finish this section by illustrating how the matrix can give us information about the associated maps.
2.4 Theorem The rank of a matrix equals the rank of any map that it represents.
Proof Suppose that the matrix H is m×n. Fix domain and codomain spaces V and W of dimension n and m with bases B = ⟨β⃗1,...,β⃗n⟩ and D. Then H represents some linear map h between those spaces with respect to these bases whose range space
{h(⃗v)|⃗v∈V}={h(c1β⃗1 +···+cnβ⃗n)|c1,...,cn ∈R} ={c1h(β⃗1)+···+cnh(β⃗n)|c1,...,cn ∈R}
 
218 Chapter Three. Maps Between Spaces
 is the span [{h(β⃗ 1), . . . , h(β⃗ n)}]. The rank of the map h is the dimension of this range space.
The rank of the matrix is the dimension of its column space, the span of the set of its columns [ {RepD(h(β⃗ 1)), . . . , RepD(h(β⃗ n))} ].
To see that the two spans have the same dimension, recall from the proof of Lemma I.2.5 that if we fix a basis then representation with respect to that basis gives an isomorphism RepD : W → Rm. Under this isomorphism there is a linear relationship among members of the range space if and only if the same relationship holds in the column space, e.g, ⃗0 = c1 · h(β⃗ 1) + · · · + cn · h(β⃗ n) if and only if ⃗0 = c1 · RepD(h(β⃗ 1)) + · · · + cn · RepD(h(β⃗ n)). Hence, a subset of the range space is linearly independent if and only if the corresponding subset of the column space is linearly independent. Therefore the size of the largest linearly independent subset of the range space equals the size of the largest linearly independent subset of the column space, and so the two spaces have the same dimension. QED
That settles the apparent ambiguity in our use of the same word ‘rank’ to apply both to matrices and to maps.
2.5 Example Any map represented by
1 2 2 1 2 1 0 0 3 002
must have three-dimensional domain and a four-dimensional codomain. In addition, because the rank of this matrix is two (we can spot this by eye or get it with Gauss’s Method), any map represented by this matrix has a two-dimensional range space.
Proof For the onto half, the dimension of the range space of h is the rank of h, which equals the rank of H by the theorem. Since the dimension of the codomain of h equals the number of rows in H, if the rank of H equals the number of rows then the dimension of the range space equals the dimension of the codomain. But a subspace with the same dimension as its superspace must equal that superspace (because any basis for the range space is a linearly independent subset of the codomain whose size is equal to the dimension of the
 2.6 Corollary Let h be a linear map represented by a matrix H. Then h is onto if and only if the rank of H equals the number of its rows, and h is one-to-one if and only if the rank of H equals the number of its columns.
Section III. Computing Linear Maps 219
 codomain, and thus so this basis for the range space must also be a basis for the codomain).
For the other half, a linear map is one-to-one if and only if it is an isomorphism between its domain and its range, that is, if and only if its domain has the same dimension as its range. The number of columns in H is the dimension of h’s domain and by the theorem the rank of H equals the dimension of h’s range. QED
2.8 Remark Some authors use ‘nonsingular’ as a synonym for one-to-one while others use it the way that we have here. The difference is slight because any map is onto its range space, so a one-to-one map is an isomorphism with its range.
In the first chapter we defined a matrix to be nonsingular if it is square and is the matrix of coefficients of a linear system with a unique solution. The next result justifies our dual use of the term.
Proof Assume that the map h: V → W is nonsingular. Corollary 2.6 says that for any matrix H representing that map, because h is onto the number of rows of H equals the rank of H, and because h is one-to-one the number of columns of H is also equal to the rank of H. Hence H is square.
Next assume that H is square, n×n. The matrix H is nonsingular if and only if its row rank is n, which is true if and only if H’s rank is n by Theorem Two.III.3.11, which is true if and only if h’s rank is n by Theorem 2.4, which is true if and only if h is an isomorphism by Theorem I.2.3. (This last holds because the domain of h is n-dimensional as it is the number of columns in H.) QED
2.10 Example Any map from R2 to P1 represented with respect to any pair of
 2.7 Definition A linear map that is one-to-one and onto is nonsingular , otherwise it is singular. That is, a linear map is nonsingular if and only if it is an isomorphism.
 2.9 Lemma A nonsingular linear map is represented by a square matrix. A square matrix represents nonsingular maps if and only if it is a nonsingular matrix. Thus, a matrix represents isomorphisms if and only if it is square and nonsingular.
bases by
is nonsingular because this matrix has rank two.
 1 2  03
220 Chapter Three. Maps Between Spaces 2.11 Example Any map g: V → W represented by
 1 2  36
is singular because this matrix is singular.
We’ve now seen that the relationship between maps and matrices goes both ways: for a particular pair of bases, any linear map is represented by a matrix and any matrix describes a linear map. That is, by fixing spaces and bases we get a correspondence between maps and matrices. In the rest of this chapter we will explore this correspondence. For instance, we’ve defined for linear maps the operations of addition and scalar multiplication and we shall see what the corresponding matrix operations are. We shall also see the matrix operation that represent the map operation of composition. And, we shall see how to find the matrix that represents a map’s inverse.
Exercises
2.12 For each matrix, state the dimension of the domain and codomain of any map that the matrix represents.
  2 1   1 1 −3  (a) 3 4 (b) 2 5 0
1 3  0 0 0  (c)1 4 (d) 0 0 0
1 −1
(e)
 1 −1 4 5  0000
2.13 Consider a linear map f : V → W represented with respect to some bases B, D by the matrix. Decide if that map is nonsingular.
 2 1   1 1  3 0 0 2 0 −2 (a) 3 4 (b) −3 −3 (c)2 1 0 (d)1 1 0
4 4 4 4 1 −4
  2.14 Let h be the linear map defined by this matrix on the domain P1 and
codomain R2 with respect to the given bases.
 2 1   1   1 
H=42 B=⟨1+x,x⟩,D=⟨1,0⟩
What is the image under h of the vector ⃗v = 2x − 1?
  2.15 Decide if each vector lies in the range of the map from R3 to R2 represented
with respect to the standard bases by the matrix.
 1 1 3  1   2 0 3  1  (a) 0 1 4 , 3 (b) 4 0 6 , 1
  2.16 Consider this matrix, representing a transformation of R2 with respect to the bases.
1   1 1   0   1   1    1   2· −1 1 B=⟨ 1 , 0 ⟩ D=⟨ 1 , −1 ⟩
 (a) To what vector in the codomain is the first member of B mapped?
Section III. Computing Linear Maps 221
 (b) The second member?
(c) Where is a general vector from the domain (a vector with components x and
y) mapped? That is, what transformation of R2 is represented with respect to
B, D by this matrix?
2.17 Consider a homomorphism h: R2 → R2 represented with respect to the standard
bases E2,E2 by this matrix.
 1 3  24
Find the image under h of each vector.  2   0   −1 
311
2.18 What transformation of F = {acosθ+bsinθ|a,b∈R} is represented with
respect to B = ⟨cos θ − sin θ, sin θ⟩ and D = ⟨cos θ + sin θ, cos θ⟩ by this matrix?  0 0 
10
  2.19 Decide whether 1 + 2x is in the range of the map from R3 to P2 represented
with respect to E3 and ⟨1, 1 + x2, x⟩ by this matrix.
1 3 0 0 1 0 101
2.20 Find the map that this matrix represents with respect to B, B.   2 1   1   1 
−1 0 B=⟨ 0 , 1 ⟩
2.21 Example 2.11 gives a matrix that is singular and is therefore associated with maps that are singular. We cannot state the action of the associated map g on domain elements ⃗v ∈ V, because do not know the domain V or codomain W or the starting and ending bases B and D. But we can compute what happens to the representations RepB,D(⃗v).
(a) Find the set of column vectors representing the members of the null space of any map g represented by this matrix.
(b) Find the nullity of any such map g.
(c) Find the set of column vectors representing the members of the range space
of any map g represented by the matrix.
(d) Find the rank of any such map g.
(e) Check that rank plus nullity equals the dimension of the domain.
  2.22 Take each matrix to represent h: Rm → Rn with respect to the standard bases. For each (i) state m and n. Then set up an augmented matrix with the given matrix on the left and a vector representing a range space element on the right (e.g., if the codomain is R3 then in the right-hand column put the three entries a, b, and c). Perform Gauss-Jordan reduction. Use that to (ii) find R(h) and rank(h) (and state whether the underlying map is onto), and (iii) find N (h) and nullity(h) (and state whether the underlying map is one-to-one).
 2 1  −1 3
(a) (b) (c)
(a)
222
Chapter Three. Maps Between Spaces
 0 1 3 (b)2 3 4
−2 −1 2 1 1
(c) 2 1 31
2.23 Use the method from the prior exercise on this matrix.
1 0 −1 2 1 0 222
2.24 Verify that the map represented by this matrix is an isomorphism.
2 1 0 3 1 1 721
2.25 This is an alternative proof of Lemma 2.9. Given an n×n matrix H, fix a domain V and codomain W of appropriate dimension n, and bases B, D for those spaces, and consider the map h represented by the matrix.
(a) Show that h is onto if and only if there is at least one RepB(⃗v) associated by H with each RepD(w⃗ ).
(b) Show that h is one-to-one if and only if there is at most one RepB(⃗v) associated by H with each RepD(w⃗ ).
(c) Consider the linear system H·RepB(⃗v) = RepD(w⃗ ). Show that H is nonsingular if and only if there is exactly one solution RepB(⃗v) for each RepD(w⃗ ).
  2.26 Because the rank of a matrix equals the rank of any map it represents, if one matrix represents two different maps H = RepB,D(h) = RepBˆ,Dˆ (hˆ) (where h, hˆ : V → W) then the dimension of the range space of h equals the dimension of the range space of hˆ. Must these equal-dimensional range spaces actually be the same?
2.27 Let V be an n-dimensional space with bases B and D. Consider a map that sends, for ⃗v ∈ V, the column vector representing ⃗v with respect to B to the column vector representing ⃗v with respect to D. Show that map is a linear transformation of Rn.
2.28 Example 2.3 shows that changing the pair of bases can change the map that a matrix represents, even though the domain and codomain remain the same. Could the map ever not change? Is there a matrix H, vector spaces V and W, and associated pairs of bases B1,D1 and B2,D2 (with B1 ̸= B2 or D1 ̸= D2 or both) such that the map represented by H with respect to B1,D1 equals the map represented by H with respect to B2,D2?
  2.29 A square matrix is a diagonal matrix if it is all zeroes except possibly for the entries on its upper-left to lower-right diagonal — its 1, 1 entry, its 2, 2 entry, etc. Show that a linear map is an isomorphism if there are bases such that, with respect to those bases, the map is represented by a diagonal matrix with no zeroes on the diagonal.
Section III. Computing Linear Maps 223 2.30 Describe geometrically the action on R2 of the map represented with respect
to the standard bases E2,E2 by this matrix.  3 0 
 Do the same for these.
02
 1 0   0 1   1 3  001001
2.31 The fact that for any linear map the rank plus the nullity equals the dimension of the domain shows that a necessary condition for the existence of a homomorphism between two spaces, onto the second space, is that there be no gain in dimension. That is, where h: V → W is onto, the dimension of W must be less than or equal to the dimension of V.
(a) Show that this (strong) converse holds: no gain in dimension implies that there is a homomorphism and, further, any matrix with the correct size and correct rank represents such a map.
(b) Are there bases for R3 such that this matrix
1 0 0 H=2 0 0
010
represents a map from R3 to R3 whose range is the xy plane subspace of R3?
2.32 Let V be an n-dimensional space and suppose that ⃗x ∈ Rn. Fix a basis B for V and consider the map h⃗x : V → R given ⃗v  → ⃗x • RepB(⃗v) by the dot product.
(a) Show that this map is linear. (b)Showthatforanylinearmapg:V→Rthereisan⃗x∈Rn suchthatg=h⃗x. (c) In the prior item we fixed the basis and varied the ⃗x to get all possible linear
maps. Can we get all possible linear maps by fixing an ⃗x and varying the basis? 2.33 Let V, W, X be vector spaces with bases B, C, D.
(a) Suppose that h : V → W is represented with respect to B, C by the matrix H. Give the matrix representing the scalar multiple rh (where r ∈ R) with respect to B, C by expressing it in terms of H.
(b) Suppose that h, g : V → W are represented with respect to B, C by H and G. Give the matrix representing h + g with respect to B, C by expressing it in terms of H and G.
(c) Suppose that h: V → W is represented with respect to B,C by H and g: W → X is represented with respect to C, D by G. Give the matrix representing g ◦ h with respect to B, D by expressing it in terms of H and G.
224 Chapter Three. Maps Between Spaces IV Matrix Operations
The prior section shows how matrices represent linear maps. We now explore how this representation interacts with things that we already know. First we will see how the representation of a scalar product r · f of a linear map relates to the representation of f, and also how the representation of a sum f + g relates to the representations of the two summands. Later we will do the same comparison for the map operations of composition and inverse.
IV.1 Sums and Scalar Products
1.1 Example Let f: V → W be a linear function represented with respect to some
 bases by this matrix.
 1 0  RepB,D(f) = 1 1
Consider the map that is the scalar multiple 5f: V → W. We will relate the representation RepB,D(5f) with RepB,D(f).
Let f associate ⃗v  → w⃗ with these representations.
 v   RepB(⃗v) = 1
RepD(w⃗ ) =
 w   1
w2
v2
Where the codomain’s basis is D = ⟨⃗δ1,⃗δ2⟩, that representation gives that the output vector is w⃗ = w1⃗δ1 + w2⃗δ2.
The action of the map 5f is ⃗v  → 5w⃗ and 5w⃗ = 5·(w1⃗δ1 +w2⃗δ2) = (5w1)⃗δ1 + (5w2)⃗δ2. So 5f associates the input vector ⃗v with the output vector having this representation.
 5w   RepD(5w⃗ ) = 1 5w2
Changing from the map f to the map 5f has the effect on the representation of the output vector of multiplying each entry by 5.
Because of that, RepB,D(5f) is this matrix.  v    5v  
RepB,D(5f) · 1 = 1
v2 5v1+5v2
RepB,D(5f) =
 5 0  55
Therefore, going from the matrix representing f to the one representing 5f means multiplying all the matrix entries by 5.
Section IV. Matrix Operations 225
 1.2 Example We can do a similar exploration for the sum of two maps. Suppose that two linear maps with the same domain and codomain f, g : R2 → R2 are represented with respect to bases B and D by these matrices.
 1 3   −2 −1  RepB,D(f) = 2 0 RepB,D(g) = 2 4
Recallthedefinitionofsum: iffdoes⃗v →⃗uandgdoes⃗v →w⃗ thenf+gis the function whose action is ⃗v  → ⃗u + w⃗ . Let these be the representations of the input and output vectors.
 v    u    w   RepB(⃗v) = 1 RepD(⃗u) = 1 RepD(w⃗ ) = 1
v2 u2 w2
Where D = ⟨⃗δ1,⃗δ2⟩ we have ⃗u + w⃗ = (u1⃗δ1 + u2⃗δ2) + (w1⃗δ1 + w2⃗δ2) =
(u1 + w1)⃗δ1 + (u2 + w2)⃗δ2 and so this is the representation of the vector sum.
 u +w   RepD(⃗u + w⃗ ) = 1 1 u2 + w2
Thus, since these represent the actions of of the maps f and g on the input ⃗v  1 3  v   v +3v   −2 −1  v   −2v −v 
1=12 1=12 20 v2 2v1 2 4 v2 2v1+4v2
adding the entries represents the action of the map f + g.
 v   −v+2v  RepB,D(f+g)· 1 = 1 2 v2 4v1 + 4v2
Therefore, we compute the matrix representing the function sum by adding the entries of the matrices representing the functions.
 −1 2  RepB,D(f + g) = 4 4
These operations extend the first chapter’s operations of addition and scalar multiplication of vectors.
We need a result that proves these matrix operations do what the examples suggest that they do.
 1.3 Definition The scalar multiple of a matrix is the result of entry-by-entry scalar multiplication. The sum of two same-sized matrices is their entry-by-entry sum.
226 Chapter Three. Maps Between Spaces
  1.4 Theorem Let h, g : V → W be linear maps represented with respect to bases B,D by the matrices H and G and let r be a scalar. Then with respect to B, D the map r · h : V → W is represented by rH and the map h + g : V → W is represented by H + G.
Proof Generalize the examples. This is Exercise 10. QED
1.5 Remark These two operations on matrices are simple, but we did not define them in this way because they are simple. We defined them this way because they represent function addition and function scalar multiplication. That is, our program is to define matrix operations by referencing function operations. Simplicity is a bonus.
We will see this again in the next subsection, where we will define the operation of multiplying matrices. Since we’ve just defined matrix scalar multi- plication and matrix sum to be entry-by-entry operations, a naive thought is to define matrix multiplication to be the entry-by-entry product. In theory we could do whatever we please but we will instead be practical and combine the entries in the way that represents the function operation of composition.
A special case of scalar multiplication is multiplication by zero. For any map 0 · h is the zero homomorphism and for any matrix 0 · H is the matrix with all entries zero.
1.7 Example The zero map from any three-dimensional space to any two- dimensional space is represented by the 2×3 zero matrix
 0 0 0  Z=000
no matter what domain and codomain bases we use.
Exercises
 1.6 Definition A zero matrix has all entries 0. We write Zn×m or simply Z (another common notation is 0n×m or just 0).
  1.8 Perform the indicated operations, if defined, or state “not defined.”  5 −1 2   2 1 4 
(a) 6 1 1 + 3 0 5  2 −1 −1 
(b)6· 1 2 3  2 1   2 1 
(c) 0 3 + 0 3
 1 2   −1 4 
(d)4 3 −1 +5 −2 1
Section IV. Matrix Operations 227  2 1   1 1 4 
(e)3 3 0 +2 3 0 5
1.9 Give the matrix representing the zero map from R4 to R2, with respect to the
standard bases.
1.10 Prove Theorem 1.4.
(a) Prove that matrix addition represents addition of linear maps.
(b) Prove that matrix scalar multiplication represents scalar multiplication of
linear maps.
  1.11 Prove each, assuming that the operations are defined, where G, H, and J are matrices, where Z is the zero matrix, and where r and s are scalars.
(a) Matrix addition is commutative G + H = H + G.
(b) Matrix addition is associative G + (H + J) = (G + H) + J. (c) The zero matrix is an additive identity G + Z = G.
(d) 0·G=Z
(e) (r+s)G=rG+sG
(f) Matrices have an additive inverse G + (−1) · G = Z.
(g) r(G+H) = rG+rH
(h) (rs)G = r(sG)
1.12 Fix domain and codomain spaces. In general, one matrix can represent many different maps with respect to different bases. However, prove that a zero matrix represents only a zero map. Are there other such matrices?
  1.13 Let V and W be vector spaces of dimensions n and m. Show that the space L(V,W) of linear maps from V to W is isomorphic to Mm×n.
  1.14 Show that it follows from the prior question that for any six transformations t1,...,t6 : R2 → R2 there are scalars c1,...,c6 ∈ R such that not every ci equals 0 but c1 t1 + · · · + c6 t6 is the zero map. (Hint: the six is slightly misleading.)
1.15 The trace of a square matrix is the sum of the entries on the main diagonal (the 1, 1 entry plus the 2, 2 entry, etc.; we will see the significance of the trace in Chapter Five). Show that trace(H + G) = trace(H) + trace(G). Is there a similar result for scalar multiplication?
1.16 Recall that the transpose of a matrix M is another matrix, whose i, j entry is the j, i entry of M. Verify these identities.
(a) (G+H)T =GT +HT (b) (r·H)T =r·HT
  1.17 A square matrix is symmetric if each i, j entry equals the j, i entry, that is, if the matrix equals its transpose.
(a) Prove that for any square H, the matrix H + HT is symmetric. Does every symmetric matrix have this form?
(b) Prove that the set of n×n symmetric matrices is a subspace of Mn×n.
  1.18 (a) How does matrix rank interact with scalar multiplication — can a scalar product of a rank n matrix have rank less than n? Greater?
(b) How does matrix rank interact with matrix addition — can a sum of rank n matrices have rank less than n? Greater?
 
228 Chapter Three. Maps Between Spaces IV.2 Matrix Multiplication
After representing addition and scalar multiplication of linear maps in the prior subsection, the natural next operation to consider is function composition.
2.1 Lemma The composition of linear maps is linear.
Proof (Note: this argument has already appeared, as part of the proof of
Theorem I.2.2.) Let h: V → W and g: W → U be linear. The calculation g◦h c1 ·⃗v1 +c2 ·⃗v2 =g h(c1 ·⃗v1 +c2 ·⃗v2) =g c1 ·h(⃗v1)+c2 ·h(⃗v2) 
=c1 ·g h(⃗v1))+c2 ·g(h(⃗v2) =c1 ·(g◦h)(⃗v1)+c2 ·(g◦h)(⃗v2) shows that g ◦ h : V → U preserves linear combinations, and so is linear. QED
As we did with the operation of matrix addition and scalar multiplication, we will see how the representation of the composite relates to the representations of the compositors by first considering an example.
2.2 Example Let h:R4 →R2 and g:R2 →R3, fix bases B ⊂ R4, C ⊂ R2, D ⊂ R3, and let these be the representations.
  1 1
5 7 9 3 B,C 1 0 C,D
  
H=RepB,C(h)= 4 6 8 2 G=RepC,D(g)=0 1
To represent the composition g ◦ h : R4 → R3 we start with a ⃗v, represent h of ⃗v, and then represent g of that. The representation of h(⃗v) is the product of h’s matrix and ⃗v’s vector.
v1      
4 6 8 2 v2 4v1 +6v2 +8v3 +2v4   =
RepC(h(⃗v))=
The representation of g( h(⃗v) ) is the product of g’s matrix and h(⃗v)’s vector.
  
4v1 +6v2 +8v3 +2v4 5v1 +7v2 +9v3 +3v4
RepD(g(h(⃗v)))=0 1
5 7 9 3 B,Cv3 5v1+7v2+9v3+3v4 C v4 B
1 1  
1 0 C,D
1·(4v1 +6v2 +8v3 +2v4)+1·(5v1 +7v2 +9v3 +3v4)
=0·(4v1 +6v2 +8v3 +2v4)+1·(5v1 +7v2 +9v3 +3v4) 1·(4v1 +6v2 +8v3 +2v4)+0·(5v1 +7v2 +9v3 +3v4) D
C
Section IV. Matrix Operations 229 Distributing and regrouping on the v’s gives
(1·4+1·5)v +(1·6+1·7)v +(1·8+1·9)v +(1·2+1·3)v  1234
=(0·4+1·5)v1 +(0·6+1·7)v2 +(0·8+1·9)v3 +(0·2+1·3)v4 (1·4+0·5)v1 +(1·6+0·7)v2 +(1·8+0·9)v3 +(1·2+0·3)v4 D
which is this matrix-vector product.
 v1    v2
1·4+1·5 1·6+1·7 1·8+1·9 1·2+1·3 =0·4+1·5 0·6+1·7 0·8+1·9 0·2+1·3 v3
1·4+0·5 1·6+0·7 1·8+0·9 1·2+0·3 B,D v4 D
The matrix representing g ◦ h has the rows of G combined with the columns of
H.
 2.3 Definition The matrix-multiplicative product of the m×r matrix G and the r×n matrix H is the m×n matrix P, where
pi,j = gi,1h1,j + gi,2h2,j + · · · + gi,rhr,j
so that the i, j-th entry of the product is the dot product of the i-th row of the
first matrix with the j-th column of the second.
. h1,j.
..
 ··· h2,j ···  
GH = gi,1 gi,2 ··· gi,r .  = ··· pi,j ··· . ..
. hr,j .
2.4 Example
2 0    2·1+0·5 2·3+0·7 2 6 13 
4 6 5 7 =4·1+6·5 4·3+6·7=34 54 8 2 8·1+2·5 8·3+2·7 18 38
2.5 Example Some products are not defined, such as the product of a 2×3 matrix with a 2×2, because the number of columns in the first matrix must equal the number of rows in the second. But the product of two n×n matrices is always defined. Here are two 2×2’s.
 1 2  −1 0   1·(−1)+2·2 1·0+2·(−2)   3 −4  3 4 2 −2 = 3·(−1)+4·2 3·0+4·(−2) = 5 −8
230 Chapter Three. Maps Between Spaces 2.6 Example The matrices from Example 2.2 combine in this way.
 1 1     4682
0 1 5 7 9 3 10
1·4+1·5 1·6+1·7 1·8+1·9 1·2+1·3 =0·4+1·5 0·6+1·7 0·8+1·9 0·2+1·3
1·4+0·5 1·6+0·7 1·8+0·9 1·2+0·3
9 13 17 5 =5 7 9 3
4682
Proof This argument generalizes Example 2.2. Let h: V → W and g: W → X be represented by H and G with respect to bases B ⊂ V, C ⊂ W, and D ⊂ X, of sizes n, r, and m. For any ⃗v ∈ V the k-th component of RepC( h(⃗v) ) is
hk,1v1 +···+hk,nvn and so the i-th component of RepD( g ◦ h (⃗v) ) is this.
gi,1 ·(h1,1v1 +···+h1,nvn)+gi,2 ·(h2,1v1 +···+h2,nvn)
+···+gi,r ·(hr,1v1 +···+hr,nvn)
Distribute and regroup on the v’s.
= (gi,1h1,1 + gi,2h2,1 + · · · + gi,rhr,1) · v1
+···+(gi,1h1,n +gi,2h2,n +···+gi,rhr,n)·vn Finish by recognizing that the coefficient of each vj
gi,1h1,j + gi,2h2,j + · · · + gi,rhr,j
matches the definition of the i, j entry of the product GH. QED
This arrow diagram pictures the relationship between maps and matrices (‘wrt’ abbreviates ‘with respect to’).
 2.7 Theorem A composition of linear maps is represented by the matrix product of the representatives.
Wwrt C
g
  Vwrt B
g◦h GH
Xwrt D
h
HG
 
Section IV. Matrix Operations 231
 Above the arrows, the maps show that the two ways of going from V to X, straight over via the composition or else in two steps by way of W, have the
same effect
g◦h h g
⃗v  −→ g(h(⃗v)) ⃗v  −→ h(⃗v)  −→ g(h(⃗v))
(this is just the definition of composition). Below the arrows, the matrices indicate that multiplying GH into the column vector RepB(⃗v) has the same effect as multiplying the column vector first by H and then multiplying the result by G.
RepB,D(g ◦ h) = GH RepC,D(g) RepB,C(h) = GH
As mentioned in Example 2.5, because the number of columns on the left does not equal the number of rows on the right, the product as here of a 2×3 matrix with a 2×2 matrix is not defined.
 −1 2 0  0 0  0 10 1.1 0 2
The definition requires that the sizes match because we want that the underlying function composition is possible.
yield the m×n result GF. Briefly: m×r times r×n equals m×n.
2.8 Remark The order of the dimensions can be confusing. In ‘m×r times r× n equals m×n’ the number written first is m. But m appears last in the map dimension description line (∗) above, and the other dimensions also appear in reverse. The explanation is that while h is done first, followed by g, we write the composition as g ◦ h, with g on the left (arising from the notation g(h(⃗v))). That carries over to matrices, so that g ◦ h is represented by GH.
We can get insight into matrix-matrix product operation by studying how the entries combine. For instance, an alternative way to understand why we require above that the sizes match is that the row of the left-hand matrix must have the same number of entries as the column of the right-hand matrix, or else some entry will be left without a matching entry from the other matrix.
Another aspect of the combinatorics of matrix multiplication, in the sum defining the i, j entry, is brought out here by the boxing the equal subscripts.
pi,j =gi, h ,j +gi, h ,j +···+gi, h ,j
The highlighted subscripts on the g’s are column indices while those on the h’s are for rows. That is, the summation takes place over the columns of G but
hg
dimension n space −→ dimension r space −→ dimension m space (∗) Thus, matrix product combines the m×r matrix G with the r×n matrix F to
r
r
      1
1
2
2
232 Chapter Three. Maps Between Spaces over the rows of H — the definition treats left differently than right. So we may
reasonably suspect that GH can be unequal to HG. 2.9 Example Matrix multiplication is not commutative.
 1 2  5 6   19 22   5 6  1 2   23 34  3 4 7 8 = 43 50 7 8 3 4 = 31 46
 2.10 Example Commutativity can fail more dramatically:  56  120   23340 
while
isn’t even defined.
7 8 3 4 0 = 31 46 0  1 2 0  5 6 
34078
2.11 Remark The fact that matrix multiplication is not commutative can seem odd at first, perhaps because most mathematical operations in prior courses are commutative. But matrix multiplication represents function composition and function composition is not commutative: if f(x) = 2x and g(x) = x + 1 then g ◦ f(x) = 2x + 1 while f ◦ g(x) = 2(x + 1) = 2x + 2.
Except for the lack of commutativity, matrix multiplication is algebraically well-behaved. The next result gives some nice properties and more are in Exercise 25 and Exercise 26.
Proof Associativity holds because matrix multiplication represents function composition, which is associative: the maps (f ◦ g) ◦ h and f ◦ (g ◦ h) are equal as both send ⃗v to f(g(h(⃗v))).
Distributivity is similar. For instance, the first one goes f ◦ (g + h) (⃗v) = f  (g + h)(⃗v)   = f  g(⃗v) + h(⃗v)   = f(g(⃗v)) + f(h(⃗v)) = f ◦ g(⃗v) + f ◦ h(⃗v) (the third equality uses the linearity of f). Right-distributivity goes the same way. QED
2.13 Remark We could instead prove that result by slogging through indices. For
 2.12 Theorem If F, G, and H are matrices, and the matrix products are defined, then the product is associative (FG)H = F(GH) and distributes over matrix addition F(G + H) = FG + FH and (G + H)F = GF + HF.
Section IV. Matrix Operations 233 example, for associativity the i, j entry of (FG)H is
(fi,1g1,1 + fi,2g2,1 + · · · + fi,rgr,1)h1,j
+ (fi,1g1,2 + fi,2g2,2 + · · · + fi,rgr,2)h2,j
.
+ (fi,1g1,s + fi,2g2,s + · · · + fi,rgr,s)hs,j where F, G, and H are m×r, r×s, and s×n matrices. Distribute
fi,1g1,1h1,j + fi,2g2,1h1,j + · · · + fi,rgr,1h1,j
+ fi,1g1,2h2,j + fi,2g2,2h2,j + · · · + fi,rgr,2h2,j
.
+ fi,1g1,shs,j + fi,2g2,shs,j + · · · + fi,rgr,shs,j and regroup around the f’s
fi,1(g1,1h1,j + g1,2h2,j + · · · + g1,shs,j)
+ fi,2(g2,1h1,j + g2,2h2,j + · · · + g2,shs,j)
.
+ fi,r(gr,1h1,j + gr,2h2,j + · · · + gr,shs,j)
to get the i, j entry of F(GH).
Contrast the two proofs. The index-heavy argument is hard to understand in
that while the calculations are easy to check, the arithmetic seems unconnected to any idea. The argument in the proof is shorter and also says why this property “really” holds. This illustrates the comments made at the start of the chapter on vector spaces — at least sometimes an argument from higher-level constructs is clearer.
We have now seen how to represent the composition of linear maps. The next subsection will continue to explore this operation.
Exercises
  2.14 Compute, or state “not defined”.
 3 1  0 5   1 1 −1 2 −1 −1
(a) −4 2 0 0.5 (b) 4 0 3 3 1 1 311
 2 −7 1 0 5  5 2  −1 2  (c) 7 4 −1 1 1 (d) 3 1 3 −5
384
 
234 Chapter Three. Maps Between Spaces
  2.15 Where
 1 −1   5 2   −2 3  A= 2 0 B= 4 4 C= −4 1
compute or state “not defined”.
(a) AB (b) (AB)C (c) BC (d) A(BC) 2.16 Which products are defined?
(a) 3×2 times 2×3 (b) 2×3 times 3×2 (c) 2×2 times 3×3
(d) 3×3 times 2×2
  2.17 Give the size of the product or state “not defined”.
(a) a 2×3 matrix times a 3×1 matrix (b) a 1×12 matrix times a 12×1 matrix (c) a 2×3 matrix times a 2×1 matrix (d) a 2×2 matrix times a 2×2 matrix
  2.18 Find the system of equations resulting from starting with
h1,1x1 + h1,2x2 + h1,3x3 = d1
h2,1x1 + h2,2x2 + h2,3x3 = d2 and making this change of variable (i.e., substitution).
x1 = g1,1y1 + g1,2y2 x2 = g2,1y1 + g2,2y2 x3 = g3,1y1 + g3,2y2
  2.19 Consider the two linear functions h: R3 → P2 and g: P2 → M2×2 given as here.
 a
b →(a+b)x2 +(2a+2b)x+c c
Use these bases for the spaces.
1 0 0 B = ⟨1 , 1 , 0⟩
 p p−2q  px2 +qx+r → q 0
C = ⟨1 + x, 1 − x, x2 ⟩  1 0  0 2  0 0  0 0 
111 D=⟨00,00,30,04⟩
(a) Give the formula for the composition map g ◦ h : R3 → M2×2 derived directly from the above definition.
(b) Represent h and g with respect to the appropriate bases.
(c) Represent the map g ◦ h computed in the first part with respect to the
appropriate bases.
(d) Check that the product of the two matrices from the second part is the matrix
from the third part.
2.20 As Definition 2.3 points out, the matrix product operation generalizes the dot
product. Is the dot product of a 1×n row vector and a n×1 column vector the
same as their matrix-multiplicative product?
  2.21 Represent the derivative map on Pn with respect to B, B where B is the natural
basis ⟨1,x,...,xn⟩. Show that the product of this matrix with itself is defined; what map does it represent?
Section IV. Matrix Operations 235
 2.22 [Cleary] Match each type of matrix with all these descriptions that could fit: (i) can be multiplied by its transpose to make a 1×1 matrix, (ii) can represent a linear map from R3 to R2 that is not onto, (iii) can represent an isomorphism from R3 to P2.
(a) a 2×3 matrix whose rank is 1
(b) a 3×3 matrix that is nonsingular (c) a 2×2 matrix that is singular
(d) an n×1 column vector
2.23 Show that composition of linear transformations on R1 is commutative. Is this true for any one-dimensional space?
2.24 Why is matrix multiplication not defined as entry-wise multiplication? That would be easier, and commutative too.
2.25 (a) Prove that HpHq = Hp+q and (Hp)q = Hpq for positive integers p, q. (b) Prove that (rH)p = rp · Hp for any positive integer p and scalar r ∈ R.
  2.26 (a) How does matrix multiplication interact with scalar multiplication: is r(GH) = (rG)H? Is G(rH) = r(GH)?
(b) How does matrix multiplication interact with linear combinations: is F(rG + sH)=r(FG)+s(FH)? Is(rF+sG)H=rFH+sGH?
2.27 We can ask how the matrix product operation interacts with the transpose operation.
(a) Show that (GH)T = HTGT.
(b) A square matrix is symmetric if each i, j entry equals the j, i entry, that is, if
the matrix equals its own transpose. Show that the matrices HHT and HTH are
symmetric.
  2.28 Rotation of vectors in R3 about an axis is a linear map. Show that linear maps
do not commute by showing geometrically that rotations do not commute.
2.29 In the proof of Theorem 2.12 we used some maps. What are the domains and
codomains?
2.30 How does matrix rank interact with matrix multiplication?
(a) Can the product of rank n matrices have rank less than n? Greater?
(b) Show that the rank of the product of two matrices is less than or equal to the
minimum of the rank of each factor.
2.31 Is ‘commutes with’ an equivalence relation among n×n matrices?
2.32 (We will use this exercise in the Matrix Inverses exercises.) Here is another
property of matrix multiplication that might be puzzling at first sight.
(a) Prove that the composition of the projections πx, πy : R3 → R3 onto the x and
y axes is the zero map despite that neither one is itself the zero map.
(b) Prove that the composition of the derivatives d2/dx2, d3/dx3 : P4 → P4 is the
zero map despite that neither is the zero map.
(c) Give a matrix equation representing the first fact. (d) Give a matrix equation representing the second.
When two things multiply to give zero despite that neither is zero we say that each
is a zero divisor.
2.33 Show that, for square matrices, (S + T)(S − T) need not equal S2 − T2.
236 Chapter Three. Maps Between Spaces
   2.34 Represent the identity transformation id : V → V with respect to B, B for any basis B. This is the identity matrix I. Show that this matrix plays the role in matrix multiplication that the number 1 plays in real number multiplication: HI = IH = H (for all matrices H for which the product is defined).
2.35 In real number algebra, quadratic equations have at most two solutions. That is not so with matrix algebra. Show that the 2×2 matrix equation T2 = I has more than two solutions, where I is the identity matrix (this matrix has ones in its 1, 1 and 2, 2 entries and zeroes elsewhere; see Exercise 34).
2.36 (a) Prove that for any 2×2 matrix T there are scalars c0, . . . , c4 that are not all 0 such that the combination c4T4 + c3T3 + c2T2 + c1T + c0I is the zero matrix (where I is the 2×2 identity matrix, with 1’s in its 1, 1 and 2, 2 entries and zeroes elsewhere; see Exercise 34).
(b) Let p(x) be a polynomial p(x) = cnxn +···+c1x+c0. If T is a square matrix we define p(T) to be the matrix cnTn + ··· + c1T + c0I (where I is the appropriately-sized identity matrix). Prove that for any square matrix there is a polynomial such that p(T) is the zero matrix.
(c) The minimal polynomial m(x) of a square matrix is the polynomial of least degree, and with leading coefficient 1, such that m(T) is the zero matrix. Find the minimal polynomial of this matrix.
 √3/2 −1/2  √
1/2 3/2
(This is the representation with respect to E2,E2, the standard basis, of a rotation
through π/6 radians counterclockwise.)
2.37 The infinite-dimensional space P of all finite-degree polynomials gives a memo- rable example of the non-commutativity of linear maps. Let d/dx: P → P be the usual derivative and let s: P → P be the shift map.
ns 2 n+1 a0 +a1x+···+anx  −→ 0+a0x+a1x +···+anx
Show that the two maps don’t commute d/dx ◦ s ̸= s ◦ d/dx; in fact, not only is (d/dx ◦ s) − (s ◦ d/dx) not the zero map, it is the identity map.
2.38 Recall the notation for the sum of the sequence of numbers a1, a2, . . . , an.
 n
ai = a1 + a2 + · · · + an
i=1
In this notation, the i, j entry of the product of G and H is this.
 r k=1
Using this notation,
(a) reprove that matrix multiplication is associative; (b) reprove Theorem 2.7.
  pi,j =
gi,khk,j
Section IV. Matrix Operations 237 IV.3 Mechanics of Matrix Multiplication
We can consider matrix multiplication as a mechanical process, putting aside for the moment any implications about the underlying maps.
The striking thing about this operation is the way that rows and columns combine. The i, j entry of the matrix product is the dot product of row i of the left matrix with column j of the right one. For instance, here a second row and a third column combine to make a 2, 3 entry.
 1 1    9 13 17 5 462
 8 9
  57 3=57 3 10 4682
01
We can view this as the left matrix acting by multiplying its rows into the columns of the right matrix. Or, it is the right matrix using its columns to act on the rows of the left matrix. Below, we will examine actions from the left and from the right for some simple matrices.
Simplest is the zero matrix.
3.1 Example Multiplying by a zero matrix from the left or from the right results in a zero matrix.
 0 0  1 3 2   0 0 0   2 3  0 0   0 0  0 0 −1 1 −1 = 0 0 0 1 4 0 0 = 0 0
The next easiest matrices are the ones with a single nonzero entry.
3.3 Example This is the 1,2 unit matrix with three rows and two columns, multiplying from the left.
0 1    7 8 56
0 0 7 8 =0 0 00 00
Acting from the left, an i, j unit matrix copies row j of the multiplicand into row i of the result. From the right an i, j unit matrix picks out column i of the multiplicand and copies it into column j of the result.
12301 01 4 5 6 0 0 = 0 4 789 00 07
9
 3.2 Definition A matrix with all 0’s except for a 1 in the i, j entry is an i, j unit matrix (or matrix unit).
238 Chapter Three. Maps Between Spaces 3.4 Example Rescaling unit matrices simply rescales the result. This is the action
from the left of the matrix that is twice the one in the prior example.
0 2    14 16 56 0 0 7 8 =0 0
00 00
Next in complication are matrices with two nonzero entries.
3.5 Example There are two cases. If a left-multiplier has entries in different rows then their actions don’t interact.
1 0 01 2 3 1 0 0 0 0 01 2 3  0 0 2   4 5 6  = (  0 0 0  +  0 0 2  )  4 5 6  000 789 000 000 789
1 2 3 0 0 0
= 0 0 0 + 14 16 18
000 000
123
= 14 16 18
000
But if the left-multiplier’s nonzero entries are in the same row then that row of the result is a combination.
1 0 21 2 3 1 0 0 0 0 21 2 3  0 0 0   4 5 6  = (  0 0 0  +  0 0 0  )  4 5 6  000 789 000 000 789
1 2 3 14 16 18
=  0 0 0  +  0 0 0 
000 000
15 18 21
=  0 0 0 
000
Right-multiplication acts in the same way, but with columns.
3.6 Example Consider the columns of the product of two 2×2 matrices.
 g g   h h    g h +g h g h +g h   1,1 1,2 1,1 1,2 = 1,1 1,1 1,2 2,1 1,1 1,2 1,2 2,2
g2,1 g2,2 h2,1 h2,2 g2,1h1,1 + g2,2h2,1 g2,1h1,2 + g2,2h2,2 Each column is the result of multiplying G by the corresponding column of H.
  h    g h +g h   G 1,1 = 1,1 1,1 1,2 2,1
h2,1 g2,1h1,1 + g2,2h2,1
 h    g h +g h   G 1,2 = 1,1 1,2 1,2 2,2
h2,2 g2,1h1,2 + g2,2h2,2
Section IV. Matrix Operations 239
  3.7 Lemma In a product of two matrices G and H, the columns of GH are formed by taking G times the columns of H
. .  . .  G·⃗h1 ··· ⃗hn=G·⃗h1 ··· G·⃗hn
. .  . .  ....
and the rows of GH are formed by taking the rows of G times H ··· ⃗g1 ··· ··· ⃗g1 ·H ···
..  .  · H =  .
 
···⃗gr ··· ···⃗gr·H··· (ignoring the extra parentheses).
Proof We will check that in a product of 2×2 matrices, the rows of the product equal the product of the rows of G with the entire matrix H.
 g 1,1
g2,1
g   h 1,2 1,1
h    (g 1,2 = 1,1
g )H  1,2
g2,2 h2,1 h2,2 (g2,1
 (g1,1h1,1 + g1,2h2,1
We leave the more general check as an exercise.
g2,2 )H
= (g2,1h1,1 + g2,2h2,1
g1,1h1,2 + g1,2h2,2)  g2,1h1,2 + g2,2h2,2)
QED
An application of those observations is that there is a matrix that just copies out the rows and columns.
 3.8 Definition The main diagonal (or principle diagonal or diagonal) of a square matrix goes from the upper left to the lower right.
 3.9 Definition An identity matrix is square and every entry is 0 except for 1’s
in the main diagonal.
1 0 ... 0 0 1 ... 0
In×n =  .   . 
00...1
3.10 Example Here is the 2×2 identity matrix leaving its multiplicand unchanged
240 Chapter Three. Maps Between Spaces when it acts from the right.
 1 −2 0 −2 1 0 0 −2 1 −1 0 1 = 1 −1
1 −2
43 43
  
3.11 Example Here the 3×3 identity leaves its multiplicand unchanged both from
the left
1002 36 2 36  0 1 0   1 3 8  =  1 3 8  001−710 −710
and from the right.
2 36100 2 36  1 3 8   0 1 0  =  1 3 8  −710001 −710
In short, an identity matrix is the identity element of the set of n×n matrices with respect to the operation of matrix multiplication.
We can generalize the identity matrix by relaxing the ones to arbitrary reals. The resulting matrix rescales whole rows or columns.
 3.12 Definition A diagonal matrix is square and has 0’s off the main diagonal.
a1,1 0 ... 0 0 a2,2 ... 0
 .   . 
0 0 ... an,n
3.13 Example From the left, the action of multiplication by a diagonal matrix is to rescales the rows.
 2 0  2 1 4 −1   4 2 8 −2  0 −1 −1 3 4 4 = 1 −3 −4 −4
From the right such a matrix rescales the columns.
   300    1 2 1  0 2 0  = 3 4 − 2
2 2 2 0 0 −2 6 4 −4
Section IV. Matrix Operations 241 We can also generalize identity matrices by putting a single one in each row
and column in ways other than putting them down the diagonal.
3.15 Example From the left these matrices permute rows.
001123 789  1 0 0   4 5 6  =  1 2 3  010 789 456
From the right they permute columns.
123001 231  4 5 6   1 0 0  =  5 6 4  789 010 897
We finish this subsection by applying these observations to get matrices that perform Gauss’s Method and Gauss-Jordan reduction. We have already seen how to produce a matrix that rescales rows, and a row swapper.
3.16 Example Multiplying by this matrix rescales the second row by three.
1 0 00 2 1 1 0 2 1 1 0 3 0 0 1/3 1 −1 = 0 1 3 −3 0011020 1020
3.17 Example This multiplication swaps the first and third rows.
001021 1 102 0  0 1 0   0 1 3 − 3  =  0 1 3 − 3  1001020 0211
To see how to perform a row combination, we observe something about those two examples. The matrix that rescales the second row by a factor of three arises in this way from the identity.
  3.14 Definition A permutation matrix is square and is all 0’s except for a single 1 in each row and column.
1 0 0 1 0 0  3ρ2 
0 1 0 −→ 0 3 0 001 001
Similarly, the matrix that swaps first and third rows arises in this way.
1 0 0 0 0 1  ρ1↔ρ3 
0 1 0 −→ 0 1 0 001 100
242 Chapter Three. Maps Between Spaces 3.18 Example The 3×3 matrix that arises as
1 0 0 1 0 0  −2ρ2+ρ3  0 1 0 −→ 0 1 0
0 0 1 0 −2 1
will, when it acts from the left, perform the combination operation −2ρ2 + ρ3.
 1 0 01 0  0 1 0   0 1
0 −2 1 0 2
2 0 1 0 2 0 3 − 3  =  0 1 3 − 3 
1 1
0 0 −5 7
 3.19 Definition The elementary reduction matrices (or just elementary matri- ces) result from applying a single Gaussian operation to an identity matrix.
kρi
(1) I −→ Mi(k)fork̸=0
ρi ↔ρj
(2)I −→ Pi,jfori̸=j
kρi +ρj
(3) I −→ Ci,j(k) for i ̸= j
 3.20 Lemma Matrix multiplication can do Gaussian reduction. kρi
(1) IfH −→ GthenMi(k)H=G. ρi ↔ρj
(2) IfH −→ GthenPi,jH=G.
kρi +ρj
(3) If H −→ G then Ci,j(k)H = G.
Proof Clear.
3.21 Example This is the first system, from the first chapter, on which we
performed Gauss’s Method.
3x3 = 9 x1 +5x2 −2x3 =2 (1/3)x1 + 2x2 = 3
We can reduce it with matrix multiplication. Swap the first and third rows,
0 0 10 0 3   9 1/3 2 0   3  0 1 0   1 5 − 2   2  =  1 5 − 2   2  1 0 0 1/3 2 0 3 0 0 3 9
QED
  
Section IV. Matrix Operations 243 triple the first row,
3001/32 0 3 16 0 9  0 1 0   1 5 − 2   2  =  1 5 − 2   2  001 0039 0039
and then add −1 times the first row to the second.
1 0 01 6 0 9 1 6 0 9  − 1 1 0   1 5 − 2   2  =  0 − 1 − 2   − 7  00100390039
Now back substitution will give the solution.
3.22 Example Gauss-Jordan reduction works the same way. For the matrix ending the prior example, first turn the leading entries to ones,
1 0 01 6 0 9 1 6 0 9  0 − 1 0   0 − 1 − 2   − 7  =  0 1 2   7  0 0 1/3 0 0 3 9 0 0 1 3
then clear the third column, and then the second column.
1−6010 01609 1003  0 1 0   0 1 − 2   0 1 2   7  =  0 1 0   1  00100100130013
Until now we have taken the point of view that our primary objects of study are vector spaces and the maps between them, and we seemed to have adopted matrices only for computational convenience. This subsection show that this isn’t the entire story.
Understanding matrix operations by understanding the mechanics of how the entries combine is also useful. In the rest of this book we shall continue to focus on maps as the primary objects but we will be pragmatic—if the matrix point of view gives some clearer idea then we will go with it.
Exercises
  3.24 Predict the result of each product with a permutation matrix and then check by multiplying it out.
 0 1  1 2   1 2  0 1  1 0 01 2 3 (a) 1 0 3 4 (b) 3 4 1 0 (c)0 0 14 5 6
010789
  3.25 Predict the result of each multiplication by an elementary reduction matrix, and then check by multiplying it out.
                  3.23Corollary ForanymatrixHthereareelementaryreductionmatricesR1,..., Rr such that Rr ·Rr−1 ···R1 ·H is in reduced echelon form.
244
Chapter Three. Maps Between Spaces
  3 0  1 2  0 1 3 4
3401 3410
(a)
(d) (e)
 1 2  1 −1 
(b)
 1 0  1 2  0 2 3 4
 1 2  0 1 
(c)
 1 0  1 2  −2 1 3 4
3.26 Predict the result of each multiplication by a diagonal matrix, and then check by multiplying it out.
 3 0  1 2   4 0  1 2  (a) (b)
0134 0234
3.27 Produce each.
(a) a 3×3 matrix that, acting from the left, swaps rows one and two
(b) a 2×2 matrix that, acting from the right, swaps column one and two
  3.28 Show how to use matrix multiplication to bring this matrix to echelon form.
1 2 1 0 2 3 1−1 7 11 4 −3
3.29 Find the product of this matrix with its transpose.  cos θ − sin θ 
sin θ cos θ
3.30 The need to take linear combinations of rows and columns in tables of numbers arises often in practice. For instance, this is a map of part of Vermont and New York.
  Swanton
 In part because of Lake Champlain, there are no roads directly connect- ing some pairs of towns. For in- stance, there is no way to go from Winooski to Grand Isle without go- ing through Colchester. (To sim- plify the graph many other roads and towns have been omitted. From top to bottom of this map is about forty miles.)
Grand Isle
    Colchester
Winooski
Burlington
    (a) The adjacency matrix of a map is the square matrix whose i, j entry is the number of roads from city i to city j (all (i,i) entries are 0). Produce the adjacency matrix of this map, with the cities in alphabetical order.
(b) A matrix is symmetric if it equals its transpose. Show that an adjacency matrix is symmetric. (These are all two-way streets. Vermont doesn’t have many one-way streets.)
(c) What is the significance of the square of the incidence matrix? The cube?
Section IV. Matrix Operations 245
   3.31 This table gives the number of hours of each type done by each worker, and the associated pay rates. Use matrices to compute the wages due.
regular overtime
wage
regular $25.00 overtime $45.00
    Alan
Betty   35 6
40 12
  Catherine
40 18
 Donald
Remark. This illustrates that in practice we often want to compute linear combi- nations of rows and columns in a context where we really aren’t interested in any associated linear maps.
3.32 Express this nonsingular matrix as a product of elementary reduction matrices.
3.33 Express
1 2 0 T=2 −1 0
312  1 0 
28 0
−3 3
as the product of two elementary reduction matrices.
  3.34 Prove that the diagonal matrices form a subspace of Mn×n. What is its dimension?
3.35 Does the identity matrix represent the identity map if the bases are unequal? 3.36 Show that every multiple of the identity commutes with every square matrix.
Are there other matrices that commute with all square matrices? 3.37 Prove or disprove: nonsingular matrices commute.
  3.38 Show that the product of a permutation matrix and its transpose is an identity matrix.
3.39 Show that if the first and second rows of G are equal then so are the first and second rows of GH. Generalize.
3.40 Describe the product of two diagonal matrices.
  3.41 Show that if G has a row of zeros then GH (if defined) has a row of zeros. Does
that work for columns?
3.42 Show that the set of unit matrices forms a basis for Mn×m. 3.43 Find the formula for the n-th power of this matrix.
 1 1  10
  3.44 The trace of a square matrix is the sum of the entries on its diagonal (its significance appears in Chapter Five). Show that Tr(GH) = Tr(HG).
3.45 A square matrix is upper triangular if its only nonzero entries lie above, or on, the diagonal. Show that the product of two upper triangular matrices is upper triangular. Does this hold for lower triangular also?
3.46 A square matrix is a Markov matrix if each entry is between zero and one and the sum along each row is one. Prove that a product of Markov matrices is Markov.
246 Chapter Three. Maps Between Spaces 3.47 Give an example of two matrices of the same rank and size with squares of
differing rank.
3.48 On a computer multiplications have traditionally been more costly than ad- ditions, so people have tried to in reduce the number of multiplications used to compute a matrix product.
(a) How many real number multiplications do we need in the formula we gave for the product of a m×r matrix and a r×n matrix?
(b) Matrix multiplication is associative, so all associations yield the same result. The cost in number of multiplications, however, varies. Find the association requiring the fewest real number multiplications to compute the matrix product of a 5×10 matrix, a 10×20 matrix, a 20×5 matrix, and a 5×1 matrix.
(c) (Very hard.) Find a way to multiply two 2×2 matrices using only seven multiplications instead of the eight suggested by the naive approach.
? 3.49 [Putnam, 1990, A-5] If A and B are square matrices of the same size such that ABAB = 0, does it follow that BABA = 0?
3.50 [Am. Math. Mon., Dec. 1966] Demonstrate these four assertions to get an al- ternate proof that column rank equals row rank.
(a) ⃗y · ⃗y = 0 iff ⃗y = ⃗0.
(b) A⃗x = ⃗0 iff ATA⃗x = ⃗0.
(c) dim(R(A)) = dim(R(ATA)).
(d) col rank(A) = col rank(AT) = row rank(A).
3.51 [Ackerson] Prove (where A is an n×n matrix and so defines a transformation of any n-dimensional space V with respect to B,B where B is a basis) that dim(R(A)∩ N (A)) = dim(R(A)) − dim(R(A2)). Conclude
(a) N (A) ⊂ R(A) iff dim(N (A)) = dim(R(A)) − dim(R(A2)); (b) R(A) ⊆ N (A) iff A2 = 0;
(c) R(A)=N (A) iff A2 =0 and dim(N (A))=dim(R(A)) ; (d) dim(R(A) ∩ N (A)) = 0 iff dim(R(A)) = dim(R(A2)) ;
(e) (Requires the Direct Sum subsection, which is optional.) V = R(A) ⊕ N (A) iff dim(R(A)) = dim(R(A2)).
IV.4 Inverses
We finish this section by considering how to represent the inverse of a linear map. We first recall some things about inverses. Where π: R3 → R2 is the projection map and ι: R2 → R3 is the embedding
x       x
πxxι yz  −→ y y  −→ y0
 
Section IV. Matrix Operations 247 then the composition π ◦ ι is the identity map π ◦ ι = id on R2.
    x    xιπx
y   − →  y0    − → y
We say that ι is a right inverse of π or, what is the same thing, that π is a left inverse of ι. However, composition in the other order ι ◦ π doesn’t give the identity map — here is a vector that is not sent to itself under ι ◦ π.
01  −→ 0  −→ 0
In fact, π has no left inverse at all. For, if f were to be a left inverse of π then
we would have
yz  −→ y  −→ yz
for all of the infinitely many z’s. But a function f cannot send a single argument
 x  to more than one value.
y
So a function can have a right inverse but no left inverse, or a left inverse but no right inverse. A function can also fail to have an inverse on either side; one example is the zero transformation on R2.
Some functions have a two-sided inverse, another function that is the inverse both from the left and from the right. For instance, the transformation given by ⃗v  → 2 · ⃗v has the two-sided inverse ⃗v  → (1/2) · ⃗v. The appendix shows that a function has a two-sided inverse if and only if it is both one-to-one and onto. The appendix also shows that if a function f has a two-sided inverse then it is unique, so we call it ‘the’ inverse and write f−1.
In addition, recall that we have shown in Theorem II.2.20 that if a linear map has a two-sided inverse then that inverse is also linear.
Thus, our goal in this subsection is, where a linear h has an inverse, to find the relationship between RepB,D(h) and RepD,B(h−1).
Because of the correspondence between linear maps and matrices, statements about map inverses translate into statements about matrix inverses.
0     0 π0ι
x     x πxf
 4.1 Definition A matrix G is a left inverse matrix of the matrix H if GH is the identity matrix. It is a right inverse if HG is the identity. A matrix H with a two-sided inverse is an invertible matrix. That two-sided inverse is denoted H−1 .
248 Chapter Three. Maps Between Spaces
  4.2 Lemma If a matrix has both a left inverse and a right inverse then the two are equal.
 4.3 Theorem A matrix is invertible if and only if it is nonsingular.
Proof (For both results.) Given a matrix H, fix spaces of appropriate dimension for the domain and codomain and fix bases for these spaces. With respect to these bases, H represents a map h. The statements are true about the map and therefore they are true about the matrix. QED
Proof Because the two matrices are invertible they are square, and because their product is defined they must both be n×n. Fix spaces and bases — say, Rn with the standard bases — to get maps g, h : Rn → Rn that are associated with the matrices, G = RepEn ,En (g) and H = RepEn ,En (h).
Consider h−1g−1. By the prior paragraph this composition is defined. This map is a two-sided inverse of gh since (h−1g−1)(gh) = h−1(id)h = h−1h = id and (gh)(h−1g−1) = g(id)g−1 = gg−1 = id. The matrices representing the maps reflect this equality. QED
This is the arrow diagram giving the relationship between map inverses and matrix inverses. It is a special case of the diagram relating function composition to matrix multiplication.
Wwrt C
h h−1
 4.4 Lemma A product of invertible matrices is invertible: if G and H are invertible and GH is defined then GH is invertible and (GH)−1 = H−1G−1.
  H H−1
V id V
 wrt B I wrt B
Beyond its place in our program of seeing how to represent map operations, another reason for our interest in inverses comes from linear systems. A linear system is equivalent to a matrix equation, as here.
x+x=3  1 1  x   3  12⇐⇒ 1=
2x1−x2=2 2−1 x2 2
By fixing spaces and bases (for instance, R2,R2 with the standard bases), we take the matrix H to represent a map h. The matrix equation then becomes this linear map equation.
h ( ⃗x ) = ⃗d
Section IV. Matrix Operations 249
 If we had a left inverse map g then we could apply it to both sides g◦h(⃗x) = g(⃗d) to get ⃗x = g(⃗d). Restating in terms of the matrices, we want to multiply by the inverse matrix RepC,B(g) · RepC(⃗d) to get RepB(⃗x).
4.5 Example We can find a left inverse for the matrix just given  m n  1 1   1 0 
p q 2 −1 = 0 1
by using Gauss’s Method to solve the resulting linear system.
m+2n =1 m−n =0 p+2q=0 p− q=1
Answer: m = 1/3, n = 1/3, p = 2/3, and q = −1/3. (This matrix is actually the two-sided inverse of H; the check is easy.) With it, we can solve the system from the prior example.
 x   1/3 1/3   3   5/3  y = 2/3 −1/3 2 = 4/3
4.6 Remark Why do inverse matrices when we have Gauss’s Method? Beyond the conceptual appeal of representing the map inverse operation, solving linear systems this way has two advantages.
First, once we have done the work of finding an inverse then solving a system with the same coefficients but different constants is fast: if we change the constants on the right of the system above then we get a related problem
 1 1  x   5  2 −1 y = 1
that our inverse method solves quickly.
 x   1/3 1/3   5   2  y = 2/3 −1/3 1 = 3
Another advantage of inverses is that we can explore a system’s sensitivity to changes in the constants. For example, tweaking the 3 on the right of the prior example’s system to
 1 1  x   3.01  1=
2−1x2 2
250
and solving with the inverse
 1/3 1/3    3.01  2/3 −1/3 2 =
Chapter Three. Maps Between Spaces
 (1/3)(3.01) + (1/3)(2)  (2/3)(3.01) − (1/3)(2)
 shows that the first component of the solution changes by 1/3 of the tweak, while the second component moves by 2/3 of the tweak. This is sensitivity analysis. We could use it to decide how accurately we must specify the data in a linear model to ensure that the solution has a desired accuracy.
Proof The matrix H is invertible if and only if it is nonsingular and thus Gauss-Jordan reduces to the identity. By Corollary 3.23 we can do this reduction with elementary matrices.
Rr · Rr−1 . . . R1 · H = I (∗)
For the first sentence of the result, note that elementary matrices are invertible
because elementary row operations are reversible, and that their inverses are also
elementary. Apply R−1 from the left to both sides of (∗). Then apply R−1 , etc. r r−1
The result gives H as the product of elementary matrices H = R−1 · · · R−1 · I.
 4.7 Lemma A matrix H is invertible if and only if it can be written as the product of elementary reduction matrices. We can compute the inverse by applying to the identity matrix the same row steps, in the same order, that Gauss-Jordan reduce H.
(The I there covers the case r = 0.)
For the second sentence, group (∗) as (Rr · Rr−1 . . . R1) · H = I and recognize
what’s in the parentheses as the inverse H−1 = Rr · Rr−1 . . . R1 · I. Restated: applying R1 to the identity, followed by R2, etc., yields the inverse of H. QED
4.8 Example To find the inverse of
 1 1 
2 −1
do Gauss-Jordan reduction, meanwhile performing the same operations on
the identity. For clerical convenience we write the matrix and the identity
side-by-side and do the reduction steps together.
    
1r
  1110−2ρ1+ρ2 1110
  2 −1 0 1
−→ 0 −3 −2 1   
−1/3ρ2 11 1 0
−ρ2+ρ1 −→
 −→
0 1 2/3 −1/3   
1 0 1/3 1/3 0 1 2/3 −1/3
   
Section IV. Matrix Operations 251 This calculation has found the inverse.
 1 1  −1  1/3 1/3   2−1 =2/3−1/3
4.9 Example This one happens to start with a row swap.
 0 
3 −1 1 0 0 1
0 1 0 1 0 
  ρ1↔ρ2 
  1
3 −1 1 0 0 1 −1 0 0 0 1
0 1 −1
1 0 1 0 0 0 0 1
−→ 0
1 0 1 0 1 0
   −ρ1+ρ3  
 −→
0 3 −1 1 0 0 0 −1 −1 0 −1 1
 .
1 0 0 1/4 1/4 3/4 −→ 0 1 0 1/4 1/4 −1/4
0 0 1 −1/4 3/4 −3/4
   4.10 Example This algorithm detects a non-invertible matrix when the left half won’t reduce to the identity.
    
1110 −2ρ1+ρ2 11 10 2 2 0 1 −→ 0 0 −2 1
With this procedure we can give a formula for the inverse of a general 2×2 matrix, which is worth memorizing.
     4.11 Corollary The inverse for a 2×2 matrix exists and equals  a b −1 1  d −b 
cd =ad−bc−ca if and only if ad − bc ̸= 0.
 Proof This computation is Exercise 21. QED
We have seen in this subsection, as in the subsection on Mechanics of Matrix Multiplication, how to exploit the correspondence between linear maps and matrices. We can fruitfully study both maps and matrices, translating back and forth to use whichever is handiest.
Over the course of this entire section we have developed an algebra system for matrices. We can compare it with the familiar algebra of real numbers.
252 Chapter Three. Maps Between Spaces
 Matrix addition and subtraction work in much the same way as the real number operations except that they only combine same-sized matrices. Scalar multipli- cation is in some ways an extension of real number multiplication. We also have a matrix multiplication operation and its inverse that are somewhat like the familiar real number operations (associativity, and distributivity over addition, for example), but there are differences (failure of commutativity). This section provides an example that algebra systems other than the usual real number one can be interesting and useful.
Exercises
4.12 Supply the intermediate steps in Example 4.9.
  4.13 Use Corollary 4.11 to decide if each matrix has an inverse.
(a)
 2 1  −1 1
(b)
 0 4  1 −3
(c)
 2 −3  −4 6
  4.14 For each invertible matrix in the prior problem, use Corollary 4.11 to find its inverse.
  4.15 Find the inverse, if it exists, by using the Gauss-Jordan Method. Check the answers for the 2×2 matrices with Corollary 4.11.
 3 1  (a) 0 2
0 1 (e)0 −2
 2 1/2   2 −4  1 1 3 (b)31 (c)−12 (d)024
5 2 2 3
4 (f)1 −2 −3 2 3 −2 4 −2 −3
  4.16 What matrix has this one for its inverse?  1 3 
25
4.17 How does the inverse operation interact with scalar multiplication and addition of matrices?
(a) What is the inverse of rH?
(b) Is (H + G)−1 = H−1 + G−1?   4.18 Is (Tk)−1 = (T−1)k?
4.19 Is H−1 invertible?
4.20 For each real number θ let tθ : R2 → R2 be represented with respect to the
standard bases by this matrix.
 cos θ − sin θ  sin θ cos θ
Show that tθ1+θ2 = tθ1 · tθ2 . Show also that tθ−1 = t−θ. 4.21 Do the calculations for the proof of Corollary 4.11. 4.22 Show that this matrix
 1 0 1  H=010
has infinitely many right inverses. Show also that it has no left inverse.
−1 1 0
Section IV. Matrix Operations 253
 4.23 In the review of inverses example, starting this subsection, how many left inverses has ι?
4.24 If a matrix has infinitely many right-inverses, can it have infinitely many left-inverses? Must it have?
4.25 Assume that g: V → W is linear. One of these is true, the other is false. Which is which?
(a) If f: W → V is a left inverse of g then f must be linear.
(b) If f: W → V is a right inverse of g then f must be linear.
  4.26 Assume that H is invertible and that HG is the zero matrix. Show that G is a
zero matrix.
4.27 Prove that if H is invertible then the inverse commutes with a matrix GH−1 =
H−1G if and only if H itself commutes with that matrix GH = HG.
  4.28 Show that if T is square and if T4 is the zero matrix then (I−T)−1 = I+T+T2+T3.
Generalize.
  4.29 Let D be diagonal. Describe D2, D3, . . . , etc. Describe D−1, D−2, . . . , etc.
Define D0 appropriately.
4.30 Prove that any matrix row-equivalent to an invertible matrix is also invertible. 4.31 The first question below appeared as Exercise 30.
(a) Show that the rank of the product of two matrices is less than or equal to the minimum of the rank of each.
(b) Show that if T and S are square then TS=I if and only if ST =I. 4.32 Show that the inverse of a permutation matrix is its transpose.
4.33 (a) Show that (GH)T = HTGT.
(b) A square matrix is symmetric if each i, j entry equals the j, i entry (that is, if the matrix equals its transpose). Show that the matrices HHT and HTH are symmetric.
(c) Show that the inverse of the transpose is the transpose of the inverse.
(d) Show that the inverse of a symmetric matrix is symmetric.
  4.34 (a) Prove that the composition of the projections πx, πy : R3 → R3 is the zero
map despite that neither is the zero map.
(b) Prove that the composition of the derivatives d2/dx2, d3/dx3 : P4 → P4 is the
zero map despite that neither map is the zero map.
(c) Give matrix equations representing each of the prior two items.
When two things multiply to give zero despite that neither is zero, each is said to be a zero divisor. Prove that no zero divisor is invertible.
4.35 In real number algebra, there are exactly two numbers, 1 and −1, that are their own multiplicative inverse. Does H2 = I have exactly two solutions for 2×2 matrices?
4.36 Is the relation ‘is a two-sided inverse of’ transitive? Reflexive? Symmetric? 4.37 [Am. Math. Mon., Nov. 1951] Prove: if the sum of the elements of each row of a square matrix is k, then the sum of the elements in each row of the inverse
matrix is 1/k.
254 Chapter Three. Maps Between Spaces V Change of Basis
Representations vary with the bases. For instance, with respect to the bases E2
 and
⃗e1 ∈ R2 has these different representations.
 1    1   B=⟨ 1 , −1 ⟩
 1 
RepE2 (⃗e1) = 0 RepB(⃗e1) = 1/2
 1/2 
The same holds for maps: with respect to the basis pairs E2,E2 and E2,B, the
identity map has these representations.
 1 0   1/2 1/2  RepE2,E2 (id) = 0 1 RepE2,B(id) = 1/2 −1/2
This section shows how to translate among the representations. That is, we will compute how the representations vary as the bases vary.
V.1 Changing Representations of Vectors
In converting RepB(⃗v) to RepD(⃗v) the underlying vector ⃗v doesn’t change. Thus, the translation between these two ways of expressing the vector is accomplished by the identity map on the space, described so that the domain space vectors are represented with respect to B and the codomain space vectors are represented with respect to D.
Vwrt B 
id 
Vwrt D
(This diagram is vertical to fit with the ones in the next subsection.)
 1.1 Definition The change of basis matrix for bases B, D ⊂ V is the representa- tion of the identity map id: V → V with respect to those bases.
 . .  RepB,D(id) = RepD(β⃗ 1) · · · RepD(β⃗ n)
. . ..
Section V. Change of Basis 255 1.2 Remark A better name would be ‘change of representation matrix’ but the
above name is standard.
The next result supports the definition.
Proof The first sentence holds because matrix-vector multiplication represents a map application and so RepB,D(id) · RepB(⃗v) = RepD( id(⃗v) ) = RepD(⃗v) for each ⃗v. For the second sentence, with respect to B, D the matrix M represents a linear map whose action is to map each vector to itself, and is therefore the
  1.3 Lemma Left-multiplication by the change of basis matrix for B, D converts a representation with respect to B to one with respect to D. Conversely, if left-multiplication by a matrix changes bases M · RepB(⃗v) = RepD(⃗v) then M is a change of basis matrix.
identity map.
1.4 Example With these bases for R2,
QED
because RepD(id(
 2   1   −1   1  B=⟨1,0⟩ D=⟨1,1⟩
 2   −1/2   1 
1 )) = 3/2 RepD(id( 0 )) =
 −1/2  1/2
the change of basis matrix is this.
RepB,D(id) = 3/2 1/2
 −1/2 −1/2  For instance, this is the representation of ⃗e2
 0   1  RepB( 1 )= −2
and the matrix does the conversion.
 −1/2 −1/2   1    1/2  3/2 1/2 −2 = 1/2
Checking that vector on the right is RepD(⃗e2) is easy.
DD
We finish this subsection by recognizing the change of basis matrices as a familiar set.
256 Chapter Three. Maps Between Spaces 1.5 Lemma A matrix changes bases if and only if it is nonsingular.
Proof For the ‘only if’ direction, if left-multiplication by a matrix changes bases then the matrix represents an invertible function, simply because we can invert the function by changing the bases back. Because it represents a function that is invertible, the matrix itself is invertible, and so is nonsingular.
For ‘if’ we will show that any nonsingular matrix M performs a change of basis operation from any given starting basis B (having n vectors, where the matrix is n×n) to some ending basis.
If the matrix is the identity I then the statement is obvious. Otherwise because the matrix is nonsingular Corollary IV.3.23 says there are elementary reduction matrices such that Rr · · · R1 · M = I with r   1. Elementary matrices are invertible and their inverses are also elementary so multiplying both sides of that equation from the left by Rr−1, then by Rr−1−1, etc., gives M as a product of elementary matrices M = R1−1 · · · Rr−1.
We will be done if we show that elementary matrices change a given basis to another basis, since then Rr−1 changes B to some other basis Br and Rr−1−1 changes Br to some Br−1, etc. We will cover the three types of elementary matrices separately; recall the notation for the three.
  c1 c1 ..
. .  ci cj
.  .      ci  ci 
c1 c1 . .
c1  c1  .  . 
.  . 
 . . . .
M(k)c =kc P .=. C (k).= .  i i i i,j . . i,j .  . 
. .
.. c c c kc+c
..jijij  .   .   .   . 
cn cn  .   .   .   .  cn cn cn cn
Applying a row-multiplication matrix Mi(k) changes a representation with respect to ⟨β⃗1,...,β⃗i,...,β⃗n⟩ to one with respect to ⟨β⃗1,...,(1/k)β⃗i,...,β⃗n⟩.
⃗v=c1 ·β⃗1 +···+ci ·β⃗i +···+cn ·β⃗n
 → c1 ·β⃗1 +···+kci ·(1/k)β⃗i +···+cn ·β⃗n =⃗v
The second one is a basis because the first is a basis and because of the k ̸= 0 restriction in the definition of a row-multiplication matrix. Similarly, left- multiplication by a row-swap matrix Pi,j changes a representation with respect to the basis ⟨β⃗1,...,β⃗i,...,β⃗j,...,β⃗n⟩ into one with respect to this basis
Section V. Change of Basis 257 ⟨β⃗1,...,β⃗j,...,β⃗i,...,β⃗n⟩.
⃗v=c1 ·β⃗1 +···+ci ·β⃗i +···+cjβ⃗j +···+cn ·β⃗n
 → c1 ·β⃗1 +···+cj ·β⃗j +···+ci ·β⃗i +···+cn ·β⃗n =⃗v
And, a representation with respect to ⟨β⃗1,...,β⃗i,...,β⃗j,...,β⃗n⟩ changes via left-multiplication by a row-combination matrix Ci,j(k) into a representation with respect to ⟨β⃗1,...,β⃗i −kβ⃗j,...,β⃗j,...,β⃗n⟩
⃗v=c1 ·β⃗1 +···+ci ·β⃗i +cjβ⃗j +···+cn ·β⃗n
 → c1 ·β⃗1 +···+ci ·(β⃗i −kβ⃗j)+···+(kci +cj)·β⃗j +···+cn ·β⃗n =⃗v
(the definition of Ci,j(k) specifies that i ̸= j and k ̸= 0). QED
  1.6 Corollary A matrix is nonsingular if and only if it represents the identity map with respect to some pair of bases.
Exercises
  1.7 In R2, where
 1 2   0 −1  (a) 3 4 (b) 1 −1
0 2 0 (e)0 0 6
 2 3 −1  (c) 0 1 0
2 3 −1 (d)0 1 0
4 7 −2
100
  1.9 Find the change of basis matrix for B, D ⊆ R2 .
 2   −2  D=⟨1,4⟩
find the change of basis matrices from D to E2 and from E2 to D. Multiply the two.
1.8 Which of these matrices could be used to change bases?
 1   1  (a)B=E2,D=⟨⃗e2,⃗e1⟩ (b)B=E2,D=⟨ 2 , 4 ⟩
 1   1   −1   2 
(c)B=⟨2 , 4⟩,D=E2 (d)B=⟨ 1 , 2⟩,D=⟨4 , 3⟩
  1.10 Find the change of basis matrix for each B, D ⊆ P2 .
(a) B = ⟨1,x,x2⟩,D = ⟨x2,1,x⟩ (b) B = ⟨1,x,x2⟩,D = ⟨1,1 + x,1 + x + x2⟩ (c) B=⟨2,2x,x2⟩,D=⟨1+x2,1−x2,x+x2⟩
1.11 For the bases in Exercise 9, find the change of basis matrix in the other direction, from D to B.
  1.12 Decide if each changes bases on R2. To what basis is E2 changed?
 0   1 
258
Chapter Three. Maps Between Spaces
  5 0 
(a) (b)
 −1 4   1 −1  (d)
 2 1 
04 31 2−8 11
(c)
1.13 For each space find the matrix changing a vector representation with respect to B to one with respect to D.
1 1 0 (a) V =R3, B=E3, D=⟨2,1, 1 ⟩
3 1 −1 1 1 0
(b) V =R3, B=⟨2,1, 1 ⟩, D=E3 3 1 −1
(c) V = P2 , B = ⟨x2 , x2 + x, x2 + x + 1⟩, D = ⟨2, −x, x2 ⟩
1.14 Find bases such that this matrix represents the identity map with respect to
those bases.
3 1 4 2 −1 1 004
1.15 Consider the vector space of real-valued functions with basis ⟨sin(x), cos(x)⟩. Show that ⟨2 sin(x) + cos(x), 3 cos(x)⟩ is also a basis for this space. Find the change of basis matrix in each direction.
1.16 Where does this matrix
 cos(2θ) sin(2θ)   sin(2θ) − cos(2θ)
send the standard basis for R2? Any other bases? Hint. Consider the inverse.
  1.17 What is the change of basis matrix with respect to B, B?
1.18 Prove that a matrix changes bases if and only if it is invertible.
1.19 Finish the proof of Lemma 1.5.
  1.20 Let H be an n×n nonsingular matrix. What basis of Rn does H change to the
standard basis?
1.21 (a) In P3 with basis B = ⟨1+x, 1−x, x2 +x3, x2 −x3⟩ we have this representation.
0 Rep (1−x+3x2 −x3)=1
B 1 2B
Find a basis D giving this different representation for the same polynomial. 1
Rep (1−x+3x2 −x3)=0 D 2
0D
(b) State and prove that we can change any nonzero vector representation to any
other.
Hint. The proof of Lemma 1.5 is constructive — it not only says the bases change, it shows how they change.
1.22 Let V, W be vector spaces, and let B, Bˆ be bases for V and D, Dˆ be bases for W. Where h: V → W is linear, find a formula relating RepB,D(h) to RepBˆ,Dˆ (h).
Section V. Change of Basis 259
   1.23 Show that the columns of an n×n change of basis matrix form a basis for Rn. Do all bases appear in that way: can the vectors from any Rn basis make the columns of a change of basis matrix?
1.24 Find a matrix having this effect.
 1   4 
3  → −1
That is, find a M that left-multiplies the starting vector to yield the ending vector.
Is there a matrix having these two effects?
 1   1   2   −1   1   1   2   −1  (a) 3  → 1 −1  → −1 (b) 3  → 1 6  → −1
Give a necessary and sufficient condition for there to be a matrix such that ⃗v1  → w⃗ 1 and⃗v2  →w⃗2.
V.2 Changing Map Representations
The first subsection shows how to convert the representation of a vector with respect to one basis to the representation of that same vector with respect to another basis. We next convert the representation of a map with respect to one pair of bases to the representation with respect to a different pair — we convert from RepB,D(h) to RepBˆ,Dˆ (h). Here is the arrow diagram.
h
Vwrt B −−−−→ Wwrt D
H

id 
VwrtBˆ −−−−→WwrtDˆ
h Hˆ
To move from the lower-left to the lower-right we can either go straight over, or else up to VB then over to WD and then down. So we can calculate Hˆ = RepBˆ,Dˆ(h)eitherbydirectlyusingBˆandDˆ,orelsebyfirstchangingbaseswith RepBˆ,B(id) then multiplying by H = RepB,D(h) and then changing bases with RepD,Dˆ (id).
Hˆ = RepD,Dˆ (id) · H · RepBˆ,B(id) 2.1 Example The matrix
 cos(π/6) − sin(π/6)   √3/2 T = sin(π/6) cos(π/6) = 1/2
represents, with respect to E2,E2, the transformation t: R2 → R2 that rotates vectors through the counterclockwise angle of π/6 radians.
id 
(∗) −1/2 
 √3/2
 
260
Chapter Three. Maps Between Spaces
   1  3
tπ/6 −→
 (−3 + √3)/2  √
(1+3 3)/2
       We can translate T to a representation with respect to these ˆ  1   0  ˆ  −1   2 
B=⟨12⟩D=⟨03⟩ by using the arrow diagram above.
2t2
Rwrt E2 −−−−→ Rwrt E2 T 
id  id 
2t2 RwrtBˆ −−−−→RwrtDˆ
Tˆ
The picture illustrates that we can compute Tˆ either directly by going along the square’s bottom, or as in formula (∗) by going up on the left, then across the top,andthendownontheright,withTˆ=RepE2,Dˆ(id)·T·RepBˆ,E2(id).(Note again that the matrix multiplication reads right to left, as the three functions are composed and function composition reads right to left.)
Findthematrixfortheleft-handside,thematrixRepBˆ,E2(id),intheusual way: find the effect of the identity matrix on the starting basis Bˆ — which of course is no effect at all — and then represent those basis elements with respect to the ending basis E3.
 1 0  RepBˆ,E2(id)= 1 2
This calculation is easy when the ending basis is the standard one.
There are two ways to compute the matrix for going down the square’s right side, RepE2,Dˆ (id). We could calculate it directly as we did for the other change of basis matrix. Or, we could instead calculate it as the inverse of the matrix forgoingupRepDˆ,E2(id).Findthatmatrixiseasy,andwehaveaformulafor
the 2×2 inverse so that’s what is in the equation below.
 RepBˆ,Dˆ (t) = =
 −1 2 −1  √3/2 −1/2   1 0  0 3 1/2 √3/2 1 2
 (5 − √3)/6 (3 + 2√3)/3  √√
(1+ 3)/6 3/3
     
Section V. Change of Basis 261
 The matrix is messier but the map that it represents is the same. For instance, to replicate the effect of t in the picture, start with Bˆ,
 1   1  RepBˆ( 3 )= 1 Bˆ
apply Tˆ,
 (5 − √3)/6 (3 + 2√3)/3   1   (11 + 3√3)/6 
   (1+√3)/6 √3/3 1 = (1+3√3)/6 Bˆ,Dˆ Bˆ Dˆ
   and check it against Dˆ .
11+3√3  −1  1+3√3  2   (−3+√3)/2 
   6 · 0 + 6 · 3 = (1+3√3)/2
2.2 Example Changing bases can make the matrix simpler. On R3 the map
   x y + z t
 y    −→  x + z  z x+y
is represented with respect to the standard basis in this way.
0 1 1 RepE3 ,E3 (t) = 1 0 1
110 1 1 1
B = ⟨−1, 1 ,1⟩ 0 −2 1
gives a matrix that is diagonal.
RepB,B(t) =  0 −1 0
Representing it with respect to
−1 0 0 002
Naturally we usually prefer representations that are easier to understand. We say that a map or matrix has been diagonalized when we find a basis B such that the representation is diagonal with respect to B,B, that is, with respect to the same starting basis as ending basis. Chapter Five finds which maps and matrices are diagonalizable.
262 Chapter Three. Maps Between Spaces
 The rest of this subsection develops the easier case of finding two bases B, D such that a representation is simple. Recall that the prior subsection shows that a matrix is a change of basis matrix if and only if it is nonsingular.
Proof This is immediate from equation (∗) above. QED Exercise 23 checks that matrix equivalence is an equivalence relation. Thus
it partitions the set of matrices into matrix equivalence classes.
 2.3 Definition Same-sized matrices H and Hˆ are matrix equivalent if there are nonsingular matrices P and Q such that Hˆ = PHQ.
 2.4 Corollary Matrix equivalent matrices represent the same map, with respect to appropriate pairs of bases.
 All matrices:
H matrix equivalent to Hˆ
H
Hˆ ...
 We can get insight into the classes by comparing matrix equivalence with row equivalence (remember that matrices are row equivalent when they can be reduced to each other by row operations). In Hˆ = PHQ, the matrices P and Q are nonsingular and thus each is a product of elementary reduction matrices by Lemma IV.4.7. Left-multiplication by the reduction matrices making up P performs row operations. Right-multiplication by the reduction matrices making up Q performs column operations. Hence, matrix equivalence is a generalization of row equivalence — two matrices are row equivalent if one can be converted to the other by a sequence of row reduction steps, while two matrices are matrix equivalent if one can be converted to the other by a sequence of row reduction steps followed by a sequence of column reduction steps.
Consequently, if matrices are row equivalent then they are also matrix equivalent since we can take Q to be the identity matrix. The converse, however, does not hold: two matrices can be matrix equivalent but not row equivalent.
2.5 Example These two are matrix equivalent  10   11 
00 00
because the second reduces to the first by the column operation of taking −1 times the first column and adding to the second. They are not row equivalent because they have different reduced echelon forms (both are already in reduced form).
Section V. Change of Basis 263 We close this section by giving a set of representatives for the matrix equiva-
lence classes.
  2.6 Theorem Any m×n matrix of rank k is matrix equivalent to the m×n matrix that is all zeros except that the first k diagonal entries are ones.
1 0 ... 0 0 ... 0 0 1 ... 0 0 ... 0
.  . 

0 0 ... 1 0 ... 0
0 0 ... 0 0 ... 0  . 

00...00...0
This is a block partial-identity form.
 I Z 
ZZ
Proof Gauss-Jordanreducethegivenmatrixandcombinealltherowreduction matrices to make P. Then use the leading entries to do column reduction and finish by swapping the columns to put the leading ones on the diagonal. Combine the column reduction matrices into Q. QED
2.7 Example We illustrate the proof by finding P and Q for this matrix.
1 2 1 −1 0 0 1 −1 2 4 2 −2
First Gauss-Jordan row-reduce.
1−101 00121−1 120 0  0 1 0   0 1 0   0 0 1 − 1  =  0 0 1 − 1  0 0 1 −2 0 1 2 4 2 −2 0 0 0 0
Then column-reduce, which involves right-multiplication.
120 01−2001000 1000
 0 1 000100   0 0 1 −1  =0 0 1 0 0 0 0 0 0 0 1 00 0 1 1 0 0 0 0
   00010001
264 Chapter Three. Maps Between Spaces Finish by swapping columns.
 10001000 1000  0010  
0 0 1 0 =0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0
0001
Finally, combine the left-multipliers together as P and the right-multipliers
together as Q to get PHQ.
1 −10121−110−20 1000
 0 0 1 0   00 0 1 −1 =0 1 0 0
Proof Two same-sized matrices with the same rank are equivalent to the same block partial-identity matrix. QED
2.9 Example The 2×2 matrices have only three possible ranks: zero, one, or two. Thus there are three matrix equivalence classes.

0 1
−2 0
1 2 4 2 −2 0 1 0 1 0 0 0 0 0001
 2.8 Corollary Matrix equivalence classes are characterized by rank: two same-sized matrices are matrix equivalent if and only if they have the same rank.
 All 2×2 matrices:
Three equivalence classes
 00     ⋆10
00⋆
00
⋆
 1 0  01
Each class consists of all of the 2×2 matrices with the same rank. There is only one rank zero matrix. The other two classes have infinitely many members; we’ve shown only the canonical representative.
One nice thing about the representative in Theorem 2.6 is that we can completely understand the linear map when it is expressed in this way: where the bases are B = ⟨β⃗1,...,β⃗n⟩ and D = ⟨⃗δ1,...,⃗δm⟩ then the map’s action is
c1β⃗1 +···+ckβ⃗k +ck+1β⃗k+1 +···+cnβ⃗n  → c1⃗δ1 +···+ck⃗δk +⃗0+···+⃗0 where k is the rank. Thus we can view any linear map as a projection.
 c1  c1
..
 .  . 
 ck  ck    −→   ck+1  0
. . . .
cnB 0D
Section V. Change of Basis
265
 Exercises
  2.10 Decide if these are matrix equivalent.  1 3 0  2 2 1 
(a) 2 3 0 , 0 5 −1  0 3   4 0 
(b) 1 1 , 0 5  1 3  1 3 
(c) 2 6 , 2 −6
2.11 Which of these are matrix equivalent to each other? 1 2 3  1 3   −5 1 0 
(a)4 5 6 (b) −1 −3 (c) −1 0 1 789
101 310 (e)2 0 2 (f)9 3 0
 0 −1  (d) 0 5
1 3 1 −3 −1 0
  2.12 Find the canonical representative of the matrix equivalence class of each ma-
trix.
 2 1 0  0 1 0 2 (a)420 (b)1104
3 3 3 −1 2.13 Suppose that, with respect to
 1    1   B=E2 D=⟨ 1 , −1 ⟩
the transformation t: R2 → R2 is represented by this matrix.  1 2 
34
Use change of basis matrices to represent t with respect to each pair.
ˆ  0   1  ˆ  −1   2  (a)B=⟨ 1 , 1 ⟩,D=⟨ 0 , 1 ⟩
ˆ  1   1  ˆ  1   2  (b)B=⟨2 , 0⟩,D=⟨2 , 1⟩
2.14 What sizes are P and Q in the equation Hˆ = PHQ?
  2.15 Consider the spaces V = P2 and W = M2×2, with these bases.
2  0 0  0 0  0 1  1 1  B=⟨1,1+x,1+x⟩ D=⟨0 1,1 1,1 1,1 1⟩
ˆ 2 ˆ  −1 0  0 −1  0 0  0 0  B=⟨1,x,x⟩ D=⟨00,00,10,01⟩
We will find P and Q to convert the representation of a map with respect to B, D to one with respect to Bˆ,Dˆ
(a) Draw the appropriate arrow diagram.
(b) Compute P and Q.
  2.16 Find the change of basis matrices that will convert the representation of a
t:R2 →R2 withrespecttoB,DtoonewithrespecttoBˆ,Dˆ.
 1   1   0   −1  ˆ ˆ   1    0 
B=⟨ 0 , 1 ⟩ D=⟨ 1 , 0 ⟩ B=E2 D=⟨ −1 , 1 ⟩
266 Chapter Three. Maps Between Spaces
   2.17 Find the P and Q to express H via PHQ as a block partial identity matrix. 2 1 1
H=3 −1 0 132
  2.18 Use Theorem 2.6 to show that a square matrix is nonsingular if and only if it is equivalent to an identity matrix.
2.19 Show that, where A is a nonsingular square matrix, if P and Q are nonsingular square matrices such that PAQ = I then QP = A−1.
2.20 Why does Theorem 2.6 not show that every matrix is diagonalizable (see Example 2.2)?
2.21 Must matrix equivalent matrices have matrix equivalent transposes? 2.22 What happens in Theorem 2.6 if k = 0?
2.23 Show that matrix equivalence is an equivalence relation.
  2.24 Show that a zero matrix is alone in its matrix equivalence class. Are there other matrices like that?
2.25 What are the matrix equivalence classes of matrices of transformations on R1? R3?
2.26 How many matrix equivalence classes are there?
2.27 Are matrix equivalence classes closed under scalar multiplication? Addition? 2.28 Let t: Rn → Rn represented by T with respect to En,En.
(a) Find RepB,B(t) in this specific case.
 1 1    1   −1 
T= 3 −1 B=⟨ 2 , −1 ⟩
(b) Describe RepB,B(t) in the general case where B = ⟨β⃗1,...,β⃗n⟩.
2.29 (a) Let V have bases B1 and B2 and suppose that W has the basis D. Where
h: V → W, find the formula that computes RepB2,D(h) from RepB1,D(h). (b) Repeat the prior question with one basis for V and two bases for W.
2.30 (a) If two matrices are matrix equivalent and invertible, must their inverses be matrix equivalent?
(b) If two matrices have matrix equivalent inverses, must the two be matrix equivalent?
(c) If two matrices are square and matrix equivalent, must their squares be matrix equivalent?
(d) If two matrices are square and have matrix equivalent squares, must they be matrix equivalent?
2.31 Square matrices are similar if they represent the same transformation, but each with respect to the same ending as starting basis. That is, RepB1,B1(t) is similar to RepB2,B2(t).
(a) Give a definition of matrix similarity like that of Definition 2.3.
(b) Prove that similar matrices are matrix equivalent.
(c) Show that similarity is an equivalence relation.
(d) Show that if T is similar to Tˆ then T2 is similar to Tˆ2, the cubes are similar,
etc.
(e) Prove that there are matrix equivalent matrices that are not similar.
Section VI. Projection 267 VI Projection
This section is optional. It is a prerequisite only for the final two sections of Chapter Five, and some Topics.
We have described projection from R3 into its xy-plane subspace as a shadow map. This shows why but it also shows that some shadows fall upward.
1 2 2
1 2 −1
So perhaps a better description is: the projection of ⃗v is the vector ⃗p in the plane with the property that someone standing on ⃗p and looking straight up or down — that is, looking orthogonally to the plane — sees the tip of ⃗v. In this section we will generalize this to other projections, orthogonal and non-orthogonal.
VI.1 Orthogonal Projection Into a Line
We first consider orthogonal projection of a vector ⃗v into a line l. This shows a figure walking out on the line to a point ⃗p such that the tip of ⃗v is directly above them, where “above” does not mean parallel to the y-axis but instead means orthogonal to the line.
Since the line is the span of some vector l = {c ·⃗s | c ∈ R}, we have a coefficient c⃗p with the property that ⃗v − c⃗p⃗s is orthogonal to c⃗p⃗s.
⃗v ⃗v − c ⃗p ⃗s c ⃗p ⃗s
       
268 Chapter Three. Maps Between Spaces
 To solve for this coefficient, observe that because ⃗v − c⃗p⃗s is orthogonal to a scalar multiple of ⃗s, it must be orthogonal to ⃗s itself. Then (⃗v − c⃗p⃗s) • ⃗s = 0 gives that c⃗p = ⃗v • ⃗s/⃗s • ⃗s.
(That says ‘spanned by ⃗s ’ instead the more formal ‘span of the set {⃗s }’. This more casual phrase is common.)
1.2 Example To orthogonally project the vector  2  into the line y = 2x, first
 1.1 Definition The orthogonal projection of ⃗v into the line spanned by a
nonzero ⃗s is this vector.
proj[⃗s ](⃗v) = ⃗v • ⃗s · ⃗s ⃗s • ⃗s
 pick a direction vector for the line.
3
The calculation is easy.
 1  ⃗s= 2
 into the y-axis is
y z
x 0
 y  •  1   0   0 
z0
0 0 · 1 = y
00 1 • 1
00
2 1
3•2  1  8  1 
  8/5   11· 2 =5· 2 = 16/5
   •  22
1.3 Example In R3, the orthogonal projection of a general vector x
 which matches our intuitive expectation.
The picture above showing the figure walking out on the line until ⃗v’s tip is overhead is one way to think of the orthogonal projection of a vector into a line. We finish this subsection with two other ways.
Section VI. Projection 269
 1.4 Example A railroad car left on an east-west track without its brake is pushed by a wind blowing toward the northeast at fifteen miles per hour; what speed will the car reach?
For the wind we use a vector of length 15 that points toward the northeast.  15 1/2 
⃗v= 15 1/2
The car is only affected by the part of the wind blowing in the east-west direction — the part of ⃗v in the direction of the x-axis is this (the picture has the same perspective as the railroad car picture above).
            north
 15 1/2  ⃗p= 0
  east
So the car will reach a velocity of 15 1/2 miles per hour toward the east.
Thus, another way to think of the picture that precedes the definition is that it shows ⃗v as decomposed into two parts, the part ⃗p with the line, and the part that is orthogonal to the line (shown above on the north-south axis). These two are non-interacting in the sense that the east-west car is not at all affected by the north-south part of the wind (see Exercise 10). So we can think of the orthogonal projection of ⃗v into the line spanned by ⃗s as the part of ⃗v that lies in the direction of ⃗s.
Still another useful way to think of orthogonal projection into a line is to have the person stand on the vector, not the line. This person holds a rope looped over the line. As they pull, the loop slides on the line.
When it is tight, the rope is orthogonal to the line. That is, we can think of the projection ⃗p as being the vector in the line that is closest to ⃗v (see Exercise 17).
1.5 Example A submarine is tracking a ship moving along the line y = 3x + 2. Torpedo range is one-half mile. If the sub stays where it is, at the origin on the chart below, will the ship pass within range?
  
270
Chapter Three. Maps Between Spaces
 north
The formula for projection into a line does not immediately apply because the line doesn’t pass through the origin, and so isn’t the span of any ⃗s. To adjust for this, we start by shifting the entire map down two units. Now the line is y = 3x, a subspace. We project to get the point ⃗p on the line closest to
 the sub’s shifted position.
 0  ⃗v= −2
  0    1 
−2 • 3  1   −3/5 
⃗p= 1  1 · 3 = −9/5 •
33
The distance between ⃗v and ⃗p is about 0.63 miles. The ship will never be in
range.
Exercises
  1.6 Project the first vector orthogonally into the line spanned by the second vec- tor.
 2   3   2   3  1 1 1 3 (a)1,−2 (b)1,0 (c)1,2(d)1,3
4−1 412   1.7 Project the vector orthogonally into the line.
2 −3  −1 
(a) −1, {c 1 |c∈R} (b) −1 , the line y=3x
4 −3
1.8 Although pictures guided our development of Definition 1.1, we are not restricted
to spaces that we can draw. In R4 project this vector into this line.
1 ⃗v =  2 
east
 −1
l = { c ·  1  | c ∈ R }
−1
  1.9 Definition 1.1 uses two vectors ⃗s and ⃗v. Consider the transformation of R2
resulting from fixing
 3  ⃗s= 1
and projecting ⃗v into the line that is the span of ⃗s. Apply it to these vec- tors.
1 31
Section VI. Projection 271  1   0 
 (a) (b)
24
Show that in general the projection transformation is this.
 x1    (x1 + 3x2)/10   x  →(3x+9x)/10
212
Express the action of this transformation with a matrix.
1.10 Example 1.4 suggests that projection breaks ⃗v into two parts, proj[⃗s ](⃗v ) and
⃗v − proj[⃗s ](⃗v ), that are non-interacting. Recall that the two are orthogonal. Show
that any two nonzero orthogonal vectors make up a linearly independent set. 1.11 (a) What is the orthogonal projection of ⃗v into a line if ⃗v is a member of that
line?
(b) Show that if⃗v is not a member of the line then the set {⃗v,⃗v−proj[⃗s](⃗v)} is
linearly independent.
1.12 Definition 1.1 requires that ⃗s be nonzero. Why? What is the right definition
of the orthogonal projection of a vector into the (degenerate) line spanned by the
zero vector?
1.13 Are all vectors the projection of some other vector into some line?
1.14 Show that the projection of ⃗v into the line spanned by ⃗s has length equal to
the absolute value of the number ⃗v • ⃗s divided by the length of the vector ⃗s .
1.15 Find the formula for the distance from a point to a line.
1.16 Find the scalar c such that the point (cs1,cs2) is a minimum distance from the
point (v1,v2) by using Calculus (i.e., consider the distance function, set the first
derivative equal to zero, and solve). Generalize to Rn.
  1.17 Let ⃗p be the orthogonal projection of ⃗v ∈ Rn onto a line l. Show that ⃗p is the
point in the line closest to ⃗v.
1.18 Prove that the orthogonal projection of a vector into a line is shorter than the
vector.
  1.19 Show that the definition of orthogonal projection into a line does not depend
on the spanning vector: if ⃗s is a nonzero multiple of ⃗q then (⃗v • ⃗s/⃗s • ⃗s ) · ⃗s equals
(⃗v • ⃗q/⃗q • ⃗q ) · ⃗q.
1.20 Consider the function mapping the plane to itself that takes a vector to its
projection into the line y = x. These two each show that the map is linear, the first one in a way that is coordinate-bound (that is, it fixes a basis and then computes) and the second in a way that is more conceptual.
(a) Produce a matrix that describes the function’s action.
(b) Show that we can obtain this map by first rotating everything in the plane
π/4 radians clockwise, then projecting into the x-axis, and then rotating π/4 ra-
dians counterclockwise.
1.21 For a⃗,⃗b ∈ Rn let ⃗v1 be the projection of a⃗ into the line spanned by ⃗b, let ⃗v2 be
the projection of ⃗v1 into the line spanned by a⃗, let ⃗v3 be the projection of ⃗v2 into the line spanned by ⃗b, etc., back and forth between the spans of a⃗ and ⃗b. That is, ⃗vi+1 is the projection of ⃗vi into the span of a⃗ if i + 1 is even, and into the span of ⃗b if i + 1 is odd. Must that sequence of vectors eventually settle down — must there be a sufficiently large i such that ⃗vi+2 equals ⃗vi and ⃗vi+3 equals ⃗vi+1? If so, what is the earliest such i?
272 Chapter Three. Maps Between Spaces VI.2 Gram-Schmidt Orthogonalization
The prior subsection suggests that projecting ⃗v into the line spanned by ⃗s decomposes that vector into two parts
   ⃗v ⃗v − proj[⃗s](⃗p) pro j[⃗s] (⃗p)
⃗v = proj[⃗s ](⃗v) +  ⃗v − proj[⃗s ](⃗v) 
that are orthogonal and so are “non-interacting.” We now develop that suggestion.
 2.1 Definition Vectors ⃗v1, . . . ,⃗vk ∈ Rn are mutually orthogonal when any two are orthogonal: if i ̸= j then the dot product ⃗vi • ⃗vj is zero.
 2.2 Theorem If the vectors in a set {⃗v1, . . . ,⃗vk } ⊂ Rn are mutually orthogonal and nonzero then that set is linearly independent.
Proof Consider⃗0=c1⃗v1+c2⃗v2+···+ck⃗vk.Fori∈{1,..,k},takingthedot product of ⃗vi with both sides of the equation ⃗vi • (c1⃗v1 +c2⃗v2 +· · ·+ck⃗vk) = ⃗vi •⃗0, which gives ci · (⃗vi • ⃗vi) = 0, shows that ci = 0 since ⃗vi ̸= ⃗0. QED
Proof Any linearly independent size k subset of a k dimensional space is a basis. QED
Of course, the converse of Corollary 2.3 does not hold — not every basis of every subspace of Rn has mutually orthogonal vectors. However, we can get the partial converse that for every subspace of Rn there is at least one basis consisting of mutually orthogonal vectors.
2.4 Example The members β⃗ 1 and β⃗ 2 of this basis for R2 are not orthogonal.
orthogonal vectors. The first member of the new basis is just β⃗1.  4 
⃗κ1= 2
 2.3 Corollary In a k dimensional vector space, if the vectors in a size k set are mutually orthogonal and nonzero then that set is a basis for the space.
   4     1   β⃗ 2 B = ⟨ 2 , 3 ⟩
β⃗ 1
We will derive from B a new basis for the space ⟨⃗κ1,⃗κ2⟩ consisting of mutually
Section VI. Projection 273 For the second member of the new basis, we subtract from β⃗ 2 the part in the
direction of ⃗κ1. This leaves the part of β⃗2 that is orthogonal to ⃗κ1.  1   1   1   2   −1  ⃗κ2
⃗κ2= 3 −proj[⃗κ1](3)= 3 − 1 = 2 By the corollary ⟨⃗κ1,⃗κ2⟩ is a basis for R2.
2.6 Example To produce from this basis for R3 1 0 1
B = ⟨1 , 2 , 0⟩ 103
an orthogonal basis, start by taking the first vector unchanged.
1 ⃗κ 1 =  1 
1
Get ⃗κ2 by subtracting from β⃗2 its part in the direction of ⃗κ1.
0 0 0 2/3 −2/3 ⃗κ2 = 2 − proj[⃗κ1](2) = 2 − 2/3 =  4/3 
0 0 0 2/3 −2/3
Find ⃗κ3 by subtracting from β⃗ 3 the part in the direction of ⃗κ1 and also the part in the direction of ⃗κ2.
1 1 1 −1 ⃗κ3 = 0 − proj[⃗κ1](0) − proj[⃗κ2](0) =  0 
3331
As above, the corollary gives that the result is a basis for R3.
1 −2/3 −1 ⟨1, 4/3 , 0 ⟩ 1 −2/3 1
   2.5 Definition An orthogonal basis for a vector space is a basis of mutually orthogonal vectors.
274 Chapter Three. Maps Between Spaces
  2.7Theorem (Gram-Schmidtorthogonalization) If⟨β⃗1,...β⃗k⟩isabasisforasub- space of Rn then the vectors
⃗κ 1 = β⃗ 1
⃗κ2 =β⃗2 −proj[⃗κ1](β⃗2)
⃗κ3 =β⃗3 −proj[⃗κ1](β⃗3)−proj[⃗κ2](β⃗3)
.
⃗κk =β⃗k −proj[⃗κ1](β⃗k)−···−proj[⃗κk−1](β⃗k)
form an orthogonal basis for the same subspace.
2.8 Remark This is restricted to Rn only because we have not given a definition of orthogonality for other spaces.
Proof We will use induction to check that each ⃗κi is nonzero, is in the span of ⟨β⃗1,... β⃗i⟩,andisorthogonaltoallprecedingvectors⃗κ1•⃗κi =···=⃗κi−1•⃗κi =0. Then Corollary 2.3 gives that ⟨⃗κ1 , . . . ⃗κk ⟩ is a basis for the same space as is the starting basis.
We shall only cover the cases up to i = 3, to give the sense of the argument. The full argument is Exercise 28.
The i = 1 case is trivial; taking ⃗κ1 to be β⃗ 1 makes it a nonzero vector since β⃗ 1 is a member of a basis, it is obviously in the span of ⟨β⃗ 1⟩, and the ‘orthogonal to all preceding vectors’ condition is satisfied vacuously.
In the i = 2 case the expansion
⃗κ2=β⃗2−proj[⃗κ ](β⃗2)=β⃗2−β⃗2•⃗κ1 ·⃗κ1=β⃗2−β⃗2•⃗κ1 ·β⃗1
shows that ⃗κ2 ̸= ⃗0 or else this would be a non-trivial linear dependence among the β⃗ ’s (it is nontrivial because the coefficient of β⃗ 2 is 1). It also shows that ⃗κ2 is in the span of ⟨β⃗ 1 , β⃗ 2 ⟩. And, ⃗κ2 is orthogonal to the only preceding vector
⃗κ1 •⃗κ2 =⃗κ1 • (β⃗2 −proj[⃗κ1](β⃗2))=0
because this projection is orthogonal. Thei=3caseisthesameasthei=2caseexceptforonedetail. Asinthe
i = 2 case, expand the definition.
⃗κ 3 = β⃗ 3 − β⃗ 3 • ⃗κ 1 · ⃗κ 1 − β⃗ 3 • ⃗κ 2 · ⃗κ 2
⃗κ1 • ⃗κ1 ⃗κ2 • ⃗κ2
=β⃗3−β⃗3•⃗κ1 ·β⃗1−β⃗3•⃗κ2 · β⃗2−β⃗2•⃗κ1 ·β⃗1  ⃗κ1 •⃗κ1 ⃗κ2 •⃗κ2 ⃗κ1 •⃗κ1
  1 ⃗κ 1 • ⃗κ 1 ⃗κ 1 • ⃗κ 1
     
Section VI. Projection 275
 By the first line ⃗κ3 ̸= ⃗0, since β⃗ 3 isn’t in the span [β⃗ 1, β⃗ 2] and therefore by the inductive hypothesis it isn’t in the span [⃗κ1,⃗κ2]. By the second line ⃗κ3 is in the span of the first three β⃗ ’s. Finally, the calculation below shows that ⃗κ3 is orthogonal to ⃗κ1.
⃗κ1 •⃗κ3 =⃗κ1 •  β⃗3 −proj[⃗κ1](β⃗3)−proj[⃗κ2](β⃗3)  =⃗κ1 •  β⃗3 −proj[⃗κ1](β⃗3) −⃗κ1 • proj[⃗κ2](β⃗3) =0
(Here is the difference with the i = 2 case: as happened for i = 2 the first term is 0 because this projection is orthogonal, but here the second term in the second line is 0 because ⃗κ1 is orthogonal to ⃗κ2 and so is orthogonal to any vector in the line spanned by ⃗κ2.) A similar check shows that ⃗κ3 is also orthogonal to ⃗κ2. QED
In addition to having the vectors in the basis be orthogonal, we can also normalize each vector by dividing by its length, to end with an orthonormal basis..
2.9 Example From the orthogonal basis of Example 2.6, normalizing produces this orthonormal basis.
1/√3 −1/√6 −1/√2
   √√
⟩
  ⟨1/ 3, 2/ 6 , 0 √√√
1/ 3 −1/ 6 1/ 2
   Besides its intuitive appeal, and its analogy with the standard basis En for Rn, an orthonormal basis also simplifies some computations. Exercise 22 is an example.
Exercises
2.10 Normalize the lengths of these vectors.  1  −1  1 
(a) 2 (b)  3  (c) −1 0
  2.11 Perform Gram-Schmidt on this basis for R2.  1   −1 
⟨1,2⟩ Check that the resulting vectors are orthogonal.
  2.12 Perform the Gram-Schmidt process on this basis for R3. 1 2 3
⟨2, 1 ,3⟩ 3 −3 3
  2.13 Perform Gram-Schmidt on each of these bases for R2.
276 Chapter Three. Maps Between Spaces  1   2   0   −1   0   −1 
(a)⟨1,1⟩ (b)⟨1, 3 ⟩ (c)⟨1, 0 ⟩
Then turn those orthogonal bases into orthonormal bases.
2.14 Perform the Gram-Schmidt process on each of these bases for R3.
 2 1 0
(a) ⟨2, 0 ,3⟩ 2 −1 1
1 0 2
(b) ⟨−1,1,3⟩ 0 0 1
Then turn those orthogonal bases into orthonormal bases.
  2.15 Find an orthonormal basis for this subspace of R3: the plane x − y + z = 0.
2.16 Find an orthonormal basis for this subspace of R4. x
 z 
{y | x − y − z + w = 0 and x + z = 0}
w
2.17 Show that any linearly independent subset of Rn can be orthogonalized without changing its span.
2.18 What happens if we try to apply the Gram-Schmidt process to a finite set that is not a basis?
  2.19 What happens if we apply the Gram-Schmidt process to a basis that is already orthogonal?
2.20 Let ⟨⃗κ1,...,⃗κk⟩ be a set of mutually orthogonal vectors in Rn.
(a) Prove that for any⃗v in the space, the vector⃗v−(proj[⃗κ1](⃗v)+···+proj[⃗vk](⃗v))
is orthogonal to each of ⃗κ1, . . . , ⃗κk.
(b) Illustrate the prior item in R3 by using ⃗e1 as ⃗κ1, using ⃗e2 as ⃗κ2, and taking ⃗v
to have components 1, 2, and 3.
(c) Show that proj[⃗κ1](⃗v ) + · · · + proj[⃗vk](⃗v ) is the vector in the span of the set of
⃗κ’s that is closest to ⃗v. Hint. To the illustration done for the prior part, add a
vector d1⃗κ1 + d2⃗κ2 and apply the Pythagorean Theorem to the resulting triangle. 2.21 Find a nonzero vector in R3 that is orthogonal to both of these.
1 2
5 2 −1 0
  2.22 One advantage of orthogonal bases is that they simplify finding the representa- tion of a vector with respect to that basis.
(a) For this vector and this non-orthogonal basis for R2  2   1   1 
⃗v = 3 B = ⟨ 1 , 0 ⟩
first represent the vector with respect to the basis. Then project the vector into
the span of each basis vector [β⃗1] and [β⃗2]. (b) With this orthogonal basis for R2
 1    1   K=⟨ 1 , −1 ⟩
represent the same vector ⃗v with respect to the basis. Then project the vector into the span of each basis vector. Note that the coefficients in the representation and the projection are the same.
Section VI. Projection 277
 (c) Let K = ⟨⃗κ1,...,⃗κk⟩ be an orthogonal basis for some subspace of Rn. Prove that for any ⃗v in the subspace, the i-th component of the representation RepK(⃗v ) is the scalar coefficient (⃗v • ⃗κi)/(⃗κi • ⃗κi) from proj[⃗κi](⃗v ).
(d) Provethat⃗v=proj[⃗κ1](⃗v)+···+proj[⃗κk](⃗v).
2.23 Bessel’s Inequality. Consider these orthonormal sets
B1 = {⃗e1 } B2 = {⃗e1,⃗e2 } B3 = {⃗e1,⃗e2,⃗e3 } B4 = {⃗e1,⃗e2,⃗e3,⃗e4 } along with the vector ⃗v ∈ R4 whose components are 4, 3, 2, and 1.
(a) Find the coefficient c1 for the projection of ⃗v into the span of the vector in B1. Checkthat∥⃗v∥2  |c1|2.
(b) Find the coefficients c1 and c2 for the projection of ⃗v into the spans of the two vectors in B2 . Check that ∥⃗v ∥2   |c1 |2 + |c2 |2 .
(c) Find c1, c2, and c3 associated with the vectors in B3, and c1, c2, c3, and c4 forthevectorsinB4.Checkthat∥⃗v∥2 |c1|2+···+|c3|2 andthat∥⃗v∥2  |c1|2 +···+|c4|2.
Show that this holds in general: where {⃗κ1,...,⃗κk} is an orthonormal set and ci is coefficient of the projection of a vector ⃗v from the space then ∥⃗v ∥2   |c1|2 +· · ·+|ck|2. Hint. One way is to look at the inequality 0   ∥⃗v − (c1⃗κ1 + · · · + ck⃗κk)∥2 and expand the c’s.
2.24 Prove or disprove: every vector in Rn is in some orthogonal basis.
2.25 Show that the columns of an n×n matrix form an orthonormal set if and only
if the inverse of the matrix is its transpose. Produce such a matrix.
2.26 Does the proof of Theorem 2.2 fail to consider the possibility that the set of
vectors is empty (i.e., that k = 0)?
2.27 Theorem 2.7 describes a change of basis from any basis B = ⟨β⃗ 1, . . . , β⃗ k⟩ to
one that is orthogonal K = ⟨⃗κ1 , . . . , ⃗κk ⟩. Consider the change of basis matrix RepB,K (id).
(a) Prove that the matrix RepK,B(id) changing bases in the direction opposite to that of the theorem has an upper triangular shape — all of its entries below the main diagonal are zeros.
(b) Prove that the inverse of an upper triangular matrix is also upper triangular (if the matrix is invertible, that is). This shows that the matrix RepB,K(id) changing bases in the direction described in the theorem is upper triangular.
2.28 Complete the induction argument in the proof of Theorem 2.7.
VI.3 Projection Into a Subspace
This subsection uses material from the optional earlier subsection on Com- bining Subspaces.
The prior subsections project a vector into a line by decomposing it into two parts: the part in the line proj[⃗s ](⃗v ) and the rest ⃗v − proj[⃗s ](⃗v ). To generalize projection to arbitrary subspaces we will follow this decomposition idea.
278 Chapter Three. Maps Between Spaces
  3.1 Definition Let a vector space be a direct sum V = M ⊕ N. Then for any ⃗v ∈ V with ⃗v = m⃗ + n⃗ where m⃗ ∈ M, n⃗ ∈ N, the projection of ⃗v into M along N is projM,N(⃗v) = m⃗ .
This definition applies in spaces where we don’t have a ready definition of orthogonal. (Definitions of orthogonality for spaces other than the Rn are perfectly possible but we haven’t seen any in this book.)
3.2 Example The space M2×2 of 2×2 matrices is the direct sum of these two.  a b   0 0 
M={ 0 0 |a,b∈R} N={ c d |c,d∈R}
To project
into M along N, we first fix bases for the two subspaces.
 1 0  0 1   0 0  0 0  BM=⟨ 0 0 , 0 0 ⟩ BN=⟨ 1 0 , 0 1 ⟩
Their concatenation
B=BM
⌢  1 0  0 1  0 0  0 0  BN=⟨ 0 0 , 0 0 , 1 0 , 0 1 ⟩
 3 1  A=04
is a basis for the entire space because M2×2 is the direct sum. So we can use it to represent A.
 3 1   1 0   0 1   0 0   0 0  04=3·00+1·00+0·10+4·01
The projection of A into M along N keeps the M part and drops the N part.  3 1   1 0   0 1   3 1 
projM,N( 0 4 )=3· 0 0 +1· 0 0 = 0 0
3.3 Example Both subscripts on projM,N(⃗v) are significant. The first subscript M matters because the result of the projection is a member of M. For an example showing that the second one matters, fix this plane subspace of R3 and its basis.
x 1 0
M={y|y−2z=0} BM =⟨0,2⟩ z01
Section VI. Projection 279 We will compare the projections of this element of R3
2 ⃗ v =  2 
5
into M along these two subspaces (verification that R3 = M⊕N and R3 = M⊕Nˆ
is routine).
0 0 N={k0|k∈R} Nˆ ={k 1 |k∈R}
1 −2 Here are natural bases for N and Nˆ .
0 0 BN = ⟨0⟩ BNˆ = ⟨ 1 ⟩
1 −2
To project into M along N, represent ⃗v with respect to the concatenation B ⌢B
2 1 0 0  2  = 2 ·  0  + 1 ·  2  + 4 ·  0 
 5011 1 0 2
and drop the N term.
projM,N(⃗v ) = 2 · 0 + 1 · 2 = 2
011
To project into M along Nˆ represent ⃗v with respect to B ⌢ B
M Nˆ 2 1 0 0
2=2·0+(9/5)·2−(8/5)· 1  5 0 1 −2
and omit the Nˆ part.
projM,Nˆ (⃗v ) = 2 · 0 + (9/5) · 2 = 18/5
1 0  2  0 1 9/5
So projecting along different subspaces can give different results.
These pictures compare the two maps. Both show that the projection is
indeed ‘into’ the plane and ‘along’ the line.
MN
280
Chapter Three. Maps Between Spaces
 N
  Nˆ M
M
Notice that the projection along N is not orthogonal since there are members of the plane M that are not orthogonal to the dotted line. But the projection along Nˆ is orthogonal.
We have seen two projection operations, orthogonal projection into a line as well as this subsections’s projection into an M and along an N, and we naturally ask whether they are related. The right-hand picture above suggests the answer — orthogonal projection into a line is a special case of this subsection’s projection; it is projection along a subspace perpendicular to the line.
N
M
    3.4 Definition The orthogonal complement of a subspace M of Rn is M⊥ = {⃗v ∈ Rn | ⃗v is perpendicular to all vectors in M }
(read “M perp”). The orthogonal projection projM(⃗v) of a vector is its projec- tion into M along M⊥.
3.5 Example In R3, to find the orthogonal complement of the plane x
P = {y | 3x + 2y − z = 0} z
we start with a basis for P.
1 0 B = ⟨0 , 1⟩
32
Any ⃗v perpendicular to every vector in B is perpendicular to every vector in the span of B (the proof of this is Exercise 22). Therefore, the subspace P⊥ consists
Section VI. Projection
281
 of the vectors that satisfy these two conditions.
1 v 0 v 11
•=0•=0 0 v2  1 v2 
3v3 2v3 Those conditions give a linear system.
v   v    11
P ⊥ = {  v 2  | 1 0 3  v 2  = 0 v012v0
}
33
We are thus left with finding the null space of the map represented by the matrix, that is, with calculating the solution set of the homogeneous linear system.
−3
v1 +3v3=0 =⇒ P⊥={k−2|k∈R}
v2 + 2v3 = 0 1
3.6 Example Where M is the xy-plane subspace of R3, what is M⊥? A common first reaction is that M⊥ is the yz-plane but that’s not right because some vectors from the yz-plane are not perpendicular to every vector in the xy-plane.
  1 0
1 ̸⊥ 3 θ = arccos( √2 · √13 ) ≈ 0.94 rad
02
Instead M⊥ is the z-axis, since proceeding as in the prior example and taking
the natural basis for the xy-plane gives this.
x   x   x
M⊥ ={y| 1 0 0 y= 0 }={y|x=0andy=0} z010z0z
Proof First, the orthogonal complement M⊥ is a subspace of Rn because it is a null space, namely the null space of the orthogonal projection map.
To show that the space M is the direct sum of the two, start with any basis BM = ⟨⃗μ1, . . . , ⃗μk⟩ for M and expand it to a basis for the entire space. Apply
1·0+1·3+0·2
    3.7 Lemma If M is a subspace of Rn then its orthogonal complement M⊥ is also a subspace. The space is the direct sum of the two Rn = M ⊕ M⊥. And, for any ⃗v ∈ Rn the vector ⃗v − projM(⃗v ) is perpendicular to every vector in M.
282 Chapter Three. Maps Between Spaces
 the Gram-Schmidt process to get an orthogonal basis K = ⟨⃗κ1,...,⃗κn⟩ for Rn. This K is the concatenation of two bases: ⟨⃗κ1,...,⃗κk⟩ with the same number of members k as BM, and ⟨⃗κk+1,...,⃗κn⟩. The first is a basis for M so if we show that the second is a basis for M⊥ then we will have that the entire space is the direct sum.
Exercise 22 from the prior subsection proves this about any orthogonal basis: each vector ⃗v in the space is the sum of its orthogonal projections into the lines spanned by the basis vectors.
⃗v = proj[⃗κ1](⃗v ) + · · · + proj[⃗κn](⃗v ) (∗)
To check this, represent the vector as ⃗v = r1⃗κ1 + · · · + rn⃗κn, apply ⃗κi to both sides⃗v•⃗κi =(r1⃗κ1+···+rn⃗κn)•⃗κi =r1·0+···+ri·(⃗κi•⃗κi)+···+rn·0, and solve to get ri = (⃗v • ⃗κi)/(⃗κi • ⃗κi), as desired.
Since obviously any member of the span of ⟨⃗κk+1,...,⃗κn⟩ is orthogonal to any vector in M, to show that this is a basis for M⊥ we need only show the other containment — that any w⃗ ∈ M⊥ is in the span of this basis. The prior paragraph does this. Any w⃗ ∈ M⊥ gives this on projections into basis vectors from M: proj[⃗κ1](w⃗ ) = ⃗0, . . . , proj[⃗κk](w⃗ ) = ⃗0. Therefore equation (∗) gives that w⃗ is a linear combination of ⃗κk+1,...,⃗κn. Thus this is a basis for M⊥ and Rn is the direct sum of the two.
The final sentence of the lemma is proved in much the same way. Write ⃗v=proj[⃗κ1](⃗v)+···+proj[⃗κn](⃗v). ThenprojM(⃗v)keepsonlytheMpartand drops the M⊥ part projM(⃗v ) = proj[⃗κk+1](⃗v ) + · · · + proj[⃗κk](⃗v ). Therefore ⃗v − projM(⃗v ) consists of a linear combination of elements of M⊥ and so is perpendicular to every vector in M. QED
Given a subspace, we could compute the orthogonal projection into that subspace by following the steps of that proof: finding a basis, expanding it to a basis for the entire space, applying Gram-Schmidt to get an orthogonal basis, and projecting into each linear subspace. However we will instead use a convenient formula.
Proof The vector projM(⃗v) is a member of M and so is a linear combination of basis vectors c1 ·β⃗1 +···+ck ·β⃗k. Since A’s columns are the β⃗’s, there is a ⃗c ∈ Rk such that projM(⃗v ) = A⃗c. To find ⃗c note that the vector ⃗v − projM(⃗v )
 3.8 Theorem Let M be a subspace of Rn with basis ⟨β⃗1,...,β⃗k⟩ and let A be the matrix whose columns are the β⃗ ’s. Then for any ⃗v ∈ Rn the orthogonal projection is projM(⃗v ) = c1β⃗ 1 + · · · + ckβ⃗ k, where the coefficients ci are the entries of the vector (ATA)−1AT · ⃗v. That is, projM(⃗v ) = A(ATA)−1AT · ⃗v.
Section VI. Projection 283
 is perpendicular to each member of the basis so
⃗0 = A T   ⃗v − A ⃗c   = A T ⃗v − A T A ⃗c
and solving gives this (showing that ATA is invertible is an exercise).
⃗c =   A T A   − 1 A T · ⃗v
Therefore projM(⃗v ) = A · ⃗c = A(ATA)−1AT · ⃗v, as required. QED
3.9 Example To orthogonally project this vector into this subspace 1 x
⃗ v =  − 1  P = {  y  | x + z = 0 } 1z
first make a matrix whose columns are a basis for the subspace
and then compute.
0 1 A=1 0
0 −1
01      
  T  −1 T   1 0 0 1 0 AAA A=1 001/2 10−1
0 −1
1/2 0 −1/2 =  0 1 0 
−1/2 0 1/2
With the matrix, calculating the orthogonal projection of any vector into P is
easy.
1/2 0 −1/21 0 projP(⃗v) =  0 1 0 −1 = −1
−1/201/2 1 0 Note, as a check, that this result is indeed in P.
Exercises
  3.10 Project the vectors into M along N.
 3   x   x 
(a) −2 , M={ y |x+y=0}, N={ y |−x−2y=0}  1   x   x 
(b) 2 , M={ y |x−y=0}, N={ y |2x+y=0} 3 x 1
(c) 0, M={y|x+y=0}, N={c·0|c∈R} 1z1
  3.11 Find M⊥.
284 Chapter Three. Maps Between Spaces
  x   x 
(a)M={ y |x+y=0} (b)M={ y |−2x+3y=0}
 x  ⃗  x 
(c) M={ y |x−y=0} (d) M={0} (e) M={ y |x=0}
x x
(f) M={y|−x+3y+z=0} (g) M={y|x=0andy+z=0} zz
  3.12 Find the orthogonal projection of the vector into the subspace. 1 0  1 
2 S = [{2 , −1}] 001
  3.13 With the same subspace as in the prior problem, find the orthogonal projection of this vector.
1
2 −1
  3.14 Let ⃗p be the orthogonal projection of ⃗v ∈ Rn onto a subspace S. Show that ⃗p is the point in the subspace closest to ⃗v.
3.15 This subsection shows how to project orthogonally in two ways, the method of Example 3.2 and 3.3, and the method of Theorem 3.8. To compare them, consider the plane P specified by 3x + 2y − z = 0 in R3.
(a) Find a basis for P.
(b) Find P⊥ and a basis for P⊥.
(c) Represent this vector with respect to the concatenation of the two bases from
the prior item.
1 ⃗v =  1 
2
(d) Find the orthogonal projection of ⃗v into P by keeping only the P part from the prior item.
(e) Check that against the result from applying Theorem 3.8.
3.16 We have three ways to find the orthogonal projection of a vector into a line, the Definition 1.1 way from the first subsection of this section, the Example 3.2 and 3.3 way of representing the vector with respect to a basis for the space and then keeping the M part, and the way of Theorem 3.8. For these cases, do all three ways.
 1   x 
(a)⃗v= −3 , M={ y |x+y=0}
0 x
(b)⃗v=1, M={y|x+z=0andy=0} 2z
3.17 Check that the operation of Definition 3.1 is well-defined. That is, in Exam- ple 3.2 and 3.3, doesn’t the answer depend on the choice of bases?
3.18 What is the orthogonal projection into the trivial subspace?
Section VI. Projection 285
 3.19 What is the projection of ⃗v into M along N if ⃗v ∈ M?
3.20 Show that if M ⊆ Rn is a subspace with orthonormal basis ⟨⃗κ1 , . . . , ⃗κn ⟩ then
the orthogonal projection of ⃗v into M is this.
(⃗v• ⃗κ1)·⃗κ1 +···+(⃗v• ⃗κn)·⃗κn
  3.21 Prove that the map p: V → V is the projection into M along N if and only if the map id−p is the projection into N along M. (Recall the definition of the difference of two maps: (id −p) (⃗v) = id(⃗v) − p(⃗v) = ⃗v − p(⃗v).)
3.22 Show that if a vector is perpendicular to every vector in a set then it is perpendicular to every vector in the span of that set.
3.23 True or false: the intersection of a subspace and its orthogonal complement is trivial.
3.24 Show that the dimensions of orthogonal complements add to the dimension of the entire space.
3.25 Suppose that ⃗v1,⃗v2 ∈ Rn are such that for all complements M,N ⊆ Rn, the projections of ⃗v1 and ⃗v2 into M along N are equal. Must ⃗v1 equal ⃗v2? (If so, what if we relax the condition to: all orthogonal projections of the two are equal?)
  3.26 Let M,N be subspaces of Rn. The perp operator acts on subspaces; we can ask how it interacts with other such operations.
(a) Show that two perps cancel: (M⊥)⊥ = M. (b) Prove that M ⊆ N implies that N⊥ ⊆ M⊥. (c) Show that (M + N)⊥ = M⊥ ∩ N⊥.
  3.27 The material in this subsection allows us to express a geometric relationship that we have not yet seen between the range space and the null space of a linear map.
(a) Represent f: R3 → R given by
v1 
v2 →1v1 +2v2 +3v3
v3
with respect to the standard bases and show that
1
2
3
is a member of the perp of the null space. Prove that N (f)⊥ is equal to the
span of this vector.
(b) Generalize that to apply to any f: Rn → R. (c) Represent f: R3 → R2
v3
with respect to the standard bases and show that
v1  1v1 + 2v2 + 3v3  v2  → 4v1 + 5v2 + 6v3
1 4
2, 5 36
286 Chapter Three. Maps Between Spaces
 are both members of the perp of the null space. Prove that N (f)⊥ is the span
of these two. (Hint. See the third item of Exercise 26.) (d) Generalize that to apply to any f: Rn → Rm.
In [Strang 93] this is called the Fundamental Theorem of Linear Algebra
3.28 Define a projection to be a linear transformation t: V → V with the property that repeating the projection does nothing more than does the projection alone: (t◦
t)(⃗v) = t(⃗v) for all ⃗v ∈ V.
(a) Show that orthogonal projection into a line has that property. (b) Show that projection along a subspace has that property.
(c) Show that for any such t there is a basis B = ⟨β⃗1,...,β⃗n⟩ for V such that  
t(β⃗i)= β⃗i i=1,2,...,r
⃗0 i = r + 1, r + 2, . . . , n
where r is the rank of t.
(d) Conclude that every projection is a projection along a subspace. (e) Also conclude that every projection has a representation
 I Z  RepB,B(t) = Z   Z
in block partial-identity form.
3.29 A square matrix is symmetric if each i, j entry equals the j, i entry (i.e., if the
matrix equals its transpose). Show that the projection matrix A(ATA)−1AT is symmetric. [Strang 80] Hint. Find properties of transposes by looking in the index under ‘transpose’.
  
Topic
Line of Best Fit
This Topic requires the formulas from the subsections on Orthogonal Pro- jection Into a Line and Projection Into a Subspace.
Scientists are often presented with a system that has no solution and they must find an answer anyway. More precisely, they must find a best answer. For instance, this is the result of flipping a penny, including some intermediate numbers.
number of flips   30 60 90 number of heads   16 34 51
Because of the randomness in this experiment we expect that the ratio of heads to flips will fluctuate around a penny’s long-term ratio of 50-50. So the system for such an experiment likely has no solution, and that’s what happened here.
30m=16 60m=34 90m=51
That is, the vector of data that we collected is not in the subspace where ideally
  it would be.
16 30
34 ̸∈ {m60 | m ∈ R}
51 90
However, we have to do something so we look for the m that most nearly works. An orthogonal projection of the data vector into the line subspace gives a best guess, the vector in the subspace closest to the data vector.
16 30
34 • 60 30 30
51 90
   ·60= 7110 ·60
30 30 90 12600 90
60 • 60 90 90
  
288 Chapter Three. Maps Between Spaces
 The estimate (m = 7110/12600 ≈ 0.56) is a bit more than one half, but not much more than half, so probably the penny is fair enough.
The line with the slope m ≈ 0.56 is the line of best fit for this data. heads
 60
30
                 30 60 90 flips
Minimizing the distance between the given vector and the vector used as the right-hand side minimizes the total of these vertical lengths, and consequently we say that the line comes from fitting by least-squares.
This diagram exaggerates the vertical scale by a factor of ten to make the lengths more visible.
In the above equation the line must pass through (0, 0), because we take it to be the line whose slope is this coin’s true proportion of heads to flips. We can also handle cases where the line need not pass through the origin.
Here is the progression of world record times for the men’s mile race [Oakley & Baker]. In the early 1900’s many people wondered when, or if, this record would fall below the four minute mark. Here are the times that were in force on January first of each decade through the first half of that century.
year 1870 1880 1890 1900 1910 1920 1930 1940 1950 secs 268.8 264.5 258.4 255.6 255.6 252.6 250.4 246.4 241.4
We can use this to give a circa 1950 prediction of the date for 240 seconds, and then compare that to the actual date. As with the penny data, these numbers do not lie in a perfect line. That is, this system does not have an exact solution for the slope and intercept.
b + 1870m = 268.8 b + 1880m = 264.5
.
b + 1950m = 241.4
We find a best approximation by using orthogonal projection.
(Comments on the data. Restricting to the times at the start of each decade reduces the data entry burden, smooths the data to some extent, and gives much
    
Topic: Line of Best Fit 289
 the same result as entering all of the dates and records. There are different sequences of times from competing standards bodies but the ones here are from [Wikipedia, Mens Mile]. We’ve started the plot at 1870 because at one point there were two classes of records, called ‘professional’ and ‘amateur’, and after a while the first class stopped being active so we’ve followed the second class.)
Write the linear system’s matrix of coefficients and also its vector of constants, the world record times.
1 1870 268.8
1 1880 264.5 A=. . ⃗v=. .. .
1 1950 241.4
The ending result in the subsection on Projection into a Subspace gives the formula for the the coefficients b and m that make the linear combination of A’s columns as close as possible to ⃗v. Those coefficients are the entries of the vector (ATA)−1AT ·⃗v.
Sage can do the computation for us.
sage: year = [1870, 1880, 1890, 1900, 1910, 1920, 1930, 1940, 1950]
sage: secs = [268.8, 264.5, 258.4, 255.6, 255.6, 252.6, 250.4, 246.4, 241.4] sage: var('a, b, t')
(a, b, t)
sage: model(t) = a*t+b
sage: data = zip(year, secs)
sage: fit = find_fit(data, model, solution_dict=True)
sage: model.subs(fit)
t |--> -0.3048333333333295*t + 837.0872222222147
          sage: ....: sage: sage:
g=points(data)+plot(model.subs(fit),(t,1860,1960),color='red', figsize=3,fontsize=7,typeset='latex')
g.save("four_minute_mile.pdf") g
    270 265 260 255 250 245 240
                        1860 1880 1900 1920
1940 1960
The progression makes a surprisingly good line. From the slope and intercept we predict 1958.73; the actual date of Roger Bannister’s record was 1954-May-06. The final example compares team salaries from US major league baseball
against the number of wins the team had, for the year 2002. In this year the
290 Chapter Three. Maps Between Spaces
 Oakland Athletics used mathematical techniques to optimize the players that they fielded for the money that they could spend, as told in the film Moneyball. (Salaries are in millions of dollars and the number of wins is out of 162 games).
To do the computations we again use Sage.
t |--> 0.2634981251436269*t + 63.06477642781477
sage: p = points(data,size=25)+plot(model.subs(fit),(t,30,130),color='red',typeset='latex') sage: p.save('moneyball.pdf')
The graph is below. The team in the upper left, who paid little for many wins, is the Oakland A’s.
 sage:
....:
sage:
....:
sage:
(a, b, t)
sage: model(t) = a*t+b
sage: data = zip(sal,wins)
sage: fit = find_fit(data, model, solution_dict=True) sage: model.subs(fit)
sal = [40, 40, 39, 42, 45, 42, 62, 34, 41, 57, 58, 63, 47, 75, 57, 78, 80, 50, 60, 93, 77, 55, 95, 103, 79, 76, 108, 126, 95, 106]
  wins = [103, 94, 83, 79, 78, 72, 99, 55, 66, 81, 80, 84, 62, 97, 73, 95, 93, 56, 67, 101, 78, 55, 92, 98, 74, 67, 93, 103, 75, 72]
  var('a, b, t')
           100
90
80
70
60
                                              40 60 80 100 120
Judging this line by eye would be error-prone. So the equations give us a certainty about the ‘best’ in best fit. In addition, the model’s equation tells us roughly that by spending an additional million dollars a team owner can expect to buy 1/4 of a win (and that expectation is not very sure, thank goodness).
Exercises
The calculations here are best done on a computer. Some of the problems require
data from the Internet.
1 Use least-squares to judge if the coin in this experiment is fair. flips 8 16 24 32 40
heads 4 9 13 17 20
2 For the men’s mile record, rather than give each of the many records and its exact
date, we’ve “smoothed” the data somewhat by taking a periodic sample. Do the
longer calculation and compare the conclusions.
3 Find the line of best fit for the men’s 1500 meter run. How does the slope compare with that for the men’s mile? (The distances are close; a mile is about 1609 meters.)
   
Topic: Line of Best Fit 291
 4 Find the line of best fit for the records for women’s mile.
5 Do the lines of best fit for the men’s and women’s miles cross?
6 (This illustrates that there are data sets for which a linear model is not right, and that the line of best fit doesn’t in that case have any predictive value.) In a highway restaurant a trucker told me that his boss often sends him by a roundabout route, using more gas but paying lower bridge tolls. He said that New York State calibrates the toll for each bridge across the Hudson, playing off the extra gas to get there from New York City against a lower crossing cost, to encourage people to go upstate. This table, from [Cost Of Tolls] and [Google Maps], lists for each toll crossing of the Hudson River, the distance to drive from Times Square in miles and the cost in US dollars for a passenger car (if a crossings has a one-way toll then it shows half that number).
Crossing
Lincoln Tunnel
Holland Tunnel
George Washington Bridge Verrazano-Narrows Bridge Tappan Zee Bridge
Bear Mountain Bridge Newburgh-Beacon Bridge Mid-Hudson Bridge Kingston-Rhinecliff Bridge Rip Van Winkle Bridge
Distance Toll
2 6.00 7 6.00 8 6.00
16 6.50 27 2.50 47 1.00 67 1.00 82 1.00
102 1.00 120 1.00
 Find the line of best fit and graph the data to show that the driver was practicing on my credulity.
7 When the space shuttle Challenger exploded in 1986, one of the criticisms made of NASA’s decision to launch was in the way they did the analysis of number of O-ring failures versus temperature (O-ring failure caused the explosion). Four O-ring failures would be fatal. NASA had data from 24 previous flights.
temp◦F 53 75 57 58 63 70 70 66 67 67 67 failures 3 2 1 1 1 1 1 0 0 0 0
68 69 70 70 72 73 75 76 76 78 79 80 81 0000000000000
The temperature that day was forecast to be 31◦F.
(a) NASA based the decision to launch partially on a chart showing only the
flights that had at least one O-ring failure. Find the line that best fits these seven flights. On the basis of this data, predict the number of O-ring failures when the temperature is 31, and when the number of failures will exceed four.
(b) Find the line that best fits all 24 flights. On the basis of this extra data, predict the number of O-ring failures when the temperature is 31, and when the number of failures will exceed four.
Which do you think is the more accurate method of predicting? (An excellent discussion is in [Dalal, et. al.].)
      
292 Chapter Three. Maps Between Spaces
 8 This table lists the average distance from the sun to each of the first seven planets, using Earth’s average as a unit.
Mercury Venus Earth Mars Jupiter Saturn Uranus
0.39 0.72 1.00 1.52 5.20 9.54 19.2
(a) Plot the number of the planet (Mercury is 1, etc.) versus the distance. Note that it does not look like a line, and so finding the line of best fit is not fruitful. (b) It does, however look like an exponential curve. Therefore, plot the number
of the planet versus the logarithm of the distance. Does this look like a line? (c) The asteroid belt between Mars and Jupiter is what is left of a planet that broke apart. Renumber so that Jupiter is 6, Saturn is 7, and Uranus is 8, and
plot against the log again. Does this look better?
(d) Use least squares on that data to predict the location of Neptune. (e) Repeat to predict where Pluto is.
(f) Is the formula accurate for Neptune and Pluto?
This method was used to help discover Neptune (although the second item is misleading about the history; actually, the discovery of Neptune in position 9 prompted people to look for the “missing planet” in position 5). See [Gardner, 1970]
9 Suppose that W is a subspace of Rn for some n and suppose that ⃗v is not an element of W. Let the orthogonal projection of ⃗v into W be the vector projW (⃗v) = ⃗p. Show that ⃗p is the element of W that is closest to ⃗v.
 
Topic
Geometry of Linear Maps
These pairs of pictures contrast the geometric action of the nonlinear maps f1(x) = ex and f2(x) = x2
5555
0000
with the linear maps h1(x) = 2x and h2(x) = −x. 5555
0000
-5 -5 -5 -5
Each of the four pictures shows the domain R on the left mapped to the codomain Rontheright. Arrowstracewhereeachmapsendsx=0,x=1,x=2,x=−1, and x = −2.
                         
294 Chapter Three. Maps Between Spaces
 The nonlinear maps distort the domain in transforming it into the range. For instance, f1(1) is further from f1(2) than it is from f1(0)—this map spreads the domain out unevenly so that a domain interval near x = 2 is spread apart more than is a domain interval near x = 0. The linear maps are nicer, more regular, in that for each map all of the domain spreads by the same factor. The map h1 on the left spreads all intervals apart to be twice as wide while on the right h2 keeps intervals the same length but reverses their orientation, as with the rising interval from 1 to 2 being transformed to the falling interval from −1 to −2.
The only linear maps from R to R are multiplications by a scalar but in higher dimensions more can happen. For instance, this linear transformation of R2 rotates vectors counterclockwise.
 
x  → xcosθ−ysinθ y xsinθ+ycosθ
−−−−−−−−−−−−−−−−−→
The transformation of R3 that projects vectors into the xz-plane is also not simply a rescaling.
x x y →0

zz
−−−−−−−→
Despite this additional variety, even in higher dimensions linear maps behave nicely. Consider a linear h: Rn → Rm and use the standard bases to represent it by a matrix H. Recall from Theorem V.2.6 that H factors into H = PBQ where P and Q are nonsingular and B is a partial-identity matrix. Recall also that nonsin- gular matrices factor into elementary matrices PBQ = TnTn−1 · · · TsBTs−1 · · · T1, which are matrices that come from the identity I after one Gaussian row opera- tion, so each T matrix is one of these three kinds
kρi ρi ↔ρj kρi +ρj
I −→ Mi (k) I −→ Pi,j I −→ Ci,j (k)
with i ̸= j, k ̸= 0. So if we understand the geometric effect of a linear map described by a partial-identity matrix and the effect of the linear maps described by the elementary matrices then we will in some sense completely understand the effect of any linear map. (The pictures below stick to transformations of R2 for ease of drawing but the principles extend for maps from any Rn to any Rm.)
           
Topic: Geometry of Linear Maps 295 The geometric effect of the linear transformation represented by a partial-
identity matrix is projection.
  1 0 0  
x010 x
000  y −−−−−→ y
z0
The geometric effect of the Mi(k) matrices is to stretch vectors by a factor
of k along the i-th axis. This map stretches by a factor of 3 along the x-axis. 
x  → 3x y y
−−−−−−−−→
If 0   k < 1 or if k < 0 then the i-th component goes the other way, here to the left.

x  → −2x y  y 
−−−−−−−−−→
Either of these stretches is a dilation.
A transformation represented by a Pi,j matrix interchanges the i-th and j-th
axes. This is reflection about the line xi = xj. 
y x x  → y
−−−−−−−→
Permutations involving more than two axes decompose into a combination of swaps of pairs of axes; see Exercise 7.
The remaining matrices have the form Ci,j(k). For instance C1,2(2) performs 2ρ1 +ρ2.
                                                              10      x21x
y −−−→ 2x + y
In the picture below, the vector ⃗u with the first component of 1 is affected less than the vector ⃗v with the first component of 2. The vector ⃗u is mapped to a h(⃗u) that is only 2 higher than ⃗u while h(⃗v) is 4 higher than ⃗v.
 h(⃗v) h ( u⃗ )
      u⃗

xx
  →  ⃗v y2x+y
−−−−−−−−−−−→
             
296 Chapter Three. Maps Between Spaces
 Any vector with a first component of 1 would be affected in the same way as ⃗u: it would slide up by 2. And any vector with a first component of 2 would slide up 4, as was ⃗v. That is, the transformation represented by Ci,j(k) affects vectors depending on their i-th component.
Another way to see this point is to consider the action of this map on the unit square. In the next picture, vectors with a first component of 0, such as the origin, are not pushed vertically at all but vectors with a positive first component slide up. Here, all vectors with a first component of 1, the entire right side of the square, slide to the same extent. In general, vectors on the same vertical line slide by the same amount, by twice their first component. The resulting shape has the same base and height as the square (and thus the same area) but the right angle corners are gone.

x → x y 2x + y
−−−−−−−−−−−→
For contrast, the next picture shows the effect of the map represented by C2,1(2). Here vectors are affected according to their second component:  x 
                  slides horizontally by twice y.
y
    
x  → x+2y y  y 
−−−−−−−−−−−→
              In general, for any Ci,j(k), the sliding happens so that vectors with the same i-th component are slid by the same amount. This kind of map is a shear.
With that we understand the geometric effect of the four types of matrices on the right-hand side of H = TnTn−1 ···TjBTj−1 ···T1 and so in some sense we understand the action of any matrix H. Thus, even in higher dimensions the geometry of linear maps is easy: it is built by putting together a number of components, each of which acts in a simple way.
We will apply this understanding in two ways. The first way is to prove something general about the geometry of linear maps. Recall that under a linear map, the image of a subspace is a subspace and thus the linear transformation h represented by H maps lines through the origin to lines through the origin. (The dimension of the image space cannot be greater than the dimension of the domain space, so a line can’t map onto, say, a plane.) We will show that h maps any line — not just one through the origin — to a line. The proof is simple: the partial-identity projection B and the elementary Ti’s each turn a line input into a line output; verifying the four cases is Exercise 5. Therefore their composition also preserves lines.
Topic: Geometry of Linear Maps 297
 The second way that we will apply the geometric understanding of linear maps is to elucidate a point from Calculus. Below is a picture of the action of the one-variable real function y(x) = x2 + x. As with the nonlinear functions pictured earlier, the geometric effect of this map is irregular in that at different domain points it has different effects; for example as the input x goes from 2 to −2, the associated output f(x) at first decreases, then pauses for an instant, and then increases.
55
00
But in Calculus we focus less on the map overall and more on the local effect of the map. Below we look closely at what this map does near x = 1. The derivative is dy/dx = 2x+1 so that near x = 1 we have ∆y ≈ 3·∆x. That is, in a neighborhood of x = 1, in carrying the domain over this map causes it to grow by a factor of 3—it is, locally, approximately, a dilation. The picture below shows this as a small interval in the domain (1 − ∆x .. 1 + ∆x) carried over to an interval in the codomain (2 − ∆y .. 2 + ∆y) that is three times as wide.
y=2
x=1
In higher dimensions the core idea is the same but more can happen. For a function y: Rn → Rm and a point ⃗x ∈ Rn, the derivative is defined to be the linear map h: Rn → Rm that best approximates how y changes near y(⃗x). So the geometry described above directly applies to the derivative.
We close by remarking how this point of view makes clear an often misun- derstood result about derivatives, the Chain Rule. Recall that, under suitable
         
298 Chapter Three. Maps Between Spaces conditions on the two functions, the derivative of the composition is this.
d(g◦f)(x)= dg(f(x))· df(x) dx dx dx
For instance the derivative of sin(x2 + 3x) is cos(x2 + 3x) · (2x + 3). Where does this come from? Consider f, g : R → R.
        g(f(x))
  f(x)
  x
The first map f dilates the neighborhood of x by a factor of df (x)
 dx
and the second map g follows that by dilating a neighborhood of f(x) by a factor
of
dg(f(x)) dx
 and when combined, the composition dilates by the product of the two. In higher dimensions the map expressing how a function changes near a point is a linear map, and is represented by a matrix. The Chain Rule multiplies the matrices.
Exercises
1 Use the H = PBQ decomposition to find the combination of dilations, flips, skews, and projections that produces the map h: R3 → R3 represented with respect to the standard bases by this matrix.
1 2 1 H=3 6 0
122
2 What combination of dilations, flips, skews, and projections produces a rotation counterclockwise by 2π/3 radians?
3 If a map is nonsingular then to get from its representation to the identity matrix we do not need any column operations, so that in H = PBQ the matrix Q is the identity. An example of a nonsingular map is the transformation t−π/4 : R2 → R2 that rotates vectors clockwise by π/4 radians.
(a) Find the matrix H representing this map with respect to the standard bases.
Topic: Geometry of Linear Maps 299
 (b) Use Gauss-Jordan to reduce H to the identity, without column operations. (c) Translate that to a matrix equation Tj Tj−1 · · · T1 H = I.
(d) Solve the matrix equation for H.
(e) Describe H as a combination of dilations, flips, skews, and projections (the
identity is a trivial projection).
4 Show that any linear transformation of R1 is a map hk that multiplies by a scalar
x  → kx.
5 Show that linear maps preserve the linear structures of a space.
(a) Show that for any linear map from Rn to Rm, the image of any line is a line. The image may be a degenerate line, that is, a single point.
(b) Show that the image of any linear surface is a linear surface. This generalizes the result that under a linear map the image of a subspace is a subspace.
(c) Linear maps preserve other linear ideas. Show that linear maps preserve “betweeness”: if the point B is between A and C then the image of B is between the image of A and the image of C.
6 Use a picture like the one that appears in the discussion of the Chain Rule to answer: if a function f: R → R has an inverse, what’s the relationship between how the function — locally, approximately — dilates space, and how its inverse dilates space (assuming, of course, that it has an inverse)?
7 Show that any permutation, any reordering, p of the numbers 1, . . . , n, the map x1  xp(1) 
x2  xp(2)  . → . 
 ..
xn xp(n)
can be done with a composition of maps, each of which only swaps a single pair of coordinates. Hint: you can use induction on n. (Remark: in the fourth chapter we will show this and we will also show that the parity of the number of swaps used is determined by p. That is, although a particular permutation could be expressed in two different ways with two different numbers of swaps, either both ways use an
even number of swaps, or both use an odd number.)
Topic
Magic Squares
A Chinese legend tells the story of a flood by the Lo river. People offered sacrifices to appease the river. Each time a turtle emerged, walked around the sacrifice, and returned to the water. Fuh-Hi, the founder of Chinese civilization, interpreted this to mean that the river was still cranky. Fortunately, a child noticed that on its shell the turtle had the pattern on the left below, which is today called Lo Shu (“river scroll”).
The dots make the matrix on the right where the rows, columns, and diagonals add to 15. Now that the people knew how much to sacrifice, the river’s anger cooled.
A square matrix is magic if each row, column, and diagonal add to the same number, the matrix’s magic number.
Another magic square appears in the engraving Melencolia I by Dürer.
     4
 9
 2
  3
 5
 7
  8
 1
 6
    
Topic: Magic Squares 301
 One interpretation is that it depicts melancholy, a depressed state. The figure, genius, has a wealth of fascinating things to explore including the compass, the geometrical solid, the scale, and the hourglass. But the figure is unmoved; all of the things lie unused. One of the potential delights, in the upper right, is a 4×4 matrix whose rows, columns, and diagonals add to 34.
The middle entries on the bottom row give 1514, the date of the engraving. The above two squares are arrangements of 1 ... n2. They are normal. The 1×1 square whose sole entry is 1 is normal, Exercise 2 shows that there is no nor- mal 2×2 magic square, and there are normal magic squares of every other size; see [Wikipedia, Magic Square]. Finding how many normal magic squares there are of
each size is an unsolved problem; see [Online Encyclopedia of Integer Sequences]. If we don’t require that the squares be normal then we can say much more. Every 1×1 square is magic, trivially. If the rows, columns, and diagonals of a
2×2 matrix
 a b  cd
add to s then a + b = s, c + d = s, a + c = s, b + d = s, a + d = s, and b + c = s. Exercise 2 shows that this system has the unique solution a = b = c = d = s/2. So the set of 2×2 magic squares is a one-dimensional subspace of M2×2.
A sum of two same-sized magic squares is magic and a scalar multiple of a magic square is magic so the set of n×n magic squares Mn is a vector space, a subspace of Mn×n. This Topic shows that for n   3 the dimension of Mn is n2 − n. The set Mn,0 of n×n magic squares with magic number 0 is another subspace and we will verify the formula for its dimension also: n2 − 2n − 1 when n   3.
We will first prove that dim Mn = dim Mn,0 + 1. Define the trace of a matrix to be the sum down its upper-left to lower-right diagonal Tr(M) = m1,1 + · · · + mn,n. Consider the restriction of the trace to the magic squares Tr: Mn → R. The null space N (Tr) is the set of magic squares with magic number zero Mn,0. Observe that the trace is onto because for any r in the codomain R the n×n matrix whose entries are all r/n is a magic square with magic number r. Theorem Two.II.2.14 says that for any linear map the dimension
    16
 3
2
  13
  5
 10
11
  8
  9
 6
7
  12
  4
 15
14
  1
    
302 Chapter Three. Maps Between Spaces
 of the domain equals the dimension of the range space plus the dimension of the null space, the map’s rank plus its nullity. Here the domain is Mn, the range space is R and the null space is Mn,0, so we have that dim Mn = 1 + dim Mn,0.
We will finish by finding the dimension of the vector space Mn,0. For n = 1 the dimension is clearly 0. Exercise 3 shows that dim Mn,0 is also 0 for n = 2. That leaves showing that dim Mn,0 = n2 − 2n − 1 for n   3. The fact that
the squares in this vector space are magic gives us a linear system of restrictions, and the fact that they have magic number zero makes this system homogeneous: for instance consider the 3×3 case. The restriction that the rows, columns, and diagonals of
a b c d e f ghi
add to zero gives this (2n + 2)×n2 linear system.
a+b+c =0 d+e+f =0 g+h+i=0 a+d+g=0 b +e +h=0 c +f +i=0 a +e +i=0 c+e+g =0
We will find the dimension of the space by finding the number of free variables in the linear system.
The matrix of coefficients for the particular cases of n = 3 and n = 4 are below, with the rows and columns numbered to help in reading the proof. With
2
respect to the standard basis, each represents a linear map h: Rn
The domain has dimension n2 so if we show that the rank of the matrix is 2n + 1 then we will have what we want, that the dimension of the null space Mn,0 is n2 −(2n+1).
123456789 ⃗ρ1   111 000 000 ⃗ρ2   000 111 000 ⃗ρ3   000 000 111
⃗ρ4   100 100 100 ⃗ρ5   010 010 010 ⃗ρ6   001 001 001
⃗ρ7   100 010 001 ⃗ρ8   001 010 100
→ R2n+2.
 
Topic: Magic Squares
303
 1234 ⃗ρ1   1 1 1 1 ⃗ρ2   0 0 0 0 ⃗ρ3   0 0 0 0 ⃗ρ4   0 0 0 0
⃗ρ5   1 0 0 0 ⃗ρ6   0 1 0 0 ⃗ρ7   0 0 1 0 ⃗ρ8   0 0 0 1
⃗ρ9   1 0 0 0 ⃗ρ10   0001
5678 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0
1  0  0  0
0  1  0  0
0  0  1  0
0  0  0  1
0 1 0 0 0010
9 101112 13141516 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 1 1 1 1
1 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 1
0 0 1 0 0 0 0 1 0100 1000
 We want to show that the rank of the matrix of coefficients, the number of rows in a maximal linearly independent set, is 2n + 1. The first n rows of the matrix of coefficients add to the same vector as the second n rows, the vector of all ones. So a maximal linearly independent must omit at least one row. We will show that the set of all rows but the first {⃗ρ2 ... ⃗ρ2n+2} is linearly independent. So consider this linear relationship.
c2⃗ρ2 + · · · + c2n⃗ρ2n + c2n+1⃗ρ2n+1 + c2n+2⃗ρ2n+2 = ⃗0 (∗)
Now it gets messy. Focus on the lower left of the tables. Observe that in the final two rows, in the first n columns, is a subrow that is all zeros except that it starts with a one in column 1 and a subrow that is all zeros except that it ends with a one in column n.
First, with ⃗ρ1 omitted, both column 1 and column n contain only two ones. Since the only rows in (∗) with nonzero column 1 entries are rows ⃗ρn+1 and ⃗ρ2n+1, which have ones, we must have c2n+1 = −cn+1. Likewise considering the n-th entries of the vectors in (∗) gives that c2n+2 = −c2n.
Next consider the columns between those two—in the n = 3 table this includes only column 2 while in the n = 4 table it includes both columns 2 and 3. Each such column has a single one. That is, for each column index j ∈ {2 . . . n − 2} the column consists of only zeros except for a one in row n + j, and hence cn+j = 0.
On to the next block of columns, from n + 1 through 2n. Column n + 1 has only two ones (because n   3 the ones in the last two rows do not fall in the first column of this block). Thus c2 = −cn+1 and therefore c2 = c2n+1. Likewise, from column 2n we conclude that c2 = −c2n and so c2 = c2n+2.
Because n   3 there is at least one column between column n + 1 and column 2n−1. In at least one of those columns a one appears in ⃗ρ2n+1. If a one also appears in that column in ⃗ρ2n+2 then we have c2 = −(c2n+1 +c2n+2) since
304 Chapter Three. Maps Between Spaces
 cn+j =0forj∈{2...n−2}. Ifaonedoesnotappearinthatcolumnin⃗ρ2n+2 then we have c2 = −c2n+1. In either case c2 = 0, and thus c2n+1 = c2n+2 = 0 and cn+1 = c2n = 0.
If the next block of n-many columns is not the last then similarly conclude from its first column that c3 = cn+1 = 0.
Keep this up until we reach the last block of columns, those numbered (n−1)n+1throughn2. Becausecn+1 =···=c2n =0columnn2 givesthat cn = −c2n+1 = 0.
Therefore the rank of the matrix is 2n + 1, as required.
The classic source on normal magic squares is [Ball & Coxeter]. More on the Lo Shu square is at [Wikipedia, Lo Shu Square]. The proof given here began with [Ward].
Exercises
1 Let M be a 3×3 magic square with magic number s.
(a) Prove that the sum of M’s entries is 3s.
(b) Prove that s = 3 · m2,2.
(c) Prove that m2,2 is the average of the entries in its row, its column, and in
each diagonal.
(d) Prove that m2,2 is the median of M’s entries.
2 Solvethesystema+b=s,c+d=s,a+c=s,b+d=s,a+d=s,andb+c=s. 3 Show that dim M2,0 = 0.
4 Let the trace function be Tr(M) = m1,1 + · · · + mn,n. Define also the sum down
the other diagonal Tr∗(M) = m1,n + · · · + mn,1.
(a) Show that the two functions Tr, Tr∗ : Mn×n → R are linear.
(b) Show that the function θ: Mn×n → R2 given by θ(M) = (Tr(M),Tr∗(m)) is
linear.
(c) Generalize the prior item.
5 A square matrix is semimagic if the rows and columns add to the same value, that is, if we drop the condition on the diagonals.
(a) Show that the set of semimagic squares Hn is a subspace of Mn×n.
(b) Show that the set Hn,0 of n×n semimagic squares with magic number 0 is also a subspace of Mn×n.
Topic
Markov Chains
Here is a simple game: a player bets on coin tosses, a dollar each time, and the game ends either when the player has no money or is up to five dollars. If the player starts with three dollars, what is the chance that the game takes at least five flips? Twenty-five flips?
At any point, this player has either $0, or $1, . . . , or $5. We say that the player is in the state s0, s1, . . . , or s5. In the game the player moves from state to state. For instance, a player now in state s3 has on the next flip a 0.5 chance of moving to state s2 and a 0.5 chance of moving to s4. The boundary states are different; a player never leaves state s0 or state s5.
Let pi(n) be the probability that the player is in state si after n flips. Then for instance the probability of being in state s0 after flip n + 1 is p0(n + 1) = p0(n) + 0.5 · p1(n). This equation summarizes.
1.0 0.5
0.0 0.0
 0.0 0.0 0.0 0.5 0.0 0.0 0.0 0.5 0.0 0.5 0.0 0.5 0.0 0.5 0.0 0.0 0.0 0.5
0.0 p0 (n) 0.0 p1 (n) 0.0 p2 (n) 0.0 p3 (n) 0.0 p4 (n) 1.0 p5(n)
p0 (n + 1) p1 (n + 1) p2 (n + 1) p3 (n + 1) p4 (n + 1) p5(n + 1)
0.0 0.5
 =
0.0 0.0 0.0 0.0 0.0 0.0
Sage will compute the evolution of this game.
sage: M = matrix(RDF, [[1.0, 0.5, 0.0, 0.0, 0.0, 0.0],
....: [0.5, 0.0, 0.5, 0.0, 0.0, 0.0],
....: [0.0, 0.5, 0.0, 0.5, 0.0, 0.0],
....: [0.0, 0.0, 0.5, 0.0, 0.5, 0.0],
....: [0.0, 0.0, 0.0, 0.5, 0.0, 0.5],
....: [0.0, 0.0, 0.0, 0.0, 0.5, 1.0]])
sage: M = M.transpose()
sage: v0 = vector(RDF, [0.0, 0.0, 0.0, 1.0, 0.0, 0.0]) sage: v1 = v0*M
sage: v1
(0.0, 0.0, 0.5, 0.0, 0.5, 0.0)
sage: v2 = v1*M
sage: v2
(0.0, 0.25, 0.0, 0.5, 0.0, 0.25)
              
306 Chapter Three. Maps Between Spaces
 (Two notes: (1) Sage can use various number systems to make the matrix entries and here we have used Real Double Float, and (2) Sage likes to do matrix multiplication from the right, as ⃗vM instead of our usual M⃗v, so we needed to take the matrix’s transpose.)
These are components of the resulting vectors.
 n = 0 0
0
0
1
0
0
n = 1 0
0 0.5 0 0.5 0
n = 2 0
n = 3 n = 4 0.125 0.125 0 0.187 5 0.375 0
· · ·
n = 24 0.396 00 0.002 76 0
0.004 47 0
0.596 76
  0.25
0
0.5 0 0.312 5
0 0.25
0.25 0 0.25 0.375
This game is not likely to go on for long since the player quickly moves to an ending state. For instance, after the fourth flip there is already a 0.50 probability that the game is over.
This is a Markov chain. Each vector is a probability vector, whose entries are nonnegative real numbers that sum to 1. The matrix is a transition matrix or stochastic matrix, whose entries are nonnegative reals and whose columns sum to 1.
A characteristic feature of a Markov chain model is that it is historyless in that the next state depends only on the current state, not on any prior ones. Thus, a player who arrives at s2 by starting in state s3 and then going to state s2 has exactly the same chance of moving next to s3 as does a player whose history wastostartins3 thengotos4 thentos3 andthentos2.
Here is a Markov chain from sociology. A study ([Macdonald & Ridge], p. 202) divided occupations in the United Kingdom into three levels: executives and professionals, supervisors and skilled manual workers, and unskilled workers. They asked about two thousand men, “At what level are you, and at what level was your father when you were fourteen years old?” Here the Markov model assumption about history may seem reasonable — we may guess that while a parent’s occupation has a direct influence on the occupation of the child, the grandparent’s occupation likely has no such direct influence. This summarizes the study’s conclusions.
.60 .29 .16pU(n) pU(n+1) .26 .37 .27 pM(n) = pM(n + 1) .14 .34 .57 pL(n) pL(n + 1)
For instance, looking at the middle class for the next generation, a child of an upper class worker has a 0.26 probability of becoming middle class, a child of
Topic: Markov Chains 307
 a middle class worker has a 0.37 chance of being middle class, and a child of a lower class worker has a 0.27 probability of becoming middle class.
Sage will compute the successive stages of this system (the current class distribution is ⃗v0).
 sage: M = matrix(RDF, [[0.60, 0.29,
....: [0.26, 0.37,
....: [0.14, 0.34, sage: M = M.transpose()
sage: v0 = vector(RDF, [0.12, 0.32, sage: v0*M
(0.2544, 0.3008, 0.4448)
sage: v0*M^2
(0.31104, 0.297536, 0.391424)
sage: v0*M^3
(0.33553728, 0.2966432, 0.36781952)
0.16], 0.27], 0.57]])
0.56])
          Here are the next five generations. They show upward mobility, especially in the first generation. In particular, lower class shrinks a good bit.
n=0 n=1 n=2 n=3 n=4 n=5
   .12 .25 .32 .30 .56 .44
.31 .34 .30 .30 .39 .37
.35 .35 .30 .30 .36 .35
One more example. In professional American baseball there are two leagues, the American League and the National League. At the end of the annual season the team winning the American League and the team winning the National League play the World Series. The winner is the first team to take four games. That means that a series is in one of twenty-four states: 0-0 (no games won yet by either team), 1-0 (one game won for the American League team and no games for the National League team), etc.
Consider a series with a probability p that the American League team wins each game. We have this.
 0 0 0
 p 0 0
1−p 0 0 
0 p 0  0 1−p p
0 ...p0-0(n) p0-0(n+1)
0 ...p1-0(n) p1-0(n+1)
0 ...p (n) p (n+1) 0-1 0-1 
0 . . . p2-0 (n) = p2-0 (n + 1)
0 ...p1-1(n) p1-1(n+1)
 0 0 1−p ......
An especially interesting special case is when the teams are evenly matched, p = 0.50. This table below lists the resulting components of the n = 0 through n = 7 vectors.
Note that evenly-matched teams are likely to have a long series — there is a probability of 0.625 that the series goes at least six games.
0 ...p0-2(n) p0-2(n+1)  . . . .  .   . 
308 Chapter Three. Maps Between Spaces
  n=0 n=1 n=2 n=3 n=4 n=5 n=6 n=7 0−010000000
  1−0 0 0−1 0 2−0 0 1−1 0 0−2 0 3−0 0 2−1 0 1−2 0 0−3 0 4−0 0 3−1 0 2−2 0 1−3 0 0−4 0 4−1 0 3−2 0 2−3 0 1−4 0 4−2 0 3−3 0 2−4 0 4−3 0 3−4 0
0.5 0 0.5 0
0 0.25 0 0.5 0 0.25 0 0
0 0
0 0
0 0
0 0
0 0
0 0
0 0
0 0
0 0
0 0
0 0
0 0
0 0
0 0
0 0
0 0
0 0
0 0 0
0 0 0
0 0 0
0 0 0
0 0 0 0.125 0 0 0.375 0 0 0.375 0 0 0.125 0 0
0 0.0625 0.0625 0 0.25 0
0 0.375 0
0 0.25 0
0 0.0625 0.0625 0 0 0.125 0 0 0.3125 0 0 0.3125 0 0 0.125 0 0 0
0 0 0
0 0 0
0 0 0
0 0 0
0 0
0 0
0 0
0 0
0 0
0 0
0 0
0 0
0 0 0.0625 0.0625 0 0
0 0
0 0 0.0625 0.0625 0.125 0.125
0 0
0 0
0.125 0.125 0.156 25 0.156 25 0.3125 0
0.156 25 0.156 25 0 0.15625 0 0.15625
Markov chains are a widely used application of matrix operations. They also give us an example of the use of matrices where we do not consider the significance of the maps represented by the matrices. For more on Markov chains, there are many sources such as [Kemeny & Snell] and [Iosifescu].
Exercises
1 These questions refer to the coin-flipping game.
(a) Check the computations in the table at the end of the first paragraph.
(b) Consider the second row of the vector table. Note that this row has alternating
0’s. Must p1(j) be 0 when j is odd? Prove that it must be, or produce a
counterexample.
(c) Perform a computational experiment to estimate the chance that the player
ends at five dollars, starting with one dollar, two dollars, and four dollars.
2 [Feller] We consider throws of a die, and say the system is in state si if the largest number yet appearing on the die was i.
(a) Give the transition matrix.
(b) Start the system in state s1, and run it for five throws. What is the vector at
the end?
Topic: Markov Chains 309
 3 [Kelton] There has been much interest in whether industries in the United States are moving from the Northeast and North Central regions to the South and West, motivated by the warmer climate, by lower wages, and by less unionization. Here is the transition matrix for large firms in Electric and Electronic Equipment.
NE NC S W Z
   NE 0.787 0 NC 0 0.966 S 0 0.063 W 0 0
Z 0.021 0.009
0 0.111 0.034 0 0.937 0 0.074 0.612 0.005 0.010
0.102 0
0 0.314 0.954
For example, a firm in the Northeast region will be in the
year with probability 0.111. (The Z entry is a “birth-death” state. For instance, with probability 0.102 a large Electric and Electronic Equipment firm from the Northeast will move out of this system next year: go out of business, move abroad, or move to another category of firm. There is a 0.021 probability that a firm in the National Census of Manufacturers will move into Electronics, or be created, or move in from abroad, into the Northeast. Finally, with probability 0.954 a firm out of the categories will stay out, according to this research.)
(a) Does the Markov model assumption of lack of history seem justified?
(b) Assume that the initial distribution is even, except that the value at Z is 0.9.
Compute the vectors for n = 1 through n = 4. (c) Suppose that the initial distribution is this.
NE NC S W Z
0.0000 0.6522 0.3478 0.0000 0.0000 Calculate the distributions for n = 1 through n = 4.
(d) Find the distribution for n = 50 and n = 51. Has the system settled down to an equilibrium?
4 [Wickens] Here is a model of some kinds of learning The learner starts in an undecided state sU. Eventually the learner has to decide to do either response A (that is, end in state sA) or response B (ending in sB). However, the learner doesn’t jump right from undecided to sure that A is the correct thing to do (or B). Instead, the learner spends some time in a “tentative-A” state, or a “tentative-B” state, trying the response out (denoted here tA and tB). Imagine that once the learner has decided, it is final, so once in sA or sB, the learner stays there. For the other state changes, we can posit transitions with probability p in either direction.
(a) Construct the transition matrix.
(b) Take p = 0.25 and take the initial vector to be 1 at sU. Run this for five steps.
What is the chance of ending up at sA?
(c) Do the same for p = 0.20.
(d) Graph p versus the chance of ending at sA. Is there a threshold value for p,
above which the learner is almost sure not to take longer than five steps?
5 A certain town is in a certain country (this is a hypothetical problem). Each year ten percent of the town dwellers move to other parts of the country. Each year one percent of the people from elsewhere move to the town. Assume that there are two
West region next
 
310
Chapter Three. Maps Between Spaces
 states sT , living in town, and sC, living elsewhere.
(a) Construct the transition matrix.
(b) Starting with an initial distribution sT = 0.3 and sC = 0.7, get the results for
the first ten years.
(c) Do the same for sT = 0.2.
(d) Are the two outcomes alike or different?
6 For the World Series application, use a computer to generate the seven vectors for p=0.55 and p=0.6.
(a) What is the chance of the National League team winning it all, even though they have only a probability of 0.45 or 0.40 of winning any one game?
(b) Graph the probability p against the chance that the American League team wins it all. Is there a threshold value—a p above which the better team is essentially ensured of winning?
7 Above we define a transition matrix to have each entry nonnegative and each column sum to 1.
(a) Check that the three transition matrices shown in this Topic meet these two conditions. Must any transition matrix do so?
(b) Observe that if A⃗v0 = ⃗v1 and A⃗v1 = ⃗v2 then A2 is a transition matrix from ⃗v0 to ⃗v2. Show that a power of a transition matrix is also a transition matrix. (c) Generalize the prior item by proving that the product of two appropriately-
sized transition matrices is a transition matrix.
Topic
Orthonormal Matrices
In The Elements, Euclid considers two figures to be the same if they have the same size and shape. That is, while the triangles below are not equal because they are not the same set of points, they are, for Euclid’s purposes, essentially indistinguishable because we can imagine picking the plane up, sliding it over and rotating it a bit, although not warping or stretching it, and then putting it back down, to superimpose the first figure on the second. (Euclid never explicitly states this principle but he uses it often [Casey].)
P2
P1 P Q1
3
 Q2
Q3
  In modern terms “picking the plane up . . . ” is taking a map from the plane to itself. Euclid considers only transformations that may slide or turn the plane but not bend or stretch it. Accordingly, define a map f: R2 → R2 to be distance- preserving or a rigid motion or an isometry if for all points P1, P2 ∈ R2, the distance from f(P1) to f(P2) equals the distance from P1 to P2. We also define a plane figure to be a set of points in the plane and we say that two figures are congruent if there is a distance-preserving map from the plane to itself that carries one figure onto the other.
Many statements from Euclidean geometry follow easily from these definitions. Some are: (i) collinearity is invariant under any distance-preserving map (that is, if P1, P2, and P3 are collinear then so are f(P1), f(P2), and f(P3)), (ii) betweeness is invariant under any distance-preserving map (if P2 is between P1 and P3 then so is f(P2) between f(P1) and f(P3)), (iii) the property of being a triangle is invariant under any distance-preserving map (if a figure is a triangle then the image of that figure is also a triangle), (iv) and the property of being a circle is invariant under any distance-preserving map. In 1872, F. Klein suggested that we can define Euclidean geometry as the study of properties that are invariant
312 Chapter Three. Maps Between Spaces
 under these maps. (This forms part of Klein’s Erlanger Program, which proposes the organizing principle that we can describe each kind of geometry — Euclidean, projective, etc.—as the study of the properties that are invariant under some group of transformations. The word ‘group’ here means more than just ‘collection’ but that lies outside of our scope.)
We can use linear algebra to characterize the distance-preserving maps of the plane.
To begin, observe that there are distance-preserving transformations of the plane that are not linear. The obvious example is this translation.
 x   x   1   x + 1  y  → y + 0 = y
However, this example turns out to be the only one, in that if f is distance- preserving and sends ⃗0 to ⃗v0 then the map ⃗v  → f(⃗v) − ⃗v0 is linear. That will follow immediately from this statement: a map t that is distance-preserving and sends ⃗0 to itself is linear. To prove this equivalent statement, consider the standard basis and suppose that
 a   c  t(⃗e1) = b t(⃗e2) = d
for some a,b,c,d ∈ R. To show that t is linear we can show that it can be represented by a matrix, that is, that t acts in this way for all x, y ∈ R.
    
x t ax+cy
⃗v= y  −→ bx+dy (∗)
Recall that if we fix three non-collinear points then we can determine any point by giving its distance from those three. So we can determine any point ⃗v in the domain by its distance from ⃗0, ⃗e1, and ⃗e2. Similarly, we can determine any point t(⃗v) in the codomain by its distance from the three fixed points t(⃗0), t(⃗e1), and t(⃗e2) (these three are not collinear because, as mentioned above, collinearity is invariant and ⃗0, ⃗e1, and ⃗e2 are not collinear). Because t is distance-preserving we can say more: for the point ⃗v in the plane that is determined by being the distance d0 from ⃗0, the distance d1 from ⃗e1, and the distance d2 from ⃗e2, its image t(⃗v) must be the unique point in the codomain that is determined by being d0 from t(⃗0), d1 from t(⃗e1), and d2 from t(⃗e2). Because of the uniqueness, checking that the action in (∗) works in the d0, d1, and d2 cases
 x   x   ax + cy  dist( y ,⃗0)=dist(t( y ),t(⃗0))=dist( bx+dy ,⃗0)
Topic: Orthonormal Matrices 313 (we assumed that t maps ⃗0 to itself)
 x   x   ax+cy   a  dist( y ,⃗e1)=dist(t( y ),t(⃗e1))=dist( bx+dy , b )
 and
dist( y ,⃗e2)=dist(t( y ),t(⃗e2))=dist( bx+dy , d )
suffices to show that (∗) describes t. Those checks are routine.
Thus any distance-preserving f: R2 → R2 is a linear map plus a translation, f(⃗v) = t(⃗v) + ⃗v0 for some constant vector ⃗v0 and linear map t that is distance-
preserving. So in order to understand distance-preserving maps what remains is to understand distance-preserving linear maps.
Not every linear map is distance-preserving. For example ⃗v  → 2⃗v does not preserve distances.
But there is a neat characterization: a linear transformation t of the plane is distance-preserving if and only if both ∥t(⃗e1)∥ = ∥t(⃗e2)∥ = 1, and t(⃗e1) is orthogonal to t(⃗e2). The ‘only if’ half of that statement is easy—because t is distance-preserving it must preserve the lengths of vectors and because t is distance-preserving the Pythagorean theorem shows that it must preserve orthogonality. To show the ‘if’ half we can check that the map preserves lengths of vectors because then for all ⃗p and ⃗q the distance between the two is preserved ∥t(⃗p−⃗q)∥=∥t(⃗p)−t(⃗q)∥=∥⃗p−⃗q∥. Forthatchecklet
 x   a   c  ⃗v = y t ( ⃗e 1 ) = b t ( ⃗e 2 ) = d
andwiththe‘if’assumptionsthata2+b2 =c2+d2 =1andac+bd=0we have this.
∥t(⃗v)∥2 =(ax+cy)2 +(bx+dy)2
=a2x2 +2acxy+c2y2 +b2x2 +2bdxy+d2y2 =x2(a2 +b2)+y2(c2 +d2)+2xy(ac+bd)
= x2 + y2
= ∥ ⃗v ∥ 2
One thing that is neat about this characterization is that we can easily recognize matrices that represent such a map with respect to the standard bases: the columns are of length one and are mutually orthogonal. This is an orthonormal matrix (or, more informally, orthogonal matrix since people
 x   x   ax+cy   c 
314 Chapter Three. Maps Between Spaces often use this term to mean not just that the columns are orthogonal but also
that they have length one).
We can leverage this characterization to understand the geometric actions of distance-preserving maps. Because ∥t(⃗v )∥ = ∥⃗v ∥, the map t sends any ⃗v somewhere on the circle about the origin that has radius equal to the length of ⃗v. In particular, ⃗e1 and ⃗e2 map to the unit circle. What’s more, once we fix the unit vector ⃗e1 as mapped to the vector with components a and b then there are only two places where ⃗e2 can go if its image is to be perpendicular to the first vector’s image: it can map either to one where ⃗e2 maintains its position a quarter circle clockwise from ⃗e1
  −b 
a  a 
b
 a −b  RepE2,E2(t)= b a
 or to one where it goes a quarter circle counterclockwise.
 −b 
a  a 
b
    
x t xcosθ−ysinθ y  −→ xsinθ+ycosθ
 a  b
 b  −a
 a b  RepE2,E2(t)= b −a
 The geometric description of these two cases is easy. Let θ be the counter- clockwise angle between the x-axis and the image of ⃗e1. The first matrix above represents, with respect to the standard bases, a rotation of the plane by θ radians.
 The second matrix above represents a reflection of the plane through the line bisecting the angle between ⃗e1 and t(⃗e1).
Topic: Orthonormal Matrices
315
  a  b
 b  −a
    
x t xcosθ+ysinθ y  −→ xsinθ−ycosθ
 (This picture shows ⃗e1 reflected up into the first quadrant and ⃗e2 reflected down into the fourth quadrant.)
Note: in the domain the angle between ⃗e1 and ⃗e2 runs counterclockwise, and in the first map above the angle from t(⃗e1) to t(⃗e2) is also counterclockwise, so it preserves the orientation of the angle. But the second map reverses the orientation. A distance-preserving map is direct if it preserves orientations and opposite if it reverses orientation.
With that, we have characterized the Euclidean study of congruence. It considers, for plane figures, the properties that are invariant under combinations of (i) a rotation followed by a translation, or (ii) a reflection followed by a translation (a reflection followed by a non-trivial translation is a glide reflection).
Another idea encountered in elementary geometry, besides congruence of figures, is that figures are similar if they are congruent after a change of scale. The two triangles below are similar since the second is the same shape as the first but 3/2-ths the size.
P2
P1 P Q1
3
Q2
Q3
  From the above work we have that figures are similar if there is an orthonormal matrix T such that the points ⃗q on one figure are the images of the points ⃗p on the other figure by ⃗q = (kT )⃗v + ⃗p0 for some nonzero real number k and constant vector ⃗p0.
Although these ideas are from Euclid, mathematics is timeless and they are still in use today. One application of the maps studied above is in computer graphics. We can, for example, animate this top view of a cube by putting together film frames of it rotating; that’s a rigid motion.
   Frame 1 Frame 2 Frame 3
316 Chapter Three. Maps Between Spaces We could also make the cube appear to be moving away from us by producing
film frames of it shrinking, which gives us figures that are similar.
Frame 1: Frame 2: Frame 3:
Computer graphics incorporates techniques from linear algebra in many other ways (see Exercise 4).
A beautiful book that explores some of this area is [Weyl]. More on groups, of transformations and otherwise, is in any book on Modern Algebra, for instance [Birkhoff & MacLane]. More on Klein and the Erlanger Program is in [Yaglom].
Exercises
(a)
(b)
(c)
− 2/ 3 −1/ 3
2 Write down the formula for each of these distance-preserving maps.
    1 Decide if each of these is an orthonormal matrix.
  1/√2 −1/√2  √√
    −1/ 2 −1/ 2   1/√3 −1/√3 
  3
(a) the map that rotates π/6 radians, and then translates by ⃗e2
(b) the map that reflects about the line y = 2x
(c) the map that reflects about y = −2x and translates over 1 and up 1
(a) The proof that a map that is distance-preserving and sends the zero vector to itself incidentally shows that such a map is one-to-one and onto (the point in the domain determined by d0, d1, and d2 corresponds to the point in the codomain determined by those three). Therefore any distance-preserving map has an inverse. Show that the inverse is also distance-preserving.
√√
  −1/ 3 −1/ 3
  1/√3 −√2/√3 
   √√√
   (b) Prove that congruence is an equivalence relation between plane figures.
4 In practice the matrix for the distance-preserving linear transformation and the translation are often combined into one. Check that these two computations yield
the same first two components.
 a c  x   e  a c ex b d y + f b d fy
0011
(These are homogeneous coordinates; see the Topic on Projective Geometry).
5
(a) Verify that the properties described in the second paragraph of this Topic as
invariant under distance-preserving maps are indeed so.
(b) Give two more properties that are of interest in Euclidean geometry from
your experience in studying that subject that are also invariant under distance-
preserving maps.
(c) Give a property that is not of interest in Euclidean geometry and is not
invariant under distance-preserving maps.
Chapter Four
Determinants
In the first chapter we highlighted the special case of linear systems with the same number of equations as unknowns, those of the form T⃗x = ⃗b where T is a square matrix. We noted that there are only two kinds of T’s. If T is associated with a unique solution for any ⃗b, such as for the homogeneous system T⃗x = ⃗0, then T is associated with a unique solution for every such ⃗b. We call such a matrix nonsingular. The other kind of T, where every linear system for which it is the matrix of coefficients has either no solution or infinitely many solutions, we call singular.
In our work since then this distinction has been a theme. For instance, we now know that an n×n matrix T is nonsingular if and only if each of these holds:
• any system T⃗x = ⃗b has a solution and that solution is unique;
• Gauss-Jordan reduction of T yields an identity matrix;
• the rows of T form a linearly independent set;
• the columns of T form a linearly independent set, a basis for Rn; • any map that T represents is an isomorphism;
• an inverse matrix T−1 exists.
So when we look at a square matrix, one of the first things that we ask is whether it is nonsingular.
This chapter develops a formula that determines whether T is nonsingular. More precisely, we will develop a formula for 1×1 matrices, one for 2×2 matrices, etc. These are naturally related; that is, we will develop a family of formulas, a scheme that describes the formula for each size.
Since we will restrict the discussion to square matrices, in this chapter we will often simply say ‘matrix’ in place of ‘square matrix’.
 
318 Chapter Four. Determinants
 I Definition
Determining nonsingularity is trivial for 1×1 matrices.  a  is nonsingular iff a ̸= 0
Corollary Three.IV.4.11 gives the 2×2 formula.
tation is intricate (see Exercise 10).
a b c
d e f isnonsingulariff aei+bfg+cdh−hfa−idb−gec̸=0
ghi
With these cases in mind, we posit a family of formulas: a, ad−bc, etc. For each n the formula defines a determinant function detn×n : Mn×n → R such that an n×n matrix T is nonsingular if and only if detn×n(T) ̸= 0. (We usually omit the subscript n×n because the size of T describes which determinant function we mean.)
I.1 Exploration
This subsection is an optional motivation and development of the general definition. The definition is in the next subsection.
Above, in each case the matrix is nonsingular if and only if some formula is nonzero. But the three formulas don’t show an obvious pattern. We may spot that the 1×1 term a has one letter, that the 2×2 terms ad and bc have two letters, and that the 3×3 terms each have three letters. We may even spot that in those terms there is a letter from each row and column of the matrix, e.g., in the cdh term one letter comes from each row and from each column.
 c d 
h
But these observations are perhaps more puzzling than enlightening. For instance, we might wonder why some terms are added but some are subtracted.
 a b  cd
is nonsingular iff ad − bc ̸= 0
We can produce the 3×3 formula as we did the prior one, although the compu-
Section I. Definition 319
 A good strategy for solving problems is to explore which properties the solution must have, and then search for something with those properties. So we shall start by asking what properties we’d like the determinant formulas to have.
At this point, our main way to decide whether a matrix is singular or not is to do Gaussian reduction and then check whether the diagonal of the echelon form matrix has any zeroes, that is, whether the product down the diagonal is zero. So we could guess that whatever determinant formula we find, the proof that it is right may involve applying Gauss’s Method to the matrix to show that in the end the product down the diagonal is zero if and only if our formula gives zero.
This suggests a plan: we will look for a family of determinant formulas that are unaffected by row operations and such that the determinant of an echelon form matrix is the product of its diagonal entries. In the rest of this subsection we will test this plan against the 2×2 and 3×3 formulas. In the end we will have to modify the “unaffected by row operations” part, but not by much.
First we check whether the 2×2 and 3×3 formulas are unaffected by the row operation of combining: if
kρi +ρj ˆ T −→ T
then is det(Tˆ) = det(T)? This check of the 2×2 determinant after the kρ1 + ρ2 operation
 a b 
det( ka+c kb+d )=a(kb+d)−(ka+c)b=ad−bc
shows that it is indeed unchanged, and the other 2×2 combination kρ2 + ρ1 gives the same result. Likewise, the 3×3 combination kρ3 + ρ2 leaves the determinant unchanged
abc
det(kg+d kh+e ki+f)=a(kh+e)i+b(ki+f)g+c(kg+d)h
g h i −h(ki+f)a−i(kg+d)b−g(kh+e)c = aei + bfg + cdh − hfa − idb − gec
as do the other 3×3 row combination operations.
So there seems to be promise in the plan. Of course, perhaps if we had
worked out the 4×4 determinant formula and tested it then we might have found that it is affected by row combinations. This is an exploration and we do not yet have all the facts. Nonetheless, so far, so good.
Next we compare det(Tˆ) with det(T) for row swaps. Here we hit a snag: the
320 Chapter Four. Determinants 2×2 row swap ρ1 ↔ ρ2 does not yield ad − bc.
 c d 
det( a b )=bc−ad
And this ρ1 ↔ ρ3 swap inside of a 3×3 matrix
g h i
det(d e f)=gec+hfa+idb−bfg−cdh−aei
abc
also does not give the same determinant as before the swap since again there is a sign change. Trying a different 3×3 swap ρ1 ↔ ρ2
d e f
det(a b c)=dbi+ecg+fah−hcd−iae−gbf
ghi
also gives a change of sign.
So row swaps appear in this experiment to change the sign of a determinant.
This does not wreck our plan entirely. We hope to decide nonsingularity by considering only whether the formula gives zero, not by considering its sign. Therefore, instead of expecting determinant formulas to be entirely unaffected by row operations we modify our plan so that on a swap they will change sign.
Obviously we finish by comparing det(Tˆ) with det(T) for the operation of multiplying a row by a scalar. This
 a b 
det( kc kd )=a(kd)−(kc)b=k·(ad−bc)
ends with the entire determinant multiplied by k, and the other 2×2 case has the same result. This 3×3 case ends the same way
abc
det( d e f ) = ae(ki) + bf(kg) + cd(kh)
kg kh ki −(kh)fa − (ki)db − (kg)ec
= k · (aei + bfg + cdh − hfa − idb − gec)
as do the other two 3×3 cases. These make us suspect that multiplying a row by k multiplies the determinant by k. As before, this modifies our plan but does not wreck it. We are asking only that the zero-ness of the determinant formula be unchanged, not focusing on the its sign or magnitude.
So in this exploration our plan got modified in some inessential ways and is now: we will look for n×n determinant functions that remain unchanged under
 
Section I. Definition 321
 the operation of row combination, that change sign on a row swap, that rescale on the rescaling of a row, and such that the determinant of an echelon form matrix is the product down the diagonal. In the next two subsections we will see that for each n there is one and only one such function.
Finally, for the next subsection note that factoring out scalars is a row-wise operation: here
3 3 9 1 1 3 det(2 1 1 )=3·det(2 1 1 )
5 11 −5 5 11 −5
the 3 comes only out of the top row only, leaving the other rows unchanged. Consequently in the definition of determinant we will write it as a function of the rows det(⃗ρ1,⃗ρ2,...⃗ρn), rather than as det(T) or as a function of the entries det(t1,1, . . . , tn,n).
Exercises
  1.1 Evaluate the determinant of each.
 3 1  2 0 1 4 0 1
(a) −1 1 (b)3 1 1 (c)0 0 1 −1 0 1 1 3 −1
1.2 Evaluate the determinant of each.
 2 0  2 1 1 2 3 4
(a) −1 3 (b)0 5 −2 (c)5 6 7 1 −3 4 8 9 1
  1.3 Verify that the determinant of an upper-triangular 3×3 matrix is the product down the diagonal.
a b c
det(0 e f) = aei
00i
Do lower-triangular matrices work the same way?
  1.4 Use the determinant to decide if each is singular or nonsingular.
(a)
 2 1  3 1
(b)
 0 1  1 −1
(c)
 4 2  2 1
1.5 Singular or nonsingular? Use the determinant to decide. 2 1 1 1 0 1 2 1 0 (a)3 2 2 (b)2 1 1 (c)3 −2 0
014 413 100
  1.6 Each pair of matrices differ by one row operation. Use this operation to compare
det(A) with det(B).
 12   12 
(a)A= 2 3,B= 0 −1
3 1 0 3 1 0 (b) A=0 0 1, B=0 1 2
012 001
322
Chapter Four. Determinants
 1−13 1−13 (c) A=2 2 −6, B=1 1 −3
104 104
  1.7 Find the determinant of this 4×4 matrix by following the plan: perform Gauss’s
Method and look for the determinant to remain unchanged on a row combination, to change sign on a row swap, to rescale on the rescaling of a row, and such that the determinant of the echelon form matrix is the product down its diagonal.
1.8 Show this.
1 2 0 2 2 4 1 0 0 0 −1 3 3 −1 1 4
111
det( a b c ) = (b − a)(c − a)(c − b)
a2 b2 c2
  1.9 Which real numbers x make this matrix singular?
 12−x 4   8 8−x
1.10 Do the Gaussian reduction to check the formula for 3×3 matrices stated in the preamble to this section.
a b c
d e fisnonsingulariffaei+bfg+cdh−hfa−idb−gec̸=0
ghi
1.11 Show that the equation of a line in R2 thru (x1,y1) and (x2,y2) is given by
this determinant.
x y 1
det(x1 y1 1) = 0 x1 ̸= x2
x2 y2 1
1.12 Many people have learned, perhaps in Calculus, this mnemonic for the determi-
nant of a 3×3 matrix: first repeat the first two columns, then sum the products on the forward diagonals, and then subtract the products on the backward diagonals. That is, first write
 and then calculate this.
h1,1 h1,2 h1,3 h1,1 h2,1 h2,2 h2,3 h2,1 h3,1 h3,2 h3,3 h3,1
h1,2  h2,2  h3,2
  h1,1h2,2h3,3 + h1,2h2,3h3,1 + h1,3h2,1h3,2 −h3,1h2,2h1,3 − h3,2h2,3h1,1 − h3,3h2,1h1,2
(a) Check that this agrees with the formula given in the preamble to this section.
(b) Does it extend to other-sized determinants? 1.13 The cross product of the vectors
x1  ⃗x = x2
y1  ⃗y = y2
x3
y3
Section I. Definition
323
 is the vector computed as this determinant.
 ⃗e 1 ⃗x × ⃗y = det(x1
⃗e 2 ⃗e 3 
Note that the first row’s entries are vectors, the vectors from the standard basis for
R3. Show that the cross product of two vectors is perpendicular to each vector. 1.14 Prove that each statement holds for 2×2 matrices.
(a) The determinant of a product is the product of the determinants det(ST) = det(S) · det(T ).
(b) If T is invertible then the determinant of the inverse is the inverse of the determinant det(T−1) = (det(T))−1.
Matrices T and T′ are similar if there is a nonsingular matrix P such that T′ = PTP−1. (We shall look at this relationship in Chapter Five.) Show that similar 2×2 matrices have the same determinant.
  1.15 Prove that the area of this region in the plane  x2 
x2 x3 ) y1 y2 y3
 y2
is equal to the value of this determinant.
 x1  y1
Compare with this.
 x1 x2  det( y y )
12
 x2 x1  det( y y )
21
1.16 Prove that for 2×2 matrices, the determinant of a matrix equals the determinant of its transpose. Does that also hold for 3×3 matrices?
1.17 Is the determinant function linear—is det(x·T +y·S) = x·det(T)+y·det(S)? 1.18 Show that if A is 3×3 then det(c · A) = c3 · det(A) for any scalar c.
1.19 Which real numbers θ make
 cos θ − sin θ  sin θ cos θ
singular? Explain geometrically.
? 1.20 [Am. Math. Mon., Apr. 1955] If a third order determinant has elements 1, 2,
. . . , 9, what is the maximum value it may have?
I.2 Properties of Determinants
We want a formula to determine whether an n×n matrix is nonsingular. We will not begin by stating such a formula. Instead we will begin by considering, for
324 Chapter Four. Determinants
 each n, the function that such a formula calculates. We will define this function by a list of properties. We will then prove that a function with these properties exists and is unique, and also describe how to compute it. (Because we will eventually prove this, from the start we will just say ‘det(T)’ instead of ‘if there is a unique determinant function then det(T)’.)
 2.1 Definition A n×n determinant is a function det: Mn×n → R such that (1) det(⃗ρ1,...,k·⃗ρi+⃗ρj,...,⃗ρn)=det(⃗ρ1,...,⃗ρj,...,⃗ρn)fori̸=j
(2) det(⃗ρ1,...,⃗ρj,...,⃗ρi,...,⃗ρn) = −det(⃗ρ1,...,⃗ρi,...,⃗ρj,...,⃗ρn) for i ̸= j (3) det(⃗ρ1,...,k⃗ρi,...,⃗ρn)=k·det(⃗ρ1,...,⃗ρi,...,⃗ρn)foranyscalark
(4) det(I) = 1 where I is an identity matrix
(the ⃗ρ’s are the rows of the matrix). We often write |T| for det(T).
2.2 Remark Condition (2) is redundant since
ρi +ρj −ρj +ρi ρi +ρj −ρi
ˆ
T −→ −→ −→ −→ T
swaps rows i and j. We have listed it for consistency with the Gauss’s Method
presentation in earlier chapters.
2.3 Remark Condition (3) does not have a k ̸= 0 restriction, although the Gauss’s Method operation of multiplying a row by k does have it. The next result shows that we do not need that restriction here.
Proof To verify the first sentence swap the two equal rows. The sign of the determinant changes but the matrix is the same and so its determinant is the same. Thus the determinant is zero.
For the second sentence multiply the zero row by two. That doubles the de- terminant but it also leaves the row unchanged, and hence leaves the determinant unchanged. Thus the determinant must be zero.
Do Gauss-Jordan reduction for the third sentence, T → · · · → Tˆ. By the first three properties the determinant of T is zero if and only if the determinant of Tˆ is zero (although the two could differ in sign or magnitude). A nonsingular matrix T Gauss-Jordan reduces to an identity matrix and so has a nonzero determinant.
 2.4 Lemma A matrix with two identical rows has a determinant of zero. A matrix with a zero row has a determinant of zero. A matrix is nonsingular if and only if its determinant is nonzero. The determinant of an echelon form matrix is the product down its diagonal.
Section I. Definition 325
A singular T reduces to a Tˆ with a zero row; by the second sentence of this lemma its determinant is zero.
The fourth sentence has two cases. If the echelon form matrix is singular then it has a zero row. Thus it has a zero on its diagonal and the product down its diagonal is zero. By the third sentence of this result the determinant is zero and therefore this matrix’s determinant equals the product down its diagonal.
If the echelon form matrix is nonsingular then none of its diagonal entries is zero. This means that we can divide by those entries and use condition (3) to get 1’s on the diagonal.
  t t
  1,1 1,2
t   1,n 
 1 t /t
t /t   1,n 1,1 
1
..
Then the Jordan half of Gauss-Jordan elimination leaves the identity matrix.
 1 0 0    
 0   1,1 2,2 n,n  
t2,n/t2,2   
t2,n 
  0 tn,n   0 1  
  0 t2,2
   .  . 
..
=t·t···t·
 0 1 0 
=t1,1 ·t2,2···tn,n ·  ..  =t1,1 ·t2,2···tn,n ·1
  .   0 1 
So in this case also, the determinant is the product down the diagonal. QED
That gives us a way to compute the value of a determinant function on a matrix: do Gaussian reduction, keeping track of any changes of sign caused by row swaps and any scalars that we factor out, and finish by multiplying down the diagonal of the echelon form result. This algorithm is as fast as Gauss’s Method and so is practical on all of the matrices that we will see.
2.5 Example Doing 2×2 determinants with Gauss’s Method     
  2 4   2 4 
    =     = 10  −1 3   0 5 
doesn’t give a big time savings because the 2×2 determinant formula is easy. However, a 3×3 determinant is often easier to calculate with Gauss’s Method than with its formula.
 2 2 6   2 2 6   2 2 6        
 4 4 3 = 0 0 −9 =− 0 −3 5 =−54  0−35  0−35   00−9 
  1,2
1,1
326 Chapter Four. Determinants 2.6 Example Determinants bigger than 3×3 go quickly with the Gauss’s Method
procedure.
 1 0 1 3   1 0 1 3   1 0 1 3        
  0 1 1 4   0 1 1     =  
 0 0 0 5   0 0 0
4   0 1 1 4 
  = −     = −(−5) = 5
5   0 0 −1 −3   0 1 0 1   0 0 −1 −3   0 0 0 5 
The prior example illustrates an important point. Although we have not yet found a 4×4 determinant formula, if one exists then we know what value it gives to the matrix — if there is a function with properties (1) – (4) then on the above matrix the function must return 5.
2.7 Lemma For each n, if there is an n×n determinant function then it is unique.
Proof Perform Gauss’s Method on the matrix, keeping track of how the sign alternates on row swaps and any row-scaling factors, and then multiply down the diagonal of the echelon form result. By the definition and the lemma, all n×n determinant functions must return this value on the matrix. QED
The ‘if there is an n×n determinant function’ emphasizes that, although we can use Gauss’s Method to compute the only value that a determinant function could possibly return, we haven’t yet shown that such a function exists for all n. The rest of this section does that.
Exercises
For these, assume that an n×n determinant function exists for all n.   2.8 Find each determinant by performing one row operation.
   
  1−212   2−4 1 0 
 1 (b)  0
1 −2  04 
(a)      00−10 
3 −6 
  2.9 Use Gauss’s Method to find each determinant.
 0 0 0 5     
 0  1001 
 2110     
 −1010   1110 
 312  (a)  3 1 0 
     014 
(b)
2.10 Use Gauss’s Method to find each.  2 −1   1 1 0 
(a)     (b)  3 0 2   −1 −1   5 2 2 
2.11 For which values of k does this system have a unique solution? x + z−w=2
y − 2z = 3 x +kz =4 z−w=2
Section I. Definition 327   2.12 Express each of these in terms of |H|.
  h3,1 h3,2 (a)  h2,1 h2,2
h3,3   h2,3 
 h
  −h1,1
h   1,2 1,3
−h1,2 −2h2,2
h (b)  −2h2,1
1,1
 −3h
 h1,1 + h3,1
−h1,3   −2h2,3 
 
 5h 5h 5h 
3,1
3,2
−3h
3,3
−3h
h1,2 + h3,2
h2,2
h1,3 + h3,3  h2,3  
(c)   h2,1
3,1 3,2 3,3
  2.13 Find the determinant of a diagonal matrix.
2.14 Describe the solution set of a homogeneous linear system if the determinant of
the matrix of coefficients is nonzero.
  2.15 Show that this determinant is zero.
 y+z x+z x+y    x y z     1 1 1  
2.16 (a) Find the 1×1, 2×2, and 3×3 matrices with i,j entry given by (−1)i+j. (b) Find the determinant of the square matrix with i,j entry (−1)i+j.
2.17 (a) Find the 1×1, 2×2, and 3×3 matrices with i, j entry given by i + j. (b) Find the determinant of the square matrix with i, j entry i + j.
  2.18 Show that determinant functions are not linear by giving a case where |A + B| ̸= |A| + |B|.
2.19 The second condition in the definition, that row swaps change the sign of a determinant, is somewhat annoying. It means we have to keep track of the number of swaps, to compute how the sign alternates. Can we get rid of it? Can we replace it with the condition that row swaps leave the determinant unchanged? (If so then we would need new 1×1, 2×2, and 3×3 formulas, but that would be a minor matter.)
2.20 Prove that the determinant of any triangular matrix, upper or lower, is the product down its diagonal.
2.21 Refer to the definition of elementary matrices in the Mechanics of Matrix Multiplication subsection.
(a) What is the determinant of each kind of elementary matrix?
(b) Prove that if E is any elementary matrix then |ES| = |E||S| for any appropriately
sized S.
(c) (This question doesn’t involve determinants.) Prove that if T is singular
then a product TS is also singular.
(d) Show that |TS| = |T||S|.
(e) Show that if T is nonsingular then |T−1| = |T|−1.
2.22 Prove that the determinant of a product is the product of the determinants |T S| = |T | |S| in this way. Fix the n × n matrix S and consider the function d: Mn×n → R given by T  → |TS|/|S|.
(a) Check that d satisfies condition (1) in the definition of a determinant function.
328
Chapter Four. Determinants
 (b) Check condition (2).
(c) Check condition (3).
(d) Check condition (4).
(e) Conclude the determinant of a product is the product of the determinants.
2.23 A submatrix of a given matrix A is one that we get by deleting some of the rows and columns of A. Thus, the first matrix here is a submatrix of the second.  3 1  3 4 1
2 5 0 9 −2 2 −1 5
Prove that for any square matrix, the rank of the matrix is r if and only if r is the
largest integer such that there is an r×r submatrix with a nonzero determinant. 2.24 Prove that a matrix with rational entries has a rational determinant.
? 2.25 [Am. Math. Mon., Feb. 1953] Find the element of likeness in (a) simplifying a
fraction, (b) powdering the nose, (c) building new steps on the church, (d) keeping
emeritus professors on campus, (e) putting B, C, D in the determinant 23
 1 a a a   a3 1 a a2 .  B a3 1 a 
 C D a3 1 
I.3 The Permutation Expansion
The prior subsection defines a function to be a determinant if it satisfies four conditions and shows that there is at most one n×n determinant function for each n. What is left is to show that for each n such a function exists.
But, we easily compute determinants: we use Gauss’s Method, keeping track of the sign changes from row swaps, and end by multiplying down the diagonal. How could they not exist?
The difficulty is to show that the computation gives a well-defined — that
is, unique — result. Consider these two Gauss’s Method reductions of the same
matrix, the first without any row swap
    
1 2 −3ρ1+ρ2 1 2 3 4 −→ 0 −2
and the second with one.
      
1 2 ρ1↔ρ2 3 4 −(1/3)ρ1+ρ2 3 4 34 −→ 12 −→ 02/3
Both yield the determinant −2 since in the second one we note that the row swap changes the sign of the result we get by multiplying down the diagonal.
Section I. Definition 329
 The fact that we are able to proceed in two ways opens the possibility that the two give different answers. That is, the way that we have given to compute determinant values does not plainly eliminate the possibility that there might be, say, two reductions of some 7×7 matrix that lead to different determinant values. In that case we would not have a function, since the definition of a function is that for each input there must be exactly associated one output. The rest of this section shows that the definition Definition 2.1 never leads to a conflict.
To do this we will define an alternative way to find the value of a determinant. (This alternative is less useful in practice because it is slow. But it is very useful for theory.) The key idea is that condition (3) of Definition 2.1 shows that the determinant function is not linear.
3.1 Example With condition (3) scalars come out of each row separately,       
  4 2    2 1    2 1     =2·   =4·     −2 6   −2 6   −1 3 
not from the entire matrix at once. So, where
 2 1  A= −1 3
then det(2A) ̸= 2 · det(A) (instead, det(2A) = 4 · det(A)).
Since scalars come out a row at a time we might guess that determinants are
linear a row at a time.
 3.2 Definition Let V be a vector space. A map f: Vn → R is multilinear if (1) f(⃗ρ1,...,⃗v+w⃗,...,⃗ρn)=f(⃗ρ1,...,⃗v,...,⃗ρn)+f(⃗ρ1,...,w⃗,...,⃗ρn) (2) f(⃗ρ1,...,k⃗v,...,⃗ρn)=k·f(⃗ρ1,...,⃗v,...,⃗ρn)
for ⃗v, w⃗ ∈ V and k ∈ R.
 3.3 Lemma Determinants are multilinear.
Proof Property (2) here is just Definition 2.1’s condition (3) so we need only verify property (1).
There are two cases. If the set of other rows {⃗ρ1,...,⃗ρi−1,⃗ρi+1,...,⃗ρn} is linearly dependent then all three matrices are singular and so all three determinants are zero and the equality is trivial.
Therefore assume that the set of other rows is linearly independent. We can make a basis by adding one more vector ⟨⃗ρ1, . . . , ⃗ρi−1, β⃗ , ⃗ρi+1, . . . , ⃗ρn⟩. Express
330 Chapter Four. Determinants ⃗v and w⃗ with respect to this basis
⃗v=v1⃗ρ1 +···+vi−1⃗ρi−1 +viβ⃗ +vi+1⃗ρi+1 +···+vn⃗ρn
w⃗ =w1⃗ρ1 +···+wi−1⃗ρi−1 +wiβ⃗ +wi+1⃗ρi+1 +···+wn⃗ρn
and add.
⃗v+w⃗ =(v1 +w1)⃗ρ1 +···+(vi +wi)β⃗ +···+(vn +wn)⃗ρn Consider the left side of (1) and expand ⃗v + w⃗ .
det(⃗ρ1,..., (v1 +w1)⃗ρ1 +···+(vi +wi)β⃗ +···+(vn +wn)⃗ρn, ...,⃗ρn) (∗)
By the definition of determinant’s condition (1), the value of (∗) is unchanged by the operation of adding −(v1 + w1)⃗ρ1 to the i-th row ⃗v + w⃗ . The i-th row becomes this.
⃗v+w⃗ −(v1 +w1)⃗ρ1 =(v2 +w2)⃗ρ2 +···+(vi +wi)β⃗ +···+(vn +wn)⃗ρn Next add −(v2 + w2)⃗ρ2, etc., to eliminate all of the terms from the other rows.
Apply condition (3) from the definition of determinant.
det(⃗ρ1,...,⃗v+w⃗,...,⃗ρn)
=det(⃗ρ1,...,(vi +wi)·β⃗,...,⃗ρn)
=(vi +wi)·det(⃗ρ1,...,β⃗,...,⃗ρn)
=vi ·det(⃗ρ1,...,β⃗,...,⃗ρn)+wi ·det(⃗ρ1,...,β⃗,...,⃗ρn)
Now this is a sum of two determinants. To finish, bring vi and wi back inside in front of the β⃗ ’s and use row combinations again, this time to reconstruct the expressions of ⃗v and w⃗ in terms of the basis. That is, start with the operations of adding v1⃗ρ1 to viβ⃗ and w1⃗ρ1 to wi⃗ρ1, etc., to get the expansions of ⃗v and w⃗ . QED
Multilinearity allows us to expand a determinant into a sum of determinants, each of which involves a simple matrix.
3.4 Example Use property (1) of multilinearity to break up the first row       
 2 1   2 0   0 1    =  +    43   43   43 
and then use (1) again to break each along the second row.
        
 2 0   2 0   0 1   0 1  =  +  +  +    40   03   40   03 
The result is four determinants. In each row of each of the four there is a single entry from the original matrix.
 
Section I. Definition 331
 3.5 Example In the same way, a 3×3 determinant separates into a sum of many simpler determinants. Splitting along the first row produces three determinants (we have highlighted the zero in the 1, 3 position to set it off visually from the zeroes that appear as part of the splitting).
 2 1 −1   2 0 0   0 1 0   0 0 −1          
 4 3   =  4 3   +  4 3   +  4 3    2 1 5   2 1 5   2 1 5   2 1 5 
In turn, each of the above splits in three along the second row. Then each of the nine splits in three along the third row. The result is twenty seven determinants, such that each row contains a single entry from the starting matrix.
 2 0 0   2 0 0   2 0 0   2 0 0   0 0 −1            
= 4 0 0 + 4 0 0 + 4 0 0 + 0 3 0 +···+ 0 0    2 0 0   0 1 0   0 0 5   2 0 0   0 0 5 
So multilinearity will expand an n×n determinant into a sum of nn-many determinants, where each row of each determinant contains a single entry from the starting matrix.
In this expansion, although there are lots of terms, most of them have a determinant of zero.
3.6 Example In each of these examples from the prior expansion, two of the entries from the original matrix are in the same column.
 2 0 0   0 0 −1   0 1 0        
 4 0 0   0 3 0   0 0    0 1 0   0 0 5   0 0 5 
For instance, in the first matrix the 2 and the 4 both come from the first column of the original matrix. In the second matrix the −1 and 5 both come from the third column. And in the third matrix the 0 and 5 both come from the third column. Any such matrix is singular because one row is a multiple of the other. Thus any such determinant is zero, by Lemma 2.4.
With that observation the above expansion of the 3×3 determinant into the sum of the twenty seven determinants simplifies to the sum of these six where
0
0
0
  0
0
  0
  
332 Chapter Four. Determinants the entries from the original matrix come one per row, and also one per column.
 2 1 −1   2 0 0   2 0 0        
 4 3  = 0 3 0 + 0 0    2 1 5   0 0 5   0 1 0 
 0 1 0   0 1 0      
+ 4 0 0 + 0 0    0 0 5   2 0 0 
 0 0 −1   0 0 −1      
+ 4 0 0 + 0 3 0   0 1 0   2 0 0 
In that expansion we can bring out the scalars.
 1 0 0   1 0 0      
 0
  = (2)(3)(5)  0 1 0  + (2)(  0 0 1 
)(1)  0 0 1   0 1 0 
 0 1 0      
+ (1)(4)(5)  1 0 0  + (1)(  0 0 1 
0
 0
 0
 0 1 0  )(2)  0 0 1 
 1 0 0 
 0 0 1 
  0 0 1      
+ (−1)(4)(1)  1 0 0  + (−1)(3)(2)  0 1 0   0 1 0   1 0 0 
To finish, evaluate those six determinants by row-swapping them to the identity matrix, keeping track of the sign changes.
= 30 · (+1) + 0 · (−1) +20·(−1)+0·(+1)
− 4 · (+1) − 6 · (−1) = 12
That example captures this subsection’s new calculation scheme. Multi- linearity expands a determinant into many separate determinants, each with one entry from the original matrix per row. Most of these have one row that is a multiple of another so we omit them. We are left with the determinants that have one entry per row and column from the original matrix. Factoring out the scalars further reduces the determinants that we must compute to the one-entry-per-row-and-column matrices where all entries are 1’s.
Recall Definition Three.IV.3.14, that a permutation matrix is square, with entries 0’s except for a single 1 in each row and column. We now introduce a notation for permutation matrices.
0
Section I. Definition 333
  3.7 Definition An n-permutation is a function on the first n positive integers φ: {1,...,n} → {1,...,n} that is one-to-one and onto.
In a permutation each number 1, . . . , n appears as output for one and only one input. We can denote a permutation as a sequence φ = ⟨φ(1), φ(2), . . . , φ(n)⟩.
3.8Example The2-permutationsarethefunctionsφ1:{1,2}→{1,2}givenby φ1(1) = 1, φ1(2) = 2, and φ2 : {1, 2} → {1, 2} given by φ2(1) = 2, φ2(2) = 1. The sequence notation is shorter: φ1 = ⟨1, 2⟩ and φ2 = ⟨2, 1⟩.
3.9 Example In the sequence notation the 3-permutations are φ1 = ⟨1,2,3⟩, φ2 = ⟨1,3,2⟩, φ3 = ⟨2,1,3⟩, φ4 = ⟨2,3,1⟩, φ5 = ⟨3,1,2⟩, and φ6 = ⟨3,2,1⟩.
We denote the row vector that is all 0’s except for a 1 in entry j with ιj so that the four-wide ι2 is (0 1 0 0). Now our notation for permutation matrices is: with any φ = ⟨φ(1), . . . , φ(n)⟩ associate the matrix whose rows are ιφ(1), . . . , ιφ(n). For instance, associated with the 4-permutation φ = ⟨3, 2, 1, 4⟩ is the matrix whose rows are the corresponding ι’s.
ι3 0010
in Example 3.8.
Pφ1 = 1 = Pφ2 = 2 =
ι2  0
ι1 1000
1 0 0 Pφ= = 
ι4 0001
3.10 Example These are the permutation matrices for the 2-permutations listed
 ι   1 0   ι   0 1  ι2 01 ι1 10
For instance, Pφ2 ’s first row is ιφ2 (1) = ι2 and its second is ιφ2 (2) = ι1 .
3.11 Example Consider the 3-permutation φ5 = ⟨3, 1, 2⟩. The permutation matrix
Pφ5 has rows ιφ5(1) = ι3, ιφ5(2) = ι1, and ιφ5(3) = ι2.
0 0 1 P φ 5 =  1 0 0 
010
 3.12 Definition The permutation expansion for determinants is
 t
  1,1
 t2,1
 
... t  
t t2,2
.
 tt...t   n,1 n,2 n,n 
1,2
1,n 
t2,n  
  =
  .  
. . .
t1,φ1(1)t2,φ1(2) · · · tn,φ1(n)|Pφ1 | +t1,φ2(1)t2,φ2(2) ···tn,φ2(n)|Pφ2|
.
+t1,φk(1)t2,φk(2) ···tn,φk(n)|Pφk|
334 Chapter Four. Determinants where φ1, . . . , φk are all of the n-permutations.
We can restate the formula in summation notation
 
  as does the 3×3 formula.
 t
  1,1
 t2,1  t3,1
t t2,2 t3,2
t   1,3 
1,2
|T| =
permutations φ
t1,φ(1)t2,φ(2) ···tn,φ(n) |Pφ|
read aloud as, “the sum, over all permutations φ, of terms having the form t1,φ(1)t2,φ(2) · · · tn,φ(n)|Pφ|.”
3.13 Example The familiar 2×2 determinant formula follows from the above
  
t1,2  
 =t1,1t2,2 ·|Pφ1|+t1,2t2,1 ·|Pφ2|
 t1,1  
 t2,1
t2,2 
         1 0   0 1 
=t1,1t2,2 ·   +t1,2t2,1 ·     0 1   1 0 
= t1,1t2,2 − t1,2t2,1
t2,3  = t1,1t2,2t3,3 |Pφ1 | + t1,1t2,3t3,2 |Pφ2 | + t1,2t2,1t3,3 |Pφ3 |
t3,3  
+ t1,2t2,3t3,1 |Pφ4 | + t1,3t2,1t3,2 |Pφ5 | + t1,3t2,2t3,1 |Pφ6 | = t1,1t2,2t3,3 − t1,1t2,3t3,2 − t1,2t2,1t3,3
+ t1,2t2,3t3,1 + t1,3t2,1t3,2 − t1,3t2,2t3,1
Computing a determinant with the permutation expansion typically takes longer than with Gauss’s Method. However, we will use it to prove that the determinant function is well-defined. We will just state the result here and defer its proof to the following subsection.
3.14 Theorem For each n there is an n×n determinant function.
Also in the next subsection is the proof of the result below (they are together
because the two proofs overlap).
Because of this theorem, while we have so far stated determinant results in terms of rows, all of the results also hold in terms of columns.
  3.15 Theorem The determinant of a matrix equals the determinant of its trans- pose.
Section I. Definition 335
  3.16 Corollary A matrix with two equal columns is singular. Column swaps change the sign of a determinant. Determinants are multilinear in their columns.
Proof For the first statement, transposing the matrix results in a matrix with the same determinant, and with two equal rows, and hence a determinant of zero. Prove the other two in the same way. QED
We finish this subsection with a summary: determinant functions exist, are unique, and we know how to compute them. As for what determinants are about, perhaps these lines [Kemp] help make it memorable.
Determinant none, Solution: lots or none.
Determinant some, Solution: just one.
Exercises
This summarizes our notation for the 2- and 3-permutations. i12 i123
      φ1(i) 1 2 φ2(i) 2 1
φ1(i)123 φ2(i)132 φ3(i) 2 1 3 φ4(i) 2 3 1 φ5(i) 3 1 2 φ6(i) 3 2 1
        3.17 For this matrix, find the term associated with each 3-permutation.
1 2 3 M=4 5 6
789
That is, fill in the rest of this table.
permutation φi φ1 φ2 term m1,φi(1)m2,φi(2)m3,φi(3) 1 · 5 · 9
φ3 φ4 φ5 φ6
     3.18 For each 3-permutation φ find |Pφ|.
3.19 This determinant is 7 by the 2×2 formula. Compute it with the permutation
expansion.
 2 3   1 5 
3.20 This determinant is 0 because the first two rows add to the third. Compute the determinant using the permutation expansion.
 −1 0 1   3 1 4   2 1 5 
  3.21 Compute the determinant by using the permutation expansion.
336 Chapter Four. Determinants  1 2 3   2 2 1 
(a) 4 5 6  (b) 3 −1 0   7 8 9   −2 0 5 
  3.22 Compute these both with Gauss’s Method and the permutation expansion formula.
 2 1   0 1 4  (a)    (b) 0 2 3 
 3 1   1 5 1 
  3.23 Use the permutation expansion formula to derive the formula for 3×3 determi-
nants.
3.24 List all of the 4-permutations.
3.25 A permutation, regarded as a function from the set {1,..,n} to itself, is one-to- one and onto. Therefore, each permutation has an inverse.
(a) Find the inverse of each 2-permutation. (b) Find the inverse of each 3-permutation.
3.26 Prove that f is multilinear if and only if for all ⃗v, w⃗ ∈ V and k1, k2 ∈ R, this holds.
f(⃗ρ1,...,k1⃗v1 +k2⃗v2,...,⃗ρn)=k1f(⃗ρ1,...,⃗v1,...,⃗ρn)+k2f(⃗ρ1,...,⃗v2,...,⃗ρn) 3.27 How would determinants change if we changed property (4) of the definition to
read that |I| = 2?
3.28 Verify the second and third statements in Corollary 3.16.
  3.29 Show that if an n×n matrix has a nonzero determinant then we can express any column vector ⃗v ∈ Rn as a linear combination of the columns of the matrix.
3.30 [Strang 80] True or false: a matrix whose entries are only zeros or ones has a determinant equal to zero, one, or negative one.
3.31 (a) Show that there are 120 terms in the permutation expansion formula of a 5×5 matrix.
(b) How many are sure to be zero if the 1, 2 entry is zero?
3.32 How many n-permutations are there?
3.33 Show that the inverse of a permutation matrix is its transpose. 3.34 A matrix A is skew-symmetric if AT = −A, as in this matrix.
 0 3  A= −3 0
Show that n×n skew-symmetric matrices with nonzero determinants exist only for even n.
  3.35 What is the smallest number of zeros, and the placement of those zeros, needed to ensure that a 4×4 matrix has a determinant of zero?
3.36 If we have n data points (x1,y1),(x2,y2),... ,(xn,yn) and want to find a polynomial p(x) = an−1xn−1 + an−2xn−2 + · · · + a1x + a0 passing through those points then we can plug in the points to get an n equation/n unknown linear system. The matrix of coefficients for that system is the Vandermonde matrix.
 
Section I. Definition 337 Prove that the determinant of the transpose of that matrix of coefficients
  1 1 ... 1   x x...x 
 12n 
222   x1 x2 ... xn
    .  
 .   x1n−1 x2n−1 . . . xnn−1 
equals the product, over all indices i,j ∈ {1,...,n} with i < j, of terms of the form xj − xi. (This shows that the determinant is zero, and the linear system has no solution, if and only if the xi’s in the data are not distinct.)
3.37 We can divide a matrix into blocks, as here,
1 2 0 3 4 0 0 0 −2
which shows four blocks, the square 2×2 and 1×1 ones in the upper left and lower right, and the zero blocks in the upper right and lower left. Show that if a matrix is such that we can partition it as
 J Z2  T=Z1 K
where J and K are square, and Z1 and Z2 are all zeroes, then |T | = |J| · |K|.
3.38 Prove that for any n×n matrix T there are at most n distinct reals r such that
the matrix T − rI has determinant zero (we shall use this result in Chapter Five).
? 3.39 [Math. Mag., Jan. 1963, Q307] The nine positive digits can be arranged into
3×3 arrays in 9! ways. Find the sum of the determinants of these arrays. 3.40 [Math. Mag., Jan. 1963, Q237] Show that
 x−2 x−3 x−4   x+1 x−1 x−3 =0.  x−4 x−7 x−10 
? 3.41 [Am. Math. Mon., Jan. 1949] Let S be the sum of the integer elements of a magic square of order three and let D be the value of the square considered as a determinant. Show that D/S is an integer.
? 3.42 [Am. Math. Mon., Jun. 1931] Show that the determinant of the n2 elements in the upper left corner of the Pascal triangle
       has the value unity.
1111.. 123.. 13.. 1..
. .
338 Chapter Four. Determinants I.4 Determinants Exist
This subsection contains proofs of two results from the prior subsection. It is optional. We will use the material developed here only in the Jordan Canonical Form subsection, which is also optional.
We wish to show that for any size n, the determinant function on n×n matrices is well-defined. The prior subsection develops the permutation expansion formula.
  t
  1,1
 tn,1 tn,2
... t  
 t2,1  
. . .
t1,φ1(1)t2,φ1(2) · · · tn,φ1(n)|Pφ1 | +t1,φ2(1)t2,φ2(2) ···tn,φ2(n)|Pφ2|
.
+t1,φk(1)t2,φk(2) ···tn,φk(n)|Pφk|  
t t2,2
.
1,2
1,n  t2,n  
  =   .  
. . .
tn,n 
=
t1,φ(1)t2,φ(2) · · · tn,φ(n) |Pφ| permutations φ
This reduces the problem of showing that the determinant is well-defined to only showing that the determinant is well-defined on the set of permutation matrices.
A permutation matrix can be row-swapped to the identity matrix. So one way that we can calculate its determinant is by keeping track of the number of swaps. However, we still must show that the result is well-defined. Recall what the difficulty is: the determinant of
0 1 0 0
1 0 0 0 Pφ =   0 0 1 0
could be computed with one swap
0001
1 0 0 0
or with three.
ρ1↔ρ2 0 1 0 0 Pφ−→  0 0 1 0
0001
1 0 0 0
ρ3↔ρ1 ρ2↔ρ3 ρ1↔ρ3 0 1 0 0 Pφ −→ −→ −→   0 0 1 0
0001
Both reductions have an odd number of swaps so in this case we figure that |Pφ| = −1 but if there were some way to do it with an even number of swaps then
Section I. Definition 339
 we would have the determinant giving two different outputs from a single input. Below, Corollary 4.5 proves that this cannot happen — there is no permutation matrix that can be row-swapped to an identity matrix in two ways, one with an even number of swaps and the other with an odd number of swaps.
 4.1 Definition In a permutation φ = ⟨...,k,...,j,...⟩, elements such that k > j are in an inversion of their natural order. Similarly, in a permutation matrix
two rows
 .  such that k > j are in an inversion.
 .  ι 
 .k  P =.
 . 
φ
ιj
4.2 Example This permutation matrix 1000 ι1
0 0 1 0 ι3   = 0100 ι2
0001 ι4 has a single inversion, that ι3 precedes ι2.
4.3 Example There are three inversions here:
0 0 1 ι
100 ι1 ι3 precedes ι1, ι3 precedes ι2, and ι2 precedes ι1.
Proof Consider a swap of rows j and k, where k > j. If the two rows are adjacent
3 0 1 0 = ι2
 4.4 Lemma A row-swap in a permutation matrix changes the number of inversions from even to odd, or from odd to even.
 . 

ιφ(j)  Pφ=  ιφ(k) 
 . 
ρk ↔ρj −→
. 
ιφ(k)     ιφ(j) 
.
340 Chapter Four. Determinants
 then since inversions involving rows not in this pair are not affected, the swap changes the total number of inversions by one, either removing or producing one inversion depending on whether φ(j) > φ(k) or not. Consequently, the total number of inversions changes from odd to even or from even to odd.
If the rows are not adjacent then we can swap them via a sequence of adjacent swaps, first bringing row k up
.  ιφ(j) 
ι   φ(j+1) 
ρj+1 ↔ρj −→ −→ ··· −→
 .   ιφ(k) 
 ι   φ(j) 
ιφ(j+2)  .
ρk ↔ρk−1
ρk−1 ↔ρk−2
 ιφ(j+1)  .
.  ιφ(k) 
. ιφ(k−1) 
. ..
and then bringing row j down.
.  . 
 ιφ(k)  ι 
ρj+1 ↔ρj+2 ρj+2 ↔ρj+3 ρ ↔ρ k−1 k
 φ(j+1) ι 
−→ −→ ··· −→
φ(j+2)  . 
Each of these adjacent swaps changes the number of inversions from odd to even or from even to odd. The total number of swaps (k−j)+(k−j−1) is odd. Thus, in aggregate, the number of inversions changes from even to odd, or from odd to even. QED
Proof The identity matrix has zero inversions. To change an odd number to zero requires an odd number of swaps, and to change an even number to zero requires an even number of swaps. QED
4.6 Example The matrix in Example 4.3 can be brought to the identity with one swap ρ1 ↔ ρ3. (So the number of swaps needn’t be the same as the number of inversions, but the oddness or evenness of the two numbers is the same.)
.  ιφ(j) 
 .  .
 4.5 Corollary If a permutation matrix has an odd number of inversions then swapping it to the identity takes an odd number of swaps. If it has an even number of inversions then swapping to the identity takes an even number.
Section I. Definition 341
  4.7 Definition The signum of a permutation sgn(φ) is −1 if the number of inversions in φ is odd and is +1 if the number of inversions is even.
4.8 Example Using the notation for the 3-permutations from Example 3.8 we
have
1 0 0 1 0 0 P φ 1 =  0 1 0  P φ 2 =  0 0 1 
001 010
so sgn(φ1) = 1 because there are no inversions, while sgn(φ2) = −1 because
there is one.
We still have not shown that the determinant function is well-defined because we have not considered row operations on permutation matrices other than row swaps. We will finesse this issue. Define a function d: Mn×n → R by altering the permutation expansion formula, replacing |Pφ| with sgn(φ).
d(T ) =
 
permutations φ
t1,φ(1) t2,φ(2) · · · tn,φ(n) · sgn(φ)
The advantage of this formula is that the number of inversions is clearly well- defined — just count them. Therefore, we will be finished showing that an n×n determinant function exists when we show that this d satisfies the conditions required of a determinant.
Proof We must check that it has the four conditions from the definition of determinant, Definition 2.1.
Condition (4) is easy: where I is the n×n identity, in  
 4.9 Lemma The function d above is a determinant. Hence determinants exist for every n.
ι1,φ(1) ι2,φ(2) · · · ιn,φ(n) sgn(φ)
all of the terms in the summation are zero except for the one where the permu- tation φ is the identity, which gives the product down the diagonal, which is
perm φ
 
d(I) =
perm φ
one.
For condition (3) suppose that T −→ T and consider d(T).
  ˆt1,φ(1) · · · ˆti,φ(i) · · · ˆtn,φ(n) sgn(φ)
=
t1,φ(1) · · · kti,φ(i) · · · tn,φ(n) sgn(φ)
kρiˆ ˆ
φ
342 Chapter Four. Determinants Factor out k to get the desired equality.
 
 φ
t1,φ(1) · · · ti,φ(i) · · · tn,φ(n) sgn(φ) = k · d(T ) ρi↔ρj ˆ ˆ
= k ·
For (2) suppose that T −→ T. We must show that d(T) is the negative
of d(T).
d(T) = t1,φ(1) ···ti,φ(i) ···tj,φ(j) ···tn,φ(n) sgn(φ) (∗)
ˆ ˆˆˆˆ perm φ
We will show that each term in (∗) is associated with a term in d(T), and that the two terms are negatives of each other. Consider the matrix from the multilinear expansion of d(Tˆ) giving the term ˆt1,φ(1) · · · ˆti,φ(i) · · · ˆtj,φ(j) · · · ˆtn,φ(n) sgn(φ).
 . 
 ˆt   i,φ(i) .  
 .   ˆtj,φ(j) 
 . 
It is the result of the ρi ↔ ρj operation performed on this matrix.
 .   t 
 . i,φ(j)  .
 .   tj,φ(i) 
 . 
That is, the term with hatted t’s is associated with this term from the d(T) expansion: t1,σ(1) · · · tj,σ(j) · · · ti,σ(i) · · · tn,σ(n) sgn(σ), where the permutation σ equals φ but with the i-th and j-th numbers interchanged, σ(i) = φ(j) and σ(j) = φ(i). The two terms have the same multiplicands ˆt1,φ(1) = t1,σ(1), . . . , including the entries from the swapped rows ˆti,φ(i) = tj,φ(i) = tj,σ(j) and ˆtj,φ(j) = ti,φ(j) = ti,σ(i). But the two terms are negatives of each other since sgn(φ) = − sgn(σ) by Lemma 4.4.
Now, any permutation φ can be derived from some other permutation σ by such a swap, in one and only one way. Therefore the summation in (∗) is in fact a sum over all permutations, taken once and only once.
ˆ ˆˆˆˆ
d(T) = t1,φ(1) ···ti,φ(i) ···tj,φ(j) ···tn,φ(n) sgn(φ)
perm φ  
= t1,σ(1) ···tj,σ(j) ···ti,σ(i) ···tn,σ(n) · −sgn(σ)  perm σ
Section I. Definition
343
 Thus d(Tˆ) = −d(T).
Finally, for condition (1) suppose that T −→
ˆ ˆˆˆˆ
d(T) = t1,φ(1) ···ti,φ(i) ···tj,φ(j) ···tn,φ(n) sgn(φ)
perm φ  
t1,φ(1) · · · ti,φ(i) · · · (kti,φ(j) + tj,φ(j) ) · · · tn,φ(n) sgn(φ)
 t1,φ(1) · · · ti,φ(i) · · · kti,φ(j) · · · tn,φ(n) sgn(φ)
+ t1,φ(1) · · · ti,φ(i) · · · tj,φ(j) · · · tn,φ(n) sgn(φ) 
Break it into two summations.
 
φ
 
= k· t1,φ(1) ···ti,φ(i) ···ti,φ(j) ···tn,φ(n) sgn(φ) φ +d(T)
Consider the terms t1,φ(1) · · · ti,φ(i) · · · ti,φ(j) · · · tn,φ(n) sgn(φ). Notice the sub- scripts; the entry is ti,φ(j), not tj,φ(j). The sum of these terms is the determinant ofamatrixSthatisequaltoT exceptthatrowjofSisacopyofrowiofT, that is, S has two equal rows. In the same way that we proved Lemma 2.4 we can see that d(S) = 0: a swap of S’s equal rows will change the sign of d(S) but since the matrix is unchanged by that swap the value of d(S) must also be unchanged, and so that value must be zero. QED
We have now proved that determinant functions exist for each size n×n. We already know that for each size there is at most one determinant. Therefore, for each size there is one and only one determinant function.
We end this subsection by proving the other result remaining from the prior subsection.
Proof The proof is best understood by doing the general 3×3 case. That the argument applies to the n×n case will be clear.
=
Distribute over the addition in kti,φ(j) + tj,φ(j).
=
t1,φ(1) · · · ti,φ(i) · · · kti,φ(j) · · · tn,φ(n) sgn(φ)  
=
φ
 
φ
t1,φ(1) · · · ti,φ(i) · · · tj,φ(j) · · · tn,φ(n) sgn(φ) Recognize the second one.
+
φ
kρi +ρj
ˆ T.
 4.10 Theorem The determinant of a matrix equals the determinant of its trans- pose.
344
Chapter Four. Determinants
 Compare the permutation expansion of the matrix T
 t
  1,1
 t2,1  t3,1
t t2,2 t3,2
t   1 0 0   1 1,3      
0 0   
1,2
t2,3   = t1,1 t2,2 t3,3  0 1 0  + t1,1 t2,3 t3,2  0  0 1 0 
0 1  t3,3   0 0 1   0 1 0 
with the permutation expansion of its transpose.
 t
  1,1
 t1,2  t1,3
t t2,2 t2,3
t   1 0 0   1 3,1      
0 0   
2,1
 0 1 0      
+ t1,2t2,1t3,3  1 0 0  + t1,2t2,3t3,1  0 0 1   0 0 1   1 0 0 
 0 0 1   0 0 1      
+ t1,3t2,1t3,2  1 0 0  + t1,3t2,2t3,1  0 1 0   0 1 0   1 0 0 
t3,2   = t1,1 t2,2 t3,3  0 1 0  + t1,1 t3,2 t2,3  0  0 1 0 
0 1  t3,3   0 0 1   0 1 0 
 0 1 0      
+ t2,1t1,2t3,3  1 0 0  + t2,1t3,2t1,3  0 0 1   0 0 1   1 0 0 
 0 0 1   0 0 1      
+ t3,1t1,2t2,3  1 0 0  + t3,1t2,2t1,3  0 1 0   0 1 0   1 0 0 
Compare first the six products of t’s. The ones in the expansion of T are the same as the ones in the expansion of the transpose; for instance, t1,2t2,3t3,1 is in the top and t3,1t1,2t2,3 is in the bottom. That’s perfectly sensible — the six in the top arise from all of the ways of picking one entry of T from each row and column while the six in the bottom are all of the ways of picking one entry of T from each column and row, so of course they are the same set.
Next observe that in the two expansions, each t-product expression is not necessarily associated with the same permutation matrix. For instance, on the top t1,2t2,3t3,1 is associated with the matrix for the map 1  → 2, 2  → 3, 3  → 1. On the bottom t3,1t1,2t2,3 is associated with the matrix for the map 1  → 3, 2  → 1, 3  → 2. The second map is inverse to the first. This is also perfectly sensible — both the matrix transpose and the map inverse flip the 1, 2 to 2, 1, flip the 2,3 to 3,2, and flip 3,1 to 1,3.
We finish by noting that the determinant of Pφ equals the determinant of Pφ−1 , as Exercise 16shows. QED
Section I. Definition 345 Exercises
These summarize the notation used in this book for the 2- and 3-permutations. i12 i123
       φ1(i) 1 2 φ2(i) 2 1
φ1(i)123 φ2(i)132 φ3(i) 2 1 3 φ4(i) 2 3 1 φ5(i) 3 1 2 φ6(i) 3 2 1
      4.11 Give the permutation expansion of a general 2×2 matrix and its transpose.
  4.12 This problem appears also in the prior subsection.
(a) Find the inverse of each 2-permutation.
(b) Find the inverse of each 3-permutation.
  4.13 (a) Find the signum of each 2-permutation.
(b) Find the signum of each 3-permutation.
4.14 Find the only nonzero term in the permutation expansion of this matrix.
 0 1 0 0   1 0 1 0   0 1 0 1   0 0 1 0 
Compute that determinant by finding the signum of the associated permutation. 4.15 [Strang 80] What is the signum of the n-permutation φ = ⟨n, n − 1, . . . , 2, 1⟩? 4.16 Prove these.
(a) Every permutation has an inverse.
(b) sgn(φ−1) = sgn(φ)
(c) Every permutation is the inverse of another.
4.17 Prove that the matrix of the permutation inverse is the transpose of the matrix of the permutation Pφ−1 = PφT, for any permutation φ.
  4.18 Show that a permutation matrix with m inversions can be row swapped to the identity in m steps. Contrast this with Corollary 4.5.
  4.19 For any permutation φ let g(φ) be the integer defined in this way.  
g(φ) =
(This is the product, over all indices i and j with i < j, of terms of the given form.)
(a) Compute the value of g on all 2-permutations. (b) Compute the value of g on all 3-permutations. (c) Prove that g(φ) is not 0.
(d) Prove this.
sgn(φ)= g(φ) |g(φ)|
Many authors give this formula as the definition of the signum function.
i<j
[φ(j) − φ(i)]
 
346 Chapter Four. Determinants II Geometry of Determinants
The prior section develops the determinant algebraically, by considering formulas satisfying certain conditions. This section complements that with a geometric approach. Beyond its intuitive appeal, an advantage of this approach is that while we have so far only considered whether or not a determinant is zero, here we shall give a meaning to the value of the determinant. (The prior section treats the determinant as a function of the rows but this section focuses on columns.)
II.1 Determinants as Size Functions
This parallelogram picture is familiar from the construction of the sum of the two vectors.
   1.1 Definition In Rn the box (or parallelepiped) formed by ⟨⃗v1, . . . ,⃗vn⟩ is the set{t1⃗v1+···+tn⃗vn |t1,...,tn ∈[0...1]}.
box is shown in Example 1.4. subtracting away areas not in the box.
 x2  y2
 x1  y1
Thus the parallelogram above is the box formed by ⟨ x1  ,  x2  ⟩. A three-space
y1 y2
We can find the area of the above box by drawing an enclosing rectangle and
   y2 y1
area of parallelogram
= area of rectangle − area of A − area of B
−···−area of F
= (x1 + x2)(y1 + y2) − x2y1 − x1y1/2
− x2y2/2 − x2y2/2 − x1y1/2 − x2y1 = x1y2 − x2y1
A
 C
B
E
D
  F
  x2 x1
That the area equals the value of the determinant
  
 x1 x2  
    = x1y2 − x2y1  y1 y2 
Section II. Geometry of Determinants 347
 is no coincidence. The definition of determinants contains four properties that we know lead to a unique function for each dimension n. We shall argue that these properties make good postulates for a function that measure the size of boxes in n-space.
For instance, a function that measures the size of the box should have the property that multiplying one of the box-defining vectors by a scalar will multiply the size by that scalar.
  w⃗ w⃗ ⃗v
k⃗v
Shown here is k = 1.4. On the right the rescaled region is in solid lines with the original region shaded for comparison.
That is, we can reasonably expect that size(...,k⃗v,...) = k·size(...,⃗v,...). Of course, this condition is one of those in the definition of determinants.
Another property of determinants that should apply to any function mea- suring the size of a box is that it is unaffected by row combinations. Here are before-combining and after-combining boxes (the scalar shown is k = −0.35).
  w⃗ k ⃗v + w⃗ ⃗v
⃗v
The box formed by v and k⃗v + w⃗ slants differently than the original one but the two have the same base and the same height, and hence the same area. So weexpectthatsizeisnotaffectedbyashearoperationsize(...,⃗v,...,w⃗,...)= size(...,⃗v,...,k⃗v+w⃗,...). Again, this is a determinant condition.
We expect that the box formed by unit vectors has unit size
⃗e2
⃗e1
and we naturally extend that to any n-space size(⃗e1 , . . . , ⃗en ) = 1.
Condition (2) of the definition of determinant is redundant, as remarked following the definition. We know from the prior section that for each n the determinant exists and is unique so we know that these postulates for size
functions are consistent and that we do not need any more postulates. Therefore, we are justified in interpreting det(⃗v1,...,⃗vn) as giving the size of the box formed by the vectors.
1.2 Remark Although condition (2) is redundant it raises an important point. Consider these two.
   
348
Chapter Four. Determinants
   ⃗v
⃗u
 4 1 
 2 3 =10
⃗v
⃗u
 1 4 
 3 2 =−10
     
Swapping the columns changes the sign. On the left, starting with ⃗u and following the arc inside the angle to ⃗v (that is, going counterclockwise), we get a positive size. On the right, starting at ⃗v and going to ⃗u, and so following the clockwise arc, gives a negative size. The sign returned by the size function reflects the orientation or sense of the box. (We see the same thing if we picture the effect of scalar multiplication by a negative scalar.)
1.4 Example By the formula that takes the area of the base times the height, the volume of this parallelepiped is 12. That agrees with the determinant.
 2 0 −1    
 0 3 0 =12  2 1 1 
We can also compute the volume as the absolute value of this determinant.
 0 2 −1    
 3 0 0 =−12  1 2 1 
 1.3 Definition The volume of a box is the absolute value of the determinant of a matrix with those vectors as columns.
  1.5 Theorem A transformation t: Rn → Rn changes the size of all boxes by the same factor, namely, the size of the image of a box |t(S)| is |T| times the size of the box |S|, where T is the matrix representing t with respect to the standard basis.
That is, the determinant of a product is the product of the determinants |TS| = |T| · |S|.
The two sentences say the same thing, first in map terms and then in matrix terms. This is because |t(S)| = |TS|, as both give the size of the box that is
Section II. Geometry of Determinants 349
 the image of the unit box En under the composition t ◦ s, where the maps are represented with respect to the standard basis. We will prove the second sentence.
Proof First consider the case that T is singular and thus does not have an inverse. Observe that if TS is invertible then there is an M such that (TS)M = I, so T(SM) = I, and so T is invertible. The contrapositive of that observation is that if T is not invertible then neither is TS—if |T| = 0 then |TS| = 0.
Now consider the case that T is nonsingular. Any nonsingular matrix factors into a product of elementary matrices T = E1E2 · · · Er. To finish this argument we will verify that |ES| = |E| · |S| for all matrices S and elementary matrices E. The result will then follow because |TS| = |E1 ···ErS| = |E1|···|Er| · |S| = |E1 · · · Er | · |S| = |T | · |S|.
There are three types of elementary matrix. We will cover the Mi(k) case; the Pi,j and Ci,j(k) checks are similar. The matrix Mi(k)S equals S except that row i is multiplied by k. The third condition of determinant functions then gives that |Mi(k)S| = k · |S|. But |Mi(k)| = k, again by the third condition because Mi(k) is derived from the identity by multiplication of row i by k. Thus |ES| = |E| · |S| holds for E = Mi(k). QED
1.6 Example Application of the map t represented with respect to the standard bases by
 1 1  −2 0
will double sizes of boxes, e.g., from this
 to this
⃗v
 2 1 
  
 1 2 =3 w⃗   
 t(⃗v)
  
  3 3  
   =6  −4 −2 
 1.7 Corollary If a matrix is invertible then the determinant of its inverse is the inverse of its determinant |T−1| = 1/|T|.
Proof 1=|I|=|TT−1|=|T|·|T−1|
QED
t ( w⃗ )
350
Chapter Four. Determinants
 Exercises
1.8 Is
4
1 2
inside of the box formed by these three?
3 2 1
3 6 0 115
  1.9 Find the volume of the region defined by the vectors.  1   −1 
(a) ⟨ 3 , 4 ⟩
2 3 8
(b) ⟨1,−2,−3⟩ 048
1 2 −1 0 (c) ⟨2,2, 3 ,1⟩
0 2 0 0  
1257
  1.10 Why doesn’t this picture contradict Theorem 1.5?
 2 1  01 −→
area is 2 determinant is 2 area is 5
  1.11 Find the volume of this region.
  1.12 Suppose that |A| = 3. By what factor do these change volumes? (a) A (b) A2 (c) A−2
  1.13 Consider the linear transformation of R3 represented with respect to the standard bases by this matrix.
1 0 −1 311 −1 0 3
(a) Compute the determinant of the matrix. Does the transformation preserve orientation or reverse it?
(b) Find the size of the box defined by these vectors. What is its orientation? 1 2 1
−1 0 1 2 −1 0
(c) Find the images under t of the vectors in the prior item and find the size of the box that they define. What is the orientation?
     1.14 By what factor does each transformation change the size of boxes?
Section II. Geometry of Determinants
351
  x   2x   x   3x−y  (a) y  → 3y (b) y  → −2x+y
x  x−y 
(c) y →x+y+z z y−2z
1.15 What is the area of the image of the rectangle [2..4] × [2..5] under the action of this matrix?
 2 3  4 −1
1.16 If t: R3 → R3 changes volumes by a factor of 7 and s: R3 → R3 changes volumes by a factor of 3/2 then by what factor will their composition changes volumes?
1.17 In what way does the definition of a box differ from the definition of a span? 1.18 Does |TS| = |ST|? |T(SP)| = |(TS)P|?
1.19 Show that there are no 2×2 matrices A and B satisfying these.
 1 −1   2 1  AB=20 BA=11
1.20 (a)Supposethat|A|=3andthat|B|=2.Find|A2·BT·B−2·AT|. (b) Assume that |A| = 0. Prove that |6A3 + 5A2 + 2A| = 0.
  1.21 Let T be the matrix representing (with respect to the standard bases) the map that rotates plane vectors counterclockwise thru θ radians. By what factor does T change sizes?
  1.22 Must a transformation t: R2 → R2 that preserves areas also preserve lengths? 1.23 What is the volume of a parallelepiped in R3 bounded by a linearly dependent
set?
  1.24 Find the area of the triangle in R3 with endpoints (1, 2, 1), (3, −1, 4), and (2, 2, 2). (This asks for area, not volume. The triangle defines a plane; what is the area of the triangle in that plane?)
1.25 An alternate proof of Theorem 1.5 uses the definition of determinant func- tions.
(a) Note that the vectors forming S make a linearly dependent set if and only if |S| = 0, and check that the result holds in this case.
(b) For the |S| ̸= 0 case, to show that |TS|/|S| = |T| for all transformations, consider the function d: Mn×n → R given by T  → |TS|/|S|. Show that d has the first property of a determinant.
(c) Show that d has the remaining three properties of a determinant function.
(d) Conclude that |T S| = |T | · |S|.
1.26 Give a non-identity matrix with the property that AT = A−1. Show that if
AT = A−1 then |A| = ±1. Does the converse hold?
1.27 The algebraic property of determinants that factoring a scalar out of a single row will multiply the determinant by that scalar shows that where H is 3×3, the determinant of cH is c3 times the determinant of H. Explain this geometrically, that is, using Theorem 1.5. (The observation that increasing the linear size of a three-dimensional object by a factor of c will increase its volume by a factor of c3 while only increasing its surface area by an amount proportional to a factor of c2 is the Square-cube law [Wikipedia, Square-cube Law].)
352 Chapter Four. Determinants
 1.28 We say that matrices H and G are similar if there is a nonsingular matrix P such that H = P−1GP (we will study this relation in Chapter Five). Show that similar matrices have the same determinant.
1.29 We usually represent vectors in R2 with respect to the standard basis so vectors in the first quadrant have both coordinates positive.
 ⃗v
 +3  RepE2 (⃗v) = +2
Moving counterclockwise around the origin, we cycle thru four regions:
 +   −   −   +  ···−→+ −→+ −→− −→− −→···.
Using this basis
gives the same counterclockwise cycle. We say these two bases have the same orientation.
(a) Why do they give the same cycle?
(b) What other configurations of unit vectors on the axes give the same cycle? (c) Find the determinants of the matrices formed from those (ordered) bases. (d) What other counterclockwise cycles are possible, and what are the associated
determinants?
(e) What happens in R1? (f) What happens in R3?
A fascinating general-audience discussion of orientations is in [Gardner].
1.30 This question uses material from the optional Determinant Functions Exist subsection. Prove Theorem 1.5 by using the permutation expansion formula for
the determinant.
  1.31 (a) Show that this gives the equation of a line in R2 thru (x2,y2) and (x3,y3).
x3 
y3  = 0
 (x3,y3) is
 0   −1  B=⟨,⟩ β⃗1
1 0 β⃗2
 x x2  y y2  1 1
1  
(b) [Petersen] Prove that the area of a triangle with vertices (x1,y1), (x2,y2), and
 x1 x2 1  y1 y2
x3   y3 .
1  
 2   1 1
(c) [Math. Mag., Jan. 1973] Prove that the area of a triangle with vertices at (x1, y1), (x2, y2), and (x3, y3) whose coordinates are integers has an area of N or N/2 for some positive integer N.
Section III. Laplace’s Formula 353
 III Laplace’s Formula
Determinants are a font of interesting and amusing formulas. Here is one that is often used to compute determinants by hand.
III.1 Laplace’s Expansion
The example shows a 3×3 case but the approach works for any size n > 1. 1.1 Example Consider the permutation expansion.
 t t t    1 0 0   1 0 0   1,1 1,2 1,3         
 t2,1 t2,2 t2,3  = t1,1t2,2t3,3  0 1 0  + t1,1t2,3t3,2  0 0 1 
 t3,1 t3,2 t3,3 
 0 0 1   0 1 0 
 0 1 0   0 1 0 
     + t1,2t2,1t3,3  1 0 0  + t1,2t2,3t3,1  0 0 1   0 0 1   1 0 0 
 0 0 1   0 0 1      
+ t1,3t2,1t3,2  1 0 0  + t1,3t2,2t3,1  0 1 0   0 1 0   1 0 0 
Pick a row or column and factor out its entries; here we do the entries in the first row.
  1 0 0   1 0 0      
= t1,1 · t2,2t3,3  0 1 0  + t2,3t3,2  0 0 1    0 0 1   0 1 0 
  0 1 0   0 1 0      
+ t1,2 · t2,1t3,3  1 0 0  + t2,3t3,1  0 0 1    0 0 1   1 0 0 
  0 0 1   0 0 1      
+ t1,3 · t2,1t3,2  1 0 0  + t2,2t3,1  0 1 0    0 1 0   1 0 0 
In those permutation matrices, swap to get the first rows into place. This requires one swap to each of the permutation matrices on the second line, and two swaps to each on the third line. (Recall that row swaps change the sign of
354 Chapter Four. Determinants the determinant.)
  1 0 0   1 0 0      
= t1,1 · t2,2t3,3  0 1 0  + t2,3t3,2  0 0 1    0 0 1   0 1 0 
  1 0 0   1 0 0      
− t1,2 · t2,1t3,3  0 1 0  + t2,3t3,1  0 0 1    0 0 1   0 1 0 
  1 0 0   1 0 0      
+ t1,3 · t2,1t3,2  0 1 0  + t2,2t3,1  0 0 1    0 0 1   0 1 0 
On each line the terms in square brackets involve only the second and third row and column, and simplify to a 2×2 determinant.
      
 t2,2 =t1,1 · 
 1.4 Example Where
1 2 3 T =  4 5 6 
789
 t3,2
t3,3    t3,1
t3,3    t3,1
t3,2  
t2,3    t2,1  −t1,2 · 
t2,3    t2,1  +t1,3 · 
t2,2    
The formula given in Theorem 1.5, which generalizes this example, is a recur- rence — the determinant is expressed as a combination of determinants. This formula isn’t circular because it gives the n×n case in terms of smaller ones.
1.3 Example The 1, 2 cofactor of the matrix from Example 1.1 is the negative of the second 2×2 determinant.
 1.2 Definition For any n×n matrix T, the (n − 1)×(n − 1) matrix formed by deleting row i and column j of T is the i,j minor of T. The i,j cofactorTi,j of T is (−1)i+j times the determinant of the i,j minor of T.
  
 t2,1 T1,2 = −1 ·  
these are the 1, 2 and 2, 2 cofactors.
     
1+2  4 6  T1,2 =(−1) ·   =6
T2,2 =(−1)
2+2  1 3 
·   =−12
 7 9 
 7 9 
 t3,1
t2,3    
t3,3  
Section III. Laplace’s Formula 355
  1.5 Theorem (Laplace Expansion of Determinants) Where T is an n×n matrix, we can find the determinant by expanding by cofactors on any row i or column j.
|T|=ti,1 ·Ti,1 +ti,2 ·Ti,2 +···+ti,n ·Ti,n =t1,j ·T1,j +t2,j ·T2,j +···+tn,j ·Tn,j
Proof Exercise 27. QED 1.6 Example We can compute the determinant
 1 2 3    
|T| =  4 5 6   7 8 9 
by expanding along the first row, as in Example 1.1.       
 5 6   4 6   4 5 
|T | = 1 · (+1)     + 2 · (−1)     + 3 · (+1)     = −3 + 12 − 9 = 0
 8 9   7 9   7 8  Or, we could expand down the second column.
      
 4 6   1 3   1 3 
|T | = 2 · (−1)     + 5 · (+1)     + 8 · (−1)     = 12 − 60 + 48 = 0
 7 9   7 9   4 6 
1.7 Example A row or column with many zeroes suggests a Laplace expansion.
 1 5 0             
     2 1   1 5   1 5 
 2 1 1  = 0 · (+1)     + 1 · (−1)     + 0 · (+1)     = 16  3 −1 0   3 −1   3 −1   2 1 
We finish by applying Laplace’s expansion to derive a new formula for the inverse of a matrix. With Theorem 1.5, we can calculate the determinant of a matrix by taking linear combinations of entries from a row with their associated cofactors.
ti,1 ·Ti,1 +ti,2 ·Ti,2 +···+ti,n ·Ti,n =|T| (∗) Recall that a matrix with two identical rows has a zero determinant. Thus,
weighting the cofactors by entries from row k with k ̸= i gives zero
ti,1 ·Tk,1 +ti,2 ·Tk,2 +···+ti,n ·Tk,n =0 (∗∗)
because it represents the expansion along the row k of a matrix with row i equal to row k. This summarizes (∗) and (∗∗).
 t1,1 t1,2 ... t1,n T1,1 T2,1 ... Tn,1 |T| 0 ... 0 
 t2,1 t2,2  . .
tn,1 tn,2
. . . . . .
t2,n T1,2 T2,2 ... Tn,2  0 |T| ... 0   . = .  . .
tn,n T1,n T2,n ... Tn,n 0 0 ... |T|
356 Chapter Four. Determinants
 Note that the order of the subscripts in the matrix of cofactors is opposite to the order of subscripts in the other matrix; e.g., along the first row of the matrix of cofactors the subscripts are 1, 1 then 2, 1, etc.
 1.8 Definition The matrix adjoint (or the adjugate) to the square matrix T is
. . . . . .
where Tj,i is the j, i cofactor.
T1,1 T2,1 T1,2 T2,2
Tn,1  Tn,2 
adj(T) =  .
T1,n T2,n . . . Tn,n
  . 
 1.9 Theorem Where T is a square matrix, T · adj(T) = adj(T) · T = |T| · I. Thus if T has an inverse, if |T| ̸= 0, then T−1 = (1/|T|) · adj(T).
Proof Equations (∗) and (∗∗). 1.10 Example If
QED
then adj(T) is
1 0 4 T =  2 1 − 1 
101
      
 1 −1   0 4   0 4  
  −    
T T T 01   01  1−1 10−4
  2 1   1 0   1 0     −    
 1 0   1 0   2 1 
and taking the product with T gives the diagonal matrix |T | · I.
1 0 41 0 −4 −3 0 0  2 1 − 1   − 3 − 3 9  =  0 − 3 0  101−101 00−3
The inverse of T is (1/ − 3) · adj(T).
 1/−3 0/−3 −4/−3 −1/3 0 4/3  T−1 = −3/−3 −3/−3 9/−3  =  1 1 −3 
−1/−3 0/−3 1/−3 1/3 0 −1/3
1,1 2,1 3,1     
    2 −1   1 4   1 4     T1,2 T2,2 T3,2=−        −   =−3 −3 9
       
 11  11   2−1 
T1,3 T2,3 T3,3               −1 0 1
Section III. Laplace’s Formula 357
 The formulas from this subsection are often used for by-hand calculation and are sometimes useful with special types of matrices. However, for generic matrices they are not the best choice because they require more arithmetic than, for instance, the Gauss-Jordan method.
Exercises
  1.11 Find the cofactor.
(a) T2,3 (b) T3,2 (c) T1,3
  1.12 Find the adjoint to this matrix.
102 T=−1 1 3
0 2 −1
102 T=−1 1 3
0 2 −1
1.13 This determinant is 0. Compute that by expanding on the first row.
 1 2 3   4 5 6   7 8 9 
  1.14 Find the determinant by expanding
 3 0 1 
 1 2 2   −1 3 0 
(a) on the first row (b) on the second row (c) on the third column. 1.15 Find the adjoint of the matrix in Example 1.6.
  1.16 Find the matrix adjoint to each.
2 1 4  3 −1   1 1  1 4 3
(a)−1 0 2 (b) 2 4 (c) 5 0 (d)−1 0 3 101 189
  1.17 Find the inverse of each matrix in the prior question with Theorem 1.9. 1.18 Find the matrix adjoint to this one.
2 1 0 0 1 2 1 0 0 1 2 1 0012
  1.19 Expand across the first row to derive the formula for the determinant of a 2×2 matrix.
  1.20 Expand across the first row to derive the formula for the determinant of a 3×3 matrix.
  1.21 (a) Give a formula for the adjoint of a 2×2 matrix. (b) Use it to derive the formula for the inverse.
  1.22 Can we compute a determinant by expanding down the diagonal?
358 Chapter Four. Determinants
 1.23 Give a formula for the adjoint of a diagonal matrix.
  1.24 Prove that the transpose of the adjoint is the adjoint of the transpose.
1.25 Prove or disprove: adj(adj(T)) = T.
1.26 A square matrix is upper triangular if each i, j entry is zero in the part above
the diagonal, that is, when i > j.
(a) Must the adjoint of an upper triangular matrix be upper triangular? Lower
triangular?
(b) Prove that the inverse of a upper triangular matrix is upper triangular, if an
inverse exists.
1.27 This question requires material from the optional Determinants Exist sub- section. Prove Theorem 1.5 by using the permutation expansion.
1.28 Prove that the determinant of a matrix equals the determinant of its transpose using Laplace’s expansion and induction on the size of the matrix.
?1.29Showthat
 1 −1 1 −1 1 −1 ...   1 1 0 1 0 1 ... 
   Fn= 0 1 1 0 1 0 ...   0 0 1 1 0 1 ...   . . . . . . ... 
where Fn is the n-th term of 1,1,2,3,5,...,x,y,x+y,..., the Fibonacci sequence, and the determinant is of order n − 1. [Am. Math. Mon., Jun. 1949]
Topic
Cramer’s Rule
A linear system is equivalent to a linear relationship among vectors.
x1+2x2=6  1   2   6  3x1+x2=8 ⇐⇒ x1·3+x2·1=8
In the picture below the small parallelogram is formed from sides that are the vectors  1  and  2 . It is nested inside a parallelogram with sides x1 1 
 313 and x2 2 . By the vector equation, the far corner of the larger parallelogram is
1  6 .
8
 6  8
  1  x1· 3
 1  3
 2   2  2 1
x· 1
This drawing restates the algebraic question of finding the solution of a linear system into geometric terms: by what factors x1 and x2 must we dilate the sides of the starting parallelogram so that it will fill the other one?
We can use this picture, and our geometric understanding of determinants, to get a new formula for solving linear systems. Compare the sizes of these shaded boxes.
 6  8
    1  3
 1  x1· 3
 2  111
 2   2 
360 Chapter Four. Determinants The second is defined by the vectors x1 1  and  2  and one of the properties of
31
the size function — the determinant — is that therefore the size of the second
box is x1 times the size of the first. The third box is derived from the second by
shearing, adding x2 2  to x1 1  to get x1 1  + x2 2  =  6 , along with  2 . The 133181
determinant is not affected by shearing so the size of the third box equals that of the second.
Taken together we have this.
          
 1 2   x1 ·1 2   x1 ·1+x2 ·2 2   6 2  x1 ·     =     =     =      3 1   x1·3 1   x1·3+x2·1 1   8 1 
Solving gives the value of one of the variables.
    6 2 
 8 1     
 1 2   3 1 
The generalization of this example is Cramer’s Rule: if |A| ̸= 0 then the system A⃗x = ⃗b has the unique solution xi = |Bi|/|A| where the matrix Bi is formed from A by replacing column i with the vector ⃗b. The proof is Exercise 3.
For instance, to solve this system for x2
1 0 4x 2
 x1 =
−10
= −5 = 2
  we do this computation.
1
 2 1 − 1   x 2  =  1 
101x3 −1  1 2 4 
  
 2 1 −1 
 1 −1 1 
x2= 10 4 =−3   
    2 1 −1   1 0 1 
−18
  Cramer’s Rule lets us by-eye solve systems that are small and simple. For example, we can solve systems with two equations and two unknowns, or three equations and three unknowns, where the numbers are small integers. Such cases appear often enough that many people find this formula handy.
But using it to solving large or complex systems is not practical, either by hand or by a computer. A Gauss’s Method-based approach is faster.
Exercises
1 Use Cramer’s Rule to solve each for each of the variables.
Topic: Cramer’s Rule 361
 (a)x−y=4 (b)−2x+y=−2 −x+2y=−7 x−2y=−2
2 Use Cramer’s Rule to solve this system for z.
2x+y+z=1 3x +z=4 x−y−z=2
3 Prove Cramer’s Rule.
4 Here is an alternative proof of Cramer’s Rule that doesn’t overtly contain any
geometry. Write Xi for the identity matrix with column i replaced by the vector ⃗x of unknowns x1, ..., xn.
(a) Observe that AXi = Bi.
(b) Take the determinant of both sides.
5 Suppose that a linear system has as many equations as unknowns, that all of
its coefficients and constants are integers, and that its matrix of coefficients has determinant 1. Prove that the entries in the solution are all integers. (Remark. This is often used to invent linear systems for exercises.)
6 Use Cramer’s Rule to give a formula for the solution of a two equations/two unknowns linear system.
7 Can Cramer’s Rule tell the difference between a system with no solutions and one with infinitely many?
8 The first picture in this Topic (the one that doesn’t use determinants) shows a unique solution case. Produce a similar picture for the case of infinitely many solutions, and the case of no solutions.
Topic
Speed of Calculating Determinants
For large matrices, finding the determinant by using row operations is typically much faster than using the permutation expansion. We make this statement precise by finding how many operations each method performs.
To compare the speed of two algorithms, we find for each one how the time taken grows as the size of its input data set grows. For instance, if we increase the size of the input by a factor of ten does the time taken grow by a factor of ten, or by a factor of a hundred, or by a factor of a thousand? That is, is the time taken proportional to the size of the data set, or to the square of that size, or to the cube of that size, etc.? An algorithm whose time is proportional to the square is faster than one that takes time proportional to the cube.
First consider the permutation expansion formula.
 t t ... t     1,1 1,2 1,n 
  t2,1 t2,2 ... t2,n      .   =
t1,φ(1)t2,φ(2) · · · tn,φ(n) |Pφ|  .  permutations φ
 tn,1 tn,2 . . . tn,n 
There are n! = n · (n − 1) · · · 2 · 1 different n-permutations so for a matrix with n rows this sum has n! terms (and inside each term is n-many multiplications). The factorial function grows quickly: when n is only 10 the expansion already has 10! = 3, 628, 800 terms. Observe that growth proportional to the factorial is bigger than growth proportional to the square n! > n2 because multiplying the first two factors in n! gives n · (n − 1), which for large n is approximately n2 and then multiplying in more factors will make the factorial even larger. Similarly, the factorial function grows faster than n3, etc. So an algorithm that uses the permutation expansion formula, and thus performs a number of operations at least as large as the factorial of the number of rows, would be very slow.
In contrast, the time taken by the row reduction method does not grow so fast. Below is a script for row reduction in the computer language Python. (Note: The code here is naive; for example it does not handle the case that the
Topic: Speed of Calculating Determinants 363
 m(p_row, p_row) entry is zero. Analysis of a finished version that includes all of the tests and subcases is messier but would gives us roughly the same speed results.)
import random
def random_matrix(num_rows, num_cols):
    m = for
[]
col in range(num_cols): new_row = []
for row in range(num_rows):
new_row.append(random.uniform(0,100)) m.append(new_row)
      return m
def gauss_method(m):
"""Perform Gauss's Method on m. This code is for illustration only and should not be used in practice.
m list of lists of numbers; each included list is a row """
num_rows, num_cols = len(m), len(m[0]) for p_row in range(num_rows):
for row in range(p_row+1, num_rows):
factor = -m[row][p_row] / float(m[p_row][p_row]) new_row = []
for col_num in range(num_cols):
p_entry, entry = m[p_row][col_num], m[row][col_num]
new_row.append(entry+factor*p_entry) m[row] = new_row
return m
response = raw_input('number of rows? ') num_rows = int(response)
m = for
                    random_matrix(num_rows, num_rows) row in m:
print row
gauss_method(m)
   M =
print "-----" for row in M:
   print row
Besides a routine to do Gauss’s Method, this program also has a routine to generate a matrix filled with random numbers (the numbers are between 0 and 100, to make them readable below). This program prompts a user for the number of rows, generates a random square matrix of that size, and does row reduction on it.
$ python gauss_method.py
number of rows? 4
[69.48033741746909, 32.393754742132586, 91.35245787350696, 87.04557918402462] [98.64189032145111, 28.58228108715638, 72.32273998878178, 26.310252241189257] [85.22896214660841, 39.93894635139987, 4.061683241757219, 70.5925099861901] [24.06322759315518, 26.699175587284373, 37.398583921673314, 87.42617087562161] -----
[69.48033741746909, 32.393754742132586, 91.35245787350696, 87.04557918402462] [0.0, -17.40743803545155, -57.37120602662462, -97.2691774792963]
[0.0, 0.0, -108.66513774392809, -37.31586824349682]
[0.0, 0.0, 0.0, -13.678536859817994]
Inside of the gauss_method routine, for each row prow, the routine performs factor · ρprow + ρrow on the rows below. For each of these rows below, this
           
364 Chapter Four. Determinants
 involves operating on every entry in that row. That is a triply-nested loop. So this program has a running time that is something like the cube of the number of rows in the matrix. (Comment. We are glossing over many issues. For example, we may worry that the time taken by the program is dominated by the time to store and retrieve entries from memory, rather than by the row operations. However, development of a computation model is outside of our scope.)
If we add this code at the bottom,
def do_matrix(num_rows): gauss_method(random_matrix(num_rows, num_rows))
import timeit
for num_rows in [10,20,30,40,50,60,70,80,90,100]:
s = "do_matrix("+str(num_rows)+")"
t = timeit.timeit(stmt=s, setup="from __main__ import do_matrix",
number=100)
print "num_rows=", num_rows, " seconds=", t
then Python will time the program. Here is the output from a timed test run.
num_rows= 10 seconds= 0.0162539482117 num_rows= 20 seconds= 0.0808238983154 num_rows= 30 seconds= 0.248152971268 num_rows= 40 seconds= 0.555531978607 num_rows= 50 seconds= 1.05453586578 num_rows= 60 seconds= 1.77881097794 num_rows= 70 seconds= 2.75969099998 num_rows= 80 seconds= 4.10647988319 num_rows= 90 seconds= 5.81125879288 num_rows= 100 seconds= 7.86893582344
Graphing that data gives part of the curve of a cubic.
                    8 7 6 5 4 3 2 1 0
20 40 60 80 100
                      Finding the fastest algorithm to compute the determinant is a topic of current research. So far, researchers have found algorithms that run in time between the square and cube of the number of rows.
The contrast between the times taken by the two determinant computation methods of permutation expansion and row operations makes the point that although in principle they give the same answer, in practice we want the one with the best performance.
Topic: Speed of Calculating Determinants 365 Exercises
1 To get an idea of what happens for typical matrices we can use the ability of computer systems to generate random numbers (of course, these are only pseudo- random in that they come from an algorithm but they pass a number of reasonable statistical tests for randomness).
(a) Fill a 5×5 array with random numbers say, in the range [0 ... 1)). See if it is singular. Repeat that experiment a few times. Are singular matrices frequent or rare in this sense?
(b) Time your computer algebra system at finding the determinant of ten 10×10 arrays of random numbers. Find the average time per array. Repeat the prior item for 20×20 arrays, 30×30 arrays, . . . 100×100 arrays, and compare to the numbers given above. (Notice that, when an array is singular, we can sometimes decide that quickly, for instance if the first row equals the second. In the light of your answer to the first part, do you expect that singular systems play a large role in your average?)
(c) Graph the input size versus the average time.
2 Compute the determinant of each of these by hand using the two methods discussed
 above.
 2 1  (a)    
 5 −3 
    2100 
 3 1 1  (b)  −1 0 5    −1 2 −2 
 1 3 2 0  (c)    
 0 −1 −2 1 
 0 0 −2 1 
Count the number of multiplications and divisions used in each case, for each of
the methods.
3 The use by the timing routine of do_matrix has a bug. That routine does
two things, generate a random matrix and then do gauss_method on it, and the timing number returned is for the combination. Produce code that times only the gauss_method routine.
4 What 10×10 array can you invent that takes your computer the longest time to reduce? The shortest?
5 Some computer language specifications requires that arrays be stored “by column,” that is, the entire first column is stored contiguously, then the second column, etc. Does the code fragment given take advantage of this, or can it be rewritten to make it faster, by taking advantage of the fact that computer fetches are faster from contiguous locations?
Topic
Chiò’s Method
When doing Gauss’s Method on a matrix that contains only integers people often like to keep it that way. To avoid fractions in the reduction of this matrix
2 1 1 A =  3 4 − 1 
151 they may start by multiplying the lower rows by 2
2 1 1
2ρ2 
−→ 6 8 −2 (∗)
2ρ3
so that elimination in the first column goes like this.
2 1 1 −3ρ1+ρ2  
 2 10 2
−→ 0 5 −5 (∗∗)
−ρ1 +ρ3
080
This all-integer approach is easier for mental calculations. And, using integer arithmetic on a computer avoids some sticky issues involving floating point calculations [Kahan]. So there are sound reasons for this approach.
Another advantage of this approach is that we can easily apply Laplace’s ex- pansion to the first column of (∗∗) and then get the determinant by remembering to divide by 4 because of (∗).
Here is the general 3×3 case of this approach to finding the determinant. First, assuming a1,1 ̸= 0, we can rescale the lower rows.
aaa a a a
1,1 1,2 A = a2,1 a2,2 a3,1 a3,2
1,3 1,1 a1,1ρ2 
1,2 a2,2a1,1
1,3 a2,3a1,1
a3,3 a1,1

a2,3 −→ a2,1a1,1 a1,1 ρ3

a3,3 a3,1 a1,1
a3,2 a1,1
Topic: Chiò’s Method 367
 This rescales the determinant by a21,1. Now eliminate down the first column.
−a2,1 ρ1 +ρ2 −→ −a3,1 ρ1 +ρ3
1,1
1,2 a2,2a1,1 − a2,1a1,2
a3,2a1,1 − a3,1a1,2
1,3
a2,3a1,1 − a2,1a1,3
a3,3a1,1 − a3,1a1,3
matrix of these four.
a 1,3
 a2,1 a2,2 a2,3  a3,3
a a a 
  0
0

Let C be the 1,1 minor. By Laplace the determinant of the above matrix is a1,1 det(C). We thus have a21,1 det(A) = a1,1 det(C) and since a1,1 ̸= 0 this gives det(A) = det(C)/a1,1.
To do larger matrices we must see how to compute the minor’s entries. The pattern above is that each element of the minor is a 2×2 determinant. For instance, the entry in the minor’s upper left a2,2a1,1 − a2,1a1,2, which is the 2, 2 entry in the above matrix, is the determinant of the matrix of these four elements of A.
a 1,3
 a2,3 
a3,1 a3,2 a3,3
And the minor’s lower left, the 3, 2 entry from above, is the determinant of the
a2,1
a1,1
a3,1
a1,2
  a2,2
  a1,2
  a3,2
  So, where A is n×n for n   3, we let Chiò’s matrix C be the (n−1)×(n−1)
matrix whose i, j entry is the determinant   
  a1,1 a1,j+1    ai+1,1 ai+1,j+1  
where 1 < i, j   n. Chiò’s method for finding the determinant of A is that if a1,1 ̸= 0 then det(A) = det(C)/an−2. (By the way, nothing in Chiò’s formula
requires that the numbers be integers; it applies to reals as well.) To illustrate we find the determinant of this 3×3 matrix.
This is Chiò’s matrix.
2 1 1 A =  3 4 − 1 
151
        2 1   2 1  
 3 4   3 −1     
a1,1
1,1
C=       = 5 −5         9 1  2 1   2 1 
 1 5   1 1 
368 Chapter Four. Determinants
 The formula for 3×3 matrices det(A) = det(C)/a1,1 gives det(A) = (50/2) = 25. For a larger determinant we must do multiple steps but each involves only 2×2 determinants. So we can often calculate the determinant just by writing
down a bit of intermediate information. For instance, with this 4×4 matrix
3 0 1 1
1 2 0 1 A =  2 − 1 0 3 
1001
we can mentally doing each of the 2×2 calculations and only write down the
3×3 result.
           3 0   3 1   3 1 
 1 2   1 0   1 1 
 
    6 −1 2  3 1       =−3 −2 7  2 3 
           3 0   3 1   3 1 
         3 0   3 1 
C3=        2 −1   2 0 
0 −1 2
 1 0   1 0   1 1 
Note that the determinant of this is a4−2 = 32 times the determinant of A.
1,1
To finish, iterate. Here is Chiò’s matrix of C3.
        6 −1   6 2 
 −3 −2   −3 7   
C2 =           =          6 −1   6 2 
 0 −1   0 2 
 
−15
−6 12
The determinant of this matrix is 6 times the determinant of C3. The determinant ofC2 is108.Sodet(A)=108/(32·6)=2.
Laplace’s expansion formula reduces the calculation of an n×n determinant to the evaluation of a number of (n − 1)×(n − 1) ones. Chiò’s formula is also recursive but it reduces an n×n determinant to a single (n − 1)×(n − 1) determinant, calculated from a number of 2×2 determinants. However, for large matrices Gauss’s Method is better than either of these; for instance, it takes roughly half as many operations as Chiò’s Method [Fuller & Logan].
Exercises
1 Use Chiò’s Method to find each determinant.
48
Topic: Chiò’s Method
369
     123 
(a)  4 5 6  (b)  7 8 9 
 2140   0140 
     1111 
 0 2 1 1 
2 What if a1,1 is zero?
3 The Rule of Sarrus is a mnemonic that many people learn for the 3×3 determinant
formula. To the right of the matrix, copy the first two columns.
abcab
defde
ghigh
Then the determinant is the sum of the three upper-left to lower-right diagonals
minus the three lower-left to upper-right diagonals aei+bfg+cdh−gec−hfa−idb.
Count the operations involved in Sarrus’s formula and in Chiò’s. 4 Prove Chiò’s formula.
Computer Code
This implements Chiò’s Method. It is in the computer language Python.
#!/usr/bin/python
# chio.py
# Calculate a determinant using Chio's method.
# Jim Hefferon; Public Domain
# For demonstration only; for instance, does not handle the M[0][0]=0 case
def det_two(a,b,c,d):
"""Return the determinant of the 2x2 matrix [[a,b], [c,d]]""" return a*d-b*c
def chio_mat(M):
"""Return the Chio matrix as a list of the rows
M nxn matrix, list of rows""" dim=len(M)
C=[]
for row in range(1,dim):
C.append([])
for col in range(1,dim):
C[-1].append(det_two(M[0][0], M[0][col], M[row][0], M[row][col])) return C
def chio_det(M,show=None):
"""Find the determinant of M by Chio's method
M mxm matrix, list of rows""" dim=len(M)
key_elet=M[0][0] if dim==1:
return key_elet
return chio_det(chio_mat(M))/(key_elet**(dim-2))
if __name__=='__main__': M=[[2,1,1], [3,4,-1], [1,5,1]] print "M=",M
print "Det is", chio_det(M)
This is the result of calling the program from a command line.
$ python chio.py
M=[[2, 1, 1], [3, 4, -1], [1, 5, 1]] Det is 25
                                         
Topic
Projective Geometry
There are geometries other than the familiar Euclidean one. One such geometry arose when artists observed that what a viewer sees is not necessarily what is there. As an example, here is Leonardo da Vinci’s The Last Supper.
Look at where the ceiling meets the left and right walls. In the room those lines are parallel but da Vinci has painted lines that, if extended, would intersect. The intersection is the vanishing point. This aspect of perspective is familiar as an image of railroad tracks that appear to converge at the horizon.
Da Vinci has adopted a model of how we see. Imagine a person viewing a room. From the person’s eye, in every direction, carry a ray outward until it intersects something, such as a point on the line where the wall meets the ceiling. This first intersection point is what the person sees in that direction. Overall what the person sees is the collection of three-dimensional intersection points projected to a common two dimensional image.
A
B
    C
Topic: Projective Geometry 371
 This is a central projection from a single point. As the sketch shows, this projection is not orthogonal like the ones we have seen earlier because the line from the viewer to C is not orthogonal to the image plane. (This model is only an approximation — it does not take into account such factors as that we have binocular vision or that our brain’s processing greatly affects what we perceive. Nonetheless the model is interesting, both artistically and mathematically.)
The operation of central projection preserves some geometric properties, for instance lines project to lines. However, it fails to preserve some others. One example is that equal length segments can project to segments of unequal length (above, AB is longer than BC because the segment projected to AB is closer to the viewer and closer things look bigger). The study of the effects of central projections is projective geometry.
There are three cases of central projection. The first is the projection done by a movie projector.
        projector P source S image I
We can think that each source point is pushed from the domain plane S outward to the image plane I. The second case of projection is that of the artist pulling the source back to a canvas.
      painter P image I source S
The two are different because first S is in the middle and then I. One more configuration can happen, with P in the middle. An example of this is when we use a pinhole to shine the image of a solar eclipse onto a paper.
372 Chapter Four. Determinants
      source S pinhole P image I
Although the three are not exactly the same, they are similar. We shall say that each is a central projection by P of S to I. We next look at three models of central projection, of increasing abstractness but also of increasing uniformity. The last model will bring out the linear algebra.
Consider again the effect of railroad tracks that appear to converge to a point. Model this with parallel lines in a domain plane S and a projection via a P to a codomain plane I. (The gray lines shown are parallel to the S plane and to the I plane.)
S
P
I
This single setting shows all three projection cases. The first picture below shows P acting as a movie projector by pushing points from part of S out to image points on the lower half of I. The middle picture shows P acting as the artist by pulling points from another part of S back to image points in the middle of I. In the third picture P acts as the pinhole, projecting points from S to the upper part of I. This third picture is the trickiest—the points that are projected near to the vanishing point are the ones that are far out on the lower left of S. Points in S that are near to the vertical gray line are sent high up on I.
SSS PPP
III
    
Topic: Projective Geometry 373
 There are two awkward things here. First, neither of the two points in the domain nearest to the vertical gray line (see below) has an image because a projection from those two is along the gray line that is parallel to the codomain plane (we say that these two are projected to infinity). The second is that the vanishing point in I isn’t the image of any point from S because a projection to this point would be along the gray line that is parallel to the domain plane (we say that the vanishing point is the image of a projection from infinity).
S
P
I
For a model that eliminates this awkwardness, cover the projector P with a hemispheric dome. In any direction, defined by a line through the origin, project anything in that direction to the single spot on the dome where the line intersects. This includes projecting things on the line between P and the dome, as with the movie projector. It includes projecting things on the line further from P than the dome, as with the painter. More subtly, it also includes things on the line that lie behind P, as with the pinhole case.
1
l = {k · 2 | k ∈ R}
3
More formally, for any nonzero vector ⃗v ∈ R3, let the associated point v in the projective plane be the set {k⃗v | k ∈ R and k ̸= 0} of nonzero vectors lying on the same line through the origin as ⃗v. To describe a projective point we can give any representative member of the line, so that the projective point shown above can be represented in any of these three ways.
1 1/3 −2 2 2/3 −4 3 1 −6
Each of these is a homogeneous coordinate vector for the point l.
  
374 Chapter Four. Determinants
 This picture and definition clarifies central projection but there is still something ungainly about the dome model: what happens when P looks down? Consider, in the sketch above, the part of P’s line of sight that comes up towards us, out of the page. Imagine that this part of the line falls, to the equator and below. Now the part of the line l that intersects the dome lies behind the page.
That is, as the line of sight continues down past the equator, the projective point suddenly shifts from the front of the dome to the back of the dome. (This brings out that the dome does not include the entire equator or else when the viewer is looking exactly along the equator then there would be two points in the line that are both on the dome. Instead we define the dome so that it includes the points on the equator with a positive y coordinate, as well as the point where y = 0 and x is positive.) This discontinuity means that we often have to treat equatorial points as a separate case. So while the railroad track model of central projection has three cases, the dome has two.
We can do better, we can reduce to a model having a single case. Consider a sphere centered at the origin. Any line through the origin intersects the sphere in two spots, said to be antipodal. Because we associate each line through the origin with a point in the projective plane, we can draw such a point as a pair of antipodal spots on the sphere. Below, we show the two antipodal spots connected by a dashed line to emphasize that they are not two different points, the pair of spots together make one projective point.
While drawing a point as a pair of antipodal spots on the sphere is not as intuitive as the one-spot-per-point dome mode, on the other hand the awkwardness of the dome model is gone in that as a line of view slides from north to south, no sudden changes happen. This central projection model is uniform.
So far we have described points in projective geometry. What about lines? What a viewer P at the origin sees as a line is shown below as a great circle, the intersection of the model sphere with a plane through the origin.
  
Topic: Projective Geometry 375
 (We’ve included one of the projective points on this line to bring out a subtlety. Because two antipodal spots together make up a single projective point, the great circle’s behind-the-paper part is the same set of projective points as its in-front-of-the-paper part.) Just as we did with each projective point, we can also describe a projective line with a triple of reals. For instance, the members of this plane through the origin in R3
x
{  y  | x + y − z = 0 }
z
project to a line that we can describe with (1 1 −1) (using a row vector for this typographically distinguishes lines from points). In general, for any nonzero three-wide row vector ⃗L we define the associated line in the projective plane, to be the set L = {k⃗L | k ∈ R and k ̸= 0}.
The reason this description of a line as a triple is convenient is that in the projective plane a point v and a line L are incident — the point lies on the line, the line passes through the point—if and only if a dot product of their representatives v1L1 + v2L2 + v3L3 is zero (Exercise 4 shows that this is independent of the choice of representatives ⃗v and ⃗L). For instance, the projective point described above by the column vector with components 1, 2, and 3 lies in the projective line described by (1 1 −1), simply because any vector in R3 whose components are in ratio 1 : 2 : 3 lies in the plane through the origin whose equation is of the form k·x+k·y−k·z = 0 for any nonzero k. That is, the incidence formula is inherited from the three-space lines and planes of which v and L are projections.
With this, we can do analytic projective geometry. For instance, the projective lineL=(1 1 −1)hastheequation1v1+1v2−1v3 =0,meaningthatforany projective point v incident with the line, any of v’s representative homogeneous coordinate vectors will satisfy the equation. This is true simply because those vectors lie on the three space plane. One difference from Euclidean analytic geometry is that in projective geometry besides talking about the equation of a line, we also talk about the equation of a point. For the fixed point
1 v = 2
3
the property that characterizes lines incident on this point is that the components of any representatives satisfy 1L1 + 2L2 + 3L3 = 0 and so this is the equation of v.
376 Chapter Four. Determinants
 This symmetry of the statements about lines and points is the Duality Principle of projective geometry: in any true statement, interchanging ‘point’ with ‘line’ results in another true statement. For example, just as two distinct points determine one and only one line, in the projective plane two distinct lines determine one and only one point. Here is a picture showing two projective lines that cross in antipodal spots and thus cross at one projective point.
(∗)
Contrast this with Euclidean geometry, where two unequal lines may have a unique intersection or may be parallel. In this way, projective geometry is simpler, more uniform, than Euclidean geometry.
That simplicity is relevant because there is a relationship between the two spaces: we can view the projective plane as an extension of the Euclidean plane. Draw the sphere model of the projective plane as the unit sphere in R3. Take Euclidean 2-space to be the plane z = 1. As shown below, all of the points on the Euclidean plane are projections of antipodal spots from the sphere. Conversely, we can view some points in the projective plane as corresponding to points in Euclidean space. (Note that projective points on the equator don’t correspond to points on the plane; instead we say these project out to infinity.)
(∗∗)
Thus we can think of projective space as consisting of the Euclidean plane with some extra points adjoined — the Euclidean plane is embedded in the projective plane. The extra points in projective space, the equatorial points, are called ideal points or points at infinity and the equator is called the ideal line or line at infinity (it is not a Euclidean line, it is a projective line).
The advantage of this extension from the Euclidean plane to the projective plane is that some of the nonuniformity of Euclidean geometry disappears. For instance, the projective lines shown above in (∗) cross at antipodal spots, a single projective point, on the sphere’s equator. If we put those lines into (∗∗) then they correspond to Euclidean lines that are parallel. That is, in moving
  
Topic: Projective Geometry 377
 from the Euclidean plane to the projective plane, we move from having two cases, that distinct lines either intersect or are parallel, to having only one case, that distinct lines intersect (possibly at a point at infinity).
A disadvantage of the projective plane is that we don’t have the same familiarity with it as we have with the Euclidean plane. Doing analytic geometry in the projective plane helps because the equations lead us to the right conclusions. Analytic projective geometry uses linear algebra. For instance, for three points of the projective plane t, u, and v, setting up the equations for those points by fixing vectors representing each shows that the three are collinear if and only if the resulting three-equation system has infinitely many row vector solutions representing their line. That in turn holds if and only if this determinant is zero.
 t u v    1 1 1 
 t2 u2 v2  
 t3 u3 v3  
Thus, three points in the projective plane are collinear if and only if any three representative column vectors are linearly dependent. Similarly, by duality, three lines in the projective plane are incident on a single point if and only if any three row vectors representing them are linearly dependent.
The following result is more evidence of the niceness of the geometry of the projective plane. These two triangles are in perspective from the point O because their corresponding vertices are collinear.
O T1
T2
Consider the pairs of corresponding sides: the sides T1U1 and T2U2, the sides T1V1 and T2V2, and the sides U1V1 and U2V2. Desargue’s Theorem is that when we extend the three pairs of corresponding sides, they intersect (shown here as the points TU, TV, and UV). What’s more, those three intersection points are collinear.
 V1 U1
   TU
UV
V2
U2
    TV
378 Chapter Four. Determinants
 We will prove this using projective geometry. (We’ve drawn Euclidean figures because that is the more familiar image. To consider them as projective figures we can imagine that, although the line segments shown are parts of great circles and so are curved, the model has such a large radius compared to the size of the figures that the sides appear in our sketch to be straight.)
For the proof we need a preliminary lemma [Coxeter]: if W, X, Y, Z are four points in the projective plane, no three of which are collinear, then there are homogeneous coordinate vectors w⃗ , ⃗x, ⃗y, and ⃗z for the projective points, and a basis B for R3, satisfying this.
1 0 0 1 RepB(w⃗ ) = 0 RepB(⃗x) = 1 RepB(⃗y) = 0 RepB(⃗z) = 1
0011
To prove the lemma, because W, X, and Y are not on the same projective line, any homogeneous coordinate vectors w⃗ 0, ⃗x0, and ⃗y0 do not line on the same plane through the origin in R3 and so form a spanning set for R3. Thus any homogeneous coordinate vector for Z is a combination ⃗z0 = a·w⃗ 0 +b·⃗x0 +c·⃗y0. ThenletthebasisbeB=⟨w⃗,⃗x,⃗y⟩andtakew⃗ =a·w⃗0,⃗x=b·⃗x0,⃗y=c·⃗y0, and ⃗z = ⃗z0.
To prove Desargue’s Theorem use the lemma to fix homogeneous coordinate vectors and a basis.
1 0 0 1 RepB(⃗t1) = 0 RepB(⃗u1) = 1 RepB(⃗v1) = 0 RepB(⃗o) = 1
0011
The projective point T2 is incident on the projective line OT1 so any homogeneous coordinate vector for T2 lies in the plane through the origin in R3 that is spanned by homogeneous coordinate vectors of O and T1:
1 1 RepB(⃗t2) = a 1 + b 0
10
for some scalars a and b. Hence the homogeneous coordinate vectors of members T2 of the line OT1 are of the form on the left below. The forms for U2 and V2 are similar.
t   1   1  2
RepB(⃗t2) =  1  RepB(⃗u2) = u2 RepB(⃗v2) =  1  1 1 v2
Topic: Projective Geometry 379
 The projective line T1U1 is the projection of a plane through the origin in R3. One way to get its equation is to note that any vector in it is linearly dependent on the vectors for T1 and U1 and so this determinant is zero.
 1 0 x    
 0 1 y =0 =⇒ z=0  0 0 z 
The equation of the plane in R3 whose image is the projective line T2U2 is this.
 t2 1 x 
 1 u2 y =0 =⇒ (1−u2)·x+(1−t2)·y+(t2u2 −1)·z=0  1 1 z 
Finding the intersection of the two is routine.
t2 −1 T 1 U 1 ∩ T 2 U 2 =  1 − u 2 
0
(This is, of course, a homogeneous coordinate vector of a projective point.) The other two intersections are similar.
1−t 0 2
T1V1 ∩ T2V2 = 0  U1V1 ∩ U2V2 =u2 −1 v2 − 1 1 − v2
Finish the proof by noting that these projective points are on one projective line because the sum of the three homogeneous coordinate vectors is zero.
Every projective theorem has a translation to a Euclidean version, although the Euclidean result may be messier to state and prove. Desargue’s theorem illustrates this. In the translation to Euclidean space, we must treat separately the case where O lies on the ideal line, for then the lines T1T2, U1U2, and V1V2 are parallel.
The remark following the statement of Desargue’s Theorem suggests thinking of the Euclidean pictures as figures from projective geometry for a sphere model with very large radius. That is, just as a small area of the world seems to people living there to be flat, the projective plane is locally Euclidean.
We finish by pointing out one more thing about the projective plane. Al- though its local properties are familiar, the projective plane has a perhaps unfamiliar global property. The picture below shows a projective point. At that point we have drawn Cartesian axes, xy-axes. Of course, the axes appear in the picture at both antipodal spots, one in the northern hemisphere (that is,
380 Chapter Four. Determinants
 shown on the right) and the other in the south. Observe that in the northern hemisphere a person who puts their right hand on the sphere, palm down, with their thumb on the y axis will have their fingers pointing along the x-axis in the positive direction.
The sequence of pictures below show a trip around this space: the antipodal spots rotate around the sphere with the spot in the northern hemisphere moving up and over the north pole, ending on the far side of the sphere, and its companion coming to the front. (Be careful: the trip shown is not halfway around the projective plane. It is a full circuit. The spots at either end of the dashed line are the same projective point. So by the third sphere below the trip has pretty much returned to the same projective point where we drew it starting above.)
=⇒ =⇒
At the end of the circuit, the x part of the xy-axes sticks out in the other direction. That is, for a person to put their thumb on the y-axis and have their fingers point positively on the x-axis, they must use their left hand. The projective plane is not orientable — in this geometry, left and right handedness are not fixed properties of figures (said another way, we cannot describe a spiral as clockwise or counterclockwise).
This exhibition of the existence of a non-orientable space raises the question of whether our universe orientable. Could an astronaut leave earth right-handed and return left-handed? [Gardner] is a nontechnical reference. [Clarke] is a classic science fiction story about orientation reversal.
For an overview of projective geometry see [Courant & Robbins]. The ap- proach we’ve taken here, the analytic approach, leads to quick theorems and illustrates the power of linear algebra; see [Hanes], [Ryan], and [Eggar]. But another approach, the synthetic approach of deriving the results from an axiom system, is both extraordinarily beautiful and is also the historical route of development. Two fine sources for this approach are [Coxeter] or [Seidenberg]. An easy and interesting application is in [Davies].
    
Topic: Projective Geometry
381
 Exercises
1
2
3 4
5 6 7
What is the equation of this point?
1
0 0
(a) Find the line incident on these points in the projective plane. 1 4
2 , 5 36
(b) Find the point incident on both of these projective lines. (1 2 3), (4 5 6)
Find the formula for the line incident on two projective points. Find the formula for the point incident on two projective lines.
Prove that the definition of incidence is independent of the choice of the rep- resentatives of p and L. That is, if p1, p2, p3, and q1, q2, q3 are two triples of homogeneous coordinates for p, and L1, L2, L3, and M1, M2, M3 are two triples of homogeneous coordinates for L, prove that p1L1 + p2L2 + p3L3 = 0 if and only if q1M1 +q2M2 +q3M3 =0.
Give a drawing to show that central projection does not preserve circles, that a circle may project to an ellipse. Can a (non-circular) ellipse project to a circle?
Give the formula for the correspondence between the non-equatorial part of the antipodal modal of the projective plane, and the plane z = 1.
(Pappus’s Theorem) Assume that T0, U0, and V0 are collinear and that T1, U1, and V1 are collinear. Consider these three points: (i) the intersection V2 of the lines T0U1 and T1U0, (ii) the intersection U2 of the lines T0V1 and T1V0, and (iii) the intersection T2 of U0V1 and U1V0.
(a) Draw a (Euclidean) picture.
(b) Apply the lemma used in Desargue’s Theorem to get simple homogeneous
coordinate vectors for the T’s and V0.
(c) Find the resulting homogeneous coordinate vectors for U’s (these must each
involve a parameter as, e.g., U0 could be anywhere on the T0V0 line).
(d) Find the resulting homogeneous coordinate vectors for V1. (Hint: it involves
two parameters.)
(e) Find the resulting homogeneous coordinate vectors for V2. (It also involves
two parameters.)
(f) Show that the product of the three parameters is 1. (g) Verify that V2 is on the T2U2 line.
Chapter Five
Similarity
We have shown that for any homomorphism there are bases B and D such that the matrix representing the map has a block partial-identity form.
 Identity Zero  RepB,D(h) = Zero Zero
This representation describes the map as sending c1β⃗ 1 + · · · + cnβ⃗ n to c1⃗δ1 + ··· + ck⃗δk +⃗0 + ··· +⃗0, where n is the dimension of the domain and k is the dimension of the range. Under this representation the action of the map is easy to understand because most of the matrix entries are zero.
This chapter considers the special case where the domain and codomain are the same. Here we naturally ask for the domain basis and codomain basis to be the same. That is, we want a basis B so that RepB,B(t) is as simple as possible, where we take ‘simple’ to mean that it has many zeroes. We will find that we cannot always get a matrix having the above block partial-identity form but we will develop a form that comes close, a representation that is nearly diagonal.
I Complex Vector Spaces
This chapter requires that we factor polynomials. But many polynomials do not factor over the real numbers; for instance, x2 + 1 does not factor into a product of two linear polynomials with real coefficients; instead it requires complex numbers x2 + 1 = (x − i)(x + i).
    
384 Chapter Five. Similarity
 Consequently in this chapter we shall use complex numbers for our scalars, including entries in vectors and matrices. That is, we shift from studying vector spaces over the real numbers to vector spaces over the complex numbers. Any real number is a complex number and in this chapter most of the examples use only real numbers but nonetheless, the critical theorems require that the scalars be complex. So this first section is a review of complex numbers.
In this book our approach is to shift to this more general context of taking scalars to be complex for the pragmatic reason that we must do so in order to move forward. However, the idea of doing vector spaces by taking scalars from a structure other than the real numbers is an interesting and useful one. Delightful presentations that take this approach from the start are in [Halmos] and [Hoffman & Kunze].
I.1 Polynomial Factoring and Complex Numbers
This subsection is a review only. For a full development, including proofs, see [Ebbinghaus].
Consider a polynomial p(x) = cnxn + · · · + c1x + c0 with leading coefficient cn ̸= 0. The degree of the polynomial is n. If n = 0 then p is a constant polynomial p(x) = c0. Constant polynomials that are not the zero polynomial, c0 ̸= 0, have degree zero. We define the zero polynomial to have degree −∞.
1.1 Remark Defining the degree of the zero polynomial to be −∞ allows the equation degree(fg) = degree(f) + degree(g) to hold for all polynomials.
Just as integers have a division operation — e.g., ‘4 goes 5 times into 21 with remainder 1’ — so do polynomials.
 1.2 Theorem (Division Theorem for Polynomials) Let p(x) be a polynomial. If d(x) is a non-zero polynomial then there are quotient and remainder polynomials q(x) and r(x) such that
p(x) = d(x) · q(x) + r(x)
where the degree of r(x) is strictly less than the degree of d(x).
The point of the integer statement ‘4 goes 5 times into 21 with remainder 1’ is that the remainder is less than 4 — while 4 goes 5 times, it does not go 6 times. Similarly, the final clause of the polynomial division statement is crucial.
1.3 Example If p(x) = 2x3 −3x2 +4x and d(x) = x2 +1 then q(x) = 2x−3 and
Section I. Complex Vector Spaces 385 r(x) = 2x + 3. Note that r(x) has a lower degree than does d(x).
Proof Theremaindermustbeaconstantpolynomialbecauseitisofdegreeless than the divisor x − λ. To determine the constant, take the theorem’s divisor d(x) to be x − λ and substitute λ for x. QED
If a divisor d(x) goes into a dividend p(x) evenly, meaning that r(x) is the zero polynomial, then d(x) is a called a factor of p(x). Any root of the factor, any λ ∈ R such that d(λ) = 0, is a root of p(x) since p(λ) = d(λ) · q(λ) = 0.
Proof By the above corollary p(x) = (x − λ) · q(x) + p(λ). Since λ is a root, p(λ) = 0 so x − λ is a factor. QED
A repeated root of a polynomial is a number λ such that the polynomial is evenly divisible by (x − λ)n for some power larger than one. The largest such power is called the multiplicity of λ.
Finding the roots and factors of a high-degree polynomial can be hard. But for second-degree polynomials we have the quadratic formula: the roots of ax2 +bx+c are these
√√
λ1 = −b + b2 − 4ac λ2 = −b − b2 − 4ac 2a 2a
(if the discriminant b2 − 4ac is negative then the polynomial has no real number roots). A polynomial that cannot be factored into two lower-degree polynomials with real number coefficients is said to be irreducible over the reals.
  1.4 Corollary The remainder when p(x) is divided by x − λ is the constant polynomial r(x) = p(λ).
 1.5 Corollary If λ is a root of the polynomial p(x) then x − λ divides p(x) evenly, that is, x−λ is a factor of p(x).
     1.6 Theorem Any constant or linear polynomial is irreducible over the reals. A quadratic polynomial is irreducible over the reals if and only if its discriminant is negative. No cubic or higher-degree polynomial is irreducible over the reals.
 1.7 Corollary Any polynomial with real coefficients can be factored into linear and irreducible quadratic polynomials. That factorization is unique; any two factorizations have the same powers of the same factors.
Note the analogy with the prime factorization of integers. In both cases the uniqueness clause is very useful.
386 Chapter Five. Similarity 1.8 Example Because of uniqueness we know, without multiplying them out, that
(x+3)2(x2 +1)3 does not equal (x+3)4(x2 +x+1)2.
1.9 Example By uniqueness, if c(x) = m(x)·q(x) then where c(x) = (x−3)2(x+2)3
and m(x) = (x − 3)(x + 2)2, we know that q(x) = (x − 3)(x + 2).
While x2 +1 has no real roots and so doesn’t factor over the real numbers, if we imagine a root — traditionally denoted i, so that i2 + 1 = 0 — then x2 + 1 factors into a product of linears (x−i)(x+i). When we adjoin this root i to the reals and close the new system with respect to addition and multiplication then we have the complex numbers C = {a+bi|a,b∈Randi2 =−1}. (These are often pictured on a plane with a plotted on the horizontal axis and b on the vertical; note that the distance of the point from the origin is |a + bi| = √a2 + b2.)
In C all quadratics factor. That is, in contrast with the reals, C has no irreducible quadratics.
√√ ax2+bx+c=a· x−−b+ b2−4ac · x−−b− b2−4ac 
2a 2a
1.10 Example The second degree polynomial x2 + x + 1 factors over the complex numbers into the product of two first degree polynomials.
√√√√
 x−−1+ −3  x−−1− −3 = x−(−1+ 3i)  x−(−1− 3i)  222222
                 1.11 Theorem (Fundamental Theorem of Algebra) Polynomials with complex coeffi- cients factor into linear polynomials with complex coefficients. The factorization is unique.
I.2 Complex Representations
Recall the definitions of the complex number addition
(a+bi) + (c+di)=(a+c)+(b+d)i
and multiplication.
(a + bi)(c + di) = ac + adi + bci + bd(−1) = (ac − bd) + (ad + bc)i
2.1Example Forinstance,(1−2i)+(5+4i)=6+2iand(2−3i)(4−0.5i)= 6.5 − 13i.
Section I. Complex Vector Spaces 387 With these rules, all of the operations that we’ve used for real vector spaces
carry over unchanged to vector spaces with complex scalars.
2.2 Example Matrix multiplication is the same, although the scalar arithmetic involves more bookkeeping.
 1+1i 2−0i   1+0i 1−0i  i −2+3i 3i −i
 (1+1i)·(1+0i)+(2−0i)·(3i) (1+1i)·(1−0i)+(2−0i)·(−i)  = (i)·(1+0i)+(−2+3i)·(3i) (i)·(1−0i)+(−2+3i)·(−i)
 1+7i 1−1i  = −9−5i 3+3i
We shall carry over unchanged from the previous chapters everything that we can. For instance, we shall call this
1+0i 0+0i
0 + 0i 0 + 0i ⟨ . ,..., . ⟩  .   . 
0+0i 1+0i
the standard basis for Cn as a vector space over C and again denote it En. Another example is that Pn will be the vector space of degree n polynomials with coefficients that are complex.
 
388 Chapter Five. Similarity II Similarity
We’ve defined two matrices H and Hˆ to be matrix equivalent if there are nonsingular P and Q such that Hˆ = PHQ. We were motivated by this diagram showing H and Hˆ both representing a map h, but with respect to different pairs of bases, B,D and Bˆ,Dˆ.
h
Vwrt B −−−−→ Wwrt D
H

 id 
VwrtBˆ −−−−→WwrtDˆ
The top is first. The effect of the transformation on the starting basis B
h Hˆ
We now consider the special case of transformations, where the codomain equals the domain, and we add the requirement that the codomain’s basis equals the domain’s basis. So, we are considering representations with respect to B, B and D, D.
t
Vwrt B −−−−→ Vwrt B
T

id 
Vwrt D −−−−→ Vwrt D
t Tˆ
In matrix terms, RepD,D(t) = RepB,D(id) RepB,B(t)  RepB,D(id) −1. II.1 Definition and Examples
1.1 Example Consider the derivative transformation d/dx: P2 → P2, and two bases for that space B = ⟨x2,x,1⟩ and D = ⟨1,1 + x,1 + x2⟩ We will compute the four sides of the arrow square.
P2 wrt B −−−−→ P2 wrt B
T

d/dx
id 
P2 wrt D −−−−→ P2 wrt D
d/dx Tˆ
2 d/dx d/dx d/dx
x  −→2x x −→1 1 −→0
id 
id 
id 
Section II. Similarity
389
 represented with respect to the ending basis (also B) 0 0
0 RepB(0) = 0
RepB(2x) = 2 RepB(1) = 0 010
gives the representation of the map.
T = RepB,B(d/dx) = 2 0 0
Next, the bottom. The effect of the transformation on elements of D d/dx d/dx 2 d/dx
1  −→ 0 1 + x  −→ 1 1 + x  −→ 2x represented with respect to D gives the matrix Tˆ.
0 1 −2 Tˆ = RepD,D(d/dx) = 0 0 2 
000
Third, computing the matrix for the right-hand side involves finding the effect of the identity map on the elements of B. Of course, the identity map does not transform them at all so to find the matrix we represent B’s elements with respect to D.
−1 −1 1 RepD(x2) =  0  RepD(x) =  1  RepD(1) = 0
100
So the matrix for going down the right side is the concatenation of those.
−1 −1 1 P = RepB,D(id) =  0 1 0
100
With that, we have two options to compute the matrix for going up on left side. The direct computation represents elements of D with respect to B
0 0 1
RepB(1) = 0 RepB(1 + x) = 1 RepB(1 + x2) = 0 111
0 0 0 010
390 Chapter Five. Similarity and concatenates to make the matrix.
0 0 1 0 1 0 111
The other option to compute the matrix for going up on the left is to take the inverse of the matrix P for going down on the right.
−1 −1 1 1 0 0 1 0 0 0 0 1  0 1 0 0 1 0  − → · · · − →  0 1 0 0 1 0  100001 001111
Since nonsingular matrices are square, T and Tˆ must be square and of the same size. Exercise 15 checks that similarity is an equivalence relation.
1.3 Example The definition does not require that we consider a map. Calculation
        1.2 Definition The matrices T and Tˆ are similar if there is a nonsingular P such that Tˆ = PTP−1.
with these two
gives that T is similar to this matrix.
 2 1   2 −3  P= 1 1 T= 1 −1
ˆ  12 −19  T= 7 −11
1.4 Example The only matrix similar to the zero matrix is itself: PZP−1 = PZ = Z. The identity matrix has the same property: PIP−1 = PP−1 = I.
Matrix similarity is a special case of matrix equivalence so if two matrices are similar then they are matrix equivalent. What about the converse: if they are square, must any two matrix equivalent matrices be similar? No; the matrix equivalence class of an identity matrix consists of all nonsingular matrices of that size while the prior example shows that the only member of the similarity class of an identity matrix is itself. Thus these two are matrix equivalent but not similar.
 1 0   1 2  T=01 S=03
So some matrix equivalence classes split into two or more similarity classes — similarity gives a finer partition than does matrix equivalence. This shows some matrix equivalence classes subdivided into similarity classes.
Section II. Similarity 391
     S T
...
  To understand the similarity relation we shall study the similarity classes. We approach this question in the same way that we’ve studied both the row equivalence and matrix equivalence relations, by finding a canonical form for representatives of the similarity classes, called Jordan form. With this canonical form, we can decide if two matrices are similar by checking whether they are in a class with the same representative. We’ve also seen with both row equivalence and matrix equivalence that a canonical form gives us insight into the ways in which members of the same class are alike (e.g., two identically-sized matrices are matrix equivalent if and only if they have the same rank).
Exercises
1.5 For
 13 ˆ 0 0   42  T= −2 −6 T= −11/2 −5 P= −3 2
check that Tˆ = PTP−1.
1.6 Example 1.4 shows that the only matrix similar to a zero matrix is itself and
that the only matrix similar to the identity is itself.
(a) Show that the 1×1 matrix whose single entry is 2 is also similar only to itself. (b) Is a matrix of the form cI for some scalar c similar only to itself?
(c) Is a diagonal matrix similar only to itself?
  1.7 Consider this transformation of C3
x x − z
and these bases.
t(y)= z  z 2y
1 0 0 1 1 1
B = ⟨2,1,0⟩ D = ⟨0,1,0⟩ 301 001
We will compute the parts of the arrow diagram to represent the transformation using two similar matrices.
(a) Draw the arrow diagram, specialized for this case.
(b) Compute T = RepB,B(t).
(c) Compute Tˆ = RepD,D(t).
(d) Compute the matrices for other the two sides of the arrow square.
1.8 Consider the transformation t: P2 → P2 described by x2  → x + 1, x  → x2 − 1, and 1  → 3.
(a) Find T = RepB,B(t) where B = ⟨x2, x, 1⟩.
(b) Find Tˆ = RepD,D(t) where D = ⟨1, 1 + x, 1 + x + x2⟩. (c) Find the matrix P such that Tˆ = PTP−1.
392 Chapter Five. Similarity
  1.9 Let T represent t: C2 → C2 with respect to B,B.
 1 −1   1  1   2  0  T= 2 1 B=⟨ 0 , 1 ⟩, D=⟨ 0 , −2 ⟩
We will convert to the matrix representing t with respect to D, D.
(a) Draw the arrow diagram.
(b) Give the matrix that represents the left and right sides of that diagram, in
the direction that we traverse the diagram to make the conversion. (c) Find RepD,D(t).
  1.10 Exhibit an nontrivial similarity relationship by letting t: C2 → C2 act in this way,
 1   3   −1   −1  2  → 0 1  → 2
picking two bases B,D, and representing t with respect to them, Tˆ = RepB,B(t) and T = RepD,D(t). Then compute the P and P−1 to change bases from B to D and back again.
  1.11 Show that these matrices are not similar.
1 0 4 1 0 1
1 1 3 0 1 1 217 312
1.12 Explain Example 1.4 in terms of maps.
  1.13 [Halmos] Are there two matrices A and B that are similar while A2 and B2 are
not similar?
  1.14 Prove that if two matrices are similar and one is invertible then so is the other.
1.15 Show that similarity is an equivalence relation. (The definition given earlier already reflects this, so instead start here with the definition that Tˆ is similar to T if Tˆ = PTP−1.)
1.16 Consider a matrix representing, with respect to some B,B, reflection across the x-axis in R2. Consider also a matrix representing, with respect to some D,D, reflection across the y-axis. Must they be similar?
1.17 Prove that similarity preserves determinants and rank. Does the converse hold? 1.18 Is there a matrix equivalence class with only one matrix similarity class inside?
One with infinitely many similarity classes?
1.19 Can two different diagonal matrices be in the same similarity class?
  1.20 Prove that if two matrices are similar then their k-th powers are similar when k>0. What if k 0?
  1.21 Let p(x) be the polynomial cnxn +···+c1x+c0. Show that if T is similar to Sthenp(T)=cnTn +···+c1T+c0Iissimilartop(S)=cnSn +···+c1S+c0I.
1.22 List all of the matrix equivalence classes of 1×1 matrices. Also list the similarity classes, and describe which similarity classes are contained inside of each matrix equivalence class.
1.23 Does similarity preserve sums?
1.24 Show that if T − λI and N are similar matrices then T and N + λI are also similar.
 
Section II. Similarity 393 II.2 Diagonalizability
The prior subsection shows that although similar matrices are necessarily matrix equivalent, the converse does not hold. Some matrix equivalence classes break into two or more similarity classes; for instance, the nonsingular 2×2 matrices form one matrix equivalence class but more than one similarity class.
Thus we cannot use the canonical form for matrix equivalence, a block partial-identity matrix, as a canonical form for matrix similarity. The diagram below illustrates. The stars are similarity class representatives. Each dashed-line similarity class subdivision has one star but each solid-curve matrix equivalence class division has only one partial identity matrix.
To develop a canonical form for representatives of the similarity classes we naturally build on previous work. This means first that the partial identity matrices should represent the similarity classes into which they fall. Beyond that, the representatives should be as simple as possible. The simplest extension of the partial identity form is the diagonal form.
     ⋆
⋆ ...
⋆
 2.1 Definition A transformation is diagonalizable if it has a diagonal represen- tation with respect to the same basis for the codomain as for the domain. A diagonalizable matrix is one that is similar to a diagonal matrix: T is diagonal- izable if there is a nonsingular P such that PTP−1 is diagonal.
2.2 Example The matrix is diagonalizable.
 4 −2  11
⋆ ⋆⋆⋆ ⋆⋆
 2 0   −1 2   4 −2  −1 2  −1 0 3 = 1 −1 1 1 1 −1
2.3 Example We will show that this matrix is not diagonalizable.  0 0 
N=10
The fact that N is not the zero matrix means that it cannot be similar to the zero matrix, because the zero matrix is similar only to itself. Thus if N were to
394 Chapter Five. Similarity
 be similar to a diagonal matrix then that matrix would have have at least one nonzero entry on its diagonal.
The square of N is the zero matrix. This implies that for any map n represented by N (with respect to some B, B) the composition n ◦ n is the zero map. This in turn implies that for any matrix representing n (with respect to some Bˆ,Bˆ), its square is the zero matrix. But the square of a nonzero diagonal matrix cannot be the zero matrix, because the square of a diagonal matrix is the diagonal matrix whose entries are the squares of the entries from the starting matrix. Thus there is no Bˆ,Bˆ such that n is represented by a diagonal matrix— the matrix N is not diagonalizable.
That example shows that a diagonal form will not suffice as a canonical form for similarity — we cannot find a diagonal matrix in each matrix similarity class. However, some similarity classes contain a diagonal matrix and the canonical form that we are developing has the property that if a matrix can be diagonalized then the diagonal matrix is the canonical representative of its similarity class.
Proof Consider a diagonal representation matrix.
 .
RepB,B(t) = RepB(t(β⃗1)) ···
 2.4 Lemma A transformation t is diagonalizable if and only if there is a basis B = ⟨β⃗1,...,β⃗n⟩ and scalars λ1,...,λn such that t(β⃗i) = λiβ⃗i for each i.
Consider the representation of a member of this basis with respect to the basis RepB(β⃗i). The product of the diagonal matrix and the representation vector
has the stated action.
2.5 Example To diagonalize
n 00
 3 2  T=01
QED
RepB(t(β⃗i)) =  . 0
.. . 1 = λi λ .  . 
. 
RepB(t(β⃗n)) =  . .. . 
 . ..n
λ1 0 .  .  .. .
we take T as the representation of a transformation with respect to the standard
0 0 ..
λ1 0 ...
.  0 λ
Section II. Similarity 395 basis RepE2,E2(t) and look for a basis B = ⟨β⃗1,β⃗2⟩ such that
 λ1 0  RepB,B(t) = 0 λ2
that is, such that t(β⃗ 1) = λ1β⃗ 1 and t(β⃗ 2) = λ2β⃗ 2.  3 2   3 2 
0 1 β⃗1=λ1·β⃗1 0 1 β⃗2=λ2·β⃗2 We are looking for scalars x such that this equation
01b2 b2
has solutions b1 and b2 that are not both 0 (the zero vector is not the member
  3 2  b   b  1=x·1
of any basis). That’s a linear system.
(3−x)·b1 + 2·b2 =0 (1−x)·b2 =0
(∗)
Focus first on the bottom equation. There are two cases: either b2 = 0 or x = 1. In the b2 = 0 case the first equation gives that either b1 = 0 or x = 3. Since we’ve disallowed the possibility that both b2 = 0 and b1 = 0, we are left with the first diagonal entry λ1 = 3. With that, (∗)’s first equation is 0·b1 +2·b2 = 0 and so associated with λ1 = 3 are vectors having a second component of zero
while the first component is free.
 3 2  b   b  0 1 01 =3· 01
To get a first basis vector choose any nonzero b1.  1 
β⃗ 1 = 0
The other case for the bottom equation of (∗) is λ2 = 1. Then (∗)’s first
equation is 2 · b1 + 2 · b2 = 0 and so associated with this case are vectors whose
second component is the negative of the first.
 3 2  b    b   1=1·1
0 1 −b1 −b1
Get the second basis vector by choosing a nonzero one of these.
 1  β⃗ 2 = − 1
396
Chapter Five. Similarity
 Now draw the similarity diagram
02 01
2t2
Rwrt E2 −−−−→ Rwrt E2 T 
id  id  2t2
Rwrt B −−−−→ Rwrt B D
and note that the matrix RepB,E2 (id) is easy, giving this diagonalization.  3 0   1 1 −1 3 2  1 1 
0 1 = 0 −1 0 1 0 −1
In the next subsection we will expand on that example by considering more closely the property of Lemma 2.4. This includes seeing a streamlined way to find the λ’s.
Exercises
  2.6 Repeat Example 2.5 for the matrix from Example 2.2.
2.7 Diagonalize these upper triangular matrices.  −2 1   5 4 
(a) (b)
  2.8 What form do the powers of a diagonal matrix have?
2.9 Give two same-sized diagonal matrices that are not similar. Must any two
different diagonal matrices come from different similarity classes?
2.10 Give a nonsingular diagonal matrix. Can a diagonal matrix ever be singular?
  2.11 Show that the inverse of a diagonal matrix is the diagonal of the the inverses, if no element on that diagonal is zero. What happens when a diagonal entry is zero?
2.12 The equation ending Example 2.5
 1 1 −1 3 2  1 1   3 0 
0 −1 0 1 0 −1 = 0 1
is a bit jarring because for P we must take the first matrix, which is shown as an inverse, and for P−1 we take the inverse of the first matrix, so that the two −1
powers cancel and this matrix is shown without a superscript −1. (a) Check that this nicer-appearing equation holds.
 3 0   1 1  3 2  1 1 −1 0 1 = 0 −1 0 1 0 −1
(b) Is the previous item a coincidence? Or can we always switch the P and the P−1?
2.13 Show that the P used to diagonalize in Example 2.5 is not unique. 2.14 Find a formula for the powers of this matrix. Hint: see Exercise 8.
 −3 1  −4 2
  2.15 Diagonalize these.
Section II. Similarity 397  1 1   0 1 
 (a) (b)
00 10
2.16 We can ask how diagonalization interacts with the matrix operations. Assume that t, s : V → V are each diagonalizable. Is ct diagonalizable for all scalars c? What about t+s? t◦s?
  2.17 Show that matrices of this form are not diagonalizable.
 1 c  01
2.18 Show that each of these is diagonalizable.  1 2   x y 
(a) 2 1 (b) y z x,y,z scalars
II.3 Eigenvalues and Eigenvectors
We will next focus on the property of Lemma 2.4.
(“Eigen” is German for “characteristic of” or “peculiar to.” Some authors call these characteristic values and vectors. No authors call them “peculiar.”)
3.2 Example The projection map
x x
π
y  −→ y x,y,z ∈ C
z0
has an eigenvalue of 1 associated with any eigenvector
x y
0
where x and y are scalars that are not both zero.
In contrast, a number that is not an eigenvalue of of this map is 2, since
assuming that π doubles a vector leads to the equations x = 2x, y = 2y, and 0 = 2z, and thus no non-⃗0 vector is doubled.
Note that the definition requires that the eigenvector be non-⃗0. Some authors allow ⃗0 as an eigenvector for λ as long as there are also non-⃗0 vectors associated with λ. The key point is to disallow the trivial case where λ is such that t(⃗v) = λ⃗v for only the single vector ⃗v = ⃗0.
Also, note that the eigenvalue λ could be 0. The issue is whether ⃗ζ equals ⃗0.
c ̸= 0
 3.1 Definition A transformation t: V → V has a scalar eigenvalue λ if there is a nonzero eigenvector ⃗ζ ∈ V such that t(⃗ζ) = λ · ⃗ζ.
398 Chapter Five. Similarity
 3.3 Example The only transformation on the trivial space {⃗0} is ⃗0  → ⃗0. This map has no eigenvalues because there are no non-⃗0 vectors ⃗v mapped to a scalar multiple λ · ⃗v of themselves.
3.4 Example Consider the homomorphism t: P1 → P1 given by c0 + c1x  → (c0 + c1) + (c0 + c1)x. While the codomain P1 of t is two-dimensional, its range is one-dimensional R(t) = {c + cx | c ∈ C}. Application of t to a vector in that range will simply rescale the vector c + cx  → (2c) + (2c)x. That is, t has an eigenvalue of 2 associated with eigenvectors of the form c + cx where c ̸= 0.
This map also has an eigenvalue of 0 associated with eigenvectors of the form c − cx where c ̸= 0.
The definition above is for maps. We can give a matrix version.
This extension of the definition for maps to a definition for matrices is natural but there is a point on which we must take care. The eigenvalues of a map are also the eigenvalues of matrices representing that map, and so similar matrices have the same eigenvalues. However, the eigenvectors can differ — similar matrices need not have the same eigenvectors. The next example explains.
 3.5 Definition A square matrix T has a scalar eigenvalue λ associated with the nonzero eigenvector ⃗ζ if T ⃗ζ = λ · ⃗ζ.
3.6 Example These matrices are similar
 20  ˆ  4−2 
T=00 T=4−2 since Tˆ = PTP−1 for this P.
 1 1   2 −1  P=12 P−1=−11
The matrix T has two eigenvalues, λ1 = 2 and λ2 = 0. The first one is associated with this eigenvector.
 2 0  1   2  T⃗e1=00 0=0=2⃗e1
Suppose that T represents a transformation t: C2 → C2 with respect to the standard basis. Then the action of this transformation t is simple.
    
x t 2x y  −→ 0
Section II. Similarity 399 Of course, Tˆ represents the same transformation but with respect to a different
basis B. We can easily find this basis. The arrow diagram t
T

 Vwrt E3 −−−−→ Vwrt E3
id  Vwrt B −−−−→ Vwrt B
id 
t Tˆ
shows that P−1 = RepB,E3 (id). By the definition of the matrix representation of a map, its first column is Rep (id(β⃗ )) = Rep (β⃗ ). With respect to the
E3 1 E31
standard basis any vector is represented by itself, so the first basis element β⃗ 1 is
the first column of P−1. The same goes for the other one.  2   −1 
B=⟨ −1 , 1 ⟩
Since the matrices T and Tˆ both represent the transformation t, both reflect the
action t(⃗e1) = 2⃗e1.
RepE2,E2 (t) · RepE2 (⃗e1) = T · RepE2 (⃗e1) = 2 · RepE2 (⃗e1)
RepB,B(t) · RepB(⃗e1) = Tˆ · RepB(⃗e1) = 2 · RepB(⃗e1)
But while in those two equations the eigenvalue 2’s are the same, the vector
representations differ.
 1   1  T·RepE2(⃗e1)=T 0 =2· 0
ˆ ˆ  1   1  T·RepB(⃗e1)=T· 1 =2· 1
That is, when the matrix representing the transformation is T = RepE2 ,E2 (t) then it “assumes” that column vectors are representations with respect to E2. However Tˆ = RepB,B(t) “assumes” that column vectors are representations with respect to B, and so the column vectors that get doubled are different.
We next see the basic tool for finding eigenvectors and eigenvalues. 3.7 Example If
121 T =  2 0 − 2 
−1 2 3
400 Chapter Five. Similarity
 then to find the scalars x such that T⃗ζ = x⃗ζ for nonzero eigenvectors ⃗ζ, bring everything to the left-hand side
1 2 1z z 11
 2 0 − 2   z 2  − x  z 2  = ⃗ 0 −123z3 z3
and factor (T − xI)⃗ζ = ⃗0. (Note that it says T − xI. The expression T − x doesn’t make sense because T is a matrix while x is a scalar.) This homogeneous linear
system
1−x2 1z0 1
 2 0−x −2 z2=0 −1 2 3−x z3 0
has a nonzero solution ⃗z if and only if the matrix is singular. We can determine when that happens.
0 = |T − xI|
 1−x 2 1 
   = 2 0−x −2    −1 2 3−x 
=x3 −4x2 +4x = x(x − 2)2
The eigenvalues are λ1 = 0 and λ2 = 2. To find the associated eigenvectors plug in each eigenvalue. Plugging in λ1 = 0 gives
1−02 1z0 za 11
 2 0−0 −2 z2=0 =⇒ z2=−a −123−0z3 0 z3 a
for a ̸= 0 (a must be non-0 because eigenvectors are defined to be non-⃗0). Plugging in λ2 = 2 gives
1−22 1z0 zb 11
 2 0−2 −2 z2=0 =⇒ z2=0 −1 2 3−2 z3 0 z3 b
with b ̸= 0. 3.8 Example If
 π 1  S=03
Section II. Similarity 401 (here π is not a projection map, it is the number 3.14 . . .) then
  
 π−x 1  
    = (x − π)(x − 3)  0 3−x 
so S has eigenvalues of λ1 = π and λ2 = 3. To find associated eigenvectors, first plug in λ1 for x
  π−π 1   z   0   z   a  1= =⇒ 1=
03−πz20 z20 for a scalar a ̸= 0. Then plug in λ2
 π−3 1   z    0   z    −b/(π−3)  1= =⇒ 1=
03−3z20 z2 b where b ̸= 0.
 3.9 Definition The characteristic polynomial of a square matrix T is the determinant |T − xI| where x is a variable. The characteristic equation is |T − xI| = 0. The characteristic polynomial of a transformation t is the characteristic polynomial of any matrix representation RepB,B(t).
Exercise 34 checks that the characteristic polynomial of a transformation is well-defined, that is, that the characteristic polynomial is the same no matter which basis we use for the representation.
Proof Any root of the characteristic polynomial is an eigenvalue. Over the complex numbers, any polynomial of degree one or greater has a root. QED
3.11 Remark That result is the reason that in this chapter we use scalars that are complex numbers. Had we stuck to real number scalars then there would be characteristic polynomials, such as x2 + 1, that do not factor.
3.13 Lemma An eigenspace is a subspace. It is a nontrivial subspace.
 3.10 Lemma A linear transformation on a nontrivial vector space has at least one eigenvalue.
 3.12 Definition The eigenspace of a transformation t associated with the eigenvalue λ is Vλ = { ⃗ζ | t(⃗ζ ) = λ⃗ζ }. The eigenspace of a matrix is analogous.
 
402 Chapter Five. Similarity
 Proof Notice first that Vλ is not empty; it contains the zero vector since t(⃗0) = ⃗0, which equals λ · ⃗0. To show that an eigenspace is a subspace, what remains is to check closure of this set under linear combinations. Take ⃗ζ1 , . . . , ⃗ζn ∈ Vλ and then
t(c1⃗ζ1 +c2⃗ζ2 +···+cn⃗ζn)=c1t(⃗ζ1)+···+cnt(⃗ζn) = c 1 λ ⃗ζ 1 + · · · + c n λ ⃗ζ n
= λ ( c 1 ⃗ζ 1 + · · · + c n ⃗ζ n )
that the combination is also an element of Vλ.
The space Vλ contains more than just the zero vector because by definition
λ is an eigenvector only if t(⃗ζ ) = λ⃗ζ has solutions for ⃗ζ other than ⃗0. QED 3.14 Example These are the eigenspaces associated with the eigenvalues 0 and 2
of Example 3.7.
V0 ={−a|a∈C}, V2 ={0|b∈C}.
represents projection.
1 0 0 0 1 0 000
x,y,z ∈ C
a b ab
3.15 Example These are the eigenspaces for the eigenvalues π and 3 of Exam- ple 3.8.
 a   −b/(π − 3) 
Vπ={ 0 |a∈C} V3={ b |b∈C}
The characteristic equation in Example 3.7 is 0 = x(x − 2)2 so in some sense 2 is an eigenvalue twice. However there are not twice as many eigenvectors in that the dimension of the associated eigenspace V2 is one, not two. The next example is a case where a number is a double root of the characteristic equation and the dimension of the associated eigenspace is two.
3.16 Example With respect to the standard bases, this matrix
x x π
y  −→ y z0
Section II. Similarity
403
 Its characteristic equation
0 = |T − xI|
 1−x 0 0 
   =  0 1−x 0    0 0 0−x 
= (1 − x)2(0 − x)
has the double root x = 1 along with the single root x = 0. Its eigenspace associated with the eigenvalue 1 and its eigenspace associated with the eigenvalue 0 are easy to find.
c  0 1
V1 ={c2|c1,c2 ∈C} V0 ={0|c3 ∈C} 0 c3
Note that V1 has dimension two.
In Example 3.16, there are two eigenvalues, For λ1 = 1 both the algebraic and geometric multiplicities are 2. For λ2 = 0 both the algebraic and geometric multiplicities are 1. In contrast, Example 3.14 shows that the eigenvalue λ = 2 has algebraic multiplicity 2 but geometric multiplicity 1. For every transforma- tion, each eigenvalue has geometric multiplicity greater than or equal to 1 by Lemma 3.13. (And, an eigenvalue must have geometric multiplicity less than or equal to its algebraic multiplicity, although proving this is beyond our scope.)
By Lemma 3.13 if two eigenvectors ⃗v1 and ⃗v2 are associated with the same eigenvalue then a linear combination of those two is also an eigenvector, associated with the same eigenvalue. As an illustration, referring to the prior example, this sum of two members of V1
1 0 0 + 1
00
yields another member of V1.
The next result speaks to the situation where the vectors come from different
eigenspaces.
 3.17Definition Whereacharacteristicpolynomialfactorsinto(x−λ1)m1 ···(x− λk )mk then the eigenvalue λi has algebraic multiplicity mi . Its geometric multiplicity is the dimension of the associated eigenspace Vλi .
404 Chapter Five. Similarity
  3.18 Theorem For any set of distinct eigenvalues of a map or matrix, a set of associated eigenvectors, one per eigenvalue, is linearly independent.
Proof We will use induction on the number of eigenvalues. The base step is that there are zero eigenvalues. Then the set of associated vectors is empty and so is linearly independent.
For the inductive step assume that the statement is true for any set of k   0 distinct eigenvalues. Consider distinct eigenvalues λ1, . . . , λk+1 and let ⃗v1, . . . ,⃗vk+1 be associated eigenvectors. Suppose that ⃗0 = c1⃗v1 + · · · + ck⃗vk + ck+1⃗vk+1. Derive two equations from that, the first by multiplying by λk+1 on both sides ⃗0 = c1λk+1⃗v1 + · · · + ck+1λk+1⃗vk+1 and the second by applying the map to both sides ⃗0 = c1t(⃗v1)+· · ·+ck+1t(⃗vk+1) = c1λ1⃗v1 +· · ·+ck+1λk+1⃗vk+1 (applying the matrix gives the same result). Subtract the second from the first.
⃗0 = c1(λk+1 − λ1)⃗v1 + · · · + ck(λk+1 − λk)⃗vk + ck+1(λk+1 − λk+1)⃗vk+1
The ⃗vk+1 term vanishes. Then the induction hypothesis gives that c1(λk+1 − λ1) = 0, . . . , ck(λk+1 − λk) = 0. The eigenvalues are distinct so the coefficients c1, . . . , ck are all 0. With that we are left with the equation ⃗0 = ck+1⃗vk+1 so ck+1 is also 0. QED
3.19 Example The eigenvalues of
2 −2 2 0 1 1 −4 8 3
are distinct: λ1 = 1, λ2 = 2, and λ3 = 3. A set of associated eigenvectors 2 9 2
{ 1 , 4 , 1 } 042
is linearly independent.
3.20 Corollary An n×n matrix with n distinct eigenvalues is diagonalizable. Proof Form a basis of eigenvectors. Apply Lemma 2.4. QED
This section observes that some matrices are similar to a diagonal matrix. The idea of eigenvalues arose as the entries of that diagonal matrix, although the definition applies more broadly than just to diagonalizable matrices. To find eigenvalues we defined the characteristic equation and that led to the final result, a criterion for diagonalizability. (While it is useful for the theory, note that in
 
Section II. Similarity 405
 applications finding eigenvalues this way is typically impractical; for one thing the matrix may be large and finding roots of large-degree polynomials is hard.)
In the next section we study matrices that cannot be diagonalized.
Exercises
3.21 This matrix has two eigenvalues λ1 = 3, λ2 = −4.  4 1 
−8 −5
Give two different diagonal form matrices with which it is similar.
3.22 For each, find the characteristic polynomial and the eigenvalues.
(a) (e)
 10 −9   1 2  (b)
(c)
 0 3   0 0  (d)
4−2 43 70 00  1 0 
01
  3.23 For each matrix, find the characteristic equation and the eigenvalues and associated eigenvectors.
 3 0 
(a) (b)
 3 2  −1 0
8 −1
3.24 Find the characteristic equation, and the eigenvalues and associated eigenvectors
for this matrix. Hint. The eigenvalues are complex.  −2 −1 
52
3.25 Find the characteristic polynomial, the eigenvalues, and the associated eigen- vectors of this matrix.
1 1 1 0 0 1 001
  3.26 For each matrix, find the characteristic equation, and the eigenvalues and associated eigenvectors.
3−20 010 (a) −2 3 0 (b) 0 0 1
0 0 5 4 −17 8
3.27 For each matrix, find the characteristic polynomial, and the eigenvalues and asso-
ciated eigenspaces. Also find the algebraic and geometric multiplicities.
 13 −4  1 3 −3 (a) −4 7 (b) −3 7 −3
−6 6 −2   3.28 Let t: P2 → P2 be this linear map.
2 3 −3 (c) 0 2 −3
0 0 1
a0 +a1x+a2x2  →(5a0 +6a1 +2a2)−(a1 +8a2)x+(a0 −2a2)x2 Find its eigenvalues and the associated eigenvectors.
3.29 Find the eigenvalues and eigenvectors of this map t: M2 → M2.
 a b    2c a+c  c d  → b−2c d
406 Chapter Five. Similarity
   3.30 Find the eigenvalues and associated eigenvectors of the differentiation operator d/dx:P3 →P3.
3.31 Prove that the eigenvalues of a triangular matrix (upper or lower triangular) are the entries on the diagonal.
  3.32 This matrix has distinct eigenvalues. 121
6 −1 0 −1 −2 −1
(a) Diagonalize it.
(b) Find a basis with respect to which this matrix has that diagonal representation. (c) Draw the diagram. Find the matrices P and P−1 to effect the change of basis.
  3.33 Find the formula for the characteristic polynomial of a 2×2 matrix.
3.34 Prove that the characteristic polynomial of a transformation is well-defined. 3.35 Prove or disprove: if all the eigenvalues of a matrix are 0 then it must be the
zero matrix.
  3.36 (a) Show that any non-⃗0 vector in any nontrivial vector space can be a
eigenvector. That is, given a ⃗v ̸= ⃗0 from a nontrivial V, show that there is a
transformation t: V → V having a scalar eigenvalue λ ∈ R such that ⃗v ∈ Vλ.
(b) What if we are given a scalar λ? Can any non-⃗0 member of any nontrivial
vector space be an eigenvector associated with λ?
  3.37 Suppose that t: V → V and T = RepB,B(t). Prove that the eigenvectors of T
associated with λ are the non-⃗0 vectors in the kernel of the map represented (with
respect to the same bases) by T − λI.
3.38 Provethatifa,...,dareallintegersanda+b=c+dthen
 a b  cd
has integral eigenvalues, namely a + b and a − c.
  3.39 Prove that if T is nonsingular and has eigenvalues λ1 , . . . , λn then T −1 has
eigenvalues 1/λ1,...,1/λn. Is the converse true?
  3.40 Suppose that T is n×n and c,d are scalars.
(a) Prove that if T has the eigenvalue λ with an associated eigenvector ⃗v then ⃗v is an eigenvector of cT + dI associated with eigenvalue cλ + d.
(b) Prove that if T is diagonalizable then so is cT + dI.
  3.41 Show that λ is an eigenvalue of T if and only if the map represented by T − λI
is not an isomorphism. 3.42 [Strang 80]
(a) Show that if λ is an eigenvalue of A then λk is an eigenvalue of Ak.
(b) What is wrong with this proof generalizing that? “If λ is an eigenvalue of A and μ is an eigenvalue for B, then λμ is an eigenvalue for AB, for, if A⃗x = λ⃗x and
B⃗x = μ⃗x then AB⃗x = Aμ⃗x = μA⃗x = μλ⃗x”?
3.43 Do matrix equivalent matrices have the same eigenvalues?
3.44 Show that a square matrix with real entries and an odd number of rows has at
least one real eigenvalue.
Section II. Similarity
407
 3.45 Diagonalize.
−1 2 2 222 −3 −6 −6
3.46 Suppose that P is a nonsingular n×n matrix. Show that the similarity transformation map tP : Mn×n → Mn×n sending T  → PTP−1 is an isomorphism.
? 3.47 [Math. Mag., Nov. 1967] Show that if A is an n square matrix and each row
(column) sums to c then c is a characteristic root of A. (“Characteristic root” is a synonym for eigenvalue.)
408 Chapter Five. Similarity III Nilpotence
This chapter shows that every square matrix is similar to one that is a sum of two kinds of simple matrices. The prior section focused on the first simple kind, diagonal matrices. We now consider the other kind.
III.1 Self-Composition
Because a linear transformation t: V → V has the same domain as codomain, wecancomposetwithitselft2 =t◦t,andt3 =t◦t◦t,etc.∗
⃗v t ( ⃗v )
t 2 ( ⃗v )
Note that the superscript power notation tj for iterates of the transformations fits with the notation that we’ve used for their square matrix representations because if RepB,B(t) = T then RepB,B(tj) = Tj.
     1.1 Example For the derivative map d/dx: P3 → P3 given by 2 3 d/dx 2
a+bx+cx +dx  −→ b+2cx+3dx the second power is the second derivative
2 3 d2/dx2 a+bx+cx +dx  −→ 2c+6dx
the third power is the third derivative
2 3 d3/dx3 a+bx+cx +dx  −→ 6d
and any higher power is the zero map.
1.2 Example This transformation of the space M2×2 of 2×2 matrices     
abtba c d  −→ d 0
 ∗ More information on function iteration is in the appendix.
Section III. Nilpotence
409
 has this second power and this third power.
    
abt2 ab c d  −→ 0 0
    
abt3 ba c d  −→ 0 0
After that, t4 = t2 and t5 = t3, etc.
1.3 Example Consider the shift transformation t: C3 → C3.
We have that
so the range spaces descend to the trivial subspace.
0 0
R(t) = {a | a,b ∈ C} R(t2) = {0 | c ∈ C}
x 0 t
y  −→ x zy
x 0 0 0 t t t 
y  −→ x  −→ 0  −→ 0 zyx0
bc0
These examples suggest that after some number of iterations the map settles down.
0 R(t3) = {0}
 1.4 Lemma For any transformation t: V → V, the range spaces of the powers form a descending chain
V ⊇R(t)⊇R(t2)⊇··· and the null spaces form an ascending chain.
{⃗0}⊆N (t)⊆N (t2)⊆···
Further, there is a k such that for powers less than k the subsets are proper so that if j < k then R(tj) ⊃ R(tj+1) and N (tj) ⊂ N (tj+1), while for higher powers the sets are equal so that if j   k then R(tj) = R(tj+1) and N (tj) = N (tj+1)).
Proof First recall that for any map the dimension of its range space plus the dimension of its null space equals the dimension of its domain. So if the
410 Chapter Five. Similarity
 dimensions of the range spaces shrink then the dimensions of the null spaces must rise. We will do the range space half here and leave the rest for Exercise 14. We start by showing that the range spaces form a chain. If w⃗ ∈ R(tj+1), so
that w⃗ = tj+1(⃗v) for some ⃗v, then w⃗ = tj( t(⃗v) ). Thus w⃗ ∈ R(tj).
Next we verify the “further” property: in the chain the subsets containments are proper initially, and then from some power k onward the range spaces are equal. We first show that if any pair of adjacent range spaces in the chain are equal R(tk) = R(tk+1) then all subsequent ones are also equal R(tk+1) = R(tk+2), etc. This holds because t: R(tk+1) → R(tk+2) is the same map, with the same domain, as t: R(tk) → R(tk+1) and it therefore has
the same range R(tk+1) = R(tk+2) (it holds for all higher powers by induction). So if the chain of range spaces ever stops strictly decreasing then from that point onward it is stable.
We end by showing that the chain must eventually stop decreasing. Each range space is a subspace of the one before it. For it to be a proper subspace it must be of strictly lower dimension (see Exercise 12). These spaces are finite- dimensional and so the chain can fall for only finitely many steps. That is, the
power k is at most the dimension of V.
1.5 Example The derivative map a+bx+cx2 +dx3  −→ b+2cx+3dx2 on P3
has this chain of range spaces.
R(t0)=P3 ⊃ R(t1)=P2 ⊃ R(t2)=P1 ⊃ R(t3)=P0 ⊃ R(t4)={⃗0} All later elements of the chain are the trivial space. It has this chain of null
spaces.
N (t0)={⃗0} ⊂ N (t1)=P0 ⊂ N (t2)=P1 ⊂ N (t3)=P2 ⊂ N (t4)=P3 Later elements are the entire space.
1.6Example Lett:P2 →P2 bethemapd0+d1x+d2x2  →2d0+d2x.Asthe lemma describes, on iteration the range space shrinks
R(t0)=P2 R(t)={a0 +a1x|a0,a1 ∈C} R(t2)={a0 |a0 ∈C} and then stabilizes, so that R(t2) = R(t3) = · · · . The null space grows
N (t0)={0} N (t)={b1x|b1 ∈C} N (t2)={b1x+b2x2 |b1,b2 ∈C} and then stabilizes N (t2) = N (t3) = ···.
1.7 Example The transformation π: C3 → C3 projecting onto the first two coor-
dinates
c  c 
11 π c2  −→ c2
c3 0
d/dx
QED
Section III. Nilpotence 411
 hasC3 ⊃R(π)=R(π2)=···and{⃗0}⊂N(π)=N(π2)=··· wherethisis the range space and the null space.
a 0 R(π)={b|a,b∈C} N (π)={0|c∈C}
0c
This graph illustrates. The horizontal axis gives the power j of a transfor- mation. The vertical axis gives the dimension of the range space of tj as the distance above zero, and thus also shows the dimension of the null space because the two add to the dimension n of the domain.
 1.8 Definition Let t be a transformation on an n-dimensional space. The gen- eralized range space (or closure of the range space) is R∞(t) = R(tn). The generalized null space (or closure of the null space) is N∞(t) = N (tn).
 n
    nullity(tj )
dim(N∞ (t))
    rank(tj) ...
dim(R∞ (t))
n
On iteration the rank falls and the nullity rises until there is some k such that the map reaches a steady state R(tk) = R(tk+1) = R∞(t) and N (tk) = N (tk+1) = N∞(t). This must happen by the n-th iterate.
Exercises
  1.9 Give the chains of range spaces and null spaces for the zero and identity trans- formations.
  1.10 For each map, give the chain of range spaces and the chain of null spaces, and the generalized range space and the generalized null space.
(a) t0:P2 →P2,a+bx+cx2  →b+cx2 (b) t1 : R2 → R2,
 a   0  b  → a
(c) t2:P2 →P2,a+bx+cx2  →b+cx+ax2 (d) t3 : R3 → R3,
a a
b  → a cb
     0
012 j
           
412 Chapter Five. Similarity 1.11 Prove that function composition is associative (t ◦ t) ◦ t = t ◦ (t ◦ t) and so we
can write t3 without specifying a grouping.
1.12 Check that a subspace must be of dimension less than or equal to the dimension of its superspace. Check that if the subspace is proper (the subspace does not equal the superspace) then the dimension is strictly less. (This is used in the proof of Lemma 1.4.)
  1.13 Prove that the generalized range space R∞(t) is the entire space, and the generalized null space N∞(t) is trivial, if the transformation t is nonsingular. Is this ‘only if’ also?
1.14 Verify the null space half of Lemma 1.4.
  1.15 Give an example of a transformation on a three dimensional space whose range has dimension two. What is its null space? Iterate your example until the range space and null space stabilize.
1.16 Show that the range space and null space of a linear transformation need not be disjoint. Are they ever disjoint?
III.2 Strings
This requires material from the optional Combining Subspaces subsection.
The prior subsection shows that as j increases the dimensions of the R(tj)’s fall while the dimensions of the N (tj)’s rise, in such a way that this rank and nullity split between them the dimension of V. Can we say more; do the two split a basis — is V = R(tj) ⊕ N (tj)?
The answer is yes for the smallest power j = 0 since V = R(t0) ⊕ N (t0) = V ⊕ {⃗0}. The answer is also yes at the other extreme.
Proof Let the dimension of V be n. Because R(tn) = R(tn+1), the map t: R∞(t) → R∞(t) is a dimension-preserving homomorphism. Therefore, by Theorem Two.II.2.20 it is one-to-one. QED
Proof Let the dimension of V be n. We will verify the second sentence, which is equivalent to the first. Clause (1) is true because any transformation satisfies
  2.1 Lemma For any linear t: V → V the function t: R∞(t) → R∞(t) is one-to- one.
 2.2 Corollary Where t: V → V is a linear transformation, the space is the direct sum V = R∞(t)⊕N∞(t). That is, both (1) dim(V) = dim(R∞(t))+dim(N∞(t)) and (2) R∞(t) ∩ N∞(t) = {⃗0}.
Section III. Nilpotence 413
 that its rank plus its nullity equals the dimension of the space, and in particular this holds for the transformation tn.
For clause (2), assume that ⃗v ∈ R∞(t) ∩ N∞(t) to prove that ⃗v = ⃗0. Because ⃗v is in the generalized null space, tn(⃗v) = ⃗0. On the other hand, by the lemma t: R∞(t) → R∞(t) is one-to-one and a composition of one-to-one maps is one- to-one, so tn : R∞(t) → R∞(t) is one-to-one. Only ⃗0 is sent by a one-to-one linear map to ⃗0 so the fact that tn(⃗v) = ⃗0 implies that ⃗v = ⃗0. QED
2.3 Remark Technically there is a difference between the map t: V → V and the map on the subspace t: R∞(t) → R∞(t) if the generalized range space is not equal to V, because the domains are different. But the difference is small because the second is the restriction of the first to R∞(t).
For powers between j = 0 and j = n, the space V might not be the direct sum of R(tj) and N (tj). The next example shows that the two can have a nontrivial intersection.
2.4 Example Consider the transformation of C2 defined by this action on the elements of the standard basis.
           
1n00n0 00 0  −→ 1 1  −→ 0 N=RepE2,E2(n)= 1 0
This is a shift map and is clearly nilpotent of index two.  x   0 
y  → x
Another way to depict this map’s action is with a string.
The vector
is in both the range space and null space.
⃗e 1   → ⃗e 2   → ⃗0  0 
⃗e2= 1
2.5 Example A map nˆ : C4 → C4 whose action on E4 is given by the string ⃗e 1   → ⃗e 2   → ⃗e 3   → ⃗e 4   → ⃗0
has R(nˆ) ∩ N (nˆ) equal to the span [{⃗e4 }], has R(nˆ2) ∩ N (nˆ2) = [{⃗e3,⃗e4 }], and has R(nˆ3) ∩ N (nˆ3) = [{⃗e4 }]. It is nilpotent of index four. The matrix representation is all zeros except for some subdiagonal ones.
0 0 0 0
ˆ 1000 N = RepE4,E4 (nˆ) =   0 1 0 0
0010
414 Chapter Five. Similarity 2.6 Example Transformations can act via more than one string. A transformation
t acting on a basis B = ⟨β⃗1,...,β⃗5⟩ by
β⃗ 1   → β⃗ 2   → β⃗ 3   → ⃗0
β⃗ 4   → β⃗ 5   → ⃗0
is represented by a matrix that is all zeros except for blocks of subdiagonal ones
0 0 0 0 0 1 0 0 0 0
 RepB,B(t) = 0 1 0   0 0
0 0 0 0 0 00010
(the lines just visually organize the blocks).
In those examples all vectors are eventually transformed to zero.
2.8 Example In Example 2.4 the index of nilpotency is two. In Example 2.5 it is four. In Example 2.6 it is three.
2.9 Example The differentiation map d/dx: P2 → P2 is nilpotent of index three since the third derivative of any quadratic polynomial is zero. This map’s action is described by the string x2  → 2x  → 2  → 0 and taking the basis B = ⟨x2, 2x, 2⟩ gives this representation.
0 0 0 RepB,B(d/dx) = 1 0 0
010
Not all nilpotent matrices are all zeros except for blocks of subdiagonal ones. 2.10 Example With the matrix Nˆ from Example 2.5, and this four-vector basis
1 0 1 0
       2.7 Definition A nilpotent transformation is one with a power that is the zero map. A nilpotent matrix is one with a power that is the zero matrix. In either case, the least such power is the index of nilpotency.
0 2 1 0 D=⟨ , , , ⟩
1 1 1 0 0001
a change of basis operation produces this representation with respect to D, D. 1 0 1 00 0 0 01 0 1 0−1 −1 0 1 0
0 2 1 01 0 0 00 2 1 0 −3 −2 5 0 =  1 1 1 00 1 0 01 1 1 0 −2 −1 3 0
0 0 0 1 0 0 1 0 0 0 0 1 2 1 −2 0
Section III. Nilpotence 415
 The new matrix is nilpotent; its fourth power is the zero matrix. We could verify this with a tedious computation or we can instead just observe that it is nilpotent since its fourth power is similar to Nˆ4, the zero matrix, and the only matrix similar to the zero matrix is itself.
(PNˆP−1)4 =PNˆP−1 ·PNˆP−1 ·PNˆP−1 ·PNˆP−1 =PNˆ4P−1
The goal of this subsection is to show that the prior example is prototypical in that every nilpotent matrix is similar to one that is all zeros except for blocks of subdiagonal ones.
2.12 Example Consider differentiation d/dx : P2 → P2 . The sequence ⟨x2 , 2x, 2, 0⟩ is a d/dx-string of length 4. The sequence ⟨x2 , 2x, 2⟩ is a d/dx-string of length 3 that is a basis for P2.
Note that the strings cannot form a basis under concatenation if they are not disjoint because a basis cannot have a repeated vector.
2.13 Example In Example 2.6, we can concatenate the t-strings ⟨β⃗ 1, β⃗ 2, β⃗ 3⟩ and ⟨β⃗ 4, β⃗ 5⟩ to make a basis for the domain of t.
Proof Let the space have a basis of t-strings and let t’s index of nilpotency be k. We cannot have that the longest string in that basis is longer than t’s index of nilpotency because tk sends any vector, including the vector starting the longest string, to ⃗0. Therefore instead suppose that the space has a t-string basis B where all of the strings are shorter than length k. Because t has index k, there is a vector ⃗v such that tk−1(⃗v) ̸= ⃗0. Represent ⃗v as a linear combination of elements from B and apply tk−1. We are supposing that tk−1 maps each element of B to ⃗0, and therefore maps each term in the linear combination to ⃗0, but also that it does not map ⃗v to ⃗0. That is a contradiction. QED
We shall show that each nilpotent map has an associated string basis, a basis of disjoint strings.
To see the main idea of the argument, imagine that we want to construct a counterexample, a map that is nilpotent but without an associated disjoint string basis. We might think to make something like the map t: C5 → C5 with this action.
 2.11 Definition Let t be a nilpotent transformation on V. A t-string of length k generated by ⃗v ∈ V is a sequence ⟨⃗v, t(⃗v), . . . , tk−1 (⃗v)⟩. A t-string basis is a basis that is a concatenation of t-strings.
 2.14 Lemma If a space has a t-string basis then the index of nilpotency of t is the length of the longest string in that basis.
416
Chapter Five. Similarity
 ⃗e1
⃗e 3   → ⃗0
⃗e2
⃗e 4   → ⃗e 5   → ⃗0
00000
 00000 RepE5,E5 (t) = 1 1 0 0 0 0 0 0 0 0
00010
But, the fact that the shown basis isn’t disjoint doesn’t mean that there isn’t another basis that consists of disjoint strings.
To produce such a basis for this map we will first find the number and lengths of its strings. Observer that t’s index of nilpotency is two. Lemma 2.14 says that at least one string in a disjoint string basis has length two. There are five basis elements so if there is a disjoint string basis then the map must act in one of these ways.
β⃗ 1 β⃗ 3 β⃗5
  → β⃗ 2   → ⃗0   → β⃗ 4   → ⃗0  → ⃗0
β⃗ 1 β⃗ 3 β⃗4 β⃗5
  → β⃗ 2   → ⃗0   → ⃗0
 → ⃗0
 → ⃗0
Now, the key point. A transformation with the left-hand action has a null space of dimension three since that’s how many basis vectors are mapped to zero. A transformation with the right-hand action has a null space of dimension four. Wit the matrix representation above we can determine which of the two possible shapes is right.
x −x

N (t)={ z |x,z,r∈C}
 0  r
This is three-dimensional, meaning that of the two disjoint string basis forms above, t’s basis has the left-hand one.
To produce a string basis for t, first pick β⃗ 2 and β⃗ 4 from R(t) ∩ N (t). 0 0
0 0 ⃗⃗
β2 = 1 β4 = 0 0 0
01
(Other choices are possible, just be sure that the set {β⃗ 2, β⃗ 4 } is linearly inde-
 →
 →
Section III. Nilpotence 417 pendent.) For β⃗5 pick a vector from N (t) that is not in the span of {β⃗2,β⃗4}.
1 −1
⃗ β5 =  0 
 0  0
Finally, take β⃗1 and β⃗3 such that t(β⃗1) = β⃗2 and t(β⃗3) = β⃗4. 0 0
1 0 ⃗⃗
β1 = 0 β3 = 0 0 1
00
Therefore, we have a string basis B = ⟨β⃗ 1, . . . , β⃗ 5⟩ and with respect to that basis
the matrix of t has blocks of subdiagonal 1’s.
0 0 0 0 0
1 0 0 0 0 
RepB,B(t) = 0 0     0 00 0
00000
This illustrates the proof, which describes three kinds of basis vectors (shown in squares if they are in the null space and in circles if they are not).
            2.15 Theorem Any nilpotent transformation t is associated with a t-string basis. While the basis is not unique, the number and the length of the strings is determined by t.
3k  → 3k  →
.
3k →
1k   → · · · 1k   → · · ·
1k →···  → 1k → ⃗0
⃗0
· · ·   → · · ·   →
1k   → 1k   →
  → ⃗0   → ⃗0
   →
.
 →
Proof Fix a vector space V. We will argue by induction on the index of nilpotency. If the map t: V → V has index of nilpotency 1 then it is the zero map and any basis is a string basis β⃗1  →⃗0, ..., β⃗n  →⃗0.
00 10
 →⃗0
1
1
 1
 2
 2
418 Chapter Five. Similarity
 For the inductive step, assume that the theorem holds for any transformation t:V→V withanindexofnilpotencybetween1andk−1(withk>1)and consider the index k case.
Observe that the restriction of t to the range space t: R(t) → R(t) is also nilpotent, of index k − 1. Apply the inductive hypothesis to get a string basis for R(t), where the number and length of the strings is determined by t.
B = ⟨β⃗1,t(β⃗1),...,th1(β⃗1)⟩⌢⟨β⃗2,...,th2(β⃗2)⟩⌢···⌢⟨β⃗i,...,thi(β⃗i)⟩
(In the illustration above these are the vectors of kind 1.)
Note that taking the final nonzero vector in each of these strings gives a basis
C=⟨th1(β⃗1),...,thi(β⃗i)⟩fortheintersectionR(t)∩N(t). Thisisbecausea member of R(t) maps to zero if and only if it is a linear combination of those basis vectors that map to zero. (The illustration shows these as 1’s in squares.)
Now extend C to a basis for all of N (t).
Cˆ = C ⌢ ⟨ ⃗ξ 1 , . . . , ⃗ξ p ⟩
(In the illustration the ⃗ξ’s are the vectors of kind 2 and so the set Cˆ is the set of vectors in squares.) While the vectors ⃗ξ we choose aren’t uniquely determined by t, what is uniquely determined is the number of them: it is the dimension of N (t) minus the dimension of R(t) ∩ N (t).
Finally, B⌢Cˆ is a basis for R(t)+N (t) because any sum of something in the range space with something in the null space can be represented using elements of B for the range space part and elements of Cˆ for the part from the null space. Note that
dim  R(t) + N (t)  = dim(R(t)) + dim(N (t)) − dim(R(t) ∩ N (t)) = rank(t) + nullity(t) − i
= dim(V) − i
and so we can extend B⌢Cˆ to a basis for all of V by the addition of i more vectors, provided that they are not linearly dependent on what we have already. Recall that each of β⃗1,...,β⃗i is in R(t), and extend B⌢Cˆ with vectors⃗v1,...,⃗vi such that t(⃗v1) = β⃗ 1, . . . , t(⃗vi) = β⃗ i. (In the illustration these are the 3’s.) The check that this extension preserves linear independence is Exercise 31. QED
 2.16 Corollary Every nilpotent matrix is similar to a matrix that is all zeros except for blocks of subdiagonal ones. That is, every nilpotent map is represented with respect to some basis by such a matrix.
Section III. Nilpotence 419
 This form is unique in the sense that if a nilpotent matrix is similar to two such matrices then those two simply have their blocks ordered differently. Thus this is a canonical form for the similarity classes of nilpotent matrices provided that we order the blocks, say, from longest to shortest.
2.17 Example The matrix
 1 −1  M= 1 −1
has an index of nilpotency of two, as this calculation shows.
power p Mp N (Mp)  1 −1   x 
  1M= {|x∈C} 1−1 x
 2 00  2 2M=C
00
Because the matrix is 2×2, any transformation that it represents is on a space of dimension two. The nullspace of one application of the map N (m) has dimension one, and the nullspace of two applications N (m2) has dimension two. Thus the action of m on a string basis is β⃗ 1  → β⃗ 2  → ⃗0 and the canonical form of the matrix is this.
 0 0  N=10
We can exhibit such a string basis, and also the change of basis matrices
witnessing the matrix similarity between M and N. Suppose that m: C2 → C2
is such that M represents it with respect to the standard bases. (We could take
M to be a representation with respect to some other basis but the standard one
is convenient.) Pick β⃗ 2 ∈ N (m). Also pick β⃗ 1 so that m(β⃗ 1) = β⃗ 2.  1   1 
β⃗2=1 β⃗1=0
For the change of basis matrices, recall the similarity diagram.
2m2
Cwrt E2 −−−−→ Cwrt E2 M 
id P id P 2m2
Cwrt B −−−−→ Cwrt B N
The canonical form equals RepB,B(m) = PMP−1, where
 1 1   1 −1  P−1 =RepB,E2(id)= 0 1 P=(P−1)−1 = 0 1
420 Chapter Five. Similarity and the verification of the matrix calculation is routine.
 1 −1  1 −1  1 1   0 0  0 1 1 −1 0 1 = 1 0
 2.18 Example This matrix
000 0 0 100 0 0
 −1 1 1 −1 1  0 1 0 0 0 
1 0 −1 1 −1 power p Np
is nilpotent, of index 3. 000000
  1
2
3
N (Np) 100000
{ | u,v ∈ C} −1 1 1 −1 1 u−v
 0 1 0 0 0   u 
1 0 −1 1 −1 00000
00000 
10000 10000
v 0
y
{  | y,z,u,v ∈ C}
–zero matrix– C5
 u 00000 v
z
 The table tells us this about any string basis: the null space after one map application has dimension two so two basis vectors map directly to zero, the null space after the second application has dimension four so two additional basis vectors map to zero by the second iteration, and the null space after three applications is of dimension five so the remaining one basis vector maps to zero in three hops.
β⃗ 1   → β⃗ 2   → β⃗ 3   → ⃗0 β⃗ 4   → β⃗ 5   → ⃗0
To produce such a basis, first pick two vectors from N (n) that form a linearly independent set.
0 0 0 0
⃗⃗ β3 = 1 β5 = 0 1 1
01
Section III. Nilpotence 421 Then add β⃗2,β⃗4 ∈ N (n2) such that n(β⃗2) = β⃗3 and n(β⃗4) = β⃗5.
0 0 1 1
⃗⃗ β2 = 0 β4 = 0 0 1
00 Finish by adding β⃗ 1 such that n(β⃗ 1) = β⃗ 2.
 Exercises
1 0
⃗ β1 = 1
0 0
  2.19 What is the index of nilpotency of the right-shift operator, here acting on the space of triples of reals?
(x,y,z)  → (0,x,y)
  2.20 For each string basis state the index of nilpotency and give the dimension of the range space and null space of each iteration of the nilpotent map.
(a) β⃗1  → β⃗2  → ⃗0 β⃗ 3   → β⃗ 4   → ⃗0
(b) β⃗1  → β⃗2  → β⃗3  → ⃗0 β⃗4  → ⃗0
β⃗5  → ⃗0
β⃗6  → ⃗0
(c) β⃗1  → β⃗2  → β⃗3  → ⃗0
Also give the canonical form of the matrix.
2.21 Decide which of these matrices are nilpotent.
 −2 4   3 1  (a) −1 2 (b) 1 3
45 −22 −19 (e) 33 −16 −14
−3 2 1 (c)−3 2 1
−3 2 1
1 1 4 (d)3 0 −1
5 2 7
69 −34 −29
  2.22 Find the canonical form of this matrix.
0 1 1 0 1 0 0 1 1 1
 0 0 0 0 0 0 0 0 0 0 00000
422 Chapter Five. Similarity
   2.23 Consider the matrix from Example 2.18.
(a) Use the action of the map on the string basis to give the canonical form. (b) Find the change of basis matrices that bring the matrix to canonical form. (c) Use the answer in the prior item to check the answer in the first item.
−1 1 −1 (c)  1 0 1 
1 −1 1
Put each in canonical form.
2.25 Describe the effect of left or right multiplication by a matrix that is in the
canonical form for nilpotent matrices.
2.26 Is nilpotence invariant under similarity? That is, must a matrix similar to a
nilpotent matrix also be nilpotent? If so, with the same index?
  2.27 Show that the only eigenvalue of a nilpotent matrix is zero.
2.28 Is there a nilpotent transformation of index three on a two-dimensional space? 2.29 In the proof of Theorem 2.15, why isn’t the proof’s base case that the index of
nilpotency is zero?
  2.30 Let t: V → V be a linear transformation and suppose ⃗v ∈ V is such that
tk (⃗v) = ⃗0 but tk−1 (⃗v) ̸= ⃗0. Consider the t-string ⟨⃗v, t(⃗v), . . . , tk−1 (⃗v)⟩.
(a) Prove that t is a transformation on the span of the set of vectors in the string, that is, prove that t restricted to the span has a range that is a subset of the
span. We say that the span is a t-invariant subspace.
(b) Prove that the restriction is nilpotent.
(c) Prove that the t-string is linearly independent and so is a basis for its span. (d) Represent the restriction map with respect to the t-string basis.
2.31 Finish the proof of Theorem 2.15.
2.32 Show that the terms ‘nilpotent transformation’ and ‘nilpotent matrix’, as
given in Definition 2.7, fit with each other: a map is nilpotent if and only if it is represented by a nilpotent matrix. (Is it that a transformation is nilpotent if an only if there is a basis such that the map’s representation with respect to that basis is a nilpotent matrix, or that any representation is a nilpotent matrix?)
2.33 Let T be nilpotent of index four. How big can the range space of T3 be?
2.34 Recall that similar matrices have the same eigenvalues. Show that the converse
does not hold.
2.35 Lemma 2.1 shows that any for any linear transformation t : V → V the restriction
t: R∞(t) → R∞(t) is one-to-one. Show that it is also onto, so it is an automorphism.
Must it be the identity map?
2.36 Prove that a nilpotent matrix is similar to one that is all zeros except for blocks
of super-diagonal ones.
  2.37 Prove that if a transformation has the same range space as null space. then
the dimension of its domain is even.
2.38 Prove that if two nilpotent matrices commute then their product and sum are also nilpotent.
  2.24 Each of these matrices is nilpotent.  1/2 −1/2  0 0 0
(a) 1/2 −1/2 (b) 0 −1 1 0 −1 1
Section IV. Jordan Form 423 IV Jordan Form
This section uses material from three optional subsections: Combining Subspaces, Determinants Exist, and Laplace’s Expansion.
We began this chapter by remembering that every linear map h: V → W can be represented by a partial identity matrix with respect to some bases B ⊂ V and D ⊂ W. That is, the partial identity form is a canonical form for matrix equivalence. This chapter considers transformations, where the codomain equals the domain, so we naturally ask what is possible when the two bases are equal RepB,B(t). In short, we want a canonical form for matrix similarity.
We noted that in the B, B case a partial identity matrix is not always possible. We therefore extended the matrix forms of interest to the natural generalization, diagonal matrices, and showed that a transformation or square matrix can be diagonalized if its eigenvalues are distinct. But at the same time we gave an example of a square matrix that cannot be diagonalized (because it is nilpotent) and thus diagonal form won’t suffice as the canonical form for matrix similarity.
The prior section developed that example to get a canonical form, subdiagonal ones, for nilpotent matrices.
This section finishes our program by showing that for any linear transforma- tion there is a basis such that the matrix representation RepB,B(t) is the sum of a diagonal matrix and a nilpotent matrix. This is Jordan canonical form.
IV.1 Polynomials of Maps and Matrices
Recall that the set of square matrices Mn×n is a vector space under entry-by- entry addition and scalar multiplication, and that this space has dimension n2. Thus, for any n×n matrix T the n2 +1-member set {I,T,T2,...,Tn2 } is linearly dependent and so there are scalars c0, . . . , cn2 , not all zero, such that
cn2Tn2 +···+c1T+c0I
is the zero matrix. Therefore every transformation has a kind of generalized
nilpotency: the powers of a square matrix cannot climb forever without a “repeat.”
1.1Example Rotationofplanevectorsπ/6radianscounterclockwiseisrepresented with respect to the standard basis by
  √3/2 −1/2  √
 T=
and verifying that 0T4 + 0T3 + 1T2 − 2T − 1I equals the zero matrix is easy.
 1/2 3/2
424 Chapter Five. Similarity
  1.2 Definition Let t be a linear transformation of a vector space V. Where f(x) = cnxn + · · · + c1x + c0 is a polynomial, f(t) is the transformation cntn + ···+c1t+c0(id) on V. In the same way, if T is a square matrix then f(T) is the matrix cnTn +···+c1T +c0I.
The polynomial of the matrix represents the polynomial of the map: if T = RepB,B(t) then f(T) = RepB,B(f(t)). This is because Tj = RepB,B(tj), and cT = RepB,B(ct), and T1 + T2 = RepB,B(t1 + t2).
1.3Remark Mostauthorswritethematrixpolynomialslightlydifferentlythanthe map polynomial. For instance, if f(x) = x − 3 then most authors explicitly write the identity matrix f(T ) = T − 3I but don’t write the identity map f(t) = t − 3. We shall follow this convention.
Consider again Example 1.1. The space M2×2 has dimension four so we know that for any 2×2 matrix there is a fourth degree polynomial f such that f(T) equals the zero matrix. But for the T in that example we exhibited a polynomial of degree less than four that gives the zero matrix. So while degree n2 always suffices, in some cases a smaller-degree polynomial works.
A minimal polynomial cannot be the zero polynomial because of the restriction on the leading coefficient. Obviously no other constant polynomial would do, so a minimal polynomial must have degree at least one. Thus, the zero matrix has minimal polynomial p(x) = x while the identity matrix has minimal polynomial pˆ ( x ) = x − 1 .
Proof We first prove existence. By the earlier observation that degree n2 suffices, there is at least one polynomial p(x) = ckxk + · · · + c0 that takes the map or matrix to zero, and it is not the zero polynomial by the prior paragraph. From among all such polynomials there must be at least one with minimal degree. Divide this polynomial by its leading coefficient ck to get a leading 1. Hence any map or matrix has a minimal polynomial.
Now for uniqueness. Suppose that m(x) and mˆ (x) both take the map or matrix to zero, are both of minimal degree and are thus of equal degree, and both have a leading 1. Subtract: d(x) = m(x) − mˆ (x). This polynomial takes
 1.4 Definition The minimal polynomial m(x) of a transformation t or a square matrix T is the polynomial of least degree and with leading coefficient one such that m(t) is the zero map or m(T) is the zero matrix.
 1.5 Lemma Any transformation or square matrix has a unique minimal polyno- mial.
Section IV. Jordan Form 425
 the map or matrix to zero and since the leading terms of m and mˆ cancel, d is of smaller degree than the other two. If d were to have a nonzero leading coefficient then we could divide by it to get a polynomial that takes the map or matrix to zero and has leading coefficient 1. This would contradict the minimality of the degree of m and mˆ . Thus the leading coefficient of d is zero, so m(x) − mˆ (x) is the zero polynomial, and so the two are equal. QED
1.6 Example We can compute that m(x) = x2 − 2x − 1 is minimal for the matrix of Example 1.1 by finding the powers of T up to n2 = 4.
  1/2 −√3/2   0 −1   −1/2 −√3/2  T2 = √3/2 1/2 T3 = 1 0 T4 = √3/2 −1/2
Put c4T4 + c3T3 + c2T2 + c1T + c0I equal to the zero matrix √
−(1/2)c4 + (1/2)c2 + ( 3/2)c1 + c0 = 0 √√
−(√3/2)c4 − c3 − (√3/2)c2 − (1/2)c1 = 0 ( 3/2)c4 +c3 +( 3/2)c2 + (1/2)c1 =0
−(1/2)c4 + (1/2)c2 + ( 3/2)c1 + c0 = 0 and use Gauss’ Method.
         √
  c4
√
−√ c2 − 3c1 −√2c0 =0
c3+ 3c2+ 2c1+ 3c0=0
  Setting c4, c3, and c2 to zero forces c1 and c0 to also come out as zero. To get a leading one, the most we can do is to set c4 and c3 to zero. Thus the minimal polynomial is quadratic.
Using the method of that example to find the minimal polynomial of a 3×3 matrix would mean doing Gaussian reduction on a system with nine equations in ten unknowns. We shall develop an alternative.
 1.7Lemma Supposethatthepolynomialf(x)=cnxn+···+c1x+c0 factors as k(x − λ1)q1 · · · (x − λz)qz . If t is a linear transformation then these two are equal maps.
cntn +···+c1t+c0 =k·(t−λ1)q1 ◦···◦(t−λz)qz
Consequently, if T is a square matrix then f(T) and k·(T −λ1I)q1 ···(T −λzI)qz are equal matrices.
Proof We use induction on the degree of the polynomial. The cases where the polynomial is of degree zero and degree one are clear. The full induction argument is Exercise 1.7 but we will give its sense with the degree two case.
426 Chapter Five. Similarity
 A quadratic polynomial factors into two linear terms f(x) = k(x − λ1) · (x − λ2) = k(x2 + (−λ1 − λ2)x + λ1λ2) (the roots λ1 and λ2 could be equal). We can check that substituting t for x in the factored and unfactored versions gives the same map.
  k · ( t − λ 1 ) ◦ ( t − λ 2 )   ( ⃗v ) =   k · ( t − λ 1 )   ( t ( ⃗v ) − λ 2 ⃗v )
= k ·  t(t(⃗v)) − t(λ2⃗v) − λ1t(⃗v) − λ1λ2⃗v 
=k· t◦t(⃗v)−(λ1 +λ2)t(⃗v)+λ1λ2⃗v  =k·(t2 −(λ1 +λ2)t+λ1λ2)(⃗v)
The third equality holds because the scalar λ2 comes out of the second term, since t is linear. QED
In particular, if a minimal polynomial m(x) for a transformation t factors as m(x) = (x−λ1)q1 ···(x−λz)qz then m(t) = (t−λ1)q1 ◦···◦(t−λz)qz is the zero map. Since m(t) sends every vector to zero, at least one of the maps t − λi sends some nonzero vectors to zero. Exactly the same holds in the matrix case—if m is minimal for T then m(T) = (T −λ1I)q1 ···(T −λzI)qz is the zero matrix and at least one of the matrices T − λiI sends some nonzero vectors to zero. That is, in both cases at least some of the λi are eigenvalues. (Exercise 29 expands on this.)
The next result is that every root of the minimal polynomial is an eigenvalue, and further that every eigenvalue is a root of the minimal polynomial (i.e, below it says ‘1   qi’ and not just ‘0   qi’). For that result, recall that to find eigenvalues we solve |T − xI| = 0 and this determinant gives a polynomial in x, called the characteristic polynomial, whose roots are the eigenvalues.
 1.8 Theorem (Cayley-Hamilton) If the characteristic polynomial of a transforma- tion or square matrix factors into
k·(x−λ1)p1(x−λ2)p2 ···(x−λz)pz then its minimal polynomial factors into
(x−λ1)q1(x−λ2)q2 ···(x−λz)qz where1 qi  pi foreachibetween1andz.
The proof takes up the next three lemmas. We will state them in matrix terms but they apply equally well to maps. (The matrix version is convenient for the first proof.)
The first result is the key. For the proof, observe that we can view a matrix
Section IV. Jordan Form 427 of polynomials as a polynomial with matrix coefficients.
 2x2 + 3x − 1 x2 + 2    2 1   3 0   −1 2  3x2+4x+14x2+x+1=34x2+41x+ 1 1
Proof Let C be T − xI, the matrix whose determinant is the characteristic polynomial c(x) = cnxn + · · · + c1x + c0.
t1,1−x t1,2 ...   t2,1 t2,2 − x 
C=. ..  . . 
tn,n − x
Recall Theorem Four.III.1.9, that the product of a matrix with its adjoint equals
the determinant of the matrix times the identity.
c(x) · I = adj(C)C = adj(C)(T − xI) = adj(C)T − adj(C) · x (∗)
Theleftsideof(∗)iscnIxn+cn−1Ixn−1+···+c1Ix+c0I. Fortherightside,the entries of adj(C) are polynomials, each of degree at most n − 1 since the minors of a matrix drop a row and column. As suggested before the proof, rewrite it as a polynomial with matrix coefficients: adj(C) = Cn−1xn−1 + · · · + C1x + C0 where each Ci is a matrix of scalars. Now this is the right side of (∗).
[(Cn−1T)xn−1 +···+(C1T)x+C0T]−[Cn−1xn +Cn−2xn−1 +···+C0x] Equate the left and right side of (∗)’s coefficients of xn, of xn−1, etc.
cnI = −Cn−1
cn−1I = −Cn−2 + Cn−1T
.
c1I = −C0 + C1T c0I = C0T
Multiply, from the right, both sides of the first equation by Tn, both sides of
  1.9 Lemma If T is a square matrix with characteristic polynomial c(x) then c(T) is the zero matrix.
428 Chapter Five. Similarity the second equation by Tn−1, etc.
cnTn = −Cn−1Tn
cn−1Tn−1 = −Cn−2Tn−1 + Cn−1Tn
.
c1T = −C0T + C1T2 c0I = C0T
Add. The left is cnTn +cn−1Tn−1 +···+c0I. The right telescopes; for instance −Cn−1Tn from the first line combines with the Cn−1Tn half of the second line. The total on the right is the zero matrix. QED
We refer to that result by saying that a matrix or map satisfies its charac- teristic polynomial.
Proof Let m(x) be minimal for T. The Division Theorem for Polynomials gives f(x) = q(x)m(x) + r(x) where the degree of r is strictly less than the degree of m. Because T satisfies both f and m, plugging T into that equation gives that r(T) is the zero matrix. That contradicts the minimality of m unless r is the zero polynomial. QED
Combining the prior two lemmas shows that the minimal polynomial divides the characteristic polynomial. Thus any root of the minimal polynomial is also a root of the characteristic polynomial. That is, so far we have that if m(x)=(x−λ1)q1 ···(x−λi)qi thenc(x)hastheform(x−λ1)p1 ···(x−λi)pi(x− λi+1)pi+1 · · · (x − λz)pz where each qj is less than or equal to pj. We finish the proof of the Cayley-Hamilton Theorem by showing that the characteristic polynomial has no additional roots, that is, there are no λi+1, λi+2, etc.
Proof Let T be a square matrix with minimal polynomial m(x) and assume that x−λ is a factor of the characteristic polynomial of T, that λ is an eigenvalue ofT. Wemustshowthatx−λisafactorofm,i.e.,thatm(λ)=0.
Suppose that λ is an eigenvalue of T with associated eigenvector ⃗v. Then T · T⃗v = T · λ⃗v = λT⃗v = λ2⃗v. Similarly, Tn⃗v = λn⃗v. With that, we have that for
  1.10 Lemma Where f(x) is a polynomial, if f(T) is the zero matrix then f(x) is divisible by the minimal polynomial of T. That is, any polynomial that is satisfied by T is divisible by T’s minimal polynomial.
 1.11 Lemma Each linear factor of the characteristic polynomial of a square matrix is also a linear factor of the minimal polynomial.
Section IV. Jordan Form 429 any polynomial function p(x), application of the matrix p(T) to ⃗v equals the
result of multiplying ⃗v by the scalar p(λ).
p ( T ) · ⃗v = ( c k T k + · · · + c 1 T + c 0 I ) · ⃗v = c k T k ⃗v + · · · + c 1 T ⃗v + c 0 ⃗v
= c k λ k ⃗v + · · · + c 1 λ ⃗v + c 0 ⃗v = p ( λ ) · ⃗v Since m(T ) is the zero matrix, ⃗0 = m(T )(⃗v) = m(λ) · ⃗v for all ⃗v, and hence
m(λ) = 0. QED That concludes the proof of the Cayley-Hamilton Theorem.
1.12 Example We can use the Cayley-Hamilton Theorem to find the minimal polynomial of this matrix.
2 0 0 1
1 2 0 2 T =  0 0 2 − 1 
0001
First we find its characteristic polynomial c(x) = (x − 1)(x − 2)3 with the usual determinant. Now, the Cayley-Hamilton Theorem says that T’s minimal polynomial is either (x−1)(x−2) or (x−1)(x−2)2 or (x−1)(x−2)3. We can decide among the choices just by computing
100 1000 1 0000
110 2100 2 1001 (T − 1I)(T − 2I) =     =   001−1000−1 0000
 0 0 0 0 0 0 0 −1 0 0 0 0
0000000 1 0000
and
0 0 0 0 0 0 0 −1 0 0 0 0 and so m(x) = (x − 1)(x − 2)2.
Exercises
  1.13 What are the possible minimal polynomials if a matrix has the given character- istic polynomial?
(a) (x−3)4 (b) (x+1)3(x−4) (c) (x−2)2(x−5)2
(d) (x+3)2(x−1)(x−2)2
What is the degree of each possibility?
2 1 0 0 11 0 0 2 0 0 0 0 (T−1I)(T−2I) =  =  0000000−1 0000
  1.14 Find the minimal polynomial of each matrix.
430
Chapter Five. Similarity
 3 0 0 (a)1 3 0
3 0 0 (b)1 3 0
3 0 0 2 0 1 (c)1 3 0 (d)0 6 2
004 003 013 002 −1 4 0 0 0
221 03000 
(e)0 6 2 (f)0 −4 −1 0 0 0 0 2  3 − 9 − 4 2 − 1 
15414
1.15 Find the minimal polynomial of this matrix.
0 1 0 0 0 1 100
  1.16 What is the minimal polynomial of the differentiation operator d/dx on Pn?
  1.17 Find the minimal polynomial of matrices of this form
λ 0 0 ... 0 1 λ 0 0
0 1 λ  
 ..  .
 λ 0 00... 1λ
where the scalar λ is fixed (i.e., is not a variable).
1.18 What is the minimal polynomial of the transformation of Pn that sends p(x)
to p(x + 1)?
1.19 What is the minimal polynomial of the map π: C3 → C3 projecting onto the
first two coordinates?
1.20 Find a 3×3 matrix whose minimal polynomial is x2.
1.21 What is wrong with this claimed proof of Lemma 1.9: “if c(x) = |T − xI| then c(T) = |T − TI| = 0”? [Cullen]
1.22 Verify Lemma 1.9 for 2×2 matrices by direct calculation.
  1.23 Prove that the minimal polynomial of an n×n matrix has degree at most n (not n2 as a person might guess from this subsection’s opening). Verify that this maximum, n, can happen.
  1.24 Show that, on a nontrivial vector space, a linear transformation is nilpotent if and only if its only eigenvalue is zero.
1.25 What is the minimal polynomial of a zero map or matrix? Of an identity map or matrix?
  1.26 Interpret the minimal polynomial of Example 1.1 geometrically. 1.27 What is the minimal polynomial of a diagonal matrix?
  1.28 A projection is any transformation t such that t2 = t. (For instance, consider the transformation of the plane R2 projecting each vector onto its first coordinate. If we project twice then we get the same result as if we project just once.) What is the minimal polynomial of a projection?
Section IV. Jordan Form 431
 1.29 The first two items of this question are review.
(a) Prove that the composition of one-to-one maps is one-to-one.
(b) Prove that if a linear map is not one-to-one then at least one nonzero vector
from the domain maps to the zero vector in the codomain.
(c) Verify the statement, excerpted here, that precedes Theorem 1.8.
... if a minimal polynomial m(x) for a transformation t factors as m(x) = (x−λ1)q1 ···(x−λz)qz then m(t) = (t−λ1)q1 ◦···◦(t−λz)qz is the zero map. Since m(t) sends every vector to zero, at least one of the maps t − λi sends some nonzero vectors to zero. ... That is, ... at least some of the λi are eigenvalues.
1.30 True or false: for a transformation on an n dimensional space, if the minimal polynomial has degree n then the map is diagonalizable.
1.31 Let f(x) be a polynomial. Prove that if A and B are similar matrices then f(A) is similar to f(B).
(a) Now show that similar matrices have the same characteristic polynomial. (b) Show that similar matrices have the same minimal polynomial.
(c) Decide if these are similar.
 13   4−1  2311
1.32 (a) Show that a matrix is invertible if and only if the constant term in its minimal polynomial is not 0.
(b) Show that if a square matrix T is not invertible then there is a nonzero matrix S such that ST and TS both equal the zero matrix.
  1.33 (a) Finish the proof of Lemma 1.7.
(b) Give an example to show that the result does not hold if t is not linear.
1.34 Any transformation or square matrix has a minimal polynomial. Does the converse hold?
IV.2 Jordan Canonical Form
We are looking for a canonical form for matrix similarity. This subsection completes this program by moving from the canonical form for the classes of nilpotent matrices to the canonical form for all classes.
Proof Let the linear transformation be t: V → V. If t is nilpotent then there is an n such that tn is the zero map, so t satisfies the polynomial p(x) = xn = (x − 0)n. By Lemma 1.10 the minimal polynomial of t divides p, so the minimal
 2.1 Lemma A linear transformation on a nontrivial vector space is nilpotent if and only if its only eigenvalue is zero.
432 Chapter Five. Similarity
 polynomial has only zero for a root. By Cayley-Hamilton, Theorem 1.8, the characteristic polynomial has only zero for a root. Thus the only eigenvalue of t is zero.
Conversely, if a transformation t on an n-dimensional space has only the single eigenvalue of zero then its characteristic polynomial is xn. Lemma 1.9 says that a map satisfies its characteristic polynomial so tn is the zero map. Thus t is nilpotent. QED
The ‘nontrivial vector space’ is in the statement of that lemma because on a trivial space {⃗0} the only transformation is the zero map, which has no eigenvalues because there are no associated nonzero eigenvectors.
Proof Thetransformationt−λisnilpotentifandonlyift−λ’sonlyeigenvalue is 0. That holds if and only if t’s only eigenvalue is λ, because t(⃗v) = λ⃗v if and only if (t − λ) (⃗v) = 0 · ⃗v. QED
We already have the canonical form that we want for the case of nilpotent matrices, that is, for each matrix whose only eigenvalue is zero. Corollary III.2.16 says that each such matrix is similar to one that is all zeroes except for blocks of subdiagonal ones.
Proof With N = P(T − λI)P−1 = PTP−1 − P(λI)P−1 we have N = PTP−1 − PP−1(λI) since the diagonal matrix λI commutes with anything, and so N = PTP−1 − λI. Therefore N + λI = PTP−1. QED
 2.2 Corollary The transformation t−λ is nilpotent if and only if t’s only eigenvalue is λ.
 2.3 Lemma If the matrices T − λI and N are similar then T and N + λI are also similar, via the same change of basis matrices.
2.4 Example The characteristic polynomial of  2 −1 
T=14
is (x − 3)2 and so T has only the single eigenvalue 3. Thus for
 −1 −1  T−3I= 1 1
the only eigenvalue is 0 and T − 3I is nilpotent. Finding the null spaces is routine; to ease this computation we take T to represent a transformation t: C2 → C2 with respect to the standard basis (we shall do this for the rest of the chapter).
 −y 
N (t−3)={ y |y∈C} N ((t−3)2)=C2
Section IV. Jordan Form 433
 The dimension of each null space shows that the action of the map t − 3 on a string basis is β⃗ 1  → β⃗ 2  → ⃗0. Thus, here is the canonical form for t − 3 with one choice for a string basis.
 0 0   1   −2  RepB,B(t−3)=N= 1 0 B=⟨ 1 , 2 ⟩
By Lemma 2.3, T is similar to this matrix. RepB,B(t)=N+3I= 1 3
 3 0 
We can produce the similarity computation. Recall how to find the change of
basis matrices P and P−1 to express N as P(T − 3I)P−1. The similarity diagram 2 t−3 2
Cwrt E2 −−−−→ Cwrt E2 T −3I 
id P id P 2 t−3 2
Cwrt B −−−−→ Cwrt B N
describes that to move from the lower left to the upper left we multiply by
   P−1 =  RepE2,B(id) −1 = RepB,E2 (id) = 1 −2
and to move from the upper right to the lower right we multiply by this matrix.
 1 −2 −1   1/2 1/2  P= 1 2 = −1/4 1/4
So this equation expresses the similarity.
 3 0    1/2 1/2  2 −1  1 −2  1 3 = −1/4 1/4 1 4 1 2
2.5 Example This matrix has characteristic polynomial (x − 4)4
4 1 0 −1
0 3 0 1 T =  0 0 4 0 
1005
and so has the single eigenvalue 4. The null space of t − 4 has dimension two, the null space of (t − 4)2 has dimension three, and the null space of (t − 4)3 has
12
434 Chapter Five. Similarity
 dimension four. Thus, t−4 has the action on a string basis of β⃗ 1  → β⃗ 2  → β⃗ 3  → ⃗0 and β⃗ 4  → ⃗0. This gives the canonical form N for t − 4, which in turn gives the form for t.
1/2 0 0  0 1/2 0 
0 0 1/2
In particular, this matrix
1/2 0 0  1 1/2 0 
0 0 1/2
1/2 0 0 0 1/2 0
0 1 1/2
1/2 0 0
4 0 0 0 1 4 0 0
N+4I=0 1 4 0 0004
An array that is all zeroes, except for some number λ down the diagonal and blocks of subdiagonal ones, is a Jordan block. We have shown that Jordan block matrices are canonical representatives of the similarity classes of single-eigenvalue matrices.
2.6 Example The 3×3 matrices whose only eigenvalue is 1/2 separate into three similarity classes. The three classes have these canonical representatives.
belongs to the similarity class represented by the middle one, because we have adopted the convention of ordering the blocks of subdiagonal ones from the longest block to the shortest.
We will finish the program of this chapter by extending this work to cover maps and matrices with multiple eigenvalues. The best possibility for general maps and matrices would be if we could break them into a part involving their first eigenvalue λ1 (which we represent using its Jordan block), a part with λ2, etc.
This best possibility is what happens. For any transformation t: V → V, we shall break the space V into the direct sum of a part on which t − λ1 is nilpotent, a part on which t − λ2 is nilpotent, etc.
Suppose that t: V → V is a linear transformation. The restriction of t to a subspace M need not be a linear transformation on M because there may be an m⃗ ∈ M with t(m⃗ ) ̸∈ M (for instance, the transformation that rotates the plane by a quarter turn does not map most members of the x = y line subspace back within that subspace). To ensure that the restriction of a transformation to a part of a space is a transformation on the part we need the next condition.
 1 0
1/2 0  1 1/2
Section IV. Jordan Form 435
  2.7 Definition Let t: V → V be a transformation. A subspace M is t invariant if whenever m⃗ ∈ M then t(m⃗ ) ∈ M (shorter: t(M) ⊆ M).
Recall that Lemma III.1.4 shows that for any transformation t on an n di- mensional space the range spaces of iterates are stable
as are the null spaces.
R(tn) = R(tn+1) = ··· = R∞(t) N (tn)=N (tn+1)=···=N∞(t)
Thus, the generalized null space N∞(t) and the generalized range space R∞(t) are t invariant. In particular, N∞(t − λi) and R∞(t − λi) are t − λi invariant. The action of the transformation t − λi on N∞(t − λi) is especially easy to
understand. Observe that any transformation t is nilpotent on N∞(t), because if ⃗v ∈ N∞(t) then by definition tn(⃗v) = ⃗0. Thus t − λi is nilpotent on N∞(t − λi). We shall take three steps to prove this section’s major result. The next result
is the first.
Proof For the first sentence we check the two implications separately. The ‘if’ half is easy: if the subspace is t − λ invariant for all scalars λ then using λ = 0 shows that it is t invariant. For ‘only if’ suppose that the subspace is t invariant, so that if m⃗ ∈ M then t(m⃗ ) ∈ M, and let λ be a scalar. The subspace M is closed under linear combinations and so if t(m⃗ ) ∈ M then t(m⃗ ) − λm⃗ ∈ M. Thusifm⃗ ∈Mthen(t−λ)(m⃗)∈M.
The lemma’s second sentence follows from its first. The two spaces are t − λi invariant so they are t invariant. Apply the first sentence again to conclude that they are also t − λj invariant. QED
The second step of the three that we will take to prove this section’s major
result makes use of an additional property of N∞(t − λi) and R∞(t − λi), that
they are complementary. Recall that if a space is the direct sum of two others
V = N ⊕R then any vector ⃗v in the space breaks into two parts ⃗v = n⃗ +⃗r where
n⃗ ∈N and⃗r∈R,andrecallalsothatifBN andBR arebasesforN andR
then the concatenation B ⌢B is linearly independent. The next result says NR
that for any subspaces N and R that are complementary as well as t invariant, the action of t on ⃗v breaks into the actions of t on n⃗ and on ⃗r.
 2.8 Lemma A subspace is t invariant if and only if it is t − λ invariant for all scalars λ. In particular, if λi is an eigenvalue of a linear transformation t then for any other eigenvalue λj the spaces N∞(t − λi) and R∞(t − λi) are t − λj invariant.
436 Chapter Five. Similarity
  2.9 Lemma Let t: V → V be a transformation and let N and R be t invariant complementary subspaces of V. Then we can represent t by a matrix with blocks of square submatrices T1 and T2
 T Z  } dim(N )-many rows 12
Z1   T2 } dim(R)-many rows where Z1 and Z2 are blocks of zeroes.
  Proof Since the two subspaces are complementary, the concatenation of a basis for N with a basis for R makes a basis B = ⟨⃗ν1,...,⃗νp,⃗μ1,...,⃗μq⟩ for V. We shall show that the matrix
 . .  RepB,B(t) = RepB(t(⃗ν1)) · · · RepB(t(⃗μq))
. . ..
has the desired form.
Any vector ⃗v ∈ V is a member of N if and only if when it is represented
with respect to B the final q coefficients are zero. As N is t invariant, each of the vectors RepB(t(⃗ν1)), . . . , RepB(t(⃗νp)) has this form. Hence the lower left of RepB,B(t) is all zeroes. The argument for the upper right is similar. QED
To see that we have decomposed t into its action on the parts, let BN = ⟨⃗ν1,...,⃗νp⟩ and BR = ⟨⃗μ1,...,⃗μq⟩. The restrictions of t to the subspaces N and R are represented with respect to the bases BN ,BN and BR,BR by the matrices T1 and T2. So with subspaces that are invariant and complementary we can split the problem of examining a linear transformation into two lower- dimensional subproblems. The next result illustrates this decomposition into blocks.
 2.10 Lemma If T is a matrix with square submatrices T1 and T2  T Z 
 T=12 Z1 T2
  where the Z’s are blocks of zeroes, then |T| = |T1| · |T2|.
Proof Suppose that T is n×n, that T1 is p×p, and that T2 is q×q. In the permutation formula for the determinant
|T | =
 
t1,φ(1) t2,φ(2) · · · tn,φ(n) sgn(φ)
permutations φ
Section IV. Jordan Form 437
 each term comes from a rearrangement of the column numbers 1, . . . , n into a new order φ(1), . . . , φ(n). The upper right block Z2 is all zeroes, so if a φ has at least one of p+1,...,n among its first p column numbers φ(1),...,φ(p) then the term arising from φ does not contribute to the sum because it is zero, e.g., if φ(1) = n then t1,φ(1)t2,φ(2) . . . tn,φ(n) = 0 · t2,φ(2) . . . tn,φ(n) = 0.
So the above formula reduces to a sum over all permutations with two halves: any contributing φ is the composition of a φ1 that rearranges only 1,...,p and a φ2 that rearranges only p+1,...,p+q. Now, the distributive law and the fact that the signum of a composition is the product of the signums gives that this
|T1| · |T2| =
equals |T | =   2.11 Example
perms φ1 of 1,...,p
    t1,φ1(1) · · · tp,φ1(p) sgn(φ1)
·
    tp+1,φ2(p+1) ···tp+q,φ2(p+q) sgn(φ2)
perms φ2
of p+1,...,p+q
sgn(φ). QED
 1 2 0 0   2 0   3 0 
    =     ·     = 36  0030   12  03 
 0 0 0 3 
· · · t       
t t
contributing φ 1,φ(1) 2,φ(2)  2 0 0 0 
n,φ(n)
From Lemma 2.10 we conclude that if two subspaces are complementary and t invariant then t is one-to-one if and only if its restriction to each subspace is nonsingular.
Now for the promised third, and final, step to the main result.
Proof This argument consists of proving two preliminary claims, followed by proofs of clauses (1) and (2).
The first claim is that N∞(t−λi)∩N∞(t−λj) = {⃗0} when i ̸= j. By Lemma 2.8 both N∞(t − λi) and N∞(t − λj) are t invariant. The intersection of t invariant subspaces is t invariant and so the restriction of t to N∞(t − λi) ∩ N∞(t−λj) is a linear transformation. Now, t−λi is nilpotent on N∞(t−λi) and t−λj is nilpotent on N∞(t−λj), so both t−λi and t−λj are nilpotent on the intersection. Therefore by Lemma 2.1 and the observation following it, if t has
 2.12 Lemma If a linear transformation t : V → V has the characteristic polynomial (x−λ1)p1 ...(x−λk)pk then (1) V = N∞(t−λ1)⊕···⊕N∞(t−λk) and (2) dim(N∞(t − λi)) = pi.
438 Chapter Five. Similarity
 any eigenvalues on the intersection then the “only” eigenvalue is both λi and λj. This cannot be, so the restriction has no eigenvalues: N∞(t − λi) ∩ N∞(t − λj) is the trivial space (Lemma 3.10 shows that the only transformation that is without any eigenvalues is the transformation on the trivial space).
The second claim is that N∞(t − λi) ⊆ R∞(t − λj), where i ̸= j. To verify it we will show that t − λj is one-to-one on N∞(t − λi) so that, since N∞(t − λi) is t − λj invariant by Lemma 2.8, the map t − λj is an automorphism of the subspace N∞(t − λi) and therefore that N∞(t − λi) is a subset of each R(t − λj), R((t − λj)2), etc. For the verification that the map is one-to-one suppose that ⃗v ∈ N∞(t−λi) is in the null space of t−λj, aiming to show that ⃗v = ⃗0. Consider the map [(t − λi) − (t − λj)]n. On the one hand, the only vector that (t−λi)−(t−λj) = λi −λj maps to zero is the zero vector. On the other hand, as in the proof of Lemma 1.7 we can apply the binomial expansion to get this.
n  n  n−1 1  n  n−2 2
(t−λi) (⃗v)+ 1 (t−λi) (t−λj) (⃗v)+ 2 (t−λi) (t−λj) (⃗v)+···
The first term is zero because ⃗v ∈ N∞(t − λi) while the remaining terms are zero because ⃗v is in the null space of t − λj. Therefore ⃗v = ⃗0.
With those two preliminary claims done we can prove clause (1), that the space is the direct sum of the generalized null spaces. By Corollary III.2.2 the space is the direct sum V = N∞(t−λ1)⊕R∞(t−λ1). By the second claim N∞(t−λ2) ⊆ R∞(t − λ1) and so we can get a basis for R∞(t − λ1) by starting with a basis for N∞(t−λ2) and adding extra basis elements taken from R∞(t−λ1)∩R∞(t−λ2). ThusV=N∞(t−λ1)⊕N∞(t−λ2)⊕(R∞(t−λ1)∩R∞(t−λ2)). Continuing in this way we get this.
V =N∞(t−λ1)⊕···⊕R∞(t−λk)⊕(R∞(t−λ1)∩···∩R∞(t−λk))
The first claim above shows that the final space is trivial.
We finish by verifying clause (2). Decompose V as N∞(t − λi) ⊕ R∞(t − λi)
and apply Lemma 2.9.
 T Z }dim(N (t−λ))-manyrows
 T=12∞i
Z1   T2 } dim( R∞(t − λi) )-many rows
 Lemma 2.10 says that |T − xI| = |T1 − xI| · |T2 − xI|. By the uniqueness clause of the Fundamental Theorem of Algebra, Theorem I.1.11, the determinants of the blocks have the same factors as the characteristic polynomial |T1 − xI| = (x−λ1)q1 ···(x−λz)qk and|T2−xI|=(x−λ1)r1 ···(x−λz)rk,whereq1+r1 =p1, ..., qk +rk = pk. We will finish by establishing that (i) qj = 0 for all j ̸= i, and (ii) qi = pi. Together these prove clause (2) because they show that the
Section IV. Jordan Form 439
 degree of the polynomial |T1 − xI| is qi and the degree of that polynomial equals the dimension of the generalized null space N∞(t − λi).
For (i), because the restriction of t − λi to N∞(t − λi) is nilpotent on that space, t’s only eigenvalue on that space is λi, by Lemma 2.2. So qj = 0 for j ̸= i. For (ii), consider the restriction of t to R∞(t − λi). By Lemma III.2.1, the
map t − λi is one-to-one on R∞(t − λi) and so λi is not an eigenvalue of t on that subspace. Therefore x − λi is not a factor of |T2 − xI|, so ri = 0, and so qi = pi. QED
Recall the goal of this chapter, to give a canonical form for matrix similarity. That result is next. It translates the above steps into matrix terms.
 2.13 Theorem Any square matrix is similar to one in Jordan form
Jλ1
–zeroes–  
 Jλ2
 
...   J
λk−1 –zeroes– Jλk
where each Jλ is the Jordan block associated with an eigenvalue λ of the original matrix (that is, each Jλ is all zeroes except for λ’s down the diagonal and some subdiagonal ones).
Proof Given an n×n matrix T, consider the linear map t: Cn → Cn that it represents with respect to the standard bases. Use the prior lemma to write Cn = N∞(t−λ1)⊕···⊕N∞(t−λk) where λ1, ..., λk are the eigenvalues of t. Because each N∞(t − λi) is t invariant, Lemma 2.9 and the prior lemma show that t is represented by a matrix that is all zeroes except for square blocks along the diagonal. To make those blocks into Jordan blocks, pick each Bλi to be a string basis for the action of t − λi on N∞(t − λi). QED
For Jordan form a canonical form for matrix similarity, strictly speaking it must be unique. That is, for any square matrix there needs to be one and only one matrix J similar to it and of the specified form. As stated the theorem allows us to rearrange the Jordan blocks. We could make this form unique, say by arranging the Jordan blocks so the eigenvalues are in order, and then arranging the blocks of subdiagonal ones from longest to shortest. Below, we won’t bother with that.
 2.14 Corollary Every square matrix is similar to the sum of a diagonal matrix and a nilpotent matrix.
440 Chapter Five. Similarity 2.15 Example This matrix has the characteristic polynomial (x − 2)2(x − 6).
2 0 1 T =  0 6 2 
002
First we do the eigenvalue 2. Computation of the powers of T − 2I, and of the null spaces and nullities, is routine. (Recall from Example 2.4 our convention of taking T to represent a transformation t: C3 → C3 with respect to the standard basis.)
 p (T − 2I)p 0 0 1
N ((t − 2)p) x
nullity
  {0|x∈C} 1 
1 0 4 2
000 0
0 0 0  x 
2 0 16 8 {−z/2|x,z∈C} 2
 
000 z 0 0 0
3 0 64 32 –same– 
000
–same–
 So the generalized null space N∞(t − 2) has dimension two. We know that the restriction of t − 2 is nilpotent on this subspace. From the way that the nullities growweknowthattheactionoft−2onastringbasisisβ⃗1  →β⃗2  →⃗0. Thus we can represent the restriction in the canonical form
1 −2 N2 = 0 0 =RepB,B(t−2) B2 =⟨ 1 , 0 ⟩
  
10 −2 0
(other choices of basis are possible). Consequently, the action of the restriction
of t to N∞(t − 2) is represented by this matrix. J2=N2+2I=RepB2,B2(t)= 1 2
The second eigenvalue is 6. Its computations are easier. Because the power of x − 6 in the characteristic polynomial is one, the restriction of t − 6 to N∞(t − 6)
 2 0 
Section IV. Jordan Form 441
 must be nilpotent, of index one (it can’t be of index less than one and since x − 6 is a factor of the characteristic polynomial with the exponent one it can’t be of index more than one either). Its action on a string basis must be β⃗ 3  → ⃗0 and since it is the zero map, its canonical form N6 is the 1×1 zero matrix. Consequently, the canonical form J6 for the action of t on N∞(t − 6) is the 1×1 matrix with the single entry 6. For the basis we can use any nonzero vector from the generalized null space.
0 B6 = ⟨1⟩
0
Taken together, these two give that the Jordan form of T is
2 0 0 RepB,B(t) = 1 2 0
006 where B is the concatenation of B2 and B6.
2.16 Example As a contrast with the prior example, this matrix
2 2 1 T =  0 6 2 
002
has the same characteristic polynomial (x − 2)2(x − 6), but here
p (T − 6I)p N ((t − 6)p) nullity −431 x
1 {|x∈C}1  0 0 2  (4/3)x
  00−4 0 16 −12 −2
 2
0 0 −8 0 0 16
–same– —
the action of t − 2 is stable after only one application — the restriction of t − 2 to N∞(t − 2) is nilpotent of index one. The restriction of t − 2 to the generalized null space acts on a string basis via the two strings β⃗ 1  → ⃗0 and β⃗ 2  → ⃗0. We have this Jordan block associated with the eigenvalue 2.
 2 0  J2= 0 2
442 Chapter Five. Similarity
 So the contrast with the prior example is that while the characteristic polynomial tells us to look at the action of t − 2 on its generalized null space, the characteristic polynomial does not completely describe t − 2’s action. We must do some computations to find that the minimal polynomial is (x − 2)(x − 6).
For the eigenvalue 6 the arguments for the second eigenvalue of the prior example apply again. The restriction of t − 6 to N∞(t − 6) is nilpotent of index one. Thus t − 6’s canonical form N6 is the 1×1 zero matrix, and the associated Jordan block J6 is the 1×1 matrix with entry 6.
Therefore the Jordan form for T is a diagonal matrix.
2 0 0 102
Rep (t)=0 2 0 B=B ⌢B =⟨0,1,4⟩ B,B 26
006 0 −2 0 (Checking that the third vector in B is in the null space of t − 6 is routine.)
2.17 Example A bit of computing with
−1 4 0 0 0
0 3 000 
T=0 −4 −1 0 0  3 − 9 − 4 2 − 1 
15414
shows that its characteristic polynomial is (x − 3)3(x + 1)2. This table
p (T − 3I)p −44000
0 0 0 0 0 1 
0 −4 −4 0 0  3 − 9 − 4 − 1 − 1 
N ((t − 3)p) −(u + v)/2
nullity
  15411 v
−(u+v)/2
{  | u,v ∈ C} 2
(u+v)/2  u 
 16 −16 0 0 0 0 0 0 00
2  0 16 16 00 −16 32 16 0 0
0 −16 −16 0 0 −64 64 0 0 0
0 0 000 3 
0 −64 −64 0 0  64 −128 −64 0 0
0 64 64 0 0
−z −z
{  | z,u,v ∈ C} z
u v
–same–
3
–same–
 
Section IV. Jordan Form 443
 shows that the restriction of t − 3 to N∞(t − 3) acts on a string basis via the two strings β⃗1  → β⃗2  →⃗0 and β⃗3  →⃗0.
A similar calculation for the other eigenvalue
Exercises
p
1
2
(T + 1I)p N ((t + 1)p) 0 4 0 0 0 −(u+v)
0 4 0 0 0  0  
0 −4 0 0 0  { −v  | u,v ∈ C} 
3 −9 −4 3 −1  u  
nullity
2
–same–
  15415 v 0 16 0 0 0
0 16 0 0 0 
0−16000 
8 −40 −16 8 −8 
8 24 16 8 24
–same–
 gives that the restriction of t + 1 to its generalized null space acts on a string basis via the two separate strings β⃗ 4  → ⃗0 and β⃗ 5  → ⃗0.
Therefore T is similar to this Jordan form matrix. −1 0 000
0 −1 0 0 0 
0 0 3 0 0  0 0 1 3 0  00003
2.18 Do the check for Example 2.4.
2.19 Each matrix is in Jordan form. State its characteristic polynomial and its
minimal polynomial.
 3 0  (a)13
3 0
(b)
 −1 0  2 0 0  0 −1 (c) 1 2 0 
3 0 0 (d) 1 3 0
(e)1 3
0 0 3 0
0 0 0 0
0 0 −1/2 4 0 0 0
0 1 3 5 0 0
(f)1 4 0 0 (g) 003
0 0 1 3 5 0 0 0 (h)0 2 0 0 0 0 2 0
0003 0003
0 0 −4 0 0 0 1 −4
5 0 0 0 (i)0 2 0 0 0 1 2 0
0 2 0
 
444 Chapter Five. Similarity
   2.20 Find the Jordan form from the given data.
(a) The matrix T is 5×5 with the single eigenvalue 3. The nullities of the powers
are: T − 3I has nullity two, (T − 3I)2 has nullity three, (T − 3I)3 has nullity four,
and (T − 3I)4 has nullity five.
(b) The matrix S is 5×5 with two eigenvalues. For the eigenvalue 2 the nullities
are: S − 2I has nullity two, and (S − 2I)2 has nullity four. For the eigenvalue −1 the nullities are: S + 1I has nullity one.
2.21 Find the change of basis matrices for each example.
(a) Example 2.15 (b) Example 2.16 (c) Example 2.17
  2.22 Find the Jordan form and a Jordan basis for each matrix.
 −10 4   5 −4  4 0 0 5 4 3
(a) −25 10 (b) 9 −7 (c)2 1 3 (d)−1 0 −3 5 0 4 1 −2 1
973 22−1 7122 (e) −9 −7 −4 (f) −1 −1 1 (g)1 4 −1 −1
  −215−1 4 4 4 −1 −2 2 1 1 2 8
  2.23 Find all possible Jordan forms of a transformation with characteristic polynomial (x − 1)2(x + 2)2.
2.24 FindallpossibleJordanformsofatransformationwithcharacteristicpolynomial (x−1)3(x+2).
  2.25 Find all possible Jordan forms of a transformation with characteristic polynomial (x − 2)3(x + 1) and minimal polynomial (x − 2)2(x + 1).
2.26 FindallpossibleJordanformsofatransformationwithcharacteristicpolynomial (x − 2)4(x + 1) and minimal polynomial (x − 2)2(x + 1).
  2.27 Diagonalize these.
 1 1   0 1 
(a) (b)
00 10
  2.28 Find the Jordan matrix representing the differentiation operator on P3.
  2.29 Decide if these two are similar.
 1 −1   −1 0  4−3 1−1
2.30 Find the Jordan form of this matrix.
 0 −1 
10
2.31 How many similarity classes are there for 3×3 matrices whose only eigenvalues
Also give a Jordan basis. are −3 and 4?
  2.32 Prove that a matrix is diagonalizable if and only if its minimal polynomial has only linear factors.
2.33 Give an example of a linear transformation on a vector space that has no non-trivial invariant subspaces.
2.34 Show that a subspace is t − λ1 invariant if and only if it is t − λ2 invariant.
Section IV. Jordan Form 445
 2.35 Prove or disprove: two n×n matrices are similar if and only if they have the same characteristic and minimal polynomials.
2.36 The trace of a square matrix is the sum of its diagonal entries.
(a) Find the formula for the characteristic polynomial of a 2×2 matrix.
(b) Show that trace is invariant under similarity, and so we can sensibly speak of
the ‘trace of a map’. (Hint: see the prior item.)
(c) Is trace invariant under matrix equivalence?
(d) Show that the trace of a map is the sum of its eigenvalues (counting multi-
plicities).
(e) Show that the trace of a nilpotent map is zero. Does the converse hold?
2.37 To use Definition 2.7 to check whether a subspace is t invariant, we seemingly have to check all of the infinitely many vectors in a (nontrivial) subspace to see if they satisfy the condition. Prove that a subspace is t invariant if and only if its subbasis has the property that for all of its elements, t(β⃗ ) is in the subspace.
  2.38 Is t invariance preserved under intersection? Under union? Complementation? Sums of subspaces?
2.39 Give a way to order the Jordan blocks if some of the eigenvalues are complex numbers. That is, suggest a reasonable ordering for the complex numbers.
2.40 Let Pj(R) be the vector space over the reals of degree j polynomials. Show that if j   k then Pj(R) is an invariant subspace of Pk(R) under the differentiation operator. In P7(R), does any of P0(R), ..., P6(R) have an invariant complement?
2.41 In Pn(R), the vector space (over the reals) of degree n polynomials, E={p(x)∈Pn(R)|p(−x)=p(x) for all x}
and
O = {p(x) ∈ Pn(R) | p(−x) = −p(x) for all x}
are the even and the odd polynomials; p(x) = x2 is even while p(x) = x3 is odd. Show that they are subspaces. Are they complementary? Are they invariant under the differentiation transformation?
2.42 Lemma 2.9 says that if M and N are invariant complements then t has a representation in the given block form (with respect to the same ending as starting basis, of course). Does the implication reverse?
2.43 A matrix S is the square root of another T if S2 = T. Show that any nonsingular matrix has a square root.
Topic
Method of Powers
In applications matrices can be large. Calculating eigenvalues and eigenvectors by finding and solving the characteristic polynomial is impractical, too slow and too error-prone. Some techniques avoid the characteristic polynomial. Here we shall see a method that is suitable for large matrices that are sparse, meaning that the great majority of the entries are zero.
Suppose that the n×n matrix T has n distinct eigenvalues λ1, λ2, . . . , λn. Then Cn has a basis made of the associated eigenvectors ⟨⃗ζ1, . . . , ⃗ζn⟩. For any ⃗v ∈ Cn, writing ⃗v = c1⃗ζ1 + · · · + cn⃗ζn and iterating T on ⃗v gives these.
T⃗v = c1λ1⃗ζ1 + c2λ2⃗ζ2 + · · · + cnλn⃗ζn T2⃗v=c1λ21⃗ζ1 +c2λ2⃗ζ2 +···+cnλ2n⃗ζn T3⃗v=c1λ31⃗ζ1 +c2λ32⃗ζ2 +···+cnλ3n⃗ζn
.
Tk⃗v=c1λk1⃗ζ1 +c2λk2⃗ζ2 +···+cnλkn⃗ζn Assuming that |λ1| is the largest and dividing through
T k ⃗v = c ⃗ζ + c λ k2 ⃗ζ + · · · + c λ kn ⃗ζ λk112λk2 nλkn
111
shows that as k gets larger the fractions go to zero and so λ1’s term will dominate the expression and that expression has a limit of c1⃗ζ1.
Thus if c1 ̸= 0, as k increases the vectors Tk⃗v will tend toward the direction of the eigenvectors associated with the dominant eigenvalue. Consequently, the ratios of the vector lengths |Tk⃗v|/|Tk−1⃗v| tend to that dominant eigenvalue.
For example, the eigenvalues of the matrix
 3 0  T= 8 −1
are 3 and −1. If ⃗v has the components 1 and 1 then iterating gives this.
    
Topic: Method of Powers
447
  ⃗v T⃗v T 2⃗v  1   3   9 
· · · ···
T 9⃗v T 10⃗v  19683   59049 
  1 7 17 39367 118097
The ratio between the lengths of the last two is 2.999 9.
We note two implementation issues. First, instead of finding the powers of
T and applying them to ⃗v, we will compute ⃗v1 as T⃗v and then compute ⃗v2 as T⃗v1, etc. (that is, we do not separately calculate T2, T3, ...). We can quickly do these matrix-vector products even if T is large, provided that it is sparse. The second issue is that to avoid generating numbers that are so large that they overflow our computer’s capability, we can normalize the ⃗vi’s at each step. For instance, we can divide each ⃗vi by its length (other possibilities are to divide it by its largest component, or simply by its first component). We thus implement this method by generating
w⃗ 0 = ⃗v 0 / | ⃗v 0 | ⃗v 1 = T w⃗ 0
w⃗ 1 = ⃗v 1 / | ⃗v 1 | ⃗v 2 = T w⃗ 2
.
w⃗ k−1 = ⃗vk−1/|⃗vk−1| ⃗v k = T w⃗ k
until we are satisfied. Then ⃗vk is an approximation of an eigenvector, and the approximation of the dominant eigenvalue is the ratio (T • ⃗vk)/(⃗vk • ⃗vk) ≈ (λ1⃗vk · ⃗vk)/(⃗vk • ⃗vk) = λ1.
One way that we could be ‘satisfied’ is to iterate until our approximation of the eigenvalue settles down. We could decide for instance to stop the iteration process not after some fixed number of steps, but instead when |⃗vk| differs from |⃗vk−1| by less than one percent, or when they agree up to the second significant digit.
The rate of convergence is determined by the rate at which the powers of |λ2/λ1| go to zero, where λ2 is the eigenvalue of second largest length. If that ratio is much less than one then convergence is fast but if it is only slightly less than one then convergence can be quite slow. Consequently, the method of powers is not the most commonly used way of finding eigenvalues (although it is the simplest one, which is why it is here). Instead, there are a variety of methods that generally work by first replacing the given matrix T with another that is similar to it and so has the same eigenvalues, but is in some reduced form
448 Chapter Five. Similarity
 such as tridiagonal form, where the only nonzero entries are on the diagonal, or just above or below it. Then special case techniques can find the eigenvalues. Once we know the eigenvalues then we can easily compute the eigenvectors of T. These other methods are outside of our scope. A good reference is [Goult, et al.]
Exercises
1 Use ten iterations to estimate the largest eigenvalue of these matrices, starting from the vector with components 1 and 2. Compare the answer with the one obtained by solving the characteristic equation.
04 −10
2 Redo the prior exercise by iterating until |⃗vk| − |⃗vk−1| has absolute value less than
0.01 At each step, normalize by dividing each vector by its length. How many
iterations does it take? Are the answers significantly different?
3 Use ten iterations to estimate the largest eigenvalue of these matrices, starting from the vector with components 1, 2, and 3. Compare the answer with the one
obtained by solving the characteristic equation.
401 −122 (a) −2 1 0 (b)  2 2 2 
−2 0 1 −3 −6 −6
4 Redo the prior exercise by iterating until |⃗vk| − |⃗vk−1| has absolute value less than
0.01. At each step, normalize by dividing each vector by its length. How many
iterations does it take? Are the answers significantly different?
5 What happens if c1 = 0? That is, what happens if the initial vector does not to
have any component in the direction of the relevant eigenvector?
6 How can we adapt the method of powers to find the smallest eigenvalue?
Computer Code
This is the code for the computer algebra system Octave that did the calculation above. (It has been lightly edited to remove blank lines, etc.)
>T=[3, 0; 8, -1]
T=
30
8 -1 >v0=[1; 2]
v0= 1
1 >v1=T*v0
v1= 3
7 >v2=T*v1
v2= 9
17 >T9=T**9 T9=
 15   32  (a) (b)
                    19683 0
Topic: Method of Powers 449
  39368 -1 >T10=T**10 T10=
59049 0 118096 1
>v9=T9*v0 v9=
19683
39367 >v10=T10*v0 v10=
59049 118096
>norm(v10)/norm(v9) ans=2.9999
Remark. This does not use the full power of Octave; it has built-in functions to automatically apply sophisticated methods to find eigenvalues and eigenvectors.
              
Topic
Stable Populations
Imagine a reserve park with animals from a species that we are protecting. The park doesn’t have a fence so animals cross the boundary, both from the inside out and from the outside in. Every year, 10% of the animals from inside of the park leave and 1% of the animals from the outside find their way in. Can we reach a stable level; are there populations for the park and the rest of the world that will stay constant over time, with the number of animals leaving equal to the number of animals entering?
Let pn be the year n population in the park and let rn be the population in the rest of the world.
pn+1 = .90pn + .01rn rn+1 = .10pn + .99rn
We have this matrix equation.
 p    .90 .01  p   n+1= n
rn+1 .10 .99 rn
The population will be stable if pn+1 = pn and rn+1 = rn so that the matrix equation ⃗vn+1 = T⃗vn becomes ⃗v = T⃗v. We are therefore looking for eigenvectors for T that are associated with the eigenvalue λ = 1. The equation ⃗0 = (λI − T )⃗v = (I−T)⃗v is
  0.10 −0.01  p   0  −0.10 0.01 r = 0
and gives the eigenspace of vectors with the restriction that p = .1r. For example, if we start with a park population p = 10 000 animals and a rest of the world population of r = 100 000 animals then every year ten percent of those inside leave the park (this is a thousand animals), and every year one percent of those from the rest of the world enter the park (also a thousand animals). The population is stable, self-sustaining.
 
Topic: Stable Populations 451
 Now imagine that we are trying to raise the total world population of this species. We are trying to have the world population grow at 1% per year. This makes the population level stable in some sense, although it is a dynamic stability, in contrast to the static population level of the λ = 1 case. The equation ⃗vn+1 = 1.01 · ⃗vn = T⃗vn leads to ((1.01I − T )⃗v = ⃗0, which gives this system.
  0.11 −0.01  p   0  −0.10 0.02 r = 0
This matrix is nonsingular and so the only solution is p = 0, r = 0. Thus there is no nontrivial initial population that would lead to a regular annual one percent growth rate in p and r.
We can look for the rates that allow an initial population for the park that
results in a steady growth behavior. We consider λ⃗v = T⃗v and solve for λ.   
 λ−.9 .01   2
0=   =(λ−.9)(λ−.99)−(.10)(.01)=λ −1.89λ+.89
We already know that λ = 1 is one solution of this characteristic equation. The other is 0.89. Thus there are two ways to have a dynamically stable p and r, where the two grow at the same rate despite the leaky park boundaries: have a world population that is does not grow or shrink, and have a world population that shrinks by 11% every year.
So one way to look at eigenvalues and eigenvectors is that they give a stable state for a system. If the eigenvalue is one then the system is static and if the eigenvalue isn’t one then it is a dynamic stability.
Exercises
1 For the park discussed above, what should be the initial park population in the case where the populations decline by 11% every year?
2 What will happen to the population of the park in the event of a growth in world population of 1% per year? Will it lag the world growth, or lead it? Assume that the initial park population is ten thousand, and the world population is one hundred thousand, and calculate over a ten year span.
3 The park discussed above is partially fenced so that now, every year, only 5% of the animals from inside of the park leave (still, about 1% of the animals from the outside find their way in). Under what conditions can the park maintain a stable population now?
4 Suppose that a species of bird only lives in Canada, the United States, or in Mexico. Every year, 4% of the Canadian birds travel to the US, and 1% of them travel to Mexico. Every year, 6% of the US birds travel to Canada, and 4% go to Mexico. From Mexico, every year 10% travel to the US, and 0% go to Canada.
(a) Give the transition matrix.
(b) Is there a way for the three countries to have constant populations?
  .10 λ−.99 
Topic
Page Ranking
Imagine that you are looking for the best book on Linear Algebra. You probably would try a web search engine such as Google. These lists pages ranked by impor- tance. The ranking is defined, as Google’s founders have said in [Brin & Page], that a page is important if other important pages link to it: “a page can have a high PageRank if there are many pages that point to it, or if there are some pages that point to it and have a high PageRank.” But isn’t that circular— how can they tell whether a page is important without first deciding on the important pages? With eigenvalues and eigenvectors.
We will present a simplified version of the Page Rank algorithm. For that we will model the World Wide Web as a collection of pages connected by links. This diagram, from [Wills], shows the pages as circles, and the links as arrows; for instance, page p1 has a link to page p2.
p1 p2
p4 p3
The key idea is that pages that should be highly ranked if they are cited often by other pages. That is, we raise the importance of a page pi if it is linked-to from page pj. The increment depends on the importance of the linking page pj divided by how many out-links aj are on that page.
         I(pi) =
  I(pj)
aj
 in-linking pages pj
Topic: Page Ranking 453 This matrix stores the information.
0 0 1/3 0 1 0 1/3 0 0 1 0 0 0 0 1/3 0
The algorithm’s inventors describe a way to think about that matrix.
PageRank can be thought of as a model of user behavior. We assume there is a ‘random surfer’ who is given a web page at random and keeps clicking on links, never hitting “back” . . . The probability that the random surfer visits a page is its PageRank. [Brin & Page]
In the diagram, a surfer on page p3 has a probability 1/3 of going next to each of the other pages.
That leads us to the problem of page p4. Many pages are dangling or sink links, without any outbound links. The simplest model of what happens here is to imagine that the surfer goes to a next page entirely at random.
0 0 1/3 1/4
1 0 1/3 1/4 H = 0 1 0 1/4
0 0 1/3 1/4
We will find vector ⃗I whose components are the importance rankings of each page I(pi). With this notation, our requirements for the page rank are that H⃗I = ⃗I. That is, we want an eigenvector of the matrix associated with the eigenvalue λ = 1.
Here is Sage’s calculation of the eigenvectors (slightly edited to fit on the page).
  sage: H=matrix([[0,0,1/3,1/4], [1,0,1/3,1/4], [0,1,0,1/4], [0,0,1/3,1/4]]) sage: H.eigenvectors_right()
[(1, [
(1, 2,
    ], 1), (0, 1, ], 1),
9/4, 1)
(0, [
3, -4)
(-0.3750000000000000? - 0.4389855730355308?*I,
[(1, -0.1250000000000000? + 1.316956719106593?*I, -1.875000000000000? - 1.316956719106593?*I, 1)], 1),
(-0.3750000000000000? + 0.4389855730355308?*I,
[(1, -0.1250000000000000? - 1.316956719106593?*I,
-1.875000000000000? + 1.316956719106593?*I, 1)], 1)]
       The eigenvector that Sage gives associated with the eigenvalue λ = 1 is this. 1
 2  9/4 1
454 Chapter Five. Similarity Of course, there are many vectors in that eigenspace. To get a page rank number
we normalize to length one.
sage: v=vector([1, 2, 9/4, 1])
sage: v/v.norm()
(4/177*sqrt(177), 8/177*sqrt(177), 3/59*sqrt(177), 4/177*sqrt(177))
sage: w=v/v.norm()
sage: w.n()
(0.300658411201132, 0.601316822402263, 0.676481425202546, 0.300658411201132)
So we rank the first and fourth pages as of equal importance. We rank the second and third pages as much more important than those, and about equal in importance as each other.
We’ll add one more refinement. We will allow the surfer to pick a new page at random even if they are not on a dangling page. Let this happen with probability α.
0 0 1/3 1/4 1/4 1/4 1/4 1/4
1 0 1/3 1/4 1/4 1/4 1/4 1/4 G = α ·   + (1 − α) ·   0 1 0 1/4 1/4 1/4 1/4 1/4
0 0 1/3 1/4 1/4 1/4 1/4 1/4
This is the Google matrix.
In practice α is typically between 0.85 and 0.99. Here are the ranks for the
four pages with various α’s.
α 0.85 0.90 0.95 0.99 p1 0.325 0.317 0.309 0.302 p2 0.602 0.602 0.602 0.601 p3 0.652 0.661 0.669 0.675 p4 0.325 0.317 0.309 0.302
The details of the algorithms used by commercial search engines are se- cret, no doubt have many refinements, and also change frequently. But the inventors of Google were gracious enough to outline the basis for their work in [Brin & Page]. A more current source is [Wikipedia, Google Page Rank]. Two additional excellent expositions are [Wills] and [Austin].
Exercises
1 A square matrix is stochastic if the sum of the entries in each column is one. The Google matrix is computed by taking a combination G = α∗H+(1−α)∗S of two stochastic matrices. Show that G must be stochastic.
            
Topic: Page Ranking 455 2 For this web of pages, the importance of each page should be equal. Verify it for
α = 0.85.
p1 p2
p4 p3
3 [Bryan & Leise] Give the importance ranking for this web of pages.
p1 p2
p4 p3
(a) Use α = 0.85.
(b) Use α = 0.95.
(c) Observe that while p3 is linked-to from all other pages, and therefore seems
important, it is not the highest ranked page. What is the highest ranked page? Explain.
                   
Topic
Linear Recurrences
In 1202 Leonardo of Pisa, known as Fibonacci, posed this problem.
A certain man put a pair of rabbits in a place surrounded on all sides by a wall. How many pairs of rabbits can be produced from that pair in a year if it is supposed that every month each pair begets a new pair which from the second month on becomes productive?
This moves past an elementary exponential growth model for populations to include that newborns are not fertile for some period, here a month. However, it retains other simplifying assumptions such as that there is an age after which the rabbits are infertile.
To get next month’s total number of pairs we add the number of pairs alive going into next month to the number of pairs that will be newly born next month. The latter equals the number of pairs that will be productive going into next month, which is the number that next month will have been alive for at least two months.
F(n) = F(n − 1) + F(n − 2) where F(0) = 0, F(1) = 1 (∗)
On the left is a recurrence relation. It gets that name because F recurs in its own defining equation. On the right are the initial conditions. From (∗) we can compute F(2), F(3), etc., to work up to the answer for Fibonacci’s question.
monthn 0 1 2 3 4 5 6 7 8 9 10 11 12 pairsF(n) 0 1 1 2 3 5 8 13 21 34 55 89 144
We will use linear algebra to get a formula that calculates F(n) without having to first calculate the intermediate values F(2), F(3), etc.
We start by giving (∗) a matrix formulation.
  F(n)    1 1   F(n − 1)   F(1)   1  F(n−1) = 1 0 F(n−2) where F(0) = 0
    
Topic: Linear Recurrences 457
 Write T for the matrix and ⃗vn for the vector with components F(n) and F(n − 1) so that ⃗vn = Tn−1⃗v1 for n   1. If we diagonalize T then we have a fast way to compute its powers: where T = PDP−1 then Tn = PDnP−1 and the n-th power of the diagonal matrix D is the diagonal matrix whose entries are the n-th powers of the entries of D.
The characteristic equation of T is λ2 − λ − 1 = 0. The quadratic formula gives its roots as (1+√5)/2 and (1−√5)/2. (These are sometimes called “golden ratios;” see [Falbo].) Diagonalizing gives this.
    √   √  √   1 1−√5  1 1 1+ 5 1− 5 1+ 5 0 √ −( √ )
            =
2 2 2 √ 5 2√5 1− 5 −1 1+ 5
  10 1 1 0 2 √5 2√5 Introducing the vectors and taking the n-th power, we have
       F(n)    1 1 n−1  f(1)  F(n−1) = 1 0 f(0)
   
√√ √ n−1 √
  1+5  1 1−5   
         =
1+51−5 0 √−(√)
2 2  2  5 2√5 1
   √  n−1
1 1 0 1−5 √5 2√5 0
 √ √ √ n−1       1+5
−1 1+5
      The calculation is ugly but not hard.
2
  1  1+51−5 0√
  F(n) 2 2  2  5
     F(n−1) = 1 1   1−√5 n−1 −1 0 √5
    2  1+√5 1−√5   1+√5 n−1 
   12 =√ 2 2    √  n−1
     5 1 1 −1−5 2
    1+√5 n  1−√5 n  12−2
    = √5  1+√5 n−1  1−√5 n−1 2−2
      We want the first component.
1   1 + √5 n  1 − √5 n  F(n) = √5 2 − 2
      This formula gives the value of any member of the sequence without having to first find the intermediate values.
Because (1 − √5)/2 ≈ −0.618 has absolute value less than one, its powers go to zero and so the F(n) formula is dominated by its first term. Although we
 
458 Chapter Five. Similarity
 have extended the elementary model of population growth by adding a delay period before the onset of fertility, we nonetheless still get a function that is asymptotically exponential.
In general, a homogeneous linear recurrence relation of order k has this form.
f(n) = an−1f(n − 1) + an−2f(n − 2) + · · · + an−kf(n − k)
This recurrence relation is homogeneous because it has no constant term, i.e, we can rewrite it as 0 = −f(n)+an−1f(n−1)+an−2f(n−2)+···+an−kf(n−k). It is of order k because it uses k-many prior terms to calculate f(n). The relation, cimbined with initial conditions giving values for f(0), . . . , f(k − 1), completely determines a sequence, simply because we can compute f(n) by first computing f(k), f(k + 1), etc. As with the Fibonacci case we will find a formula that solves the recurrence, that directly gives f(n)
Let V be the set of functions with domain N = {0, 1, 2, . . .} and codomain C. (Where convenient we sometimes use the domain Z+ = { 1, 2, . . . }.) This is a vector space under the usual meaning for addition and scalar multiplication, that f + g is the map x  → f(x) + g(x) and cf is the map x  → c · f(x).
If we put aside any initial conditions and look only at the recurrence, then there may be many functions satisfying the relation. For example, the Fibonacci recurrence that each value beyond the initial ones is the sum of the prior two is satisfied by the function L whose first few values are L(0) = 2, L(1) = 1, L(2) = 3, L(3) = 4, and L(4) = 7.
Fix a homogeneous linear recurrence relation of order k and consider the subset S of functions satisfying the relation (without initial conditions). This S is a subspace of V. It is nonempty because the zero function is a solution, by homogeneity. It is closed under addition because if f1 and f2 are solutions then this holds.
−(f1 +f2)(n)+an−1(f1 +f2)(n−1)+···+an−k(f1 +f2)(n−k) = (−f1(n) + · · · + an−kf1(n − k))
+(−f2(n)+···+an−kf2(n−k)) =0+0=0
It is also closed under scalar multiplication.
− (rf1)(n) + an−1(rf1)(n − 1) + · · · + an−k(rf1)(n − k)
= r · (−f1(n) + · · · + an−kf1(n − k))
=r·0 =0
Topic: Linear Recurrences 459 We can find the dimension of S. Where k is the order of the recurrence, consider
this map from the set of functions S to the set of k-tall vectors.  f(0) 
 f(1)  f  →  .   . 
f(k−1)
Exercise 4 shows that this is linear. Any solution of the recurrence is uniquely determined by the k-many initial conditions so this map is one-to-one and onto. Thus it is an isomorphism, and S has dimension k.
So we can describe the set of solutions of our linear homogeneous recurrence relation of order k by finding a basis consisting of k-many linearly independent functions. To produce those we give the recurrence a matrix formulation.
an−1 an−2 an−3 . . . an−k+1 an−k
 f(n)   1 0 0 ... 0 0 f(n−1)

 f(n−1)   0 1 0 f(n−2)
 . = .
 001 
... .. .. f(n−k+1)  . . . .  f(n−k)
000...10
Call the matrix A. We want its characteristic function, the determinant of
A − λI. The pattern in the 2×2 case  a −λ a  
and the 3×3 case
a −λ a a 
n−1 n−2 1 −λ
= λ2 − an−1λ − an−2
n−1 n−2 n−3
 1 −λ 0 =−λ3 +an−1λ2 +an−2λ+an−3
0 1 −λ
leads us to expect, and Exercise 5 verifies, that this is the characteristic equation.
 an−1 − λ an−2 an−3 . . . an−k+1 an−k    1 −λ 0 ... 0 0 
  
  0 1 −λ  
0=
  0 0 0 ... 1 −λ 
= ±(−λk + an−1λk−1 + an−2λk−2 + · · · + an−k+1λ + an−k)
 001  
 . . .. .   .... 
460 Chapter Five. Similarity
 The ± is not relevant to find the roots so we drop it. We say that the polynomial −λk + an−1λk−1 + an−2λk−2 + · · · + an−k+1λ + an−k is associated with the recurrence relation.
If the characteristic equation has no repeated roots then the matrix is diagonalizable and we can, in theory, get a formula for f(n), as in the Fibonacci case. But because we know that the subspace of solutions has dimension k we do not need to do the diagonalization calculation, provided that we can exhibit k different linearly independent functions satisfying the relation.
Where r1, r2, . . . , rk are the distinct roots, consider the functions of powers of those roots, f (n) = rn through f (n) = rn. Exercise 6 shows that each is
r1 1 rk k
a solution of the recurrence and that they form a linearly independent set. So, if
the roots of the associated polynomial are distinct, any solution of the relation hastheformf(n)=c1rn1 +c2rn2 +···+ckrnk forsomescalarsc1,...,cn. (The case of repeated roots is similar but we won’t cover it here; see any text on Discrete Mathematics.)
Now we bring in the initial conditions. Use them to solve for c1 , . . . , cn . For instance, the polynomial associated with the Fibonacci relation is −λ2 + λ + 1, whose roots are r1 = (1 + √5)/2 and r2 = (1 − √5)/2 and so any solution of the Fibonacci recurrence has the form f(n) = c1((1 + √5)/2)n + c2((1 − √5)/2)n. Use the Fibonacci initial conditions for n = 0 and n = 1
√ c1+ √ c2=0 (1+ 5/2)c1 +(1− 5/2)c2 =1
and solve to get c1 = 1/√5 and c2 = −1/√5, as we found above.
We close by considering the nonhomogeneous case, where the relation has the form f(n + 1) = anf(n) + an−1f(n − 1) + · · · + an−kf(n − k) + b for some nonzero b. We only need a small adjustment to make the transition from the
homogeneous case.
This classic example illustrates: in 1883, Edouard Lucas posed the Tower of
Hanoi problem.
In the great temple at Benares, beneath the dome which marks the center of the world, rests a brass plate in which are fixed three diamond needles, each a cubit high and as thick as the body of a bee. On one of these needles, at the creation, God placed sixty four disks of pure gold, the largest disk resting on the brass plate, and the others getting smaller and smaller up to the top one. This is the Tower of Brahma. Day and night unceasingly the priests transfer the disks from one diamond needle to another according to the fixed and immutable laws of Bram-ah, which require that the priest on duty must not move more than one disk at a time and that he must
        
Topic: Linear Recurrences 461
 place this disk on a needle so that there is no smaller disk below it. When the sixty-four disks shall have been thus transferred from the needle on which at the creation God placed them to one of the other needles, tower, temple, and Brahmins alike will crumble into dusk, and with a thunderclap the world will vanish. (Translation of [De Parville] from [Ball & Coxeter].)
We put aside the question of why the priests don’t sit down for a while and have the world last a little longer, and instead ask how many disk moves it will take. Before tackling the sixty four disk problem we will consider the problem for three disks.
To begin, all three disks are on the same needle.
After the three moves of taking the small disk to the far needle, the mid-sized disk to the middle needle, and then the small disk to the middle needle, we have this.
Now we can move the big disk to the far needle. Then to finish we repeat the three-move process on the two smaller disks, this time so that they end up on the third needle, on top of the big disk.
That sequence of moves is the best that we can do. To move the bottom disk at a minimum we must first move the smaller disks to the middle needle, then move the big one, and then move all the smaller ones from the middle needle to the ending needle. Since this minimum suffices, we get this recurrence.
T (n) = T (n − 1) + 1 + T (n − 1) = 2T (n − 1) + 1 where T (1) = 1 Here are the first few values of T.
disksn 1 2 3 4 5 6 7 8 9 10 movesT(n) 1 3 7 15 31 63 127 255 511 1023
Of course, these numbers are one less than a power of two. To derive this write the original relation as −1 = −T(n)+2T(n−1). Consider 0 = −T(n)+2T(n−1),
         
462 Chapter Five. Similarity
 a linear homogeneous recurrence of order 1. Its associated polynomial is −λ + 2, with the single root r1 = 2. Thus functions satisfying the homogeneous relation take the form c12n.
That’s the homogeneous solution. Now we need a particular solution. Because the nonhomogeneous relation −1 = −T(n) + 2T(n − 1) is so simple, we can by eye spot a particular solution T(n) = −1. Any solution of the recurrence T (n) = 2T (n − 1) + 1 (without initial conditions) is the sum of the homogeneous solution and the particular solution: c12n −1. Now the initial condition T(1) = 1 gives that c1 = 1 and we’ve gotten the formula that generates the table: the n-disk Tower of Hanoi problem requires T (n) = 2n − 1 moves.
Finding a particular solution in more complicated cases is, perhaps not surprisingly, more complicated. A delightful and rewarding, but challenging, source is [Graham, Knuth, Patashnik]. For more on the Tower of Hanoi see [Ball & Coxeter], [Gardner 1957], and [Hofstadter]. Some computer code follows the exercises.
Exercises
1 How many months until the number of Fibonacci rabbit pairs passes a thousand? Ten thousand? A million?
2 Solve each homogeneous linear recurrence relations. (a) f(n)=5f(n−1)−6f(n−2)
(b) f(n) = 4f(n − 2)
(c) f(n)=5f(n−1)−2f(n−2)−8f(n−3)
3 Give a formula for the relations of the prior exercise, with these initial condi- tions.
(a) f(0)=1, f(1)=1
(b) f(0)=0, f(1)=1
(c) f(0)=1, f(1)=1, f(2)=3.
4 Check that the isomorphism given between S and Rk is a linear map.
5 Show that the characteristic equation of the matrix is as stated, that is, is the polynomial associated with the relation. (Hint: expanding down the final column
and using induction will work.)
6 Given a homogeneous linear recurrence relation f(n) = anf(n−1)+···+an−kf(n−
k), let r1 , . . . , rk be the roots of the associated polynomial. Prove that each function
fri (n) = rnk satisfies the recurrence (without initial conditions).
7 (This refers to the value T (64) = 18, 446, 744, 073, 709, 551, 615 given in the com- puter code below.) Transferring one disk per second, how many years would it
take the priests at the Tower of Hanoi to finish the job?
Computer Code
This code generates the first few values of a function defined by a recur- rence and initial conditions. It is in the Scheme dialect of LISP, specifically, [Chicken Scheme].
Topic: Linear Recurrences 463
 After loading an extension that keeps the computer from switching to floating point numbers when the integers get large, the Tower of Hanoi function is straightforward.
(require-extension numbers)
(define (tower-of-hanoi-moves n) (if (= n 1)
1
(+ (* (tower-of-hanoi-moves (- n 1))
2) 1) ) )
; Two helper funcitons
(define (first-few-outputs proc n)
(first-few-outputs-aux proc n '()) )
(define (first-few-outputs-aux proc n lst) (if (< n 1)
lst
(first-few-outputs-aux proc (- n 1) (cons (proc n) lst)) ) )
(For readers unused to recursive code: to compute T(64), the computer wants to compute 2 ∗ T (63) − 1, which requires computing T (63). The computer puts the ‘times 2’ and the ‘plus 1’ aside for a moment. It computes T(63) by using this same piece of code (that’s what ‘recursive’ means), and to do that it wants to compute 2 ∗ T (62) − 1. This keeps up until, after 63 steps, the computer tries to compute T(1). It then returns T(1) = 1, which allows the computation of T(2) to proceed, etc., until the original computation of T(64) finishes.)
The helper functions give a table of the first few values. Here is the session at the prompt.
#;1> (load "hanoi.scm")
; loading hanoi.scm ...
; loading /var/lib//chicken/6/numbers.import.so ...
; loading /var/lib//chicken/6/chicken.import.so ...
; loading /var/lib//chicken/6/foreign.import.so ...
; loading /var/lib//chicken/6/numbers.so ...
#;2> (tower-of-hanoi-moves 64)
18446744073709551615
#;3> (first-few-outputs tower-of-hanoi-moves 64)
(1 3 7 15 31 63 127 255 511 1023 2047 4095 8191 16383 32767 65535 131071 262143 524287 1048575 2097151 4194303 8388607 16777215 33554431 67108863 134217727 268435455 536870911 1073741823 2147483647 4294967295 8589934591 17179869183 34359738367 68719476735 137438953471 274877906943 549755813887 1099511627775 2199023255551 4398046511103 8796093022207 17592186044415 35184372088831 70368744177663 140737488355327 281474976710655 562949953421311 1125899906842623 2251799813685247 4503599627370495 9007199254740991 18014398509481983 36028797018963967 72057594037927935 144115188075855871 288230376151711743 576460752303423487 1152921504606846975 2305843009213693951 4611686018427387903 9223372036854775807 18446744073709551615)
This is a list of T(1) through T(64) (the session was edited to put in line breaks for readability).
                                  
Topic
Coupled Oscillators
This is a Wilberforce pendulum. Hanging on the spring is a mass, or bob. Push it up a bit and release, and it will oscillate up and down.
But then, if the device is properly adjusted, something fascinating happens. After a few seconds, in addition to going up and down, the mass begins to rotate, to spin about the axis of the spring. This yaw increases until the mass’s motion becomes almost entirely rotary, with very little up and down. Perhaps five seconds later the motion evolves back to a combination. After some more time, order reappears. Amazingly, now the motion is almost entirely vertical. This continues, with the device trading off periods of pure vertical motion with periods of pure rotational motion, interspersed with mixtures. (Search online for “wilberforce pendulum video” to get some excellent demonstrations.)
Each pure motion state is a normal mode of oscillation. We will analyze this device’s behavior when it is in a normal mode. It is all about eigenvalues.
x(t)
θ(t)
Write x(t) for the vertical motion over time and θ(t) for the rotational motion. Fix the coordinate system so that in rest position x = 0 and θ = 0, so that positive x’s are up, and so that positive θ’s are counterclockwise when viewed from above.
    
Topic: Coupled Oscillators 465
 We start by modeling the motion of a mass on a spring constrained to have no twist. This is simpler because there is only one motion, one degree of freedom. Put the mass in rest position and push it up to compress the spring. Hooke’s Law is that for small distances the restoring force is proportional to the distance, F = −k · x. The constant k is the stiffness of the spring.
Newton’s Law is that a force is proportional to the associated acceleration F = m · d2 x(t)/dt. The constant of proportionality, m, is the mass of the object. Combining Hooke’s Law with Newton’s gives the differential equation expressing the mass’s motion m · d2 x(t)/dt = −k · x(t). We prefer the from with the variables all on one side.
m · d2 x(t) + k · x(t) = 0 (∗) dt
Our physical intuition is that over time the bob oscillates. It started high so the graph should look like this.
position x
time t
Of course, this looks like a cosine graph and we recognize that the differential equation of motion (∗) is satisfied by x(t) = cos ωt, where ω =  m/k, since dx/dt = ω · sin ωt and d2x/dt2 = −ω2 · cos ωt. Here, ω is the angular frequency. It governs the period of the oscillation since if ω = 1 then the period is 2π, while if ω = 2 then the period is π, etc.
We can give a more general solution of (∗). For a general amplitude we put afactorAinfrontx(t)=A·coswt. Andwecanallowaphaseshift,soweare not required to start the clock when the mass is high, with x(t) = A cos(wt + φ). This is the equation of simple harmonic motion.
Now back to consideration of the coupled pair of motions, vertical and rotational. These two interact because a spring that is twisted will lengthen or shorten just a bit; for instance, it could work as here or could be reversed, depending on the spring.
      spring lengthens
spring shortens
And, a spring that is stretched or compressed from its rest position will twist slightly. The interaction of the two produces coupled oscillations.
466 Chapter Five. Similarity
 To see how the interaction can produce the dramatic behavior that we see in a normal mode imagine that the mass is rotating in the direction that will will make the spring longer. If at the same moment the vertical motion is that the spring is getting shorter, then superimposing the two could result in their almost canceling. The bob ends up not moving vertically much at all, just twisting. With a properly adjusted device this could last for a number of seconds.
“Properly adjusted” means that the period of the pure vertical motion is the same as, or close to, the period of the pure rotational motion. With that, the cancellation will go on for some time.
The interaction between the motions can also produce the other normal mode behavior, where the bob moves mostly vertically without much rotation, if the spring’s motion x(t) produces a twist that opposes the bob’s twist θ(t). The bob will stop rotating, almost, so that its motion is almost entirely vertical.
To get the equations of motion in this two degrees of freedom case, we make the same assumption as in the one degree case, that for small displacements the restoring force is proportional to the displacement. But now we take that assumption both for the vertical motion and for the rotation. Let the constant of proportionality in the rotational motion be κ. Similarly we also use Newton’s Law that force is proportional to acceleration for the rotational motion as well, and take the constant of proportionality to be I.
Most crucially, we add a coupling between the two motions, which we take to be proportional to each, with constant of proportionality ε/2.
That gives a system of two differential equations, the first for vertical motion and the second for rotation. These equations describe the behavior of the coupled system at any time t.
m · d2 x(t) + k · x(t) + ε · θ(t) = 0 dt2 2
I · d2 θ(t) + κ · θ(t) + ε · x(t) = 0 dt2 2
(∗∗)
    We will use them to analyze the system’s behavior at times when it is in a normal mode.
First consider the uncoupled motions, as given by the equations without the ε terms. Without those terms these describe simple harmonic functions, and we write ω2x for k/m, and ω2θ for κ/I. We have argued above that to observe the stand-still behavior we should adjust the device so that the periods are the same ω2x = ω2θ. Write ω0 for that number.
Now consider the coupled motions x(t) and θ(t). By the same principle, to observe the stand-still behavior we want them in in sync, for instance so that the rotation is at its peak when the stretch is at its peak. That is, in a normal mode the oscillations have the same angular frequency ω. As to phase shift, as
Topic: Coupled Oscillators 467
 we also discussed there are two cases: when the twist imparted by the spring’s motion is in the same direction as the twist given by the rotational oscillation and when they are opposed. In either case to get a normal mode the peaks must coincide.
x(t) = A1 cos(ωt + φ) x(t) = A1 cos(ωt + φ)
θ(t) = A2 cos(ωt + φ) θ(t) = A2 cos(ωt + (φ + π))
We will work through the left-hand case, leaving the other as an exercise. We want to find which ω’s are possible. Take the second derivatives
d2 x(t) = −A1ω2 cos(ωt + φ) d2 θ(t) = −A2ω2 cos(ωt + φ)
  dt
dt
and plug into the equations of motion (∗∗).
m · (−A1ω2 cos(ωt + φ)) + k · (A1 cos(ωt + φ)) + ε · (A2 cos(ωt + φ)) = 0
2
I · (−A2ω2 cos(ωt + φ)) + κ · (A2 cos(ωt + φ)) + ε · (A1 cos(ωt + φ)) = 0 2
Factor out cos(ωt + φ) and divide through by m.  k−ω2 ·A1+ ε ·A2=0
 κ−ω2 ·A2+ ε ·A1=0 I 2m
We are assuming that k/m = ω2x and replace κ/I = ω2θ are equal, and writing ω20 for that number. Make the substitution and restate it as a matrix equation.
 ω2−ω2 ε/2m   A   0  01=
ε/2I ω20 − ω2 A2 0
Obviously this system has the trivial solution A1 = 0, A2 = 0, for the case where the mass is at rest. We want to know for which frequencies ω this system has a nontrivial solution.
 ω2 ε/2m  A    A   0 1=ω21
ε/2Iω20A2 A2
The normal mode angular frequencies ω are the eigenvalues of the matrix.
To calculate it take the determinant and set it to zero.
 ω20−ω2 ε/2m  4 2 2 4 ε2
  2 2 =0 =⇒ ω −(2ω0)ω +(ω0− )=0   ε/2I ω0−ω  4mI
    m 2m
   
468 Chapter Five. Similarity That equation is quadratic in ω2. Apply the formula to solve quadratic equations,
 (−b ± √b2 − 4ac)/(2a).
2 2ω20 ±  4ω40 − 4(ω40 − ε2/4mI) 2 ε
  ω= 2 =ω0±2√mI
The value ε/√mI = ε/√κk is often written ωB so that ω2 = ω20 ± ωB/2. This is the beat frequency, the difference between the two normal mode frequencies. Although the argument is beyond our scope, the general formula for the
motion of the pendulum is a linear combination of the motions during the normal modes. Thus, the pendulum’s motion is entirely determined by the eigenvalues of the above matrix. See [Berg & Marshall].
Exercises
1 Use the formula for the cosine of a sum to give an even more general formula for simple harmonic motion.
2 Find the eigenvectors associated with the eigenvalues.
3 Find the values of ω in the case where x(t) = A1 cos(ωt+φ) and θ(t) = A2 cos(ωt+
(φ + π)).
4 Build a Wilberforce pendulum out of a Slinky Jr and a soup can. You can drill
holes in the can for bolts, either two or four of them, that you can use to adjust the moment of inertia of the can so the periods of vertical and rotational motion coincide.
     
Appendix
Mathematics is made of arguments (reasoned discourse that is, not crockery- throwing). This section sketches the background material and argument tech- niques that we use in the book.
This section informally outlines the topics, skipping proofs. For more, [Velleman2] is excellent. Two other sources, available online, are [Hefferon] and [Beck].
Statements
Formal mathematical statements come labelled as a Theorem for major points, a Corollary for results that follow immediately from a prior one, or a Lemma for results chiefly used to prove others.
Statements can be complex and have many parts. The truth or falsity of the entire statement depends both on the truth value of the parts and on how the statement is put together.
Not Where P is a proposition, ‘it is not the case that P’ is true provided that P is false. For instance, ‘n is not prime’ is true only when n is the product of smaller integers.
To prove that a ‘not P’ statement holds, show that P is false.
And For a statement of the form ‘P and Q’ to be true both halves must hold: ‘7 is prime and so is 3’ is true, while ‘7 is prime and 3 is not’ is false.
To prove a ‘P and Q’, prove each half.
Or A ‘P or Q’ statement is true when either half holds: ‘7 is prime or 4 is prime’ is true, while ‘8 is prime or 4 is prime’ is false. In the case that both clauses of the statement are true, as in ‘7 is prime or 3 is prime’, we take the statement as a whole to be true. (In everyday speech people occasionally use ‘or’ in an exclusive way — “Live free or die” does not intend both halves to hold — but we will not use ‘or’ in that way.)
 
A-2
 To prove ‘P or Q’, show that in all cases at least one half holds (perhaps sometimes one half and sometimes the other, but always at least one).
If-then An ‘if P then Q’ statement may also appear as ‘P implies Q’ or ‘P =⇒ Q’ or‘PissufficienttogiveQ’or‘QifP’. ItistrueunlessPistruewhileQis false. Thus ‘if 7 is prime then 4 is not’ is true while ‘if 7 is prime then 4 is also prime’ is false. (Contrary to its use in casual speech, in mathematics ‘if P then Q’ does not connote that P precedes Q or causes Q.)
Note this consequence of the prior paragraph: if P is false then ‘if P then Q’ is true irrespective of the value of Q: ‘if 4 is prime then 7 is prime’ and ‘if 4 is prime then 7 is not’ are both true statements. (They are vacuously true.) Also observe that ‘if P then Q’ is true when Q is true: ‘if 4 is prime then 7 is prime’ and ‘if 4 is not prime then 7 is prime’ are both true.
There are two main ways to establish an implication. The first way is direct: assume that P is true and use that assumption to prove Q. For instance, to show ‘if a number is divisible by 5 then twice that number is divisible by 10’ we can assume that the number is 5n and deduce that 2(5n) = 10n. The indirect way is to prove the contrapositive statement: ‘if Q is false then P is false’ (rephrased, ‘Q can only be false when P is also false’). Thus to show ‘if a natural number is prime then it is not a perfect square’ we can argue that if it wereasquarep=n2 thenitcouldbefactoredp=n·nwheren<pandso wouldn’t be prime (p = 0 or p = 1 don’t satisfy n < p but they are nonprime).
Equivalent statements Sometimes, not only does P imply Q but also Q implies P. Some ways to say this are: ‘P if and only if Q’, ‘P iff Q’, ‘P and Q are logically equivalent’, ‘P is necessary and sufficient to give Q’, ‘P ⇐⇒ Q’. An example is ‘an integer is divisible by ten if and only if that number ends in 0’.
Although in simple arguments a chain like “P if and only if R, which holds if and only if S . . . ” may be practical, to prove that statements are equivalent we more often prove the two halves ‘if P then Q’ and ‘if Q then P’ separately.
Quantifiers
Compare these statements about natural numbers: ‘there is a natural number x such that x is divisible by x2’ is true, while ‘for all natural numbers x, that x is divisible by x2’ is false. The prefixes ‘there is’ and ‘for all’ are quantifiers.
For all The ‘for all’ prefix is the universal quantifier, symbolized ∀.
The most straightforward way to prove that a statement holds in all cases is to prove that it holds in each case. Thus to show that ‘every number divisible by p has its square divisible by p2’, take a single number of the form pn and square it (pn)2 = p2n2. This is a typical element proof. (In this kind of argument be careful not to assume properties for that element other than the ones in the
hypothesis. This argument is wrong: “If n is divisible by a prime, say 2, so that n = 2k for some natural number k, then n2 = (2k)2 = 4k2 and the square of n is divisible by the square of the prime.” That is a proof for the special case p = 2 but it isn’t a proof for all p. Contrast it with a correct one: “If n is divisible by a prime so that n = pk for some natural number k then n2 = (pk)2 = p2k2 and so the square of n is divisible by the square of the prime.”)
There exists The ‘there exists’ prefix is the existential quantifier, symbolized ∃. We can prove an existence proposition by producing something satisfying the property: for instance, to settle the question of primality of 225 + 1, Euler exhibited the divisor 641[Sandifer]. But there are proofs showing that something exists without saying how to find it; Euclid’s argument given in the next subsection shows there are infinitely many primes without giving a formula
naming them.
Finally, after “Are there any?” we often ask “How many?” That is, the
question of uniqueness often arises in conjunction with the question of existence. Sometimes the two arguments are simpler if separated so note that just as proving something exists does not show that it is unique, neither does proving that something is unique show that it exists.
Techniques of Proof
We have many ways to prove mathematical statements. Here we will outline two techniques that we use often, and that might not come naturally, even to a person with a technical turn of mind.
Induction Many proofs are iterative, “Here’s why the statement is true for the number 0, it then follows for 1 and from there to 2 . . . ”. These are proofs by mathematical induction. We will see two examples.
Wewillfirstprovethat1+2+3+···+n=n(n+1)/2. Thatformulahas a natural number variable n that is free, meaning that setting n to be 1, or 2, etc., gives a family of cases of the statement: first that 1 = 1(2)/2, second that 1 + 2 = 2(3)/2, etc. Our induction proofs involve statements with one free natural number variable.
Each such proof has two steps. In the base step we show that the statement holds for some intial number i ∈ N. Often this step is a routine verification. The second step, the inductive step, is more subtle; we will show that this implication holds:
If the statement holds from n = i up to and including n = k then the statement holds also in the n = k + 1 case
(the first line is the inductive hypothesis). Completing both steps proves that the statement is true for all natural numbers greater than or equal to i.
A-3
 (∗)
A-4
 For the sum of the initial n numbers statement the intuition behind the principle is that first, the base step directly verifies the statement for the case of the initial number n = 1. Then, because the inductive step verifies the implication (∗) for all k, that implication applied to k = 1 gives that the statement is true for the case of the number n = 2. Now, with the statement established for both 1 and 2, apply (∗) again to conclude that the statement is true for the number n = 3. In this way, we bootstrap to all numbers n   1.
Here is a proof of 1+2+3+···+n = n(n+1)/2, with separate paragraphs for the base step and the inductive step.
For the base step we show that the formula holds when n = 1. That’s easy; the sum of the first 1 natural number equals 1(1 + 1)/2.
For the inductive step, assume the inductive hypothesis that the formula holdsforthenumbersn=1,n=2,...,n=kwithk 1. Thatis, assume 1 = 1(1)/2, and 1+2 = 2(3)/2, and 1+2+3 = 3(4)/2, through 1+2+···+k = k(k+1)/2. With that, the formula holds also in the n = k + 1 case:
1+2+···+k+(k+1)= k(k+1) +(k+1)= (k+1)(k+2) 22
(the first equality follows from the inductive hypothesis).
Here is another example, proving that every integer greater than or equal to 2 is a product of primes.
The base step is easy: 2 is the product of a single prime.
For the inductive step assume that each of 2, 3, . . . , k is a product of primes, aiming to show k+1 is also a product of primes. There are two possibilities. First, if k + 1 is not divisible by a number smaller than itself then it is a prime and so is the product of primes. The second possibility is that k + 1 is divisible by a number smaller than itself, and then by the inductive hypothesis its factors can be written as a product of primes. In either case k + 1 can be rewritten as a product of primes.
Contradiction Another technique of proof is to show that something is true by showing that it cannot be false. A proof by contradiction assumes that the proposition is false and derives some contradiction to known facts.
The classic example of proof by contradiction is Euclid’s argument that there are infinitely many primes.
Suppose that there are only finitely many primes p1,...,pk. Consider the number p1 · p2 . . . pk + 1. None of the primes on the supposedly exhaustive list divides this number evenly since each leaves a remainder of 1. But every number is a product of primes so this can’t be. Therefore there cannot be only finitely many primes.
  
Another example is this proof that √2 is not a rational number. √
Suppose that 2 = m/n, so that 2n2 = m2. Factor out any 2’s, giving n = 2kn · nˆ and m = 2km · mˆ . Rewrite.
2·(2kn ·nˆ)2 =(2km ·mˆ)2
The Prime Factorization Theorem says that there must be the same number of factors of 2 on both sides, but there are an odd number of them 1 + 2kn on the left and an even number 2km on the right. That’s a contradiction, so a rational number with a square of 2 is impossible.
Sets, Functions, and Relations
The material here forms the backdrop, the vocabulary, for all of the development that we do.
Sets Mathematicians often work with collections. The most commonly-used kind of collection is a set. Sets are characterized by the Principle of Extensionality: two sets with the same elements are equal. Because of this, the order of the elements does not matter {2, π} = {π, 2}, and repeats collapse {7, 7} = {7}.
We can describe a set using a listing between curly braces {1,4,9,16} (as in the prior paragraph), or by using set-builder notation { x | x5 − 3x3 + 2 = 0 } (read “the set of all x such that . . . ”). We name sets with capital roman letters; for instance the set of primes is P = {2,3,5,7,11,... } (except that a few sets are so important that their names are reserved, such as the real numbers R and the complex numbers C). To denote that something is an element, or member,) of a set we use ‘∈’, so that 7 ∈ {3,5,7} while 8 ̸∈ {3,5,7}.
WesaythatAisasubset ofB,writtenA⊆B,whenx∈Aimpliesthat x ∈ B. In this book we use ‘⊂’ for the proper subset relationship that A is a subset of B but A ̸= B (some authors use this symbol for any kind of subset, proper or not). An example is { 2, π } ⊂ { 2, π, 7 }. These symbols may be flipped, for instance {2, π, 5} ⊃ {2, 5}.
Because of Extensionality, to prove that two sets are equal A = B show that they have the same members. Often we do this by showing mutual inclusion, thatbothA⊆BandA⊇B. Suchaproofwillhaveapartshowingthatif x ∈ A then x ∈ B, and a second part showing that if x ∈ B then x ∈ A.
When a set has no members then it is the empty set { }, symbolized ∅. Any set has the empty set for a subset by the ‘vacuously true’ property of the definition of implication.
Diagrams We picture basic set operations with a Venn diagram. This shows x ∈ P.
A-5
   
A-6
  P
x
The outer rectangle contains the universe Ω of all objects under discussion. For instance, in a statement about real numbers, the rectangle encloses all members of R. The set is pictured as a circle, enclosing its members.
HereisthediagramforP⊆Q. Itshowsthatifx∈Pthenx∈Q.
SetOperations Theunion oftwosetsisP∪Q={x|(x∈P)or(x∈Q)}. The diagram shows that an element is in the union if it is in either of the sets.
Theintersection isP∩Q={x|(x∈P)and(x∈Q)}.
Thecomplement ofasetPisPcomp ={x∈Ω|x̸∈P}
Multisets As described above, a set is a collection in which order does not matter, so that the sets {2,π} and {π,2} are equal, and in which repeats collapse, so that the sets {7,7} and {7} are equal.
 PQ
 PQ
 PQ
 P x
A collection that is like a set in that order does not matter, but in which repeats do not collapse is a multiset. Thus the multiset {1,2,2} differs from the multiset { 1, 2 }. Because order does not matter, these multisets are all equal: {1, 2, 2}, {2, 1, 2}, and {2, 2, 1}. (Note that we use the same curly brackets notation {...} as for sets.) We only mention multisets in a remark so more, such as how subsets work, or unions and intersections, is beyond our scope.
Sequences In addition to sets and multisets, we also use collections where order matters and where repeats do not collapse. These are sequences, denoted with angle brackets: ⟨2, 3, 7⟩ ̸= ⟨2, 7, 3⟩. A sequence of length 2 is an ordered pair, and is often written with parentheses: (π, 3). We also sometimes say ‘ordered triple’, ‘ordered 4-tuple’, etc. The set of ordered n-tuples of elements of a set A is denoted An. Thus R2 is the set of pairs of reals.
Functions A function or map f: D → C is is an association between input arguments x ∈ D and output values f(x) ∈ C subject to the the requirement that the function must be well-defined, that x suffices to determine f(x). Restated, the condition is that if x1 = x2 then f(x1) = f(x2).
The set of all arguments D is f’s domain and the set of output values is its range R(f). Often we don’t work with the range and instead work with a convenient superset, the codomain C. For instance, we might describe the squaring function with s: R → R instead of s: R → R+ ∪ {0}.
We picture functions with a bean diagram.
The blob on the left is the domain while on the right is the codomain. The function associates the three points of the domain with three in the codomain. Note that by the definition of a function every point in the domain is associated with a unique point in the codomain, but the converse needn’t be true.
The association is arbitrary; no formula or algorithm is required, although in this book there typically is one. We often use y to denote f(x). We also use the
f
notation x  −→ 16x2 − 100, read ‘x maps under f to 16x2 − 100’ or ‘16x2 − 100
is the image of x’.
A map such as x  → sin(1/x) is a combinations of simpler maps, here g(y) =
sin(y) applied to the image of f(x) = 1/x. The composition of g: Y → Z with f:X→Y,isthemapsendingx∈Xtog(f(x))∈Z. Itisdenotedg◦f:X→Z. This definition only makes sense if the range of f is a subset of the domain of g.
A-7
              
A-8
 An identity map id: Y → Y defined by id(y) = y has the property that for any f: X → Y, the composition id◦f is equal to f. So an identity map plays the same role with respect to function composition that the number 0 plays in real number addition or that 1 plays in multiplication.
In line with that analogy, we define a left inverse of a map f: X → Y to be a function g : range(f) → X such that g ◦ f is the identity map on X. A right inverse offisah:Y→Xsuchthatf◦histheidentity.
For some f’s there is a map that is both a left and right inverse of f. If such a map exists then it is unique because if both g1 and g2 have this property then g1(x) = g1 ◦ (f ◦ g2)(x) = (g1 ◦ f) ◦ g2 (x) = g2(x) (the middle equality comes from the associativity of function composition) so we call it a two-sided inverse or just “the” inverse, and denote it f−1. For instance, the inverse of the function f: R → R given by f(x) = 2x − 3 is the function f−1 : R → R given by f−1(x) = (x + 3)/2.
The superscript notation for function inverse ‘f−1’ fits into a larger scheme. Functions with the same codomain as domain f: X → X can be iterated, so that we can consider the composition of f with itself: f ◦ f, and f ◦ f ◦ f, etc. We write f◦f as f2 and f◦f◦f as f3, etc. Note that the familiar exponent rules for real numbers hold: fi ◦ fj = fi+j and (fi)j = fi·j. Then where f is invertible, writing f−1 for the inverse and f−2 for the inverse of f2, etc., gives that these familiar exponent rules continue to hold, since we define f0 to be the identity map.
The definition of function requires that for every input there is one and only one associated output value. If a function f: D → C has the additional property that for every output value there is at least one associated input value — that is, the additional property that f’s codomain equals its range C = R(f) — then the function is onto.
A function has a right inverse if and only if it is onto. (The f pictured above has a right inverse g: C → D given by following the arrows backwards, from right to left. For the codomain point on the top, choose either one of the arrows to follow. With that, applying g first followed by f takes elements y ∈ C to themselves, and so is the identity function.)
If a function f: D → C has the property that for every output value there is at most one associated input value — that is, if no two arguments share an image so that f(x1) = f(x2) implies that x1 = x2 — then the function is one-to-one. The bean diagram from earlier illustrates.
              
A-9
              A function has a left inverse if and only if it is one-to-one. (In the picture define g: C → D to follow the arrows backwards for those y ∈ C that are at the end of an arrow, and to send the point to an arbitrary element in D otherwise. Then applying f followed by g to elements of D will act as the identity.)
By the prior paragraphs, a map has a two-sided inverse if and only if that map is both onto and one-to-one. Such a function is a correspondence. It associates one and only one element of the domain with each element of the codomain. Because a composition of one-to-one maps is one-to-one, and a composition of onto maps is onto, a composition of correspondences is a correspondence.
We sometimes want to shrink the domain of a function. For instance, we may take the function f: R → R given by f(x) = x2 and, in order to have an inverse, limit input arguments to nonnegative reals fˆ: R+ ∪ { 0 } → R. Then fˆ is the restriction of f to the smaller domain.
Relations Some familiar mathematical things, such as ‘<’ or ‘=’, are most naturally understood as relations between things. A binary relation on a set A is a set of ordered pairs of elements of A. For example, some elements of the set that is the relation ‘<’ on the integers are (3,5), (3,7), and (1,100). Another binary relation on the integers is equality; this relation is the set { . . . , (−1, 1), (0, 0), (1, 1), . . . }. Still another example is ‘closer than 10’, the set {(x,y)||x−y|<10}. Somemembersofthisrelationare(1,10),(10,1),and (42, 44). Neither (11, 1) nor (1, 11) is a member.
Those examples illustrate the generality of the definition. All kinds of relationships (e.g., ‘both numbers even’ or ‘first number is the second with the digits reversed’) are covered.
Equivalence Relations We shall need to express that two objects are alike in some way. They aren’t identical, but they are related (e.g., two integers that give the same remainder when divided by 2).
A binary relation {(a,b),...} is an equivalence relation when it satisfies (1) reflexivity: any object is related to itself, (2) symmetry: if a is related to b then b is related to a, and (3) transitivity: if a is related to b and b is related to c then a is related to c. Some examples (on the integers): ‘=’ is an equivalence relation, ‘<’ does not satisfy symmetry, ‘same sign’ is a equivalence, while ‘nearer than 10’ fails transitivity.
Partitions In the ‘same sign’ relation { (1, 3), (−5, −7), (0, 0), . . . } there are three
A-10
 kinds of pairs, pairs with both numbers positive, pairs with both negative, and the one pair with both zero. So integers fall into exactly one of three classes, positive, or negative, or zero.
A partition of a set Ω is a collection of subsets {S0,S1,S2,...} such that every element of S is an element of a subset S1 ∪ S2 ∪ · · · = Ω and overlapping parts are equal: if Si ∩ Sj ̸= ∅ then Si = Sj. Picture that Ω is decomposed into non-overlapping parts.
Thus the prior paragraph says that ‘same sign’ partitions the integers into the set of positives, the set of negatives, and the set containing only zero. Similarly, the equivalence relation ‘=’ partitions the integers into one-element sets.
Another example is the set of strings consisting of a number, followed by a slash, followed by a nonzero number Ω = {n/d | n,d ∈ Z and d ̸= 0}. Define Sn,d by: nˆ/dˆ ∈ Sn,d if nˆd = ndˆ. Checking that this is a partition of Ω is routine (observe for instance that S4,3 = S8,6). This shows some parts, listing in each a couple of its infinitely many members.
Every equivalence relation induces a partition, and every partition is induced by an equivalence. (This is routine to check.) Below are two examples.
Consider the equivalence relationship between two integers of ‘gives the same remainder when divided by 2’, the set P = {(−1, 3), (2, 4), (0, 0), . . .}. In the set P are two kinds of pairs, the pairs with both members even and the pairs with both members odd. This equivalence induces a partition where the parts are found by: for each x we define the set of numbers related to it Sx = {y | (x,y) ∈ P}. The parts are {...,−3,−1,1,3,...} and {...,−2,0,2,4,...}. Each part can be named in many ways; for instance, {...,−3,−1,1,3,...} is S1 and also is S−3.
Now consider the partition of the natural numbers where two numbers are in the same part if they leave the same remainder when divided by 10, that is, if they have the same least significant digit. This partition is induced by the equivalence relation R defined by: two numbers n, m are related if they are together in the same part. For example, 3 is related to 33, but 3 is not

I
n writing this book I have been motivated by the desire to create a
high-quality textbook that costs almost nothing.
The book is available on my web page for free, and the paperback
version (produced through an on-demand press) costs considerably less
than comparable traditional textbooks. Any revisions or new editions
will be issued solely for the purpose of correcting mistakes and clarifying
exposition. New exercises may be added, but the existing ones will not be
unnecessarily changed or renumbered.
This text is an expansion and refinement of lecture notes I developed
while teaching proofs courses over the past fourteen years at Virginia
Commonwealth University (a large state university) and Randolph-Macon
College (a small liberal arts college). I found the needs of these two
audiences to be nearly identical, and I wrote this book for them. But I am
mindful of a larger audience. I believe this book is suitable for almost any
undergraduate mathematics program.
This second edition incorporates many minor corrections and additions
that were suggested by readers around the world. In addition, several
new examples and exercises have been added, and a section on the CantorBernstein-Schröeder
theorem has been added to Chapter 13.
Richard Hammack Richmond, Virginia
May 25, 2013
Introduction
T
his is a book about how to prove theorems.
Until this point in your education, mathematics has probably been
presented as a primarily computational discipline. You have learned to
solve equations, compute derivatives and integrals, multiply matrices
and find determinants; and you have seen how these things can answer
practical questions about the real world. In this setting, your primary goal
in using mathematics has been to compute answers.
But there is another side of mathematics that is more theoretical than
computational. Here the primary goal is to understand mathematical
structures, to prove mathematical statements, and even to invent or
discover new mathematical theorems and theories. The mathematical
techniques and procedures that you have learned and used up until now
are founded on this theoretical side of mathematics. For example, in
computing the area under a curve, you use the fundamental theorem of
calculus. It is because this theorem is true that your answer is correct.
However, in learning calculus you were probably far more concerned with
how that theorem could be applied than in understanding why it is true.
But how do we know it is true? How can we convince ourselves or others
of its validity? Questions of this nature belong to the theoretical realm of
mathematics. This book is an introduction to that realm.
This book will initiate you into an esoteric world. You will learn and
apply the methods of thought that mathematicians use to verify theorems,
explore mathematical truth and create new mathematical theories. This
will prepare you for advanced mathematics courses, for you will be better
able to understand proofs, write your own proofs and think critically and
inquisitively about mathematics.
ix
The book is organized into four parts, as outlined below.
PART I Fundamentals
• Chapter 1: Sets
• Chapter 2: Logic
• Chapter 3: Counting
Chapters 1 and 2 lay out the language and conventions used in all advanced
mathematics. Sets are fundamental because every mathematical structure,
object or entity can be described as a set. Logic is fundamental because it
allows us to understand the meanings of statements, to deduce information
about mathematical structures and to uncover further structures. All
subsequent chapters will build on these first two chapters. Chapter 3
is included partly because its topics are central to many branches of
mathematics, but also because it is a source of many examples and exercises
that occur throughout the book. (However, the course instructor may choose
to omit Chapter 3.)
PART II Proving Conditional Statements
• Chapter 4: Direct Proof
• Chapter 5: Contrapositive Proof
• Chapter 6: Proof by Contradiction
Chapters 4 through 6 are concerned with three main techniques used for
proving theorems that have the “conditional” form “If P, then Q.”
PART III More on Proof
• Chapter 7: Proving Non-Conditional Statements
• Chapter 8: Proofs Involving Sets
• Chapter 9: Disproof
• Chapter 10: Mathematical Induction
These chapters deal with useful variations, embellishments and consequences
of the proof techniques introduced in Chapters 4 through 6.
PART IV Relations, Functions and Cardinality
• Chapter 11: Relations
• Chapter 12: Functions
• Chapter 13: Cardinality of Sets
These final chapters are mainly concerned with the idea of functions, which
are central to all of mathematics. Upon mastering this material you will be
ready for advanced mathematics courses such as combinatorics, abstract
algebra, theory of computation, analysis and topology.
x Introduction
To the instructor. The book is designed for a three credit course. Here
is a possible timetable for a fourteen-week semester.
Week Monday Wednesday Friday
1 Section 1.1 Section 1.2 Sections 1.3, 1.4
2 Sections 1.5, 1.6, 1.7 Section 1.8 Sections 1.9
∗, 2.1
3 Section 2.2 Sections 2.3, 2.4 Sections 2.5, 2.6
4 Section 2.7 Sections 2.8
∗, 2.9 Sections 2.10, 2.11∗, 2.12∗
5 Sections 3.1, 3.2 Section 3.3 Sections 3.4, 3.5
∗
6 EXAM Sections 4.1, 4.2, 4.3 Sections 4.3, 4.4, 4.5
∗
7 Sections 5.1, 5.2, 5.3
∗ Section 6.1 Sections 6.2 6.3
∗
8 Sections 7.1, 7.2
∗, 7.3 Sections 8.1, 8.2 Section 8.3
9 Section 8.4 Sections 9.1, 9.2, 9.3
∗ Section 10.0
10 Sections 10.0, 10.3
∗ Sections 10.1, 10.2 EXAM
11 Sections 11.0, 11.1 Sections 11.2, 11.3 Sections 11.4, 11.5
12 Section 12.1 Section 12.2 Section 12.2
13 Sections 12.3, 12.4
∗ Section 12.5 Sections 12.5, 12.6
∗
14 Section 13.1 Section 13.2 Sections 13.3, 13.4
∗
Sections marked with ∗ may require only the briefest mention in class, or
may be best left for the students to digest on their own. Some instructors
may prefer to omit Chapter 3.
Acknowledgments. I thank my students in VCU’s MATH 300 courses
for offering feedback as they read the first edition of this book. Thanks
especially to Cory Colbert and Lauren Pace for rooting out typographical
mistakes and inconsistencies. I am especially indebted to Cory for reading
early drafts of each chapter and catching numerous mistakes before I
posted the final draft on my web page. Cory also created the index,
suggested some interesting exercises, and wrote some solutions. Thanks
to Andy Lewis and Sean Cox for suggesting many improvements while
teaching from the book. I am indebted to Lon Mitchell, whose expertise
with typesetting and on-demand publishing made the print version of this
book a reality.
And thanks to countless readers all over the world who contacted me
concerning errors and omissions. Because of you, this is a better book.
Part I
Fundamentals

CHAPTER 1
Sets
A
ll of mathematics can be described with sets. This becomes more and
more apparent the deeper into mathematics you go. It will be apparent
in most of your upper level courses, and certainly in this course. The
theory of sets is a language that is perfectly suited to describing and
explaining all types of mathematical structures.
1.1 Introduction to Sets
A set is a collection of things. The things in the collection are called
elements of the set. We are mainly concerned with sets whose elements
are mathematical entities, such as numbers, points, functions, etc.
A set is often expressed by listing its elements between commas, enclosed
by braces. For example, the collection ©
2,4,6,8
ª
is a set which has
four elements, the numbers 2,4,6 and 8. Some sets have infinitely many
elements. For example, consider the collection of all integers,
©
...,−4,−3,−2,−1,0,1,2,3,4,...ª
.
Here the dots indicate a pattern of numbers that continues forever in both
the positive and negative directions. A set is called an infinite set if it
has infinitely many elements; otherwise it is called a finite set.
Two sets are equal if they contain exactly the same elements. Thus
©
2,4,6,8
ª
=
©
4,2,8,6
ª
because even though they are listed in a different
order, the elements are identical; but ©
2,4,6,8
ª
6=
©
2,4,6,7
ª
. Also
©
...−4,−3,−2,−1,0,1,2,3,4...ª
=
©
0,−1,1,−2,2,−3,3,−4,4,...ª
.
We often let uppercase letters stand for sets. In discussing the set
©
2,4,6,8
ª
we might declare A =
©
2,4,6,8
ª
and then use A to stand for
©
2,4,6,8
ª
. To express that 2 is an element of the set A, we write 2 ∈ A, and
read this as “2 is an element of A,” or “2 is in A,” or just “2 in A.” We also
have 4 ∈ A, 6 ∈ A and 8 ∈ A, but 5 ∉ A. We read this last expression as “5 is
not an element of A,” or “5 not in A.” Expressions like 6,2 ∈ A or 2,4,8 ∈ A
are used to indicate that several things are in a set.
4 Sets
Some sets are so significant and prevalent that we reserve special
symbols for them. The set of natural numbers (i.e., the positive whole
numbers) is denoted by N, that is,
N =
©
1,2,3,4,5,6,7,...ª
.
The set of integers
Z =
©
...,−3,−2,−1,0,1,2,3,4,...ª
is another fundamental set. The symbol R stands for the set of all real
numbers, a set that is undoubtedly familiar to you from calculus. Other
special sets will be listed later in this section.
Sets need not have just numbers as elements. The set B =
©
T,F
ª
consists
of two letters, perhaps representing the values “true” and “false.” The set
C =
©
a, e,i, o,u
ª
consists of the lowercase vowels in the English alphabet.
The set D =
©
(0,0),(1,0),(0,1),(1,1)ª
has as elements the four corner points
of a square on the x-y coordinate plane. Thus (0,0) ∈ D, (1,0) ∈ D, etc., but
(1,2) ∉ D (for instance). It is even possible for a set to have other sets
as elements. Consider E =
©
1,
©
2,3
ª
,
©
2,4
ªª, which has three elements: the
number 1, the set ©
2,3
ª
and the set ©
2,4
ª
. Thus 1 ∈ E and ©
2,3
ª
∈ E and
©
2,4
ª
∈ E. But note that 2 ∉ E, 3 ∉ E and 4 ∉ E.
Consider the set M =
© £ 0 0
0 0
¤
,
£
1 0
0 1
¤
,
£
1 0
1 1
¤ª of three two-by-two matrices.
We have £
0 0
0 0
¤
∈ M, but £
1 1
0 1
¤
∉ M. Letters can serve as symbols denoting a
set’s elements: If a =
£
0 0
0 0
¤
, b =
£
1 0
0 1
¤
and c =
£
1 0
1 1
¤
, then M =
©
a,b, c
ª
.
If X is a finite set, its cardinality or size is the number of elements
it has, and this number is denoted as |X|. Thus for the sets above, |A| = 4,
|B| = 2, |C| = 5, |D| = 4, |E| = 3 and |M| = 3.
There is a special set that, although small, plays a big role. The
empty set is the set ©ª that has no elements. We denote it as ;, so ; = ©ª.
Whenever you see the symbol ;, it stands for ©ª. Observe that |;| = 0. The
empty set is the only set whose cardinality is zero.
Be careful in writing the empty set. Don’t write ©
;
ª
when you mean ;.
These sets can’t be equal because ; contains nothing while ©
;
ª
contains
one thing, namely the empty set. If this is confusing, think of a set as a
box with things in it, so, for example, ©
2,4,6,8
ª
is a “box” containing four
numbers. The empty set ; = ©ª is an empty box. By contrast, ©
;
ª
is a box
with an empty box inside it. Obviously, there’s a difference: An empty box
is not the same as a box with an empty box inside it. Thus ; 6= ©
;
ª
. (You
might also note |;| = 0 and
¯
¯
©
;
ª¯
¯ = 1 as additional evidence that ; 6= ©
;
ª
.)
Introduction to Sets 5
This box analogy can help us think about sets. The set F =
©
;,
©
;
ª
,
©©;
ªªª
may look strange but it is really very simple. Think of it as a box containing
three things: an empty box, a box containing an empty box, and a box
containing a box containing an empty box. Thus |F| = 3. The set G =
©
N,Z
ª
is a box containing two boxes, the box of natural numbers and the box of
integers. Thus |G| = 2.
A special notation called set-builder notation is used to describe sets
that are too big or complex to list between braces. Consider the infinite
set of even integers E =
©
...,−6,−4,−2,0,2,4,6,...ª
. In set-builder notation
this set is written as
E =
©
2n : n ∈ Z
ª
.
We read the first brace as “the set of all things of form,” and the colon as
“such that.” So the expression E =
©
2n : n ∈ Z
ª
is read as “E equals the set of
all things of form 2n, such that n is an element of Z.” The idea is that E
consists of all possible values of 2n, where n takes on all values in Z.
In general, a set X written with set-builder notation has the syntax
X =
©
expression : ruleª
,
where the elements of X are understood to be all values of “expression”
that are specified by “rule.” For example, the set E above is the set
of all values the expression 2n that satisfy the rule n ∈ Z. There can
be many ways to express the same set. For example, E =
©
2n : n ∈ Z
ª
=
©
n : n is an even integerª
=
©
n : n = 2k,k ∈ Z
ª
. Another common way of
writing it is
E =
©
n ∈ Z : n is evenª
,
read “E is the set of all n in Z such that n is even.” Some writers use a bar
instead of a colon; for example, E =
©
n ∈ Z | n is evenª
. We use the colon.
Example 1.1 Here are some further illustrations of set-builder notation.
1. ©
n : n is a prime numberª
=
©
2,3,5,7,11,13,17,...ª
2. ©
n ∈ N : n is primeª
=
©
2,3,5,7,11,13,17,...ª
3. ©
n
2
: n ∈ Z
ª
=
©
0,1,4,9,16,25,...ª
4. ©
x ∈ R : x
2 −2 = 0
ª
=
©p
2,−
p
2
ª
5. ©
x ∈ Z : x
2 −2 = 0
ª
= ;
6. ©
x ∈ Z : |x| < 4
ª
=
©
−3,−2,−1,0,1,2,3
ª
7. ©
2x : x ∈ Z,|x| < 4
ª
=
©
−6,−4,−2,0,2,4,6
ª
8. ©
x ∈ Z : |2x| < 4
ª
=
©
−1,0,1
ª
6 Sets
These last three examples highlight a conflict of notation that we must
always be alert to. The expression |X| means absolute value if X is a number
and cardinality if X is a set. The distinction should always be clear from
context. Consider ©
x ∈ Z : |x| < 4
ª
in Example 1.1 (6) above. Here x ∈ Z, so x
is a number (not a set), and thus the bars in |x| must mean absolute value,
not cardinality. On the other hand, suppose A =
©©1,2
ª
,
©
3,4,5,6
ª
,
©
7
ªª and
B =
©
X ∈ A : |X| < 3
ª
. The elements of A are sets (not numbers), so the |X|
in the expression for B must mean cardinality. Therefore B =
©©1,2
ª
,
©
7
ªª.
We close this section with a summary of special sets. These are sets or
types of sets that come up so often that they are given special names and
symbols.
• The empty set: ; = ©ª
• The natural numbers: N =
©
1,2,3,4,5,...ª
• The integers: Z =
©
...,−3,−2,−1,0,1,2,3,4,5,...ª
• The rational numbers: Q =
©
x : x =
m
n
, where m,n ∈ Z and n 6= 0
ª
• The real numbers: R (the set of all real numbers on the number line)
Notice that Q is the set of all numbers that can be expressed as a fraction
of two integers. You are surely aware that Q 6= R, as p
2 ∉ Q but p
2 ∈ R.
Following are some other special sets that you will recall from your
study of calculus. Given two numbers a,b ∈ R with a < b, we can form
various intervals on the number line.
• Closed interval: [a,b] =
©
x ∈ R : a ≤ x ≤ b
ª
• Half open interval: (a,b] =
©
x ∈ R : a < x ≤ b
ª
• Half open interval: [a,b) =
©
x ∈ R : a ≤ x < b
ª
• Open interval: (a,b) =
©
x ∈ R : a < x < b
ª
• Infinite interval: (a,∞) =
©
x ∈ R : a < x
ª
• Infinite interval: [a,∞) =
©
x ∈ R : a ≤ x
ª
• Infinite interval: (−∞,b) =
©
x ∈ R : x < b
ª
• Infinite interval: (−∞,b] =
©
x ∈ R : x ≤ b
ª
Remember that these are intervals on the number line, so they have in-
finitely many elements. The set (0.1,0.2) contains infinitely many numbers,
even though the end points may be close together. It is an unfortunate
notational accident that (a,b) can denote both an interval on the line and
a point on the plane. The difference is usually clear from context. In the
next section we will see still another meaning of (a,b).
Introduction to Sets 7
Exercises for Section 1.1
A. Write each of the following sets by listing their elements between braces.
1. ©
5x−1 : x ∈ Z
ª
2. ©
3x+2 : x ∈ Z
ª
3. ©
x ∈ Z : −2 ≤ x < 7
ª
4. ©
x ∈ N : −2 < x ≤ 7
ª
5. ©
x ∈ R : x
2 = 3
ª
6. ©
x ∈ R : x
2 = 9
ª
7. ©
x ∈ R : x
2 +5x = −6
ª
8. ©
x ∈ R : x
3 +5x
2 = −6x
ª
9. ©
x ∈ R : sinπx = 0
ª
10. ©
x ∈ R : cos x = 1
ª
11. ©
x ∈ Z : |x| < 5
ª
12. ©
x ∈ Z : |2x| < 5
ª
13. ©
x ∈ Z : |6x| < 5
ª
14. ©
5x : x ∈ Z,|2x| ≤ 8
ª
15. ©
5a+2b : a,b ∈ Z
ª
16. ©
6a+2b : a,b ∈ Z
ª
B. Write each of the following sets in set-builder notation.
17. ©
2,4,8,16,32,64...ª
18. ©
0,4,16,36,64,100,...ª
19. ©
...,−6,−3,0,3,6,9,12,15,...ª
20. ©
...,−8,−3,2,7,12,17,...ª
21. ©
0,1,4,9,16,25,36,...ª
22. ©
3,6,11,18,27,38,...ª
23. ©
3,4,5,6,7,8
ª
24. ©
−4,−3,−2,−1,0,1,2
ª
25. ©
...,
1
8
,
1
4
,
1
2
,1,2,4,8,...ª
26. ©
...,
1
27 ,
1
9
,
1
3
,1,3,9,27,...ª
27. ©
...,−π,−
π
2
,0,
π
2
,π,
3π
2
,2π,
5π
2
,...ª
28. ©
...,−
3
2
,−
3
4
,0,
3
4
,
3
2
,
9
4
,3,
15
4
,
9
2
,...ª
C. Find the following cardinalities.
29.
¯
¯
©©1
ª
,
©
2,
©
3,4
ªª,;
ª¯
¯
30.
¯
¯
©©1,4
ª
,a,b,
©©3,4
ªª,
©
;
ªª¯
¯
31.
¯
¯
©©©1
ª
,
©
2,
©
3,4
ªª,;
ªª¯
¯
32.
¯
¯
©©©1,4
ª
,a,b,
©©3,4
ªª,
©
;
ªªª¯
¯
33.
¯
¯
©
x ∈ Z : |x| < 10ª¯
¯
34.
¯
¯
©
x ∈ N : |x| < 10ª¯
¯
35.
¯
¯
©
x ∈ Z : x
2 < 10ª¯
¯
36.
¯
¯
©
x ∈ N : x
2 < 10ª¯
¯
37.
¯
¯
©
x ∈ N : x
2 < 0
ª¯
¯
38.
¯
¯
©
x ∈ N : 5x ≤ 20ª¯
¯
D. Sketch the following sets of points in the x-y plane.
39. ©
(x, y) : x ∈ [1,2], y ∈ [1,2]ª
40. ©
(x, y) : x ∈ [0,1], y ∈ [1,2]ª
41. ©
(x, y) : x ∈ [−1,1], y = 1
ª
42. ©
(x, y) : x = 2, y ∈ [0,1]ª
43. ©
(x, y) : |x| = 2, y ∈ [0,1]ª
44. ©
(x, x
2
) : x ∈ R
ª
45. ©
(x, y) : x, y ∈ R, x
2 + y
2 = 1
ª
46. ©
(x, y) : x, y ∈ R, x
2 + y
2 ≤ 1
ª
47. ©
(x, y) : x, y ∈ R, y ≥ x
2 −1
ª
48. ©
(x, y) : x, y ∈ R, x > 1
ª
49. ©
(x, x+ y) : x ∈ R, y ∈ Z
ª
50. ©
(x,
x
2
y
) : x ∈ R, y ∈ N
ª
51. ©
(x, y) ∈ R
2
: (y− x)(y+ x) = 0
ª
52. ©
(x, y) ∈ R
2
: (y− x
2
)(y+ x
2
) = 0
ª
8 Sets
1.2 The Cartesian Product
Given two sets A and B, it is possible to “multiply” them to produce a new
set denoted as A ×B. This operation is called the Cartesian product. To
understand it, we must first understand the idea of an ordered pair.
Definition 1.1 An ordered pair is a list (x, y) of two things x and y,
enclosed in parentheses and separated by a comma.
For example, (2,4) is an ordered pair, as is (4,2). These ordered pairs
are different because even though they have the same things in them,
the order is different. We write (2,4) 6= (4,2). Right away you can see that
ordered pairs can be used to describe points on the plane, as was done in
calculus, but they are not limited to just that. The things in an ordered
pair don’t have to be numbers. You can have ordered pairs of letters, such
as (m,`), ordered pairs of sets such as (
©
2,5
ª
,
©
3,2
ª
), even ordered pairs
of ordered pairs like ((2,4),(4,2)). The following are also ordered pairs:
(2,
©
1,2,3
ª
), (R,(0,0)). Any list of two things enclosed by parentheses is an
ordered pair. Now we are ready to define the Cartesian product.
Definition 1.2 The Cartesian product of two sets A and B is another
set, denoted as A ×B and defined as A ×B =
©
(a,b) : a ∈ A,b ∈ B
ª
.
Thus A × B is a set of ordered pairs of elements from A and B. For
example, if A =
©
k,`,m
ª
and B =
©
q, r
ª
, then
A ×B =
©
(k, q),(k, r),(`, q),(`, r),(m, q),(m, r)
ª
.
Figure 1.1 shows how to make a schematic diagram of A ×B. Line up the
elements of A horizontally and line up the elements of B vertically, as if A
and B form an x- and y-axis. Then fill in the ordered pairs so that each
element (x, y) is in the column headed by x and the row headed by y.
B
A
q
r (k, r) (`, r) (m, r)
(k, q) (`, q) (m, q)
k ` m
A ×B
Figure 1.1. A diagram of a Cartesian product
The Cartesian Product 9
For another example, ©
0,1
ª
×
©
2,1
ª
=
©
(0,2),(0,1),(1,2),(1,1)ª
. If you are
a visual thinker, you may wish to draw a diagram similar to Figure 1.1.
The rectangular array of such diagrams give us the following general fact.
Fact 1.1 If A and B are finite sets, then |A ×B| = |A|·|B|.
The set R×R =
©
(x, y) : x, y ∈ R
ª
should be very familiar. It can be viewed
as the set of points on the Cartesian plane, and is drawn in Figure 1.2(a).
The set R×N =
©
(x, y) : x ∈ R, y ∈ N
ª
can be regarded as all of the points on
the Cartesian plane whose second coordinate is a natural number. This
is illustrated in Figure 1.2(b), which shows that R×N looks like infinitely
many horizontal lines at integer heights above the x axis. The set N×N
can be visualized as the set of all points on the Cartesian plane whose
coordinates are both natural numbers. It looks like a grid of dots in the
first quadrant, as illustrated in Figure 1.2(c).
x x x
y y y
(a) (b) (c)
R×R R×N N×N
Figure 1.2. Drawings of some Cartesian products
It is even possible for one factor of a Cartesian product to be a Cartesian
product itself, as in R×(N×Z) =
©
(x,(y, z)) : x ∈ R, (y, z) ∈ N×Z
ª
.
We can also define Cartesian products of three or more sets by moving
beyond ordered pairs. An ordered triple is a list (x, y, z). The Cartesian
product of the three sets R, N and Z is R×N×Z =
©
(x, y, z) : x ∈ R, y ∈ N, z ∈ Z
ª
.
Of course there is no reason to stop with ordered triples. In general,
A1 × A2 ×··· × An =
©
(x1, x2,..., xn) : xi ∈ Ai for each i = 1,2,...,n
ª
.
Be mindful of parentheses. There is a slight difference between R×(N×Z)
and R×N×Z. The first is a Cartesian product of two sets; its elements are
ordered pairs (x,(y, z)). The second is a Cartesian product of three sets; its
elements look like (x, y, z). To be sure, in many situations there is no harm
in blurring the distinction between expressions like (x,(y, z)) and (x, y, z),
but for now we consider them as different.
10 Sets
We can also take Cartesian powers of sets. For any set A and positive
integer n, the power A
n
is the Cartesian product of A with itself n times:
A
n = A × A ×··· × A =
©
(x1, x2,..., xn) : x1, x2,..., xn ∈ A
ª
.
In this way, R
2
is the familiar Cartesian plane and R
3
is three-dimensional
space. You can visualize how, if R
2
is the plane, then Z
2 =
©
(m,n) : m,n ∈ Z
ª
is a grid of points on the plane. Likewise, as R
3
is 3-dimensional space,
Z
3 =
©
(m,n, p) : m,n, p ∈ Z
ª
is a grid of points in space.
In other courses you may encounter sets that are very similar to R
n
,
but yet have slightly different shades of meaning. Consider, for example,
the set of all two-by-three matrices with entries from R:
M =
©£ u v w
x y z ¤
: u,v,w, x, y, z ∈ R
ª
.
This is not really all that different from the set
R
6 =
©
(u,v,w, x, y, z) : u,v,w, x, y, z ∈ R
ª
.
The elements of these sets are merely certain arrangements of six real
numbers. Despite their similarity, we maintain that M 6= R
6
, for two-bythree
matrices are not the same things as sequences of six numbers.
Exercises for Section 1.2
A. Write out the indicated sets by listing their elements between braces.
1. Suppose A =
©
1,2,3,4
ª
and B =
©
a, c
ª
.
(a) A ×B
(b) B × A
(c) A × A
(d) B ×B
(e) ; ×B
(f) (A ×B)×B
(g) A ×(B ×B)
(h) B
3
2. Suppose A =
©
π, e,0
ª
and B =
©
0,1
ª
.
(a) A ×B
(b) B × A
(c) A × A
(d) B ×B
(e) A × ;
(f) (A ×B)×B
(g) A ×(B ×B)
(h) A ×B ×B
3. ©
x ∈ R : x
2 = 2
ª
×
©
a, c, e
ª
4. ©
n ∈ Z : 2 < n < 5
ª
×
©
n ∈ Z : |n| = 5
ª
5. ©
x ∈ R : x
2 = 2
ª
×
©
x ∈ R : |x| = 2
ª
6. ©
x ∈ R : x
2 = x
ª
×
©
x ∈ N : x
2 = x
ª
7. ©
;
ª
×
©
0,;
ª
×
©
0,1
ª
8. ©
0,1
ª4
B. Sketch these Cartesian products on the x-y plane R
2
(or R
3
for the last two).
9. ©
1,2,3
ª
×
©
−1,0,1
ª
10. ©
−1,0,1
ª
×
©
1,2,3
ª
11. [0,1]×[0,1]
12. [−1,1]×[1,2]
13. ©
1,1.5,2
ª
×[1,2]
14. [1,2]×
©
1,1.5,2
ª
15. ©
1
ª
×[0,1]
16. [0,1]×
©
1
ª
17. N×Z
18. Z×Z
19. [0,1]×[0,1]×[0,1]
20. ©
(x, y) ∈ R
2
: x
2 + y
2 ≤ 1
ª
×[0,1]
Subsets 11
1.3 Subsets
It can happen that every element of some set A is also an element of
another set B. For example, each element of A =
©
0,2,4
ª
is also an element
of B =
©
0,1,2,3,4
ª
. When A and B are related this way we say that A is a
subset of B.
Definition 1.3 Suppose A and B are sets. If every element of A is also
an element of B, then we say A is a subset of B, and we denote this as
A ⊆ B. We write A 6⊆ B if A is not a subset of B, that is, if it is not true
that every element of A is also an element of B. Thus A 6⊆ B means that
there is at least one element of A that is not an element of B.
Example 1.2 Be sure you understand why each of the following is true.
1. ©
2,3,7
ª
⊆
©
2,3,4,5,6,7
ª
2. ©
2,3,7
ª
6⊆
©
2,4,5,6,7
ª
3. ©
2,3,7
ª
⊆
©
2,3,7
ª
4. ©
2n : n ∈ Z
ª
⊆ Z
5. ©
(x,sin(x)) : x ∈ R
ª
⊆ R
2
6. ©
2,3,5,7,11,13,17,...ª
⊆ N
7. N ⊆ Z ⊆ Q ⊆ R
8. R×N ⊆ R×R
This brings us to a significant fact: If B is any set whatsoever, then
; ⊆ B. To see why this is true, look at the last sentence of Definition 1.3.
It says that ; 6⊆ B would mean that there is at least one element of ;
that is not an element of B. But this cannot be so because ; contains no
elements! Thus it is not the case that ; 6⊆ B, so it must be that ; ⊆ B.
Fact 1.2 The empty set is a subset of every set, that is, ; ⊆ B for any set B.
Here is another way to look at it. Imagine a subset of B as a thing you
make by starting with braces ©ª, then filling them with selections from B.
For example, to make one particular subset of B =
©
a,b, c
ª
, start with ©ª,
select b and c from B and insert them into ©ª to form the subset ©
b, c
ª
.
Alternatively, you could have chosen just a to make ©
a
ª
, and so on. But
one option is to simply select nothing from B. This leaves you with the
subset ©ª. Thus ©ª ⊆ B. More often we write it as ; ⊆ B.
12 Sets
This idea of “making” a subset can help us list out all the subsets of
a given set B. As an example, let B =
©
a,b, c
ª
. Let’s list all of its subsets.
One way of approaching this is to make a tree-like structure. Begin with
the subset ©ª, which is shown on the left of Figure 1.3. Considering the
element a of B, we have a choice: insert it or not. The lines from ©ª point
to what we get depending whether or not we insert a, either ©ª or ©
a
ª
. Now
move on to the element b of B. For each of the sets just formed we can
either insert or not insert b, and the lines on the diagram point to the
resulting sets ©ª,
©
b
ª
,
©
a
ª
, or ©
a,b
ª
. Finally, to each of these sets, we can
either insert c or not insert it, and this gives us, on the far right-hand
column, the sets ©ª,
©
c
ª
,
©
b
ª
,
©
b, c
ª
,
©
a
ª
,
©
a, c
ª
,
©
a,b
ª
and ©
a,b, c
ª
. These are
the eight subsets of B =
©
a,b, c
ª
.
Insert a? Insert b? Insert c ?
No
Yes
Yes
Yes
Yes
Yes
Yes
Yes
No
No
No
No
No
No
©ª
©ª
©ª
©ª
©
c
ª
©
b
ª
©
b, c
ª
©
a
ª
©
a, c
ª
©
a,b
ª
©
a,b, c
ª
©
b
ª
©
a
ª
©
a,b
ª
©
a
ª
Figure 1.3. A “tree” for listing subsets
We can see from the way this tree branches out that if it happened that
B =
©
a
ª
, then B would have just two subsets, those in the second column
of the diagram. If it happened that B =
©
a,b
ª
, then B would have four
subsets, those listed in the third column, and so on. At each branching of
the tree, the number of subsets doubles. Thus in general, if |B| = n, then
B must have 2
n
subsets.
Fact 1.3 If a finite set has n elements, then it has 2
n
subsets.
Subsets 13
For a slightly more complex example, consider listing the subsets of
B =
©
1,2,
©
1,3
ªª. This B has just three elements: 1, 2 and ©
1,3
ª
. At this
point you probably don’t even have to draw a tree to list out B’s subsets.
You just make all the possible selections from B and put them between
braces to get
©ª,
©
1
ª
,
©
2
ª
,
©©1,3
ªª,
©
1,2
ª
,
©
1,
©
1,3
ªª,
©
2,
©
1,3
ªª,
©
1,2,
©
1,3
ªª.
These are the eight subsets of B. Exercises like this help you identify what
is and isn’t a subset. You know immediately that a set such as ©
1,3
ª
is not
a subset of B because it can’t be made by selecting elements from B, as
the 3 is not an element of B and thus is not a valid selection. Notice that
although ©
1,3
ª
6⊆ B, it is true that ©
1,3
ª
∈ B. Also, ©©1,3
ªª ⊆ B.
Example 1.3 Be sure you understand why the following statements are
true. Each illustrates an aspect of set theory that you’ve learned so far.
1. 1 ∈
©
1,
©
1
ªª . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .1 is the first element listed in ©
1,
©
1
ªª
2. 1 6⊆
©
1,
©
1
ªª . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . because 1 is not a set
3. ©
1
ª
∈
©
1,
©
1
ªª . . . . . . . . . . . . . . . . . . . . . . . . . ©
1
ª
is the second element listed in ©
1,
©
1
ªª
4. ©
1
ª
⊆
©
1,
©
1
ªª . . . . . . . . . . . . . . . . . . . . . . . make subset ©
1
ª
by selecting 1 from ©
1,
©
1
ªª
5. ©©1
ªª ∉
©
1,
©
1
ªª . . . . . . . . . . . because ©
1,
©
1
ªª contains only 1 and ©
1
ª
, and not ©©1
ªª
6. ©©1
ªª ⊆
©
1,
©
1
ªª. . . . . . . . . . . . . . . . . .make subset ©©1
ªª by selecting ©
1
ª
from ©
1,
©
1
ªª
7. N ∉ N. . . . . . . . .because N is a set (not a number) and N contains only numbers
8. N ⊆ N . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . because X ⊆ X for every set X
9. ; ∉ N. . . . . . . . . . . . . . . . . . . . because the set N contains only numbers and no sets
10. ; ⊆ N . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . because ; is a subset of every set
11. N ∈
©
N
ª
. . . . . . . . . . . . . . . . . . . . . . . . . . .because ©
N
ª
has just one element, the set N
12. N 6⊆
©
N
ª
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . because, for instance, 1 ∈ N but 1 ∉
©
N
ª
13. ; ∉ ©
N
ª
. . . . . . . . . . . . . . . . . . . . . note that the only element of ©
N
ª
is N, and N 6= ;
14. ; ⊆ ©
N
ª
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . because ; is a subset of every set
15. ; ∈ ©
;,N
ª
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ; is the first element listed in ©
;,N
ª
16. ; ⊆ ©
;,N
ª
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .because ; is a subset of every set
17. ©
N
ª
⊆
©
;,N
ª
. . . . . . . . . . . . . . . . . . . . . . . make subset ©
N
ª
by selecting N from ©
;,N
ª
18. ©
N
ª
6⊆
©
;,
©
N
ªª . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . because N ∉
©
;,
©
N
ªª
19. ©
N
ª
∈
©
;,
©
N
ªª. . . . . . . . . . . . . . . . . . . . . .©
N
ª
is the second element listed in ©
;,
©
N
ªª
20. ©
(1,2),(2,2),(7,1)ª
⊆ N×N . . . . . . . . . . . . . . . . . . . each of (1,2), (2,2), (7,1) is in N×N
Though they should help you understand the concept of subset, the
above examples are somewhat artificial. But in general, subsets arise very
14 Sets
naturally. For instance, consider the unit circle C =
©
(x, y) ∈ R
2
: x
2 + y
2 = 1
ª
.
This is a subset C ⊆ R
2
. Likewise the graph of a function y = f (x) is a set
of points G =
©
(x, f (x)) : x ∈ R
ª
, and G ⊆ R
2
. Surely sets such as C and G
are more easily understood or visualized when regarded as subsets of R
2
.
Mathematics is filled with such instances where it is important to regard
one set as a subset of another.
Exercises for Section 1.3
A. List all the subsets of the following sets.
1. ©
1,2,3,4
ª
2. ©
1,2,;
ª
3. ©©R
ªª
4. ;
5. ©
;
ª
6. ©
R,Q,N
ª
7. ©
R,
©
Q,N
ªª
8. ©©0,1
ª
,
©
0,1,
©
2
ªª,
©
0
ªª
B. Write out the following sets by listing their elements between braces.
9. ©
X : X ⊆
©
3,2,a
ª
and |X| = 2
ª
10. ©
X ⊆ N : |X| ≤ 1
ª
11. ©
X : X ⊆
©
3,2,a
ª
and |X| = 4
ª
12. ©
X : X ⊆
©
3,2,a
ª
and |X| = 1
ª
C. Decide if the following statements are true or false. Explain.
13. R
3 ⊆ R
3
14. R
2 ⊆ R
3
15. ©
(x, y) : x−1 = 0
ª
⊆
©
(x, y) : x
2 − x = 0
ª
16. ©
(x, y) : x
2 − x = 0
ª
⊆
©
(x, y) : x−1 = 0
ª
1.4 Power Sets
Given a set, you can form a new set with the power set operation, defined
as follows.
Definition 1.4 If A is a set, the power set of A is another set, denoted
as P(A) and defined to be the set of all subsets of A. In symbols, P(A) =
©
X : X ⊆ A
ª
.
For example, suppose A =
©
1,2,3
ª
. The power set of A is the set of all
subsets of A. We learned how to find these subsets in the previous section,
and they are ©ª,
©
1
ª
,
©
2
ª
,
©
3
ª
,
©
1,2
ª
,
©
1,3
ª
,
©
2,3
ª
and ©
1,2,3
ª
. Therefore the
power set of A is
P(A) =
©
;,
©
1
ª
,
©
2
ª
,
©
3
ª
,
©
1,2
ª
,
©
1,3
ª
,
©
2,3
ª
,
©
1,2,3
ª ª.
As we saw in the previous section, if a finite set A has n elements, then
it has 2
n
subsets, and thus its power set has 2
n elements.
Power Sets 15
Fact 1.4 If A is a finite set, then |P(A)| = 2
|A|
.
Example 1.4 You should examine the following statements and make
sure you understand how the answers were obtained. In particular, notice
that in each instance the equation |P(A)| = 2
|A|
is true.
1. P
¡©0,1,3
ª¢ =
©
;,
©
0
ª
,
©
1
ª
,
©
3
ª
,
©
0,1
ª
,
©
0,3
ª
,
©
1,3
ª
,
©
0,1,3
ª ª
2. P
¡©1,2
ª¢ =
©
;,
©
1
ª
,
©
2
ª
,
©
1,2
ª ª
3. P
¡©1
ª¢ =
©
;,
©
1
ª ª
4. P (;) =
©
;
ª
5. P
¡©a
ª¢ =
©
;,
©
a
ª ª
6. P
¡©;
ª¢ =
©
;,
©
;
ª ª
7. P
¡©a
ª¢×P
¡©;
ª¢ =
©
(;,;),
¡
;,
©
;
ª¢,
¡©a
ª
,;
¢
,
¡©a
ª
,
©
;
ª¢ ª
8. P
¡
P
¡©;
ª¢¢ =
©
;,
©
;
ª
,
©©;
ªª,
©
;,
©
;
ªª ª
9. P
¡©1,
©
1,2
ªª¢ =
©
;,
©
1
ª
,
©©1,2
ªª,
©
1,
©
1,2
ªª ª
10. P
¡©Z,N
ª¢ =
©
;,
©
Z
ª
,
©
N
ª
,
©
Z,N
ª ª
Next are some that are wrong. See if you can determine why they are wrong
and make sure you understand the explanation on the right.
11. P(1) =
©
;,
©
1
ª ª . . . . . . . . . . . . . . . . . . . . . . . . . . . meaningless because 1 is not a set
12. P
¡©1,
©
1,2
ªª¢ =
©
;,
©
1
ª
,
©
1,2
ª
,
©
1,
©
1,2
ªªª . . . . . . . . wrong because ©
1,2
ª
6⊆
©
1,
©
1,2
ªª
13. P
¡©1,
©
1,2
ªª¢ =
©
;,
©©1
ªª,
©©1,2
ªª,
©
;,
©
1,2
ªªª . . . . . wrong because ©©1
ªª 6⊆
©
1,
©
1,2
ªª
If A is finite, it is possible (though maybe not practical) to list out P(A)
between braces as was done in the above example. That is not possible if
A is infinite. For example, consider P(N). If you start listing its elements
you quickly discover that N has infinitely many subsets, and it’s not clear
how (or if) they could be arranged as a list with a definite pattern:
P(N) =
©
;,
©
1
ª
,
©
2
ª
,...,©
1,2
ª
,
©
1,3
ª
,...,©
39,47ª
,
...,©
3,87,131ª
,...,©
2,4,6,8,...ª
,... ? ...ª
.
The set P(R
2
) is mind boggling. Think of R
2 =
©
(x, y) : x, y ∈ R
ª
as the set
of all points on the Cartesian plane. A subset of R
2
(that is, an element
of P(R
2
)) is a set of points in the plane. Let’s look at some of these sets.
Since ©
(0,0),(1,1)ª
⊆ R
2
, we know that ©
(0,0),(1,1)ª
∈ P(R
2
). We can even
draw a picture of this subset, as in Figure 1.4(a). For another example, the
graph of the equation y = x
2
is the set of points G =
©
(x, x
2
) : x ∈ R
ª
and this
is a subset of R
2
, so G ∈ P(R
2
). Figure 1.4(b) is a picture of G. Because
this can be done for any function, the graph of any imaginable function
f : R → R is an element of P(R
2
).
16 Sets
x x x
y y y
(a) (b) (c)
Figure 1.4. Three of the many, many sets in P(R
2
)
In fact, any black-and-white image on the plane can be thought of as a
subset of R
2
, where the black points belong to the subset and the white
points do not. So the text “INFINITE” in Figure 1.4(c) is a subset of R
2
and therefore an element of P(R
2
). By that token, P(R
2
) contains a copy
of the page you are reading now.
Thus in addition to containing every imaginable function and every
imaginable black-and-white image, P(R
2
) also contains the full text of
every book that was ever written, those that are yet to be written and
those that will never be written. Inside of P(R
2
) is a detailed biography of
your life, from beginning to end, as well as the biographies of all of your
unborn descendants. It is startling that the five symbols used to write
P(R
2
) can express such an incomprehensibly large set.
Homework: Think about P(P(R
2
)).
Exercises for Section 1.4
A. Find the indicated sets.
1. P
¡©©a,b
ª
,
©
c
ªª¢
2. P
¡©1,2,3,4
ª¢
3. P
¡©©;
ª
,5
ª¢
4. P
¡©R,Q
ª¢
5. P
¡
P
¡©2
ª¢¢
6. P
¡©1,2
ª¢×P
¡©3
ª¢
7. P
¡©a,b
ª¢×P
¡©0,1
ª¢
8. P
¡©1,2
ª
×
©
3
ª¢
9. P
¡©a,b
ª
×
©
0
ª¢
10. ©
X ∈ P
¡©1,2,3
ª¢: |X| ≤ 1
ª
11. ©
X ⊆ P
¡©1,2,3
ª¢: |X| ≤ 1
ª
12. ©
X ∈ P
¡©1,2,3
ª¢: 2 ∈ X
ª
B. Suppose that |A| = m and |B| = n. Find the following cardinalities.
13. |P(P(P(A)))|
14. |P(P(A))|
15. |P(A ×B)|
16. |P(A)×P(B)|
17.
¯
¯
©
X ∈ P(A) : |X| ≤ 1
ª¯
¯
18. |P(A ×P(B))|
19. |P(P(P(A × ;)))|
20.
¯
¯
©
X ⊆ P(A) : |X| ≤ 1
ª¯
¯
Union, Intersection, Difference 17
1.5 Union, Intersection, Difference
Just as numbers are combined with operations such as addition, subtraction
and multiplication, there are various operations that can be applied to
sets. The Cartesian product (defined in Section 1.2) is one such operation;
given sets A and B, we can combine them with × to get a new set A ×B.
Here are three new operations called union, intersection and difference.
Definition 1.5 Suppose A and B are sets.
The union of A and B is the set A ∪B =
©
x : x ∈ A or x ∈ B
ª
.
The intersection of A and B is the set A ∩B =
©
x : x ∈ A and x ∈ B
ª
.
The difference of A and B is the set A −B =
©
x : x ∈ A and x ∉ B
ª
.
In words, the union A ∪B is the set of all things that are in A or in B
(or in both). The intersection A ∩B is the set of all things in both A and B.
The difference A −B is the set of all things that are in A but not in B.
Example 1.5 Suppose A =
©
a,b, c,d, e
ª
, B =
©
d, e, f
ª
and C =
©
1,2,3
ª
.
1. A ∪B =
©
a,b, c,d, e, f
ª
2. A ∩B =
©
d, e
ª
3. A −B =
©
a,b, c
ª
4. B − A =
©
f
ª
5. (A −B)∪(B − A) =
©
a,b, c, f
ª
6. A ∪C =
©
a,b, c,d, e,1,2,3
ª
7. A ∩C = ;
8. A −C =
©
a,b, c,d, e
ª
9. (A ∩C)∪(A −C) =
©
a,b, c,d, e
ª
10. (A ∩B)×B =
©
(d,d),(d, e),(d, f ),(e,d),(e, e),(e, f )
ª
11. (A ×C)∩(B ×C) =
©
(d,1),(d,2),(d,3),(e,1),(e,2),(e,3)ª
Observe that for any sets X and Y it is always true that X ∪Y = Y ∪ X
and X ∩Y = Y ∩ X, but in general X −Y 6= Y − X.
Continuing the example, parts 12–15 below use the interval notation
discussed in Section 1.1, so [2,5] =
©
x ∈ R : 2 ≤ x ≤ 5
ª
, etc. Sketching these
examples on the number line may help you understand them.
12. [2,5]∪[3,6] = [2,6]
13. [2,5]∩[3,6] = [3,5]
14. [2,5]−[3,6] = [2,3)
15. [0,3]−[1,2] = [0,1)∪(2,3]
18 Sets
A
B
A ∪B
A ∩B
A −B
(a) (b) (c) (d)
Figure 1.5. The union, intersection and difference of sets A and B
Example 1.6 Let A =
©
(x, x
2
) : x ∈ R
ª
be the graph of the equation y = x
2
and let B =
©
(x, x+2) : x ∈ R
ª
be the graph of the equation y = x+2. These sets
are subsets of R
2
. They are sketched together in Figure 1.5(a). Figure 1.5(b)
shows A ∪B, the set of all points (x, y) that are on one (or both) of the two
graphs. Observe that A ∩ B =
©
(−1,1),(2,4)ª
consists of just two elements,
the two points where the graphs intersect, as illustrated in Figure 1.5(c).
Figure 1.5(d) shows A−B, which is the set A with “holes” where B crossed it.
In set builder notation, we could write A∪B =
©
(x, y) : x ∈ R, y = x
2 or y = x+2
ª
and A −B =
©
(x, x
2
) : x ∈ R−
©
−1,2
ªª.
Exercises for Section 1.5
1. Suppose A =
©
4,3,6,7,1,9
ª
, B =
©
5,6,8,4
ª
and C =
©
5,8,4
ª
. Find:
(a) A ∪B
(b) A ∩B
(c) A −B
(d) A −C
(e) B − A
(f) A ∩C
(g) B ∩C
(h) B ∪C
(i) C −B
2. Suppose A =
©
0,2,4,6,8
ª
, B =
©
1,3,5,7
ª
and C =
©
2,8,4
ª
. Find:
(a) A ∪B
(b) A ∩B
(c) A −B
(d) A −C
(e) B − A
(f) A ∩C
(g) B ∩C
(h) C − A
(i) C −B
3. Suppose A =
©
0,1
ª
and B =
©
1,2
ª
. Find:
(a) (A ×B)∩(B ×B)
(b) (A ×B)∪(B ×B)
(c) (A ×B)−(B ×B)
(d) (A ∩B)× A
(e) (A ×B)∩B
(f) P(A)∩P(B)
(g) P(A)−P(B)
(h) P(A ∩B)
(i) P(A ×B)
4. Suppose A =
©
b, c,d
ª
and B =
©
a,b
ª
. Find:
(a) (A ×B)∩(B ×B)
(b) (A ×B)∪(B ×B)
(c) (A ×B)−(B ×B)
(d) (A ∩B)× A
(e) (A ×B)∩B
(f) P(A)∩P(B)
(g) P(A)−P(B)
(h) P(A ∩B)
(i) P(A)×P(B)
Complement 19
5. Sketch the sets X = [1,3]×[1,3] and Y = [2,4]×[2,4] on the plane R
2
. On separate
drawings, shade in the sets X ∪Y, X ∩Y, X −Y and Y − X. (Hint: X and Y are
Cartesian products of intervals. You may wish to review how you drew sets
like [1,3]×[1,3] in the exercises for Section 1.2.)
6. Sketch the sets X = [−1,3] × [0,2] and Y = [0,3] × [1,4] on the plane R
2
. On
separate drawings, shade in the sets X ∪Y, X ∩Y, X −Y and Y − X.
7. Sketch the sets X =
©
(x, y) ∈ R
2
: x
2 + y
2 ≤ 1
ª
and Y =
©
(x, y) ∈ R
2
: x ≥ 0
ª
on R
2
. On
separate drawings, shade in the sets X ∪Y, X ∩Y, X −Y and Y − X.
8. Sketch the sets X =
©
(x, y) ∈ R
2
: x
2 + y
2 ≤ 1
ª
and Y =
©
(x, y) ∈ R
2
: −1 ≤ y ≤ 0
ª
on R
2
.
On separate drawings, shade in the sets X ∪Y, X ∩Y, X −Y and Y − X.
9. Is the statement (R×Z)∩(Z×R) = Z×Z true or false? What about the statement
(R×Z)∪(Z×R) = R×R?
10. Do you think the statement (R−Z)×N = (R×N)−(Z×N) is true, or false? Justify.
1.6 Complement
This section introduces yet another set operation, called the set complement.
The definition requires the idea of a universal set, which we now discuss.
When dealing with a set, we almost always regard it as a subset
of some larger set. For example, consider the set of prime numbers
P =
©
2,3,5,7,11,13,...ª
. If asked to name some things that are not in P, we
might mention some composite numbers like 4 or 6 or 423. It probably
would not occur to us to say that Vladimir Putin is not in P. True, Vladimir
Putin is not in P, but he lies entirely outside of the discussion of what is
a prime number and what is not. We have an unstated assumption that
P ⊆ N
because N is the most natural setting in which to discuss prime numbers.
In this context, anything not in P should still be in N. This larger set N is
called the universal set or universe for P.
Almost every useful set in mathematics can be regarded as having
some natural universal set. For instance, the unit circle is the set C =
©
(x, y) ∈ R
2
: x
2 + y
2 = 1
ª
, and since all these points are in the plane R
2
it is
natural to regard R
2 as the universal set for C. In the absence of specifics,
if A is a set, then its universal set is often denoted as U. We are now
ready to define the complement operation.
Definition 1.6 Let A be a set with a universal set U. The complement
of A, denoted A, is the set A = U − A.
20 Sets
Example 1.7 If P is the set of prime numbers, then
P = N− P =
©
1,4,6,8,9,10,12,...ª
.
Thus P is the set of composite numbers and 1.
Example 1.8 Let A =
©
(x, x
2
) : x ∈ R
ª
be the graph of the equation y = x
2
.
Figure 1.6(a) shows A in its universal set R
2
. The complement of A is A =
R
2 − A =
©
(x, y) ∈ R
2
: y 6= x
2
ª
, illustrated by the shaded area in Figure 1.6(b).
A A
(a) (b)
Figure 1.6. A set and its complement
Exercises for Section 1.6
1. Let A =
©
4,3,6,7,1,9
ª
and B =
©
5,6,8,4
ª
have universal set U =
©
0,1,2,...,10ª
.
Find:
(a) A
(b) B
(c) A ∩ A
(d) A ∪ A
(e) A − A
(f) A −B
(g) A −B
(h) A ∩B
(i) A ∩B
2. Let A =
©
0,2,4,6,8
ª
and B =
©
1,3,5,7
ª
have universal set U =
©
0,1,2,...,8
ª
. Find:
(a) A
(b) B
(c) A ∩ A
(d) A ∪ A
(e) A − A
(f) A ∪B
(g) A ∩B
(h) A ∩B
(i) A ×B
3. Sketch the set X = [1,3]×[1,2] on the plane R
2
. On separate drawings, shade in
the sets X and X ∩([0,2]×[0,3]).
4. Sketch the set X = [−1,3]×[0,2] on the plane R
2
. On separate drawings, shade
in the sets X and X ∩([−2,4]×[−1,3]).
5. Sketch the set X =
©
(x, y) ∈ R
2
: 1 ≤ x
2 + y
2 ≤ 4
ª
on the plane R
2
. On a separate
drawing, shade in the set X.
6. Sketch the set X =
©
(x, y) ∈ R
2
: y < x
2
ª
on R
2
. Shade in the set X.
Venn Diagrams 21
1.7 Venn Diagrams
In thinking about sets, it is sometimes helpful to draw informal, schematic
diagrams of them. In doing this we often represent a set with a circle
(or oval), which we regard as enclosing all the elements of the set. Such
diagrams can illustrate how sets combine using various operations. For
example, Figures 1.7(a–c) show two sets A and B that overlap in a middle
region. The sets A ∪ B, A ∩ B and A − B are shaded. Such graphical
representations of sets are called Venn diagrams, after their inventor,
British logician John Venn, 1834–1923.
A B A B A B
A ∪B A ∩B A −B
(a) (b) (c)
Figure 1.7. Venn diagrams for two sets
Though you are unlikely to draw Venn diagrams as a part of a proof
of any theorem, you will probably find them to be useful “scratch work”
devices that help you to understand how sets combine, and to develop
strategies for proving certain theorems or solving certain problems. The
remainder of this section uses Venn diagrams to explore how three sets
can be combined using ∪ and ∩.
Let’s begin with the set A ∪B ∪C. Our definitions suggest this should
consist of all elements which are in one or more of the sets A, B and C.
Figure 1.8(a) shows a Venn diagram for this. Similarly, we think of A∩B∩C
as all elements common to each of A, B and C, so in Figure 1.8(b) the
region belonging to all three sets is shaded.
A B A B
C C
A ∪B ∪C A ∩B ∩C
(a) (b)
Figure 1.8. Venn diagrams for three sets
22 Sets
We can also think of A ∩B ∩C as the two-step operation (A ∩B)∩C. In
this expression the set A ∩B is represented by the region common to both
A and B, and when we intersect this with C we get Figure 1.8(b). This is a
visual representation of the fact that A ∩B ∩C = (A ∩B)∩C. Similarly, we
have A ∩B ∩C = A ∩(B ∩C). Likewise, A ∪B ∪C = (A ∪B)∪C = A ∪(B ∪C).
Notice that in these examples, where the expression either contains
only the symbol ∪ or only the symbol ∩, the placement of the parentheses
is irrelevant, so we are free to drop them. It is analogous to the situations
in algebra involving expressions (a + b)+ c = a +(b + c) or (a· b)· c = a·(b · c).
We tend to drop the parentheses and write simply a + b + c or a · b · c. By
contrast, in an expression like (a + b)· c the parentheses are absolutely
essential because (a+ b)· c and a+(b · c) are generally not equal.
Now let’s use Venn diagrams to help us understand the expressions
(A ∪B)∩C and A ∪(B ∩C), which use a mix of ∪ and ∩. Figure 1.9 shows
how to draw a Venn diagram for (A∪B)∩C. In the drawing on the left, the
set A ∪B is shaded with horizontal lines, while C is shaded with vertical
lines. Thus the set (A ∪B)∩C is represented by the cross-hatched region
where A ∪B and C overlap. The superfluous shadings are omitted in the
drawing on the right showing the set (A ∪B)∩C.
A B A B
C C
Figure 1.9. How to make a Venn diagram for (A ∪B)∩C
Now think about A ∪(B ∩C). In Figure 1.10 the set A is shaded with
horizontal lines, and B∩C is shaded with vertical lines. The union A∪(B∩C)
is represented by the totality of all shaded regions, as shown on the right.
A B A B
C C
Figure 1.10. How to make a Venn diagram for A ∪(B ∩C)
Venn Diagrams 23
Compare the diagrams for (A ∪B)∩C and A ∪(B∩C) in Figures 1.9 and
1.10. The fact that the diagrams are different indicates that (A ∪B)∩C 6=
A ∪(B ∩C) in general. Thus an expression such as A ∪B ∩C is absolutely
meaningless because we can’t tell whether it means (A∪B)∩C or A∪(B∩C).
In summary, Venn diagrams have helped us understand the following.
Important Points:
• If an expression involving sets uses only ∪, then parentheses are optional.
• If an expression involving sets uses only ∩, then parentheses are optional.
• If an expression uses both ∪ and ∩, then parentheses are essential.
In the next section we will study types of expressions that use only ∪
or only ∩. These expressions will not require the use of parentheses.
Exercises for Section 1.7
1. Draw a Venn diagram for A.
2. Draw a Venn diagram for B − A.
3. Draw a Venn diagram for (A −B)∩C.
4. Draw a Venn diagram for (A ∪B)−C.
5. Draw Venn diagrams for A∪(B∩C) and (A∪B)∩(A∪C). Based on your drawings,
do you think A ∪(B ∩C) = (A ∪B)∩(A ∪C)?
6. Draw Venn diagrams for A∩(B∪C) and (A∩B)∪(A∩C). Based on your drawings,
do you think A ∩(B ∪C) = (A ∩B)∪(A ∩C)?
7. Suppose sets A and B are in a universal set U. Draw Venn diagrams for A ∩B
and A ∪B. Based on your drawings, do you think it’s true that A ∩B = A ∪B?
8. Suppose sets A and B are in a universal set U. Draw Venn diagrams for A ∪B
and A ∩B. Based on your drawings, do you think it’s true that A ∪B = A ∩B?
9. Draw a Venn diagram for (A ∩B)−C.
10. Draw a Venn diagram for (A −B)∪C.
Following are Venn diagrams for expressions involving sets A,B and C. Write the
corresponding expression.
11. A B
C
12. A B
C
13. A B
C
14. A B
C
24 Sets
1.8 Indexed Sets
When a mathematical problem involves lots of sets, it is often convenient to
keep track of them by using subscripts (also called indices). Thus instead
of denoting three sets as A,B and C, we might instead write them as A1, A2
and A3. These are called indexed sets.
Although we defined union and intersection to be operations that
combine two sets, you by now have no difficulty forming unions and
intersections of three or more sets. (For instance, in the previous section
we drew Venn diagrams for the intersection and union of three sets.)
But let’s take a moment to write down careful definitions. Given sets
A1, A2,..., An, the set A1 ∪ A2 ∪ A3 ∪ ··· ∪ An consists of everything that is
in at least one of the sets Ai
. Likewise A1 ∩ A2 ∩ A3 ∩ ··· ∩ An consists of
everything that is common to all of the sets Ai
. Here is a careful definition.
Definition 1.7 Suppose A1, A2,..., An are sets. Then
A1 ∪ A2 ∪ A3 ∪··· ∪ An =
©
x : x ∈ Ai for at least one set Ai
, for 1 ≤ i ≤ n
ª
,
A1 ∩ A2 ∩ A3 ∩··· ∩ An =
©
x : x ∈ Ai for every set Ai
, for 1 ≤ i ≤ n
ª
.
But if the number n of sets is large, these expressions can get messy.
To overcome this, we now develop some notation that is akin to sigma
notation. You already know that sigma notation is a convenient symbolism
for expressing sums of many numbers. Given numbers a1,a2,a3,...,an,
then
Xn
i=1
ai = a1 + a2 + a3 +··· + an.
Even if the list of numbers is infinite, the sum
X∞
i=1
ai = a1 + a2 + a3 +··· + ai +···
is often still meaningful. The notation we are about to introduce is very
similar to this. Given sets A1, A2, A3, ..., An, we define
[n
i=1
Ai = A1 ∪ A2 ∪ A3 ∪··· ∪ An and \n
i=1
Ai = A1 ∩ A2 ∩ A3 ∩··· ∩ An.
Example 1.9 Suppose A1 =
©
0,2,5
ª
, A2 =
©
1,2,5
ª
and A3 =
©
2,5,7
ª
. Then
[
3
i=1
Ai = A1 ∪ A2 ∪ A3 =
©
0,1,2,5,7
ª
and \
3
i=1
Ai = A1 ∩ A2 ∩ A3 =
©
2,5
ª
.
Indexed Sets 25
This notation is also used when the list of sets A1, A2, A3, ... is infinite:
[∞
i=1
Ai = A1 ∪ A2 ∪ A3 ∪··· = ©
x : x ∈ Ai for at least one set Ai with 1 ≤ i
ª
.
\∞
i=1
Ai = A1 ∩ A2 ∩ A3 ∩··· = ©
x : x ∈ Ai for every set Ai with 1 ≤ i
ª
.
Example 1.10 This example involves the following infinite list of sets.
A1 =
©
−1,0,1
ª
, A2 =
©
−2,0,2
ª
, A3 =
©
−3,0,3
ª
, ··· , Ai =
©
− i,0,i
ª
, ···
Observe that [∞
i=1
Ai = Z, and \∞
i=1
Ai =
©
0
ª
.
Here is a useful twist on our new notation. We can write
[
3
i=1
Ai =
[
i∈{1,2,3}
Ai
,
as this takes the union of the sets Ai for i = 1,2,3. Likewise:
\
3
i=1
Ai =
\
i∈{1,2,3}
Ai
[∞
i=1
Ai =
[
i∈N
Ai
\∞
i=1
Ai =
\
i∈N
Ai
Here we are taking the union or intersection of a collection of sets Ai
where i is an element of some set, be it ©
1,2,3
ª
or N. In general, the way
this works is that we will have a collection of sets Ai for i ∈ I, where I is
the set of possible subscripts. The set I is called an index set.
It is important to realize that the set I need not even consist of integers.
(We could subscript with letters or real numbers, etc.) Since we are
programmed to think of i as an integer, let’s make a slight notational
change: We use α, not i, to stand for an element of I. Thus we are dealing
with a collection of sets Aα for α ∈ I. This leads to the following definition.
Definition 1.8 If we have a set Aα for every α in some index set I, then
[
α∈I
Aα =
©
x : x ∈ Aα for at least one set Aα with α ∈ I
ª
\
α∈I
Aα =
©
x : x ∈ Aα for every set Aα with α ∈ I
ª
.
26 Sets
Example 1.11 Here the sets Aα will be subsets of R
2
. Let I = [0,2] =
©
x ∈ R : 0 ≤ x ≤ 2
ª
. For each number α ∈ I, let Aα =
©
(x,α) : x ∈ R,1 ≤ x ≤ 2
ª
. For
instance, given α = 1 ∈ I the set A1 =
©
(x,1) : x ∈ R,1 ≤ x ≤ 2
ª
is a horizontal
line segment one unit above the x-axis and stretching between x = 1 and
x = 2, as shown in Figure 1.11(a). Likewise Ap
2 =
©
(x,
p
2) : x ∈ R,1 ≤ x ≤ 2
ª
is
a horizontal line segment p
2 units above the x-axis and stretching between
x = 1 and x = 2. A few other of the Aα are shown in Figure 1.11(a), but they
can’t all be drawn because there is one Aα for each of the infinitely many
numbers α ∈ [0,2]. The totality of them covers the shaded region in Figure
1.11(b), so this region is the union of all the Aα. Since the shaded region
is the set ©
(x, y) ∈ R
2
: 1 ≤ x ≤ 2,0 ≤ y ≤ 2
ª
= [1,2]×[0,2], it follows that
[
α∈[0,2]
Aα = [1,2]×[0,2].
Likewise, since there is no point (x, y) that is in every set Aα, we have
\
α∈[0,2]
Aα = ;.
x x
y y
1
1
2
2
1
1
2
2
A0.25
A0.5
A1
A2
Ap
2
[
α∈[0,2]
Aα
(a) (b)
Figure 1.11. The union of an indexed collection of sets
One final comment. Observe that Aα = [1,2]×
©
α
ª
, so the above expressions
can be written as
[
α∈[0,2]
[1,2]×
©
α
ª
= [1,2]×[0,2] and \
α∈[0,2]
[1,2]×
©
α
ª
= ;.
Indexed Sets 27
Example 1.12 In this example our sets are indexed by R
2
. For any
(a,b) ∈ R
2
, let P(a,b) be the following subset of R
3
:
P(a,b) =
©
(x, y, z) ∈ R
3
: ax+ b y = 0
ª
.
In words, given a point (a,b) ∈ R
2
, the corresponding set P(a,b) consists of
all points (x, y, z) in R
3
that satisfy the equation ax+ b y = 0. From previous
math courses you will recognize this as a plane in R
3
, that is, P(a,b)
is a
plane in R
3
. Moreover, since any point (0,0, z) on the z-axis automatically
satisfies ax+ b y = 0, each P(a,b) contains the z-axis.
Figure 1.12 (left) shows the set P(1,2) =
©
(x, y, z) ∈ R
3
: x+2y = 0
ª
. It is the
vertical plane that intersects the x y-plane at the line x+2y = 0.
x
y
z
(−2,1,0)
P(1,2)
Figure 1.12. The sets P(a,b) are vertical planes containing the z-axis.
For any point (a,b) ∈ R
2 with (a,b) 6= (0,0), we can visualize P(a,b) as the
vertical plane that cuts the x y-plane at the line ax + b y = 0. Figure 1.12
(right) shows a few of the P(a,b)
. Since any two such planes intersect
along the z-axis, and because the z-axis is a subset of every P(a,b)
, it is
immediately clear that
\
(a,b)∈R2
P(a,b) =
©
(0,0, z) : z ∈ R
ª
= “the z-axis”.
For the union, note that any given point (a,b, c) ∈ R
3 belongs to the set
P(−b,a) because (x, y, z) = (a,b, c) satisfies the equation −bx + a y = 0. (In fact,
any (a,b, c) belongs to the special set P(0,0) = R
3
, which is the only P(a,b) that
is not a plane.) Since any point in R
3 belongs to some P(a,b) we have
[
(a,b)∈R2
P(a,b) = R
3
.
28 Sets
Exercises for Section 1.8
1. Suppose A1 =
©
a,b,d, e, g, f
ª
, A2 =
©
a,b, c,d
ª
, A3 =
©
b,d,a
ª
and A4 =
©
a,b,h
ª
.
(a) [
4
i=1
Ai = (b) \
4
i=1
Ai =
2. Suppose



A1 =
©
0,2,4,8,10,12,14,16,18,20,22,24ª
,
A2 =
©
0,3,6,9,12,15,18,21,24ª
,
A3 =
©
0,4,8,12,16,20,24ª
.
(a) [
3
i=1
Ai = (b) \
3
i=1
Ai =
3. For each n ∈ N, let An =
©
0,1,2,3,...,n
ª
.
(a) [
i∈N
Ai = (b) \
i∈N
Ai =
4. For each n ∈ N, let An =
©
−2n,0,2n
ª
.
(a) [
i∈N
Ai = (b) \
i∈N
Ai =
5. (a) [
i∈N
[i,i +1] = (b) \
i∈N
[i,i +1] =
6. (a) [
i∈N
[0,i +1] = (b) \
i∈N
[0,i +1] =
7. (a) [
i∈N
R×[i,i +1] = (b) \
i∈N
R×[i,i +1] =
8. (a) [
α∈R
©
α
ª
×[0,1] = (b) \
α∈R
©
α
ª
×[0,1] =
9. (a) [
X∈P(N)
X = (b) \
X∈P(N)
X =
10. (a) [
x∈[0,1]
[x,1]×[0, x
2
] = (b) \
x∈[0,1]
[x,1]×[0, x
2
] =
11. Is \
α∈I
Aα ⊆
[
α∈I
Aα always true for any collection of sets Aα with index set I?
12. If \
α∈I
Aα =
[
α∈I
Aα, what do you think can be said about the relationships between
the sets Aα?
13. If J 6= ; and J ⊆ I, does it follow that [
α∈J
Aα ⊆
[
α∈I
Aα? What about \
α∈J
Aα ⊆
\
α∈I
Aα?
14. If J 6= ; and J ⊆ I, does it follow that \
α∈I
Aα ⊆
\
α∈J
Aα? Explain.
Sets that Are Number Systems 29
1.9 Sets that Are Number Systems
In practice, the sets we tend to be most interested in often have special
properties and structures. For example, the sets Z, Q and R are familiar
number systems: Given such a set, any two of its elements can be added
(or multiplied, etc.) together to produce another element in the set. These
operations obey the familiar commutative, associative and distributive
properties that we all have dealt with for years. Such properties lead to
the standard algebraic techniques for solving equations. Even though we
are concerned with the idea of proof, we will not find it necessary to define,
prove or verify such properties and techniques; we will accept them as the
ground rules upon which our further deductions are based.
We also accept as fact the natural ordering of the elements of N,Z,Q
and R, so that (for example) the meaning of “5 < 7” is understood and does
not need to be justified or explained. Similarly, if x ≤ y and a 6= 0, we know
that ax ≤ a y or ax ≥ a y, depending on whether a is positive or negative.
Another thing that our ingrained understanding of the ordering of
numbers tells us is that any non-empty subset of N has a smallest element.
In other words, if A ⊆ N and A 6= ;, then there is an element x0 ∈ A that is
smaller than every other element of A. (To find it, start at 1, then move
in increments to 2, 3, 4, etc., until you hit a number x0 ∈ A; this is the
smallest element of A.) Similarly, given an integer b, any non-empty subset
A ⊆
©
b, b +1, b +2, b +3,...ª
has a smallest element. This fact is sometimes
called the well-ordering principle. There is no need to remember this
term, but do be aware that we will use this simple, intuitive idea often in
proofs, usually without a second thought.
The well-ordering principle seems innocent enough, but it actually says
something very fundamental and special about the positive integers N.
In fact, the corresponding statement about the positive real numbers
is false: The subset A =
©
1
n
: n ∈ N
ª
of the positive reals has no smallest
element because for any x0 =
1
n
∈ A that we might pick, there is always a
smaller element 1
n+1
∈ A.
One consequence of the well-ordering principle (as we will see below)
is the familiar fact that any integer a can be divided by a non-zero integer
b, resulting in a quotient q and remainder r. For example, b = 3 goes
into a = 17 q = 5 times with remainder r = 2. In symbols, 17 = 5 · 3+2, or
a = qb + r. This significant fact is called the division algorithm.
Fact 1.5 (The Division Algorithm) Given integers a and b with b > 0,
there exist integers q and r for which a = qb + r and 0 ≤ r < b.
30 Sets
Although there is no harm in accepting the division algorithm without
proof, note that it does follow from the well-ordering principle. Here’s how:
Given integers a,b with b > 0, form the set
A =
©
a− xb : x ∈ Z, 0 ≤ a− xbª
⊆
©
0,1,2,3,...ª
.
(For example, if a = 17 and b = 3 then A =
©
2,5,8,11,14,17,20,...ª
is the set
of positive integers obtained by adding multiples of 3 to 17. Notice that
the remainder r = 2 of 17÷3 is the smallest element of this set.) In general,
let r be the smallest element of the set A =
©
a− xb : x ∈ Z, 0 ≤ a− xbª
. Then
r = a − qb for some x = q ∈ Z, so a = qb + r. Moreover, 0 ≤ r < b, as follows.
The fact that r ∈ A ⊆
©
0,1,2,3...ª
implies 0 ≤ r. In addition, it cannot
happen that r ≥ b: If this were the case, then the non-negative number
r−b = (a−qb)−b = a−(q+1)b having form a−xb would be a smaller element
of A than r, and r was explicitly chosen as the smallest element of A.
Since it is not the case that r ≥ b, it must be that r < b. Therefore 0 ≤ r < b.
We’ve now produced a q and an r for which a = qb + r and 0 ≤ r < b.
Moving on, it is time to clarify a small issue. This chapter asserted
that all of mathematics can be described with sets. But at the same time
we maintained that some mathematical entities are not sets. (For instance,
our approach was to say that an individual number, such as 5, is not itself
a set, though it may be an element of a set.)
We have made this distinction because we need a place to stand as
we explore sets: After all, it would appear suspiciously circular to declare
that every mathematical entity is a set, and then go on to define a set as
a collection whose members are sets!
But to most mathematicians, saying “The number 5 is not a set,” is
like saying “The number 5 is not a number.”
The truth is that any number can itself be understood as a set. One
way to do this is to begin with the identification 0 = ;. Then 1 = {;} = {0},
and 2 =
©
;,{;}
ª
= {0,1}, and 3 =
©
;,{;},{;,{;}}ª
= {0,1,2}. In general the
natural number n is the set n = {0,1,2,...,n −1} of the n numbers (which
are themselves sets) that come before it.
We will not undertake such a study here, but the elements of the
number systems Z, Q and R can all be defined in terms of sets. (Even the
operations of addition, multiplication, etc., can be defined in set-theoretic
terms.) In fact, mathematics itself can be regarded as the study of things
that can be described as sets. Any mathematical entity is a set, whether
or not we choose to think of it that way.
Russell’s Paradox 31
1.10 Russell’s Paradox
This section contains some background information that may be interesting,
but is not used in the remainder of the book.
The philosopher and mathematician Bertrand Russell (1872–1970)
did groundbreaking work on the theory of sets and the foundations of
mathematics. He was probably among the first to understand how the
misuse of sets can lead to bizarre and paradoxical situations. He is famous
for an idea that has come to be known as Russell’s paradox.
Russell’s paradox involves the following set of sets:
A =
©
X : X is a set and X ∉ X
ª
. (1.1)
In words, A is the set of all sets that do not include themselves as elements.
Most sets we can think of are in A. The set Z of integers is not an integer
(i.e., Z ∉ Z) and therefore Z ∈ A. Also ; ∈ A because ; is a set and ; ∉ ;.
Is there a set that is not in A? Consider B =
©©©©...ªªªª. Think of B
as a box containing a box, containing a box, containing a box, and so on,
forever. Or a set of Russian dolls, nested one inside the other, endlessly.
The curious thing about B is that it has just one element, namely B itself:
B =
© ©©©...ªªª
| {z }
B
ª
.
Thus B ∈ B. As B does not satisfy B ∉ B, Equation (1.1) says B ∉ A.
Russell’s paradox arises from the question “Is A an element of A?”
For a set X, Equation (1.1) says X ∈ A means the same thing as X ∉ X.
So for X = A, the previous line says A ∈ A means the same thing as A ∉ A.
Conclusion: if A ∈ A is true, then it is false; if A ∈ A is false, then it is true.
This is Russell’s paradox.
Initially Russell’s paradox sparked a crisis among mathematicians.
How could a mathematical statement be both true and false? This seemed
to be in opposition to the very essence of mathematics.
The paradox instigated a very careful examination of set theory and
an evaluation of what can and cannot be regarded as a set. Eventually
mathematicians settled upon a collection of axioms for set theory—the
so-called Zermelo-Fraenkel axioms. One of these axioms is the wellordering
principle of the previous section. Another, the axiom of foundation,
states that no non-empty set X is allowed to have the property X ∩ x 6= ;
for all its elements x. This rules out such circularly defined “sets” as
X =
©
X
ª
introduced above. If we adhere to these axioms, then situations
32 Sets
like Russell’s paradox disappear. Most mathematicians accept all this on
faith and happily ignore the Zermelo-Fraenkel axioms. Paradoxes like
Russell’s do not tend to come up in everyday mathematics—you have to go
out of your way to construct them.
Still, Russell’s paradox reminds us that precision of thought and language
is an important part of doing mathematics. The next chapter deals
with the topic of logic, a codification of thought and language.
Additional Reading on Sets. For a lively account of Bertrand Russell’s
life and work (including his paradox), see the graphic novel Logicomix: An
Epic Search For Truth, by Apostolos Doxiadis and Christos Papadimitriou.
Also see cartoonist Jessica Hagy’s online strip Indexed—it is based largely
on Venn diagrams.
CHAPTER 2
Logic
L
ogic is a systematic way of thinking that allows us to deduce new information
from old information and to parse the meanings of sentences.
You use logic informally in everyday life and certainly also in doing mathematics.
For example, suppose you are working with a certain circle, call it
“Circle X,” and you have available the following two pieces of information.
1. Circle X has radius equal to 3.
2. If any circle has radius r, then its area is πr
2
square units.
You have no trouble putting these two facts together to get:
3. Circle X has area 9π square units.
In doing this you are using logic to combine existing information to
produce new information. Because deducing new information is central to
mathematics, logic plays a fundamental role. This chapter is intended to
give you a sufficient mastery of it.
It is important to realize that logic is a process of deducing information
correctly, not just deducing correct information. For example, suppose we
were mistaken and Circle X actually had a radius of 4, not 3. Let’s look at
our exact same argument again.
1. Circle X has radius equal to 3.
2. If any circle has radius r, then its area is πr
2
square units.
3. Circle X has area 9π square units.
The sentence “Circle X has radius equal to 3.” is now untrue, and so is our
conclusion “Circle X has area 9π square units.” But the logic is perfectly
correct; the information was combined correctly, even if some of it was
false. This distinction between correct logic and correct information is
significant because it is often important to follow the consequences of an
incorrect assumption. Ideally, we want both our logic and our information
to be correct, but the point is that they are different things.
34 Logic
In proving theorems, we apply logic to information that is considered
obviously true (such as “Any two points determine exactly one line.”) or is
already known to be true (e.g., the Pythagorean theorem). If our logic is
correct, then anything we deduce from such information will also be true
(or at least as true as the “obviously true” information we began with).
2.1 Statements
The study of logic begins with statements. A statement is a sentence
or a mathematical expression that is either definitely true or definitely
false. You can think of statements as pieces of information that are either
correct or incorrect. Thus statements are pieces of information that we
might apply logic to in order to produce other pieces of information (which
are also statements).
Example 2.1 Here are some examples of statements. They are all true.
If a circle has radius r, then its area is πr
2
square units.
Every even number is divisible by 2.
2 ∈ Z
p
2 ∉ Z
N ⊆ Z
The set {0,1,2} has three elements.
Some right triangles are isosceles.
Example 2.2 Here are some additional statements. They are all false.
All right triangles are isosceles.
5 = 2
p
2 ∉ R
Z ⊆ N
{0,1,2}∩N = ;
Example 2.3 Here we pair sentences or expressions that are not statements
with similar expressions that are statements.
NOT Statements: Statements:
Add 5 to both sides. Adding 5 to both sides of x−5 = 37 gives x = 42.
Z 42 ∈ Z
42 42 is not a number.
What is the solution of 2x = 84? The solution of 2x = 84 is 42.
Statements 35
Example 2.4 We will often use the letters P, Q, R and S to stand for
specific statements. When more letters are needed we can use subscripts.
Here are more statements, designated with letters. You decide which of
them are true and which are false.
P : For every integer n > 1, the number 2
n −1 is prime.
Q : Every polynomial of degree n has at most n roots.
R : The function f (x) = x
2
is continuous.
S1 : Z ⊆ ;
S2 : {0,−1,−2}∩N = ;
Designating statements with letters (as was done above) is a very useful
shorthand. In discussing a particular statement, such as “The function
f (x) = x
2
is continuous,” it is convenient to just refer to it as R to avoid
having to write or say it many times.
Statements can contain variables. Here is an example.
P : If an integer x is a multiple of 6, then x is even.
This is a sentence that is true. (All multiples of 6 are even, so no matter
which multiple of 6 the integer x happens to be, it is even.) Since the
sentence P is definitely true, it is a statement. When a sentence or
statement P contains a variable such as x, we sometimes denote it as P(x)
to indicate that it is saying something about x. Thus the above statement
can be denoted as
P(x) : If an integer x is a multiple of 6, then x is even.
A statement or sentence involving two variables might be denoted
P(x, y), and so on.
It is quite possible for a sentence containing variables to not be a
statement. Consider the following example.
Q(x) : The integer x is even.
Is this a statement? Whether it is true or false depends on just which
integer x is. It is true if x = 4 and false if x = 7, etc. But without any
stipulations on the value of x it is impossible to say whether Q(x) is true or
false. Since it is neither definitely true nor definitely false, Q(x) cannot be
a statement. A sentence such as this, whose truth depends on the value
of one or more variables, is called an open sentence. The variables in
an open sentence (or statement) can represent any type of entity, not just
numbers. Here is an open sentence where the variables are functions:
36 Logic
R(f , g) : The function f is the derivative of the function g.
This open sentence is true if f (x) = 2x and g(x) = x
2
. It is false if f (x) = x
3
and g(x) = x
2
, etc. We point out that a sentence such as R(f , g) (that
involves variables) can be denoted either as R(f , g) or just R. We use the
expression R(f , g) when we want to emphasize that the sentence involves
variables.
We will have more to say about open sentences later, but for now let’s
return to statements.
Statements are everywhere in mathematics. Any result or theorem
that has been proved true is a statement. The quadratic formula and the
Pythagorean theorem are both statements:
P : The solutions of the equation ax2 + bx + c = 0 are x =
−b ±
p
b
2 −4ac
2a
.
Q : If a right triangle has legs of lengths a and b and hypotenuse of
length c, then a
2 + b
2 = c
2
.
Here is a very famous statement, so famous, in fact, that it has a name.
It is called Fermat’s last theorem after Pierre Fermat, a seventeenthcentury
French mathematician who scribbled it in the margin of a notebook.
R : For all numbers a,b, c,n ∈ N with n > 2, it is the case that a
n+b
n
6= c
n
.
Fermat believed this statement was true. He noted that he could prove
it was true, except his notebook’s margin was too narrow to contain his
proof. It is doubtful that he really had a correct proof in mind, for after his
death generations of brilliant mathematicians tried unsuccessfully to prove
that his statement was true (or false). Finally, in 1993, Andrew Wiles of
Princeton University announced that he had devised a proof. Wiles had
worked on the problem for over seven years, and his proof runs through
hundreds of pages. The moral of this story is that some true statements
are not obviously true.
Here is another statement famous enough to be named. It was first
posed in the eighteenth century by the German mathematician Christian
Goldbach, and thus is called the Goldbach conjecture:
S : Every even integer greater than 2 is a sum of two prime numbers.
You must agree that S is either true or false. It appears to be true, because
when you examine even numbers that are bigger than 2, they seem to
be sums of two primes: 4 = 2+2, 6 = 3+3, 8 = 3+5, 10 = 5+5, 12 = 5+7,
Statements 37
100 = 17+83 and so on. But that’s not to say there isn’t some large even
number that’s not the sum of two primes. If such a number exists, then S
is false. The thing is, in the over 260 years since Goldbach first posed this
problem, no one has been able to determine whether it’s true or false. But
since it is clearly either true or false, S is a statement.
This book is about the methods that can be used to prove that S (or
any other statement) is true or false. To prove that a statement is true,
we start with obvious statements (or other statements that have been
proven true) and use logic to deduce more and more complex statements
until finally we obtain a statement such as S. Of course some statements
are more difficult to prove than others, and S appears to be notoriously
difficult; we will concentrate on statements that are easier to prove.
But the point is this: In proving that statements are true, we use logic
to help us understand statements and to combine pieces of information
to produce new pieces of information. In the next several sections we
explore some standard ways that statements can be combined to form new
statements, or broken down into simpler statements.
Exercises for Section 2.1
Decide whether or not the following are statements. In the case of a statement,
say if it is true or false, if possible.
1. Every real number is an even integer.
2. Every even integer is a real number.
3. If x and y are real numbers and 5x = 5y, then x = y.
4. Sets Z and N.
5. Sets Z and N are infinite.
6. Some sets are finite.
7. The derivative of any polynomial of degree 5 is a polynomial of degree 6.
8. N ∉ P(N).
9. cos(x) = −1
10. (R×N)∩(N×R) = N×N
11. The integer x is a multiple of 7.
12. If the integer x is a multiple of 7, then it is divisible by 7.
13. Either x is a multiple of 7, or it is not.
14. Call me Ishmael.
15. In the beginning, God created the heaven and the earth.
38 Logic
2.2 And, Or, Not
The word “and” can be used to combine two statements to form a new
statement. Consider for example the following sentence.
R1 : The number 2 is even and the number 3 is odd.
We recognize this as a true statement, based on our common-sense understanding
of the meaning of the word “and.” Notice that R1 is made up of
two simpler statements:
P : The number 2 is even.
Q : The number 3 is odd.
These are joined together by the word “and” to form the more complex
statement R1. The statement R1 asserts that P and Q are both true. Since
both P and Q are in fact true, the statement R1 is also true.
Had one or both of P and Q been false, then R1 would be false. For
instance, each of the following statements is false.
R2 : The number 1 is even and the number 3 is odd.
R3 : The number 2 is even and the number 4 is odd.
R4 : The number 3 is even and the number 2 is odd.
From these examples we see that any two statements P and Q can
be combined to form a new statement “P and Q.” In the spirit of using
letters to denote statements, we now introduce the special symbol ∧ to
stand for the word “and.” Thus if P and Q are statements, P ∧Q stands
for the statement “P and Q.” The statement P ∧Q is true if both P and Q
are true; otherwise it is false. This is summarized in the following table,
called a truth table.
P Q P ∧Q
T T T
T F F
F T F
F F F
In this table, T stands for “True,” and F stands for “False.” (T and F are
called truth values.) Each line lists one of the four possible combinations
or truth values for P and Q, and the column headed by P ∧Q tells whether
the statement P ∧Q is true or false in each case.
And, Or, Not 39
Statements can also be combined using the word “or.” Consider the
following four statements.
S1 : The number 2 is even or the number 3 is odd.
S2 : The number 1 is even or the number 3 is odd.
S3 : The number 2 is even or the number 4 is odd.
S4 : The number 3 is even or the number 2 is odd.
In mathematics, the assertion “P or Q” is always understood to mean that
one or both of P and Q is true. Thus statements S1, S2, S3 are all true,
while S4 is false. The symbol ∨ is used to stand for the word “or.” So if P
and Q are statements, P ∨Q represents the statement “P or Q.” Here is
the truth table.
P Q P ∨Q
T T T
T F T
F T T
F F F
It is important to be aware that the meaning of “or” expressed in the
above table differs from the way it is often used in everyday conversation.
For example, suppose a university official makes the following threat:
You pay your tuition or you will be withdrawn from school.
You understand that this means that either you pay your tuition or you
will be withdrawn from school, but not both. In mathematics we never use
the word “or” in such a sense. For us “or” means exactly what is stated
in the table for ∨. Thus P ∨Q being true means one or both of P and Q
is true. If we ever need to express the fact that exactly one of P and Q is
true, we use one of the following constructions:
P or Q, but not both.
Either P or Q.
Exactly one of P or Q.
If the university official were a mathematician, he might have qualified
his statement in one of the following ways.
Pay your tuition or you will be withdrawn from school, but not both.
Either you pay your tuition or you will be withdrawn from school.
40 Logic
To conclude this section, we mention another way of obtaining new
statements from old ones. Given any statement P, we can form the new
statement “It is not true that P.” For example, consider the following
statement.
The number 2 is even.
This statement is true. Now change it by inserting the words “It is not
true that” at the beginning:
It is not true that the number 2 is even.
This new statement is false.
For another example, starting with the false statement “2 ∈ ;,” we get
the true statement “It is not true that 2 ∈ ;.”
We use the symbol ∼ to stand for the words “It’s not true that,” so
∼ P means “It’s not true that P.” We often read ∼ P simply as “not P.”
Unlike ∧ and ∨, which combine two statements, the symbol ∼ just alters
a single statement. Thus its truth table has just two lines, one for each
possible truth value of P.
P ∼ P
T F
F T
The statement ∼ P is called the negation of P. The negation of a
specific statement can be expressed in numerous ways. Consider
P : The number 2 is even.
Here are several ways of expressing its negation.
∼ P : It’s not true that the number 2 is even.
∼ P : It is false that the number 2 is even.
∼ P : The number 2 is not even.
In this section we’ve learned how to combine or modify statements with
the operations ∧, ∨ and ∼. Of course we can also apply these operations
to open sentences or a mixture of open sentences and statements. For
example, (x is an even integer)∧(3 is an odd integer) is an open sentence
that is a combination of an open sentence and a statement.
Conditional Statements 41
Exercises for Section 2.2
Express each statement or open sentence in one of the forms P ∧Q, P ∨Q, or ∼ P.
Be sure to also state exactly what statements P and Q stand for.
1. The number 8 is both even and a power of 2.
2. The matrix A is not invertible.
3. x 6= y 4. x < y 5. y ≥ x
6. There is a quiz scheduled for Wednesday or Friday.
7. The number x equals zero, but the number y does not.
8. At least one of the numbers x and y equals 0.
9. x ∈ A −B 10. x ∈ A ∪B 11. A ∈
©
X ∈ P(N) : |X| < ∞ª
12. Happy families are all alike, but each unhappy family is unhappy in its own
way. (Leo Tolstoy, Anna Karenina)
13. Human beings want to be good, but not too good, and not all the time.
(George Orwell)
14. A man should look for what is, and not for what he thinks should be.
(Albert Einstein)
2.3 Conditional Statements
There is yet another way to combine two statements. Suppose we have in
mind a specific integer a. Consider the following statement about a.
R : If the integer a is a multiple of 6, then a is divisible by 2.
We immediately spot this as a true statement based on our knowledge of
integers and the meanings of the words “if” and “then.” If integer a is a
multiple of 6, then a is even, so therefore a is divisible by 2. Notice that R
is built up from two simpler statements:
P : The integer a is a multiple of 6.
Q : The integer a is divisible by 2.
R : If P, then Q.
In general, given any two statements P and Q whatsoever, we can form
the new statement “If P, then Q.” This is written symbolically as P ⇒ Q
which we read as “If P, then Q,” or “P implies Q.” Like ∧ and ∨, the symbol
⇒ has a very specific meaning. When we assert that the statement P ⇒ Q
is true, we mean that if P is true then Q must also be true. (In other words
we mean that the condition P being true forces Q to be true.) A statement
of form P ⇒ Q is called a conditional statement because it means Q will
be true under the condition that P is true.
42 Logic
You can think of P ⇒ Q as being a promise that whenever P is true, Q
will be true also. There is only one way this promise can be broken (i.e.
be false) and that is if P is true but Q is false. Thus the truth table for
the promise P ⇒ Q is as follows:
P Q P ⇒ Q
T T T
T F F
F T T
F F T
Perhaps you are bothered by the fact that P ⇒ Q is true in the last two
lines of this table. Here’s an example to convince you that the table is
correct. Suppose your professor makes the following promise:
If you pass the final exam, then you will pass the course.
Your professor is making the promise
(You pass the exam) ⇒ (You pass the course).
Under what circumstances did she lie? There are four possible scenarios,
depending on whether or not you passed the exam and whether or not you
passed the course. These scenarios are tallied in the following table.
You pass exam You pass course (You pass exam) ⇒ (You pass course)
T T T
T F F
F T T
F F T
The first line describes the scenario where you pass the exam and you
pass the course. Clearly the professor kept her promise, so we put a T in
the third column to indicate that she told the truth. In the second line,
you passed the exam, but your professor gave you a failing grade in the
course. In this case she broke her promise, and the F in the third column
indicates that what she said was untrue.
Now consider the third row. In this scenario you failed the exam but
still passed the course. How could that happen? Maybe your professor felt
sorry for you. But that doesn’t make her a liar. Her only promise was that
if you passed the exam then you would pass the course. She did not say
Conditional Statements 43
passing the exam was the only way to pass the course. Since she didn’t
lie, then she told the truth, so there is a T in the third column.
Finally look at the fourth row. In that scenario you failed the exam
and you failed the course. Your professor did not lie; she did exactly what
she said she would do. Hence the T in the third column.
In mathematics, whenever we encounter the construction “If P, then
Q” it means exactly what the truth table for ⇒ expresses. But of course
there are other grammatical constructions that also mean P ⇒ Q. Here is
a summary of the main ones.
If P, then Q.
Q if P.
Q whenever P.
Q, provided that P.
Whenever P, then also Q.
P is a sufficient condition for Q.
For Q, it is sufficient that P.
Q is a necessary condition for P.
For P, it is necessary that Q.
P only if Q.



P ⇒ Q
These can all be used in the place of (and mean exactly the same thing as)
“If P, then Q.” You should analyze the meaning of each one and convince
yourself that it captures the meaning of P ⇒ Q. For example, P ⇒ Q means
the condition of P being true is enough (i.e., sufficient) to make Q true;
hence “P is a sufficient condition for Q.”
The wording can be tricky. Often an everyday situation involving a
conditional statement can help clarify it. For example, consider your
professor’s promise:
(You pass the exam) ⇒ (You pass the course)
This means that your passing the exam is a sufficient (though perhaps
not necessary) condition for your passing the course. Thus your professor
might just as well have phrased her promise in one of the following ways.
Passing the exam is a sufficient condition for passing the course.
For you to pass the course, it is sufficient that you pass the exam.
However, when we want to say “If P, then Q” in everyday conversation,
we do not normally express this as “Q is a necessary condition for P” or
“P only if Q.” But such constructions are not uncommon in mathematics.
To understand why they make sense, notice that P ⇒ Q being true means
44 Logic
that it’s impossible that P is true but Q is false, so in order for P to be
true it is necessary that Q is true; hence “Q is a necessary condition for
P.” And this means that P can only be true if Q is true, i.e., “P only if Q.”
Exercises for Section 2.3
Without changing their meanings, convert each of the following sentences into a
sentence having the form “If P, then Q.”
1. A matrix is invertible provided that its determinant is not zero.
2. For a function to be continuous, it is sufficient that it is differentiable.
3. For a function to be integrable, it is necessary that it is continuous.
4. A function is rational if it is a polynomial.
5. An integer is divisible by 8 only if it is divisible by 4.
6. Whenever a surface has only one side, it is non-orientable.
7. A series converges whenever it converges absolutely.
8. A geometric series with ratio r converges if |r| < 1.
9. A function is integrable provided the function is continuous.
10. The discriminant is negative only if the quadratic equation has no real solutions.
11. You fail only if you stop writing. (Ray Bradbury)
12. People will generally accept facts as truth only if the facts agree with what
they already believe. (Andy Rooney)
13. Whenever people agree with me I feel I must be wrong. (Oscar Wilde)
2.4 Biconditional Statements
It is important to understand that P ⇒ Q is not the same as Q ⇒ P. To see
why, suppose that a is some integer and consider the statements
(a is a multiple of 6) ⇒ (a is divisible by 2),
(a is divisible by 2) ⇒ (a is a multiple of 6).
The first statement asserts that if a is a multiple of 6 then a is divisible
by 2. This is clearly true, for any multiple of 6 is even and therefore
divisible by 2. The second statement asserts that if a is divisible by 2 then
it is a multiple of 6. This is not necessarily true, for a = 4 (for instance) is
divisible by 2, yet not a multiple of 6. Therefore the meanings of P ⇒ Q and
Q ⇒ P are in general quite different. The conditional statement Q ⇒ P is
called the converse of P ⇒ Q, so a conditional statement and its converse
express entirely different things.
Biconditional Statements 45
But sometimes, if P and Q are just the right statements, it can happen
that P ⇒ Q and Q ⇒ P are both necessarily true. For example, consider
the statements
(a is even) ⇒ (a is divisible by 2),
(a is divisible by 2) ⇒ (a is even).
No matter what value a has, both of these statements are true. Since both
P ⇒ Q and Q ⇒ P are true, it follows that (P ⇒ Q)∧(Q ⇒ P) is true.
We now introduce a new symbol ⇔ to express the meaning of the
statement (P ⇒ Q)∧(Q ⇒ P). The expression P ⇔ Q is understood to have
exactly the same meaning as (P ⇒ Q)∧(Q ⇒ P). According to the previous
section, Q ⇒ P is read as “P if Q,” and P ⇒ Q can be read as “P only if Q.”
Therefore we pronounce P ⇔ Q as “P if and only if Q.” For example, given
an integer a, we have the true statement
(a is even) ⇔ (a is divisible by 2),
which we can read as “Integer a is even if and only if a is divisible by 2.”
The truth table for ⇔ is shown below. Notice that in the first and last
rows, both P ⇒ Q and Q ⇒ P are true (according to the truth table for ⇒),
so (P ⇒ Q) ∧ (Q ⇒ P) is true, and hence P ⇔ Q is true. However, in the
middle two rows one of P ⇒ Q or Q ⇒ P is false, so (P ⇒ Q)∧(Q ⇒ P) is false,
making P ⇔ Q false.
P Q P ⇔ Q
T T T
T F F
F T F
F F T
Compare the statement R : (a is even) ⇔ (a is divisible by 2) with this
truth table. If a is even then the two statements on either side of ⇔
are true, so according to the table R is true. If a is odd then the two
statements on either side of ⇔ are false, and again according to the table
R is true. Thus R is true no matter what value a has. In general, P ⇔ Q
being true means P and Q are both true or both false.
Not surprisingly, there are many ways of saying P ⇔ Q in English. The
following constructions all mean P ⇔ Q:
46 Logic
P if and only if Q.
P is a necessary and sufficient condition for Q.
For P it is necessary and sufficient that Q.
If P, then Q, and conversely.



P ⇔ Q
The first three of these just combine constructions from the previous
section to express that P ⇒ Q and Q ⇒ P. In the last one, the words “...and
conversely” mean that in addition to “If P, then Q” being true, the converse
statement “If Q, then P” is also true.
Exercises for Section 2.4
Without changing their meanings, convert each of the following sentences into a
sentence having the form “P if and only if Q.”
1. For matrix A to be invertible, it is necessary and sufficient that det(A) 6= 0.
2. If a function has a constant derivative then it is linear, and conversely.
3. If x y = 0 then x = 0 or y = 0, and conversely.
4. If a ∈ Q then 5a ∈ Q, and if 5a ∈ Q then a ∈ Q.
5. For an occurrence to become an adventure, it is necessary and sufficient for
one to recount it. (Jean-Paul Sartre)
2.5 Truth Tables for Statements
You should now know the truth tables for ∧, ∨, ∼, ⇒ and ⇔. They should
be internalized as well as memorized. You must understand the symbols
thoroughly, for we now combine them to form more complex statements.
For example, suppose we want to convey that one or the other of P and
Q is true but they are not both true. No single symbol expresses this, but
we could combine them as
(P ∨Q)∧ ∼ (P ∧Q),
which literally means:
P or Q is true, and it is not the case that both P and Q are true.
This statement will be true or false depending on the truth values of P
and Q. In fact we can make a truth table for the entire statement. Begin
as usual by listing the possible true/false combinations of P and Q on four
lines. The statement (P ∨Q)∧ ∼ (P ∧Q) contains the individual statements
(P ∨Q) and (P ∧Q), so we next tally their truth values in the third and
fourth columns. The fifth column lists values for ∼ (P ∧Q), and these
Truth Tables for Statements 47
are just the opposites of the corresponding entries in the fourth column.
Finally, combining the third and fifth columns with ∧, we get the values
for (P ∨Q)∧ ∼ (P ∧Q) in the sixth column.
P Q (P ∨Q) (P ∧Q) ∼ (P ∧Q) (P ∨Q)∧ ∼ (P ∧Q)
T T T T F F
T F T F T T
F T T F T T
F F F F T F
This truth table tells us that (P ∨Q)∧ ∼ (P ∧Q) is true precisely when
one but not both of P and Q are true, so it has the meaning we intended.
(Notice that the middle three columns of our truth table are just “helper
columns” and are not necessary parts of the table. In writing truth tables,
you may choose to omit such columns if you are confident about your work.)
For another example, consider the following familiar statement concerning
two real numbers x and y:
The product x y equals zero if and only if x = 0 or y = 0.
This can be modeled as (x y = 0) ⇔ (x = 0 ∨ y = 0). If we introduce letters
P,Q and R for the statements x y = 0, x = 0 and y = 0, it becomes P ⇔ (Q∨R).
Notice that the parentheses are necessary here, for without them we
wouldn’t know whether to read the statement as P ⇔ (Q∨R) or (P ⇔ Q)∨R.
Making a truth table for P ⇔ (Q∨R) entails a line for each T/F combination
for the three statements P, Q and R. The eight possible combinations
are tallied in the first three columns of the following table.
P Q R Q ∨ R P ⇔ (Q ∨ R)
T T T T T
T T F T T
T F T T T
T F F F F
F T T T F
F T F T F
F F T T F
F F F F T
We fill in the fourth column using our knowledge of the truth table
for ∨. Finally the fifth column is filled in by combining the first and fourth
columns with our understanding of the truth table for ⇔. The resulting
table gives the true/false values of P ⇔ (Q ∨ R) for all values of P,Q and R.
48 Logic
Notice that when we plug in various values for x and y, the statements
P : x y = 0, Q : x = 0 and R : y = 0 have various truth values, but the statement
P ⇔ (Q ∨R) is always true. For example, if x = 2 and y = 3, then P,Q and R
are all false. This scenario is described in the last row of the table, and
there we see that P ⇔ (Q ∨ R) is true. Likewise if x = 0 and y = 7, then P
and Q are true and R is false, a scenario described in the second line of
the table, where again P ⇔ (Q ∨ R) is true. There is a simple reason why
P ⇔ (Q∨R) is true for any values of x and y: It is that P ⇔ (Q∨R) represents
(x y = 0) ⇔ (x = 0 ∨ y = 0), which is a true mathematical statement. It is
absolutely impossible for it to be false.
This may make you wonder about the lines in the table where P ⇔ (Q∨R)
is false. Why are they there? The reason is that P ⇔ (Q ∨ R) can also
represent a false statement. To see how, imagine that at the end of the
semester your professor makes the following promise.
You pass the class if and only if you get an “A” on the final or you get
a “B” on the final.
This promise has the form P ⇔ (Q ∨R), so its truth values are tabulated in
the above table. Imagine it turned out that you got an “A” on the exam
but failed the course. Then surely your professor lied to you. In fact, P is
false, Q is true and R is false. This scenario is reflected in the sixth line
of the table, and indeed P ⇔ (Q ∨ R) is false (i.e., it is a lie).
The moral of this example is that people can lie, but true mathematical
statements never lie.
We close this section with a word about the use of parentheses. The
symbol ∼ is analogous to the minus sign in algebra. It negates the
expression it precedes. Thus ∼ P ∨Q means (∼ P) ∨Q, not ∼ (P ∨Q). In
∼ (P ∨Q), the value of the entire expression P ∨Q is negated.
Exercises for Section 2.5
Write a truth table for the logical statements in problems 1–9:
1. P ∨(Q ⇒ R)
2. (Q ∨ R) ⇔ (R ∧Q)
3. ∼ (P ⇒ Q)
4. ∼ (P ∨Q)∨(∼ P)
5. (P∧ ∼ P)∨Q
6. (P∧ ∼ P)∧Q
7. (P∧ ∼ P) ⇒ Q
8. P ∨(Q∧ ∼ R)
9. ∼ (∼ P∨ ∼ Q)
10. Suppose the statement ((P ∧Q)∨ R) ⇒ (R ∨ S) is false. Find the truth values of
P,Q,R and S. (This can be done without a truth table.)
11. Suppose P is false and that the statement (R ⇒ S) ⇔ (P ∧Q) is true. Find the
truth values of R and S. (This can be done without a truth table.)
Logical Equivalence 49
2.6 Logical Equivalence
In contemplating the truth table for P ⇔ Q, you probably noticed that
P ⇔ Q is true exactly when P and Q are both true or both false. In other
words, P ⇔ Q is true precisely when at least one of the statements P ∧Q
or ∼ P∧ ∼ Q is true. This may tempt us to say that P ⇔ Q means the same
thing as (P ∧Q)∨(∼ P∧ ∼ Q).
To see if this is really so, we can write truth tables for P ⇔ Q and
(P ∧Q) ∨ (∼ P∧ ∼ Q). In doing this, it is more efficient to put these two
statements into the same table, as follows. (This table has helper columns
for the intermediate expressions ∼ P, ∼ Q, (P ∧Q) and (∼ P∧ ∼ Q).)
P Q ∼ P ∼ Q (P ∧Q) (∼ P∧ ∼ Q) (P ∧Q)∨(∼ P∧ ∼ Q) P ⇔ Q
T T F F T F T T
T F F T F F F F
F T T F F F F F
F F T T F T T T
The table shows that P ⇔ Q and (P ∧Q)∨(∼ P∧ ∼ Q) have the same truth
value, no matter the values P and Q. It is as if P ⇔ Q and (P∧Q)∨(∼ P∧ ∼ Q)
are algebraic expressions that are equal no matter what is “plugged into”
variables P and Q. We express this state of affairs by writing
P ⇔ Q = (P ∧Q)∨(∼ P∧ ∼ Q)
and saying that P ⇔ Q and (P ∧Q)∨(∼ P∧ ∼ Q) are logically equivalent.
In general, two statements are logically equivalent if their truth
values match up line-for-line in a truth table.
Logical equivalence is important because it can give us different (and
potentially useful) ways of looking at the same thing. As an example, the
following table shows that P ⇒ Q is logically equivalent to (∼ Q) ⇒ (∼ P).
P Q ∼ P ∼ Q (∼ Q) ⇒ (∼ P) P ⇒ Q
T T F F T T
T F F T F F
F T T F T T
F F T T T T
The fact that P ⇒ Q = (∼ Q) ⇒ (∼ P) is useful because so many theorems
have the form P ⇒ Q. As we will see in Chapter 5, proving such a theorem
may be easier if we express it in the logically equivalent form (∼ Q) ⇒ (∼ P).
50 Logic
There are two pairs of logically equivalent statements that come up
again and again throughout this book and beyond. They are prevalent
enough to be dignified by a special name: DeMorgan’s laws.
Fact 2.1 (DeMorgan’s Laws)
1. ∼ (P ∧Q) = (∼ P)∨(∼ Q)
2. ∼ (P ∨Q) = (∼ P)∧(∼ Q)
The first of DeMorgan’s laws is verified by the following table. You are
asked to verify the second in one of the exercises.
P Q ∼ P ∼ Q P ∧Q ∼ (P ∧Q) (∼ P)∨(∼ Q)
T T F F T F F
T F F T F T T
F T T F F T T
F F T T F T T
DeMorgan’s laws are actually very natural and intuitive. Consider the
statement ∼ (P ∧Q), which we can interpret as meaning that it is not the
case that both P and Q are true. If it is not the case that both P and Q
are true, then at least one of P or Q is false, in which case (∼ P)∨(∼ Q) is
true. Thus ∼ (P ∧Q) means the same thing as (∼ P)∨(∼ Q).
DeMorgan’s laws can be very useful. Suppose we happen to know that
some statement having form ∼ (P ∨Q) is true. The second of DeMorgan’s
laws tells us that (∼ Q)∧(∼ P) is also true, hence ∼ P and ∼ Q are both true
as well. Being able to quickly obtain such additional pieces of information
can be extremely useful.
Here is a summary of some significant logical equivalences. Those that
are not immediately obvious can be verified with a truth table.
P ⇒ Q = (∼ Q) ⇒ (∼ P) Contrapositive law (2.1)
∼ (P ∧Q) = ∼ P∨ ∼ Q
∼ (P ∨Q) = ∼ P∧ ∼ Q
o
DeMorgan’s laws (2.2)
P ∧Q = Q ∧ P
P ∨Q = Q ∨ P
o
Commutative laws (2.3)
P ∧(Q ∨ R) = (P ∧Q)∨(P ∧ R)
P ∨(Q ∧ R) = (P ∨Q)∧(P ∨ R)
o
Distributive laws (2.4)
P ∧(Q ∧ R) = (P ∧Q)∧ R
P ∨(Q ∨ R) = (P ∨Q)∨ R
o
Associative laws (2.5)
Notice how the distributive law P ∧ (Q ∨ R) = (P ∧Q) ∨ (P ∧ R) has the
same structure as the distributive law p ·(q + r) = p · q + p · r from algebra.
Quantifiers 51
Concerning the associative laws, the fact that P ∧(Q∧R) = (P ∧Q)∧R means
that the position of the parentheses is irrelevant, and we can write this as
P ∧Q ∧ R without ambiguity. Similarly, we may drop the parentheses in
an expression such as P ∨(Q ∨ R).
But parentheses are essential when there is a mix of ∧ and ∨, as in
P ∨(Q ∧ R). Indeed, P ∨(Q ∧ R) and (P ∨Q)∧ R are not logically equivalent.
(See Exercise 13 for Section 2.6, below.)
Exercises for Section 2.6
A. Use truth tables to show that the following statements are logically equivalent.
1. P ∧(Q ∨ R) = (P ∧Q)∨(P ∧ R)
2. P ∨(Q ∧ R) = (P ∨Q)∧(P ∨ R)
3. P ⇒ Q = (∼ P)∨Q
4. ∼ (P ∨Q) = (∼ P)∧(∼ Q)
5. ∼ (P ∨Q ∨ R) = (∼ P)∧(∼ Q)∧(∼ R)
6. ∼ (P ∧Q ∧ R) = (∼ P)∨(∼ Q)∨(∼ R)
7. P ⇒ Q = (P∧ ∼ Q) ⇒ (Q∧ ∼ Q)
8. ∼ P ⇔ Q = (P ⇒∼ Q)∧(∼ Q ⇒ P)
B. Decide whether or not the following pairs of statements are logically equivalent.
9. P ∧Q and ∼ (∼ P∨ ∼ Q)
10. (P ⇒ Q)∨ R and ∼ ((P∧ ∼ Q)∧ ∼ R)
11. (∼ P)∧(P ⇒ Q) and ∼ (Q ⇒ P)
12. ∼ (P ⇒ Q) and P∧ ∼ Q
13. P ∨(Q ∧ R) and (P ∨Q)∧ R
14. P ∧(Q∨ ∼ Q) and (∼ P) ⇒ (Q∧ ∼ Q)
2.7 Quantifiers
Using symbols ∧, ∨, ∼, ⇒ and ⇔, we can deconstruct many English
sentences into a symbolic form. As we have seen, this symbolic form can
help us understand the logical structure of sentences and how different
sentences may actually have the same meaning (as in logical equivalence).
But these symbols alone are not powerful enough to capture the full
meaning of every statement. To help overcome this defect, we introduce
two new symbols that correspond to common mathematical phrases. The
symbol “∀” stands for the phrase “For all” or “For every.” The symbol “∃”
stands for the phrase “There exists a” or “There is a.” Thus the statement
For every n ∈ Z, 2n is even,
can be expressed in either of the following ways:
∀n ∈ Z, 2n is even,
∀n ∈ Z, E(2n).
52 Logic
Likewise, a statement such as
There exists a subset X of N for which |X| = 5.
can be translated as
∃ X,(X ⊆ N)∧(|X| = 5) or ∃ X ⊆ N,|X| = 5 or ∃ X ∈ P(N),|X| = 5.
The symbols ∀ and ∃ are called quantifiers because they refer in some
sense to the quantity (i.e., all or some) of the variable that follows them.
Symbol ∀ is called the universal quantifier and ∃ is called the existential
quantifier. Statements which contain them are called quantified
statements. A statement beginning with ∀ is called a universally quantified
statement, and one beginning with ∃ is called an existentially
quantified statement.
Example 2.5 The following English statements are paired with their
translations into symbolic form.
Every integer that is not odd is even.
∀n ∈ Z,∼ (n is odd ) ⇒ (n is even), or ∀n ∈ Z,∼ O(n) ⇒ E(n).
There is an integer that is not even.
∃n ∈ Z,∼ E(n).
For every real number x, there is a real number y for which y
3 = x.
∀x ∈ R,∃ y ∈ R, y
3 = x.
Given any two rational numbers a and b, it follows that ab is rational.
∀a,b ∈ Q,ab ∈ Q.
Given a set S (such as, but not limited to, N, Z, Q etc.), a quantified
statement of form ∀x ∈ S,P(x) is understood to be true if P(x) is true
for every x ∈ S. If there is at least one x ∈ S for which P(x) is false, then
∀x ∈ S,P(x) is a false statement. Similarly, ∃ x ∈ S,P(x) is true provided that
P(x) is true for at least one element x ∈ S; otherwise it is false. Thus each
statement in Example 2.5 is true. Here are some examples of quantified
statements that are false:
Example 2.6 The following false quantified statements are paired with
their translations.
Every integer is even.
∀n ∈ Z,E(n).
Quantifiers 53
There is an integer n for which n
2 = 2.
∃n ∈ Z,n
2 = 2.
For every real number x, there is a real number y for which y
2 = x.
∀x ∈ R,∃ y ∈ R, y
2 = x.
Given any two rational numbers a and b, it follows that p
ab is rational.
∀a,b ∈ Q,
p
ab ∈ Q.
Example 2.7 When a statement contains two quantifiers you must be
very alert to their order, for reversing the order can change the meaning.
Consider the following statement from Example 2.5.
∀x ∈ R,∃ y ∈ R, y
3 = x.
This statement is true, for no matter what number x is there exists a
number y =
p3
x for which y
3 = x. Now reverse the order of the quantifiers
to get the new statement
∃ y ∈ R,∀x ∈ R, y
3 = x.
This new statement says that there exists a particular number y with
the property that y
3 = x for every real number x. Since no number y can
have this property, the statement is false. The two statements above have
entirely different meanings.
Quantified statements are often misused in casual conversation. Maybe
you’ve heard someone say “All students do not pay full tuition.” when they
mean “Not all students pay full tuition.” While the mistake is perhaps
marginally forgivable in casual conversation, it must never be made in a
mathematical context. Do not say “All integers are not even.” because that
means there are no even integers. Instead, say “Not all integers are even.”
Exercises for Section 2.7
Write the following as English sentences. Say whether they are true or false.
1. ∀x ∈ R, x
2 > 0
2. ∀x ∈ R,∃n ∈ N, x
n ≥ 0
3. ∃a ∈ R,∀x ∈ R,ax = x
4. ∀ X ∈ P(N), X ⊆ R
5. ∀n ∈ N,∃ X ∈ P(N),|X| < n
6. ∃n ∈ N,∀ X ∈ P(N),|X| < n
7. ∀ X ⊆ N,∃n ∈ Z,|X| = n
8. ∀n ∈ Z, ∃ X ⊆ N,|X| = n
9. ∀n ∈ Z,∃m ∈ Z,m = n+5
10. ∃m ∈ Z,∀n ∈ Z,m = n+5
54 Logic
2.8 More on Conditional Statements
It is time to address a very important point about conditional statements
that contain variables. To motivate this, let’s return to the following
example concerning integers x:
(x is a multiple of 6) ⇒ (x is even).
As noted earlier, since every multiple of 6 is even, this is a true statement
no matter what integer x is. We could even underscore this fact by writing
this true statement as
∀x ∈ Z, (x is a multiple of 6) ⇒ (x is even).
But now switch things around to get the different statement
(x is even) ⇒ (x is a multiple of 6).
This is true for some values of x such as −6, 12, 18, etc., but false for
others (such as 2, 4, etc.). Thus we do not have a statement, but rather an
open sentence. (Recall from Section 2.1 that an open sentence is a sentence
whose truth value depends on the value of a certain variable or variables.)
However, by putting a universal quantifier in front we get
∀x ∈ Z, (x is even) ⇒ (x is a multiple of 6),
which is definitely false, so this new expression is a statement, not an open
sentence. In general, given any two open sentences P(x) and Q(x) about
integers x, the expression ∀x ∈ Z, P(x) ⇒ Q(x) is either true or false, so it is
a statement, not an open sentence.
Now we come to the very important point. In mathematics, whenever
P(x) and Q(x) are open sentences concerning elements x in some set S
(depending on context), an expression of form P(x) ⇒ Q(x) is understood
to be the statement ∀x ∈ S, P(x) ⇒ Q(x). In other words, if a conditional
statement is not explicitly quantified then there is an implied universal
quantifier in front of it. This is done because statements of the form
∀x ∈ S, P(x) ⇒ Q(x) are so common in mathematics that we would get tired
of putting the ∀x ∈ S in front of them.
Thus the following sentence is a true statement (as it is true for all x).
If x is a multiple of 6, then x is even.
Translating English to Symbolic Logic 55
Likewise, the next sentence is a false statement (as it is not true for all x).
If x is even, then x is a multiple of 6.
This leads to the following significant interpretation of a conditional
statement, which is more general than (but consistent with) the interpretation
from Section 2.3.
Definition 2.1 If P and Q are statements or open sentences, then
“If P, then Q,”
is a statement. This statement is true if it’s impossible for P to be true
while Q is false. It is false if there is at least one instance in which P is
true but Q is false.
Thus the following are true statements:
If x ∈ R, then x
2 +1 > 0.
If a function f is differentiable on R, then f is continuous on R.
Likewise, the following are false statements:
If p is a prime number, then p is odd. (2 is prime.)
If f is a rational function, then f has an asymptote. (x
2
is rational.)
2.9 Translating English to Symbolic Logic
In writing (and reading) proofs of theorems, we must always be alert to the
logical structure and meanings of the sentences. Sometimes it is necessary
or helpful to parse them into expressions involving logic symbols. This may
be done mentally or on scratch paper, or occasionally even explicitly within
the body of a proof. The purpose of this section is to give you sufficient
practice in translating English sentences into symbolic form so that you
can better understand their logical structure. Here are some examples:
Example 2.8 Consider the Mean Value Theorem from Calculus:
If f is continuous on the interval [a,b] and differentiable on (a,b), then
there is a number c ∈ (a,b) for which f
0
(c) =
f (b)−f (a)
b−a
.
Here is a translation to symbolic form:
³¡
f cont. on [a,b]
¢
∧
¡
f is diff. on (a,b)
¢
´
⇒
³
∃ c ∈ (a,b), f
0
(c) =
f (b)−f (a)
b−a
´
.
56 Logic
Example 2.9 Consider Goldbach’s conjecture, from Section 2.1:
Every even integer greater than 2 is the sum of two primes.
This can be translated in the following ways, where P is the set of prime
numbers and S = {4,6,8,10,...} is the set of even integers greater than 2.
¡
n ∈ S
¢
⇒
¡
∃ p, q ∈ P, n = p + q
¢
∀ n ∈ S, ∃ p, q ∈ P, n = p + q
These translations of Goldbach’s conjecture illustrate an important
point. The first has the basic structure (n ∈ S) ⇒ Q(n) and the second has
structure ∀ n ∈ S, Q(n), yet they have exactly the same meaning. This is
significant. Every universally quantified statement can be expressed as a
conditional statement.
Fact 2.2 Suppose S is a set and Q(x) is a statement about x for each
x ∈ S. The following statements mean the same thing:
∀ x ∈ S, Q(x)
(x ∈ S) ⇒ Q(x).
This fact is significant because so many theorems have the form of
a conditional statement. (The Mean Value Theorem is an example!) In
proving a theorem we have to think carefully about what it says. Sometimes
a theorem will be expressed as a universally quantified statement but it will
be more convenient to think of it as a conditional statement. Understanding
the above fact allows us to switch between the two forms.
We close this section with some final points. In translating a statement,
be attentive to its intended meaning. Don’t jump into, for example,
automatically replacing every “and” with ∧ and “or” with ∨. An example:
At least one of the integers x and y is even.
Don’t be led astray by the presence of the word “and.” The meaning of
the statement is that one or both of the numbers is even, so it should be
translated with “or,” not “and”:
(x is even) ∨ (y is even).
Finally, the logical meaning of “but” can be captured by “and.” The
sentence “The integer x is even, but the integer y is odd,” is translated as
(x is even) ∧ (y is odd).
Negating Statements 57
Exercises for Section 2.9
Translate each of the following sentences into symbolic logic.
1. If f is a polynomial and its degree is greater than 2, then f
0
is not constant.
2. The number x is positive but the number y is not positive.
3. If x is prime then p
x is not a rational number.
4. For every prime number p there is another prime number q with q > p.
5. For every positive number ε, there is a positive number δ for which |x− a| < δ
implies |f (x)− f (a)| < ε.
6. For every positive number ε there is a positive number M for which |f (x)−b| < ε,
whenever x > M.
7. There exists a real number a for which a+ x = x for every real number x.
8. I don’t eat anything that has a face.
9. If x is a rational number and x 6= 0, then tan(x) is not a rational number.
10. If sin(x) < 0, then it is not the case that 0 ≤ x ≤ π.
11. There is a Providence that protects idiots, drunkards, children and the United
States of America. (Otto von Bismarck)
12. You can fool some of the people all of the time, and you can fool all of the people
some of the time, but you can’t fool all of the people all of the time. (Abraham
Lincoln)
13. Everything is funny as long as it is happening to somebody else. (Will Rogers)
2.10 Negating Statements
Given a statement R, the statement ∼ R is called the negation of R. If R
is a complex statement, then it is often the case that its negation ∼ R can
be written in a simpler or more useful form. The process of finding this
form is called negating R. In proving theorems it is often necessary to
negate certain statements. We now investigate how to do this.
We have already examined part of this topic. DeMorgan’s laws
∼ (P ∧Q) = (∼ P)∨(∼ Q) (2.6)
∼ (P ∨Q) = (∼ P)∧(∼ Q) (2.7)
(from Section 2.6) can be viewed as rules that tell us how to negate the
statements P ∧Q and P ∨Q. Here are some examples that illustrate how
DeMorgan’s laws are used to negate statements involving “and” or “or.”
58 Logic
Example 2.10 Consider negating the following statement.
R : You can solve it by factoring or with the quadratic formula.
Now, R means (You can solve it by factoring) ∨ (You can solve it with Q.F.),
which we will denote as P ∨Q. The negation of this is
∼ (P ∨Q) = (∼ P)∧(∼ Q).
Therefore, in words, the negation of R is
∼ R : You can’t solve it by factoring and you can’t solve it with
the quadratic formula.
Maybe you can find ∼ R without invoking DeMorgan’s laws. That is good;
you have internalized DeMorgan’s laws and are using them unconsciously.
Example 2.11 We will negate the following sentence.
R : The numbers x and y are both odd.
This statement means (x is odd) ∧ (y is odd), so its negation is
∼
¡
(x is odd)∧(y is odd)
¢
= ∼ (x is odd) ∨ ∼ (y is odd)
= (x is even)∨(y is even).
Therefore the negation of R can be expressed in the following ways:
∼ R : The number x is even or the number y is even.
∼ R : At least one of x and y is even.
Now let’s move on to a slightly different kind of problem. It’s often
necessary to find the negations of quantified statements. For example,
consider ∼ (∀x ∈ N, P(x)). Reading this in words, we have the following:
It is not the case that P(x) is true for all natural numbers x.
This means P(x) is false for at least one x. In symbols, this is ∃ x ∈ N, ∼ P(x).
Thus ∼ (∀x ∈ N, P(x)) = ∃ x ∈ N, ∼ P(x). Similarly, you can reason out that
∼ (∃ x ∈ N, P(x)) = ∀x ∈ N, ∼ P(x). In general:
∼ (∀x ∈ S, P(x)) = ∃ x ∈ S, ∼ P(x), (2.8)
∼ (∃ x ∈ S, P(x)) = ∀x ∈ S, ∼ P(x). (2.9)
Negating Statements 59
Example 2.12 Consider negating the following statement.
R : The square of every real number is non-negative.
Symbolically, R can be expressed as ∀x ∈ R, x
2 ≥ 0, and thus its negation is
∼ (∀x ∈ R, x
2 ≥ 0) = ∃ x ∈ R, ∼ (x
2 ≥ 0) = ∃ x ∈ R, x
2 < 0. In words, this is
∼ R : There exists a real number whose square is negative.
Observe that R is true and ∼ R is false. You may be able to get ∼ R
immediately, without using Equation (2.8) as we did above. If so, that is
good; if not, you will probably be there soon.
If a statement has multiple quantifiers, negating it will involve several
iterations of Equations (2.8) and (2.9). Consider the following:
S : For every real number x there is a real number y for which y
3 = x.
This statement asserts any real number x has a cube root y, so it’s true.
Symbolically S can be expressed as
∀x ∈ R,∃ y ∈ R, y
3 = x.
Let’s work out the negation of this statement.
∼ (∀x ∈ R,∃ y ∈ R, y
3 = x) = ∃ x ∈ R,∼ (∃ y ∈ R, y
3 = x)
= ∃ x ∈ R,∀ y ∈ R, ∼ (y
3 = x)
= ∃ x ∈ R,∀ y ∈ R, y
3
6= x.
Therefore the negation is the following (false) statement.
∼ S : There is a real number x for which y
3
6= x for all real numbers y.
In writing proofs you will sometimes have to negate a conditional
statement P ⇒ Q. The remainder of this section describes how to do this.
To begin, look at the expression ∼ (P ⇒ Q), which literally says “P ⇒ Q is
false.” You know from the truth table for ⇒ that the only way that P ⇒ Q
can be false is if P is true and Q is false. Therefore ∼ (P ⇒ Q) = P∧ ∼ Q.
∼ (P ⇒ Q) = P∧ ∼ Q (2.10)
(In fact, in Exercise 12 of Section 2.6, you used a truth table to verify that
these two statements are indeed logically equivalent.)
60 Logic
Example 2.13 Negate the following statement about a particular (i.e.,
constant) number a.
R : If a is odd then a
2
is odd.
Using Equation (2.10), we get the following negation.
∼ R : a is odd and a
2
is not odd.
Example 2.14 This example is like the previous one, but the constant a
is replaced by a variable x. We will negate the following statement.
R : If x is odd then x
2
is odd.
As discussed in Section 2.8, we interpret this as the universally quantified
statement
R : ∀x ∈ Z, (x odd) ⇒ (x
2 odd).
By Equations (2.8) and (2.10), we get the following negation for R.
∼
¡
∀x ∈ Z, (x odd) ⇒ (x
2 odd)
¢
= ∃x ∈ Z,∼
¡
(x odd) ⇒ (x
2 odd)
¢
= ∃x ∈ Z,(x odd)∧ ∼ (x
2 odd).
Translating back into words, we have
∼ R : There is an odd integer x whose square is not odd.
Notice that R is true and ∼ R is false.
The above Example 2.14 showed how to negate a conditional statement
P(x) ⇒ Q(x). This type of problem can sometimes be embedded in more
complex negation. See Exercise 5 below (and its solution).
Exercises for Section 2.10
Negate the following sentences.
1. The number x is positive, but the number y is not positive.
2. If x is prime, then p
x is not a rational number.
3. For every prime number p, there is another prime number q with q > p.
4. For every positive number ε, there is a positive number δ such that |x − a| < δ
implies |f (x)− f (a)| < ε.
5. For every positive number ε, there is a positive number M for which |f (x)−b| < ε
whenever x > M.
6. There exists a real number a for which a+ x = x for every real number x.
Logical Inference 61
7. I don’t eat anything that has a face.
8. If x is a rational number and x 6= 0, then tan(x) is not a rational number.
9. If sin(x) < 0, then it is not the case that 0 ≤ x ≤ π.
10. If f is a polynomial and its degree is greater than 2, then f
0
is not constant.
11. You can fool all of the people all of the time.
12. Whenever I have to choose between two evils, I choose the one I haven’t tried
yet. (Mae West)
2.11 Logical Inference
Suppose we know that a statement of form P ⇒ Q is true. This tells us
that whenever P is true, Q will also be true. By itself, P ⇒ Q being true
does not tell us that either P or Q is true (they could both be false, or P
could be false and Q true). However if in addition we happen to know
that P is true then it must be that Q is true. This is called a logical
inference: Given two true statements we can infer that a third statement
is true. In this instance true statements P ⇒ Q and P are “added together”
to get Q. This is described below with P ⇒ Q and P stacked one atop the
other with a line separating them from Q. The intended meaning is that
P ⇒ Q combined with P produces Q.
P ⇒ Q
P
Q
P ⇒ Q
∼ Q
∼ P
P ∨Q
∼ P
Q
Two other logical inferences are listed above. In each case you should
convince yourself (based on your knowledge of the relevant truth tables)
that the truth of the statements above the line forces the statement below
the line to be true.
Following are some additional useful logical inferences. The first
expresses the obvious fact that if P and Q are both true then the statement
P ∧Q will be true. On the other hand, P ∧Q being true forces P (also Q)
to be true. Finally, if P is true, then P ∨Q must be true, no matter what
statement Q is.
P
Q
P ∧Q
P ∧Q
P
P
P ∨Q
These inferences are so intuitively obvious that they scarcely need to
be mentioned. However, they represent certain patterns of reasoning that
we will frequently apply to sentences in proofs, so we should be cognizant
of the fact that we are using them.
62 Logic
2.12 An Important Note
It is important to be aware of the reasons that we study logic. There
are three very significant reasons. First, the truth tables we studied tell
us the exact meanings of the words such as “and,” “or,” “not” and so on.
For instance, whenever we use or read the “If..., then” construction in
a mathematical context, logic tells us exactly what is meant. Second,
the rules of inference provide a system in which we can produce new
information (statements) from known information. Finally, logical rules
such as DeMorgan’s laws help us correctly change certain statements into
(potentially more useful) statements with the same meaning. Thus logic
helps us understand the meanings of statements and it also produces new
meaningful statements.
Logic is the glue that holds strings of statements together and pins down
the exact meaning of certain key phrases such as the “If..., then” or “For
all” constructions. Logic is the common language that all mathematicians
use, so we must have a firm grip on it in order to write and understand
mathematics.
But despite its fundamental role, logic’s place is in the background of
what we do, not the forefront. From here on, the beautiful symbols ∧, ∨,
⇒, ⇔, ∼, ∀ and ∃ are rarely written. But we are aware of their meanings
constantly. When reading or writing a sentence involving mathematics we
parse it with these symbols, either mentally or on scratch paper, so as to
understand the true and unambiguous meaning.
CHAPTER 3
Counting
I
t may seem peculiar that a college-level text has a chapter on counting.
At its most basic level, counting is a process of pointing to each object
in a collection and calling off “one, two, three,...” until the quantity of
objects is determined. How complex could that be? Actually, counting
can become quite subtle, and in this chapter we explore some of its more
sophisticated aspects. Our goal is still to answer the question “How many?”
but we introduce mathematical techniques that bypass the actual process
of counting individual objects.
Almost every branch of mathematics uses some form of this “sophisticated
counting.” Many such counting problems can be modeled with the
idea of a list, so we start there.
3.1 Counting Lists
A list is an ordered sequence of objects. A list is denoted by an opening
parenthesis, followed by the objects, separated by commas, followed by a
closing parenthesis. For example (a,b, c,d, e) is a list consisting of the first
five letters of the English alphabet, in order. The objects a,b, c,d, e are
called the entries of the list; the first entry is a, the second is b, and so
on. If the entries are rearranged we get a different list, so, for instance,
(a,b, c,d, e) 6= (b,a, c,d, e).
A list is somewhat like a set, but instead of being a mere collection of
objects, the entries of a list have a definite order. Note that for sets we
have
©
a,b, c,d, e
ª
=
©
b,a, c,d, e
ª
,
but—as noted above—the analogous equality for lists does not hold.
Unlike sets, lists are allowed to have repeated entries. For example
(5,3,5,4,3,3) is a perfectly acceptable list, as is (S,O,S). The number of
entries in a list is called its length. Thus (5,3,5,4,3,3) has length six, and
(S,O,S) has length three.
64 Counting
Occasionally we may get sloppy and write lists without parentheses
and commas; for instance, we may express (S,O,S) as SOS if there is no
danger of confusion. But be alert that doing this can lead to ambiguity.
Is it reasonable that (9,10,11) should be the same as 91011? If so, then
(9,10,11) = 91011 = (9,1,0,1,1), which makes no sense. We will thus almost
always adhere to the parenthesis/comma notation for lists.
Lists are important because many real-world phenomena can be described
and understood in terms of them. For example, your phone number
(with area code) can be identified as a list of ten digits. Order is essential,
for rearranging the digits can produce a different phone number. A byte is
another important example of a list. A byte is simply a length-eight list of
0’s and 1’s. The world of information technology revolves around bytes.
To continue our examples of lists, (a,15) is a list of length two. Likewise
(0,(0,1,1)) is a list of length two whose second entry is a list of length three.
The list (N,Z,R) has length three, and each of its entries is a set. We
emphasize that for two lists to be equal, they must have exactly the same
entries in exactly the same order. Consequently if two lists are equal, then
they must have the same length. Said differently, if two lists have different
lengths, then they are not equal. For example, (0,0,0,0,0,0) 6= (0,0,0,0,0).
For another example note that
( g, r, o, c, e, r, y, l,i, s,t)
bread
milk
eggs
mustard
coffee
6=
¡ ¢
because the list on the left has length eleven but the list on the right has
just one entry (a piece of paper with some words on it).
There is one very special list which has no entries at all. It is called
the empty list, and is denoted (). It is the only list whose length is zero.
One often needs to count up the number of possible lists that satisfy
some condition or property. For example, suppose we need to make a list of
length three having the property that the first entry must be an element
of the set ©
a,b, c
ª
, the second entry must be in ©
5,7
ª
and the third entry
must be in ©
a, x
ª
. Thus (a,5,a) and (b,5,a) are two such lists. How many
such lists are there all together? To answer this question, imagine making
the list by selecting the first element, then the second and finally the third.
This is described in Figure 3.1. The choices for the first list entry are
a,b or c, and the left of the diagram branches out in three directions, one
for each choice. Once this choice is made there are two choices (5 or 7)
for the second entry, and this is described graphically by two branches
from each of the three choices for the first entry. This pattern continues
Counting Lists 65
for the choice for the third entry, which is either a or x. Thus, in the
diagram there are 3·2·2 = 12 paths from left to right, each corresponding
to a particular choice for each entry in the list. The corresponding lists
are tallied at the far-right end of each path. So, to answer our original
question, there are 12 possible lists with the stated properties.
first choice second choice third choice
Resulting list
a
b
c
5
7
5
7
5
7
a
x
a
x
a
x
x
a
x
a
x
a
(a,5,a)
(a,5, x)
(a,7,a)
(a,7, x)
(b,5,a)
(b,5, x)
(b,7,a)
(b,7, x)
(c,5,a)
(c,5, x)
(c,7,a)
(c,7, x)
Figure 3.1. Constructing lists of length 3
We summarize the type of reasoning used above in an important fact
called the multiplication principle.
Fact 3.1 (Multiplication Principle) Suppose in making a list of length
n there are a1 possible choices for the first entry, a2 possible choices for
the second entry, a3 possible choices for the third entry and so on. Then
the total number of different lists that can be made this way is the product
a1 · a2 · a3 ····· an.
So, for instance, in the above example we had a1 = 3,a2 = 2 and a3 = 2,
so the total number of lists was a1 · a2 · a3 = 3·2·2 = 12. Now let’s look at
some additional examples of how the multiplication principle can be used.
Example 3.1 A standard license plate consists of three letters followed
by four numbers. For example, JRB-4412 and MMX-8901 are two standard
license plates. (Vanity plates such as LV2COUNT are not included among
the standard plates.) How many different standard license plates are
possible?
66 Counting
To answer this question, note that any standard license plate such as
JRB-4412 corresponds to a length-7 list (J,R,B,4,4,1,2), so the question
can be answered by counting how many such lists are possible. We use the
multiplication principle. There are a1 = 26 possibilities (one for each letter
of the alphabet) for the first entry of the list. Similarly, there are a2 = 26
possibilities for the second entry and a3 = 26 possibilities for the third
entry. There are a4 = 10 possibilities for the fourth entry, and likewise
a5 = a6 = a7 = 10. Therefore there are a total of a1 · a2 · a3 · a4 · a5 · a6 · a7 =
26·26·26·10·10·10·10 = 175,760,000 possible standard license plates.
There are two types of list-counting problems. On one hand, there are
situations in which the same symbol or symbols may appear multiple times
in different entries of the list. For example, license plates or telephone
numbers can have repeated symbols. The sequence CCX-4144 is a perfectly
valid license plate in which the symbols C and 4 appear more than once.
On the other hand, for some lists repeated symbols do not make sense or
are not allowed. For instance, imagine drawing 5 cards from a standard
52-card deck and laying them in a row. Since no 2 cards in the deck
are identical, this list has no repeated entries. We say that repetition is
allowed in the first type of list and repetition is not allowed in the second
kind of list. (Often we call a list in which repetition is not allowed a
non-repetitive list.) The following example illustrates the difference.
Example 3.2 Consider making lists from symbols A, B, C, D, E, F, G.
(a) How many length-4 lists are possible if repetition is allowed?
(b) How many length-4 lists are possible if repetition is not allowed?
(c) How many length-4 lists are possible if repetition is not allowed and
the list must contain an E?
(d) How many length-4 lists are possible if repetition is allowed and the
list must contain an E?
Solutions:
(a) Imagine the list as containing four boxes that we fill with selections
from the letters A,B,C,D,E,F and G, as illustrated below.
( , , , )
7 choices
7 choices
7 choices
7 choices
There are seven possibilities for the contents of each box, so the total
number of lists that can be made this way is 7·7·7·7 = 2401.
Counting Lists 67
(b) This problem is the same as the previous one except that repetition is
not allowed. We have seven choices for the first box, but once it is filled
we can no longer use the symbol that was placed in it. Hence there are
only six possibilities for the second box. Once the second box has been
filled we have used up two of our letters, and there are only five left to
choose from in filling the third box. Finally, when the third box is filled
we have only four possible letters for the last box.
( , , , )
7 choices
6 choices
5 choices
4 choices
Thus the answer to our question is that there are 7·6·5·4 = 840 lists in
which repetition does not occur.
(c) We are asked to count the length-4 lists in which repetition is not
allowed and the symbol E must appear somewhere in the list. Thus E
occurs once and only once in each such list. Let us divide these lists into
four categories depending on whether the E occurs as the first, second,
third or fourth entry. These four types of lists are illustrated below.
E , , , , E , , , , E , , , , E
Type 1 Type 2 Type 3 Type 4
( ) ( ) ( ) ( )
6 choices 6 choices 6 choices 6 choices
5 choices 5 choices 5 choices 5 choices
4 choices 4 choices 4 choices 4 choices
Consider lists of the first type, in which the E appears in the first entry.
We have six remaining choices (A,B,C,D,F or G) for the second entry, five
choices for the third entry and four choices for the fourth entry. Hence
there are 6·5·4 = 120 lists having an E in the first entry. As indicated
in the above diagram, there are also 6·5·4 = 120 lists having an E in the
second, third or fourth entry. Thus there are 120+120+120+120 = 480
such lists all together.
(d) Now we must find the number of length-four lists where repetition
is allowed and the list must contain an E. Our strategy is as follows.
By Part (a) of this exercise there are 7 · 7 · 7 · 7 = 7
4 = 2401 lists where
repetition is allowed. Obviously this is not the answer to our current
question, for many of these lists contain no E. We will subtract from
2401 the number of lists that do not contain an E. In making a list that
does not contain an E, we have six choices for each list entry (because
68 Counting
we can choose any one of the six letters A,B,C,D,F or G). Thus there
are 6·6·6·6 = 6
4 = 1296 lists that do not have an E. Therefore the final
answer to our question is that there are 2401 − 1296 = 1105 lists with
repetition allowed that contain at least one E.
Perhaps you wondered if Part (d) of Example 3.2 could be solved with
a setup similar to that of Part (c). Let’s try doing it that way. We want
to count the length-4 lists (with repetition allowed) that contain at least
one E. The following diagram is adapted from Part (c), the only difference
being that there are now seven choices in each slot because we are allowed
to repeat any of the seven letters.
E , , , , E , , , , E , , , , E
Type 1 Type 2 Type 3 Type 4
( ) ( ) ( ) ( )
7 choices 7 choices 7 choices 7 choices
7 choices 7 choices 7 choices 7 choices
7 choices 7 choices 7 choices 7 choices
This gives a total of 7
3 + 7
3 + 7
3 + 7
3 = 1372 lists, an answer that is
substantially larger than the (correct) value of 1105 that we got in our
solution to Part (d) above. It is not hard to see what went wrong. The
list (E,E, A,B) is of type 1 and type 2, so it got counted twice. Similarly
(E,E,C,E) is of type 1, 3 and 4, so it got counted three times. In fact, you
can find many similar lists that were counted multiple times.
In solving counting problems, we must always be careful to avoid this
kind of double-counting or triple-counting, or worse.
Exercises for Section 3.1
Note: A calculator may be helpful for some of the exercises in this chapter. This
is the only chapter for which a calculator may be helpful. (As for the exercises in
the other chapters, a calculator makes them harder.)
1. Consider lists made from the letters T,H,E,O,R,Y, with repetition allowed.
(a) How many length-4 lists are there?
(b) How many length-4 lists are there that begin with T ?
(c) How many length-4 lists are there that do not begin with T ?
2. Airports are identified with 3-letter codes. For example, the Richmond, Virginia
airport has the code RIC, and Portland, Oregon has PDX. How many different
3-letter codes are possible?
3. How many lists of length 3 can be made from the symbols A,B,C,D,E,F if...
Counting Lists 69
(a) ... repetition is allowed.
(b) ... repetition is not allowed.
(c) ... repetition is not allowed and the list must contain the letter A.
(d) ... repetition is allowed and the list must contain the letter A.
4. Five cards are dealt off of a standard 52-card deck and lined up in a row. How
many such line-ups are there in which all 5 cards are of the same suit?
5. Five cards are dealt off of a standard 52-card deck and lined up in a row. How
many such line-ups are there in which all 5 cards are of the same color (i.e.,
all black or all red)?
6. Five cards are dealt off of a standard 52-card deck and lined up in a row. How
many such line-ups are there in which exactly one of the 5 cards is a queen?
7. This problem involves 8-digit binary strings such as 10011011 or 00001010
(i.e., 8-digit numbers composed of 0’s and 1’s).
(a) How many such strings are there?
(b) How many such strings end in 0?
(c) How many such strings have the property that their second and fourth
digits are 1’s?
(d) How many such strings have the property that their second or fourth digits
are 1’s?
8. This problem concerns lists made from the symbols A,B,C,D,E.
(a) How many such length-5 lists have at least one letter repeated?
(b) How many such length-6 lists have at least one letter repeated?
9. This problem concerns 4-letter codes made from the letters A,B,C,D,...,Z.
(a) How many such codes can be made?
(b) How many such codes have no two consecutive letters the same?
10. This problem concerns lists made from the letters A,B,C,D,E,F,G,H,I,J.
(a) How many length-5 lists can be made from these letters if repetition is not
allowed and the list must begin with a vowel?
(b) How many length-5 lists can be made from these letters if repetition is not
allowed and the list must begin and end with a vowel?
(c) How many length-5 lists can be made from these letters if repetition is not
allowed and the list must contain exactly one A?
11. This problem concerns lists of length 6 made from the letters A,B,C,D,E,F,G,H.
How many such lists are possible if repetition is not allowed and the list
contains two consecutive vowels?
12. Consider the lists of length six made with the symbols P, R, O, F, S, where
repetition is allowed. (For example, the following is such a list: (P,R,O,O,F,S).)
How many such lists can be made if the list must end in an S and the symbol
O is used more than once?
70 Counting
3.2 Factorials
In working the examples from Section 3.1, you may have noticed that often
we need to count the number of non-repetitive lists of length n that are
made from n symbols. In fact, this particular problem occurs with such
frequency that a special idea, called a factorial, is introduced to handle it.
The table below motivates this idea. The first column lists successive
integer values n (beginning with 0) and the second column contains a
set ©
A,B,···ª
of n symbols. The third column contains all the possible
non-repetitive lists of length n which can be made from these symbols.
Finally, the last column tallies up how many lists there are of that type.
Notice that when n = 0 there is only one list of length 0 that can be made
from 0 symbols, namely the empty list ( ). Thus the value 1 is entered in
the last column of that row.
n Symbols Non-repetitive lists of length n made from the symbols n!
0
©ª ( ) 1
1
©
A
ª
(A) 1
2
©
A,B
ª
(A,B),(B, A) 2
3
©
A,B,C
ª
(A,B,C),(A,C,B),(B,C, A),(B, A,C),(C, A,B),(C,B, A) 6
4
©
A,B,C,D
ª
(A,B,C,D),(A,B,D,C),(A,C,B,D),(A,C,D,B),(A,D,B,C),(A,D,C,B)
(B,A,C,D),(B,A,D,C),(B,C,A,D),(B,C,D,A),(B,D, A,C),(B,D,C,A)
(C,A,B,D),(C,A,D,B),(C,B,A,D),(C,B,D,A),(C,D,A,B),(C,D,B,A)
(D,A,B,C),(D,A,C,B),(D,B,A,C),(D,B,C,A),(D,C,A,B),(D,C,B,A)
24
.
.
.
.
.
.
.
.
.
.
.
.
For n > 0, the number that appears in the last column can be computed
using the multiplication principle. The number of non-repetitive lists of
length n that can be made from n symbols is n(n−1)(n−2)···3·2·1. Thus, for
instance, the number in the last column of the row for n = 4 is 4·3·2·1 = 24.
The number that appears in the last column of Row n is called the
factorial of n. It is denoted as n! (read “n factorial”). Here is the definition:
Definition 3.1 If n is a non-negative integer, then the factorial of n,
denoted n!, is the number of non-repetitive lists of length n that can
be made from n symbols. Thus 0! = 1 and 1! = 1. If n > 1, then n! =
n(n−1)(n−2)···3·2·1.
Factorials 71
It follows that 0! = 1
1! = 1
2! = 2·1 = 2
3! = 3·2·1 = 6
4! = 4·3·2·1 = 24
5! = 5·4·3·2·1 = 120
6! = 6·5·4·3·2·1 = 720, and so on.
Students are often tempted to say 0! = 0, but this is wrong. The correct
value is 0! = 1, as the above definition and table tell us. Here is another
way to see that 0! must equal 1: Notice that 5! = 5·4·3·2·1 = 5·(4·3·2·1) =
5·4!. Also 4! = 4·3·2·1 = 4·(3·2·1) = 4·3!. Generalizing this reasoning, we
have the following formula.
n! = n·(n−1)! (3.1)
Plugging in n = 1 gives 1! = 1·(1−1)! = 1·0!, that is, 1! = 1·0!. If we mistakenly
thought 0! were 0, this would give the incorrect result 1! = 0.
We round out our discussion of factorials with an example.
Example 3.3 This problem involves making lists of length seven from
the symbols 0,1,2,3,4,5 and 6.
(a) How many such lists are there if repetition is not allowed?
(b) How many such lists are there if repetition is not allowed and the
first three entries must be odd?
(c) How many such lists are there in which repetition is allowed, and
the list must contain at least one repeated number?
To answer the first question, note that there are seven symbols, so the
number of lists is 7! = 5040. To answer the second question, notice that
the set ©
0,1,2,3,4,5,6
ª
contains three odd numbers and four even numbers.
Thus in making the list the first three entries must be filled by odd numbers
and the final four must be filled with even numbers. By the multiplication
principle, the number of such lists is 3·2·1·4·3·2·1 = 3!4! = 144.
To answer the third question, notice that there are 7
7 = 823,543 lists
in which repetition is allowed. The set of all such lists includes lists
that are non-repetitive (e.g., (0,6,1,2,4,3,5)) as well as lists that have
some repetition (e.g., (6,3,6,2,0,0,0)). We want to compute the number of
lists that have at least one repeated number. To find the answer we can
subtract the number of non-repetitive lists of length seven from the total
number of possible lists of length seven. Therefore the answer is 7
7 −7! =
823,543−5040 = 818,503.
72 Counting
We close this section with a formula that combines the ideas of the first
and second sections of the present chapter. One of the main problems of
Section 3.1 was as follows: Given n symbols, how many non-repetitive lists
of length k can be made from the n symbols? We learned how to apply the
multiplication principle to obtain the answer
n(n−1)(n−2)···(n− k +1).
Notice that by cancellation this value can also be written as
n(n−1)(n−2)···(n− k +1)(n− k)(n− k −1)···3·2·1
(n− k)(n− k −1)···3·2·1
=
n!
(n− k)!
.
We summarize this as follows:
Fact 3.2 The number of non-repetitive lists of length k whose entries
are chosen from a set of n possible entries is n!
(n−k)! .
For example, consider finding the number of non-repetitive lists of
length five that can be made from the symbols 1,2,3,4,5,6,7,8. We will do
this two ways. By the multiplication principle, the answer is 8·7·6·5·4 =
6720. Using the formula from Fact 3.2, the answer is 8!
(8−5)! =
8!
3! =
40,320
6
=
6720.
The new formula isn’t really necessary, but it is a nice repackaging of
an old idea and will prove convenient in the next section.
Exercises for Section 3.2
1. What is the smallest n for which n! has more than 10 digits?
2. For which values of n does n! have n or fewer digits?
3. How many 5-digit positive integers are there in which there are no repeated
digits and all digits are odd?
4. Using only pencil and paper, find the value of 100!
95! .
5. Using only pencil and paper, find the value of 120!
118! .
6. There are two 0’s at the end of 10! = 3,628,800. Using only pencil and paper,
determine how many 0’s are at the end of the number 100!.
7. Compute how many 9-digit numbers can be made from the digits 1,2,3,4,5,6,7,8,9
if repetition is not allowed and all the odd digits occur first (on the left) followed
by all the even digits (i.e. as in 137598264, but not 123456789).
8. Compute how many 7-digit numbers can be made from the digits 1,2,3,4,5,6,7 if
there is no repetition and the odd digits must appear in an unbroken sequence.
(Examples: 3571264 or 2413576 or 2467531, etc., but not 7234615.)
Counting Subsets 73
9. There is a very interesting function Γ : [0,∞) → R called the gamma function.
It is defined as Γ(x) =
R ∞
0
t
x−1
e
−tdt. It has the remarkable property that if x ∈ N,
then Γ(x) = (x−1)!. Check that this is true for x = 1,2,3,4.
Notice that this function provides a way of extending factorials to numbers other
than integers. Since Γ(n) = (n−1)! for all n ∈ N, we have the formula n! = Γ(n+1).
But Γ can be evaluated at any number in [0,∞), not just at integers, so we
have a formula for n! for any n ∈ [0,∞). Extra credit: Compute π!.
10. There is another significant function called Stirling’s formula that provides an
approximation to factorials. It states that n! ≈
p
2πn
¡
n
e
¢n
. It is an approximation
to n! in the sense that n!
p
2πn
¡
n
e
¢n approaches 1 as n approaches ∞. Use Stirling’s
formula to find approximations to 5!, 10!, 20! and 50!.
3.3 Counting Subsets
The previous two sections were concerned with counting the number of
lists that can be made by selecting k entries from a set of n possible entries.
We turn now to a related question: How many subsets can be made by
selecting k elements from a set with n elements?
To highlight the differences between these two problems, look at the set
A =
©
a,b, c,d, e
ª
. First, think of the non-repetitive lists that can be made
from selecting two entries from A. By Fact 3.2 (on the previous page),
there are 5!
(5−2)! =
5!
3! =
120
6
= 20 such lists. They are as follows.
(a,b), (a, c), (a,d), (a, e), (b, c), (b,d), (b, e), (c,d), (c, e) (d, e)
(b,a), (c,a), (d,a), (e,a), (c,b), (d,b), (e,b), (d, c), (e, c) (e,d)
Next consider the subsets of A that can made from selecting two elements
from A. There are only ten such subsets, as follows.
©
a,b
ª
,
©
a, c
ª
,
©
a,d
ª
,
©
a, e
ª
,
©
b, c
ª
,
©
b,d
ª
,
©
b, e
ª
,
©
c,d
ª
,
©
c, e
ª
,
©
d, e
ª
.
The reason that there are more lists than subsets is that changing the
order of the entries of a list produces a different list, but changing the
order of the elements of a set does not change the set. Using elements
a,b ∈ A, we can make two lists (a,b) and (b,a), but only one subset ©
a,b
ª
.
In this section we are concerned not with counting lists, but with
counting subsets. As was noted above, the basic question is this: How
many subsets can be made by choosing k elements from an n-element
set? We begin with some notation that gives a name to the answer to this
question.
74 Counting
Definition 3.2 If n and k are integers, then ¡
n
k
¢
denotes the number
of subsets that can be made by choosing k elements from a set with n
elements. The symbol ¡
n
k
¢
is read “n choose k.” (Some textbooks write
C(n,k) instead of ¡
n
k
¢
.)
To illustrate this definition, the following table computes the values of
¡
4
k
¢
for various values of k by actually listing all the subsets of the 4-element
set A =
©
a,b, c,d
ª
that have cardinality k. The values of k appear in the
far-left column. To the right of each k are all of the subsets (if any) of A of
size k. For example, when k = 1, set A has four subsets of size k, namely
©
a
ª
,
©
b
ª
,
©
c
ª
and ©
d
ª
. Therefore ¡
4
1
¢
= 4. Similarly, when k = 2 there are six
subsets of size k so ¡
4
2
¢
= 6.
k k-element subsets of ©
a,b, c,d
ª ¡
4
k
¢
−1
¡
4
−1
¢
= 0
0 ;
¡
4
0
¢
= 1
1
©
a
ª
,
©
b
ª
,
©
c
ª
,
©
d
ª ¡
4
1
¢
= 4
2
©
a,b
ª
,
©
a, c
ª
,
©
a,d
ª
,
©
b, c
ª
,
©
b,d
ª
,
©
c,d
ª ¡
4
2
¢
= 6
3
©
a,b, c
ª
,
©
a,b,d
ª
,
©
a, c,d
ª
,
©
b, c,d
ª ¡
4
3
¢
= 4
4
©
a,b, c,d
ª ¡
4
4
¢
= 1
5
¡
4
5
¢
= 0
6
¡
4
6
¢
= 0
When k = 0, there is only one subset of A that has cardinality k, namely
the empty set, ;. Therefore ¡
4
0
¢
= 1.
Notice that if k is negative or greater than |A|, then A has no subsets
of cardinality k, so ¡
4
k
¢
= 0 in these cases. In general ¡
n
k
¢
= 0 whenever k < 0
or k > n. In particular this means ¡
n
k
¢
= 0 if n is negative.
Although it was not hard to work out the values of ¡
4
k
¢
by writing out
subsets in the above table, this method of actually listing sets would not
be practical for computing ¡
n
k
¢
when n and k are large. We need a formula.
To find one, we will now carefully work out the value of ¡
5
3
¢
in such a way
that a pattern will emerge that points the way to a formula for any ¡
n
k
¢
.
Counting Subsets 75
To begin, note that ¡
5
3
¢
is the number of 3-element subsets of ©
a,b, c,d, e
ª
.
These are listed in the following table. We see that in fact ¡
5
3
¢
= 10.
©
a,b,cª©
a,b,dª©
a,b,eª ©
a,c,dª ©
a,c,eª ©
a,d,eª ©
b,c,dª ©
b,c,eª ©
b,d,eª ©
c,d,eª
¡
5
3
¢
3!
The formula will emerge when we expand this table as follows. Taking
any one of the ten 3-element sets above, we can make 3! different nonrepetitive
lists from its elements. For example, consider the first set ©
a,b, c
ª
.
The first column of the following table tallies the 3! = 6 different lists that
can be the letters ©
a,b, c
ª
. The second column tallies the lists that can be
made from ©
a,b,d
ª
, and so on.
abc abd abe acd ace ade bcd bce bde cde
acb adb aeb adc aec aed bdc bec bed ced
bac bad bae cad cae dae cbd cbe dbe dce
bca bda bea cda cea dea cdb ceb deb dec
cba dba eba dca eca eda dcb ecb edb edc
cab dab eab dac eac ead dbc ebc ebd ecd
3!
¡
5
3
¢
This table has ¡
5
3
¢
columns and 3! rows, so it has a total of 3!¡
5
3
¢
lists.
But notice also that the table consists of every non-repetitive length-3 list
that can be made from the symbols ©
a,b, c,d, e
ª
. We know from Fact 3.2
that there are 5!
(5−3)! such lists. Thus the total number of lists in the table
is 3!¡
5
3
¢
=
5!
(5−3)! . Dividing both sides of this equation by 3!, we get
Ã
5
3
!
=
5!
3!(5−3)!
.
Working this out, you will find that it does give the correct value of 10.
But there was nothing special about the values 5 and 3. We could
do the above analysis for any ¡
n
k
¢
instead of ¡
5
3
¢
. The table would have ¡
n
k
¢
columns and k! rows. We would get
Ã
n
k
!
=
n!
k!(n− k)!
.
We summarize this as follows:
76 Counting
Fact 3.3 If n,k ∈ Z and 0 ≤ k ≤ n, then Ã
n
k
!
=
n!
k!(n− k)!
. Otherwise Ã
n
k
!
= 0.
Let’s now use our new knowledge to work some exercises.
Example 3.4 How many 4-element subsets does ©
1,2,3,4,5,6,7,8,9
ª
have?
The answer is ¡
9
4
¢
=
9!
4!(9−4)! =
9!
4!5! =
9·8·7·6·5!
4!5! =
9·8·7·6
4! =
9·8·7·6
24 = 126.
Example 3.5 A single 5-card hand is dealt off of a standard 52-card deck.
How many different 5-card hands are possible?
To answer this, think of the deck as being a set D of 52 cards. Then a
5-card hand is just a 5-element subset of D. For example, here is one of
many different 5-card hands that might be dealt from the deck.
½
7
♣
,
2
♣
,
3
♥
,
A
♠
,
5
♦
¾
The total number of possible hands equals the number of 5-element
subsets of D, that is
Ã
52
5
!
=
52!
5!·47!
=
52·51·50·49·48·47!
5!·47!
=
52·51·50·49·48
5!
= 2,598,960.
Thus the answer to our question is that there are 2,598,960 different
five-card hands that can be dealt from a deck of 52 cards.
Example 3.6 This problem concerns 5-card hands that can be dealt off
of a 52-card deck. How many such hands are there in which two of the
cards are clubs and three are hearts?
Solution: Think of such a hand as being described by a list of length
two of the form
µ ½ ∗
♣
,
∗
♣
¾
,
½
∗
♥
,
∗
♥
,
∗
♥
¾ ¶,
where the first entry is a 2-element subset of the set of 13 club cards, and
the second entry is a 3-element subset of the set of 13 heart cards. There
are ¡
13
2
¢
choices for the first entry and ¡
13
3
¢
choices for the second entry, so
by the multiplication principle there are ¡
13
2
¢¡13
3
¢
=
13!
2!11!
13!
3!10! = 22,308 such
lists. Answer: There are 22,308 possible 5-card hands with two clubs
and three hearts.
Example 3.7 Imagine a lottery that works as follows. A bucket contains
36 balls numbered 1,2,3,4,...,36. Six of these balls will be drawn randomly.
For $1 you buy a ticket that has six blanks: ä ä ä ä ä ä . You fill in the
blanks with six different numbers between 1 and 36. You win $1,000,000
Counting Subsets 77
if you chose the same numbers that are drawn, regardless of order. What
are your chances of winning?
Solution: In filling out the ticket you are choosing six numbers from
a set of 36 numbers. Thus there are ¡
36
6
¢
=
36!
6!(36−6)! = 1,947,792 different
combinations of numbers you might write. Only one of these will be a
winner. Your chances of winning are one in 1,947,792.
Exercises for Section 3.3
1. Suppose a set A has 37 elements. How many subsets of A have 10 elements?
How many subsets have 30 elements? How many have 0 elements?
2. Suppose A is a set for which |A| = 100. How many subsets of A have 5 elements?
How many subsets have 10 elements? How many have 99 elements?
3. A set X has exactly 56 subsets with 3 elements. What is the cardinality of X?
4. Suppose a set B has the property that
¯
¯
©
X : X ∈ P(B),|X| = 6
ª¯
¯ = 28. Find |B|.
5. How many 16-digit binary strings contain exactly seven 1’s? (Examples of such
strings include 0111000011110000 and 0011001100110010, etc.)
6.
¯
¯
©
X ∈ P(
©
0,1,2,3,4,5,6,7,8,9
ª
) : |X| = 4
ª¯
¯ =
7.
¯
¯
©
X ∈ P(
©
0,1,2,3,4,5,6,7,8,9
ª
) : |X| < 4
ª¯
¯ =
8. This problem concerns lists made from the symbols A,B,C,D,E,F,G,H,I.
(a) How many length-5 lists can be made if repetition is not allowed and the
list is in alphabetical order? (Example: BDEFI or ABCGH, but not BACGH.)
(b) How many length-5 lists can be made if repetition is not allowed and the
list is not in alphabetical order?
9. This problem concerns lists of length 6 made from the letters A,B,C,D,E,F,
without repetition. How many such lists have the property that the D occurs
before the A?
10. A department consists of 5 men and 7 women. From this department you select
a committee with 3 men and 2 women. In how many ways can you do this?
11. How many positive 10-digit integers contain no 0’s and exactly three 6’s?
12. Twenty-one people are to be divided into two teams, the Red Team and the
Blue Team. There will be 10 people on Red Team and 11 people on Blue Team.
In how many ways can this be done?
13. Suppose n and k are integers for which 0 ≤ k ≤ n. Use the formula ¡
n
k
¢
=
n!
k!(n−k)!
to show that ¡
n
k
¢
=
¡
n
n−k
¢
.
14. Suppose n,k ∈ Z, and 0 ≤ k ≤ n. Use Definition 3.2 alone (without using Fact 3.3)
to show that ¡
n
k
¢
=
¡
n
n−k
¢
.
78 Counting
3.4 Pascal’s Triangle and the Binomial Theorem
There are some beautiful and significant patterns among the numbers ¡
n
k
¢
.
This section investigates a pattern based on one equation in particular. It
happens that
Ã
n+1
k
!
=
Ã
n
k −1
!
+
Ã
n
k
!
(3.2)
for any integers n and k with 1 ≤ k ≤ n.
To see why this is true, recall that ¡
n+1
k
¢
equals the number of k-element
subsets of a set with n +1 elements. Now, the set A =
©
0,1,2,3,...,n
ª
has
n+1 elements, so ¡
n+1
k
¢
equals the number of k-element subsets of A. Such
subsets can be divided into two types: those that contain 0 and those that
do not contain 0. To make a k-element subset that contains 0 we can start
with ©
0
ª
and then append to this set an additional k −1 numbers selected
from ©
1,2,3,...,n
ª
. There are ¡
n
k−1
¢
ways to make this selection, so there
are ¡
n
k−1
¢
k-element subsets of A that contain 0. Concerning the k-element
subsets of A that do not contain 0, there are ¡
n
k
¢
of these sets, for we can
form them by selecting k elements from the n-element set ©
1,2,3,...,n
ª
. In
light of all this, Equation (3.2) just expresses the obvious fact that the
number of k-element subsets of A equals the number of k-element subsets
that contain 0 plus the number of k-element subsets that do not contain 0.
¡
0
0
¢
¡
1
0
¢ ¡
1
1
¢
¡
2
0
¢ ¡
2
1
¢ ¡
2
2
¢
¡
3
0
¢ ¡
3
1
¢ ¡
3
2
¢ ¡
3
3
¢
¡
4
0
¢ ¡
4
1
¢ ¡
4
2
¢ ¡
4
3
¢ ¡
4
4
¢
¡
5
0
¢ ¡
5
1
¢ ¡
5
2
¢ ¡
5
3
¢ ¡
5
4
¢ ¡
5
5
¢
¡
6
0
¢ ¡
6
1
¢ ¡
6
2
¢ ¡
6
3
¢ ¡
6
4
¢ ¡
6
5
¢ ¡
6
6
¢
¡
7
0
¢ ¡
7
1
¢ ¡
7
2
¢ ¡
7
3
¢ ¡
7
4
¢ ¡
7
5
¢ ¡
7
6
¢ ¡
7
7
¢
.
.
.
.
.
.
.
.
.
1
1 1
1 2 1
1 3 3 1
1 4 6 4 1
1 5 10 10 5 1
1 6 15 20 15 6 1
1 7 21 35 35 21 7 1
.
.
.
.
.
.
.
.
.
Figure 3.2. Pascal’s triangle
Now that we have seen why Equation (3.2) is true, we are going to
arrange the numbers ¡
n
k
¢
in a triangular pattern that highlights various
relationships among them. The left-hand side of Figure 3.2 shows numbers
¡
n
k
¢
arranged in a pyramid with ¡
0
0
¢
at the apex, just above a row containing
¡
1
k
¢
with k = 0 and k = 1. Below this is a row listing the values of ¡
2
k
¢
for
k = 0,1,2. In general, each row listing the numbers ¡
n
k
¢
is just above a row
listing the numbers ¡
n+1
k
¢
.
Pascal’s Triangle and the Binomial Theorem 79
Any number ¡
n+1
k
¢
for 0 < k < n in this pyramid is immediately below
and between the the two numbers ¡
n
k−1
¢
and ¡
n
k
¢
in the previous row. But
Equation 3.2 says ¡
n+1
k
¢
=
¡
n
k−1
¢
+
¡
n
k
¢
, and therefore any number (other than 1)
in the pyramid is the sum of the two numbers immediately above it.
This pattern is especially evident on the right of Figure 3.2, where
each ¡
n
k
¢
is worked out. Notice how 21 is the sum of the numbers 6 and 15
above it. Similarly, 5 is the sum of the 1 and 4 above it and so on.
The arrangement on the right of Figure 3.2 is called Pascal’s triangle.
(It is named after Blaise Pascal, 1623–1662, a French mathematician and
philosopher who discovered many of its properties.) Although we have
written only the first eight rows of Pascal’s triangle (beginning with Row 0
at the apex), it obviously could be extended downward indefinitely. We
could add an additional row at the bottom by placing a 1 at each end and
obtaining each remaining number by adding the two numbers above its
position. Doing this would give the following row:
1 8 28 56 70 56 28 8 1
This row consists of the numbers ¡
8
k
¢
for 0 ≤ k ≤ 8, and we have computed
them without the formula ¡
8
k
¢
=
8!
k!(8−k)! . Any ¡
n
k
¢
can be computed this way.
The very top row (containing only 1) is called Row 0. Row 1 is the
next down, followed by Row 2, then Row 3, etc. With this labeling, Row n
consists of the numbers ¡
n
k
¢
for 0 ≤ k ≤ n.
Notice that Row n appears to be a list of the coefficients of (x + y)
n
.
For example (x+ y)
2 = 1x
2 +2x y+1y
2
, and Row 2 lists the coefficients 1 2 1.
Similarly (x + y)
3 = 1x
3 + 3x
2
y + 3x y2 + 1y
3
, and Row 3 is 1 3 3 1. Pascal’s
triangle is shown on the left of Figure 3.3 and on the right are the
expansions of (x+ y)
n
for 0 ≤ n ≤ 5. In every case (at least as far as you care
to check) the numbers in Row n match up with the coefficients of (x+ y)
n
.
1
1 1
1 2 1
1 3 3 1
1 4 6 4 1
1 5 10 10 5 1
.
.
.
.
.
.
.
.
.
1
1x + 1y
1x
2 + 2x y + 1y
2
1x
3 + 3x
2
y + 3x y2 + 1y
3
1x
4 + 4x
3
y + 6x
2
y
2 + 4x y3 + 1y
4
1x
5 + 5x
4
y +10x
3
y
2 +10x
2
y
3 + 5x y4 + 1y
5
.
.
.
.
.
.
.
.
.
Figure 3.3. The n
th row of Pascal’s triangle lists the coefficients of (x+ y)
n
80 Counting
In fact this turns out to be true for every n. This result is known as
the binomial theorem, and it is worth mentioning here. It tells how to
raise a binomial x+ y to a non-negative integer power n.
Theorem 3.1 (Binomial Theorem) If n is a non-negative integer, then
(x+ y)
n =
¡
n
0
¢
x
n +
¡
n
1
¢
x
n−1
y+
¡
n
2
¢
x
n−2
y
2 +
¡
n
3
¢
x
n−3
y
3 +··· +¡
n
n−1
¢
x yn−1 +
¡
n
n
¢
y
n
.
For now we will be content to accept the binomial theorem without
proof. (You will be asked to prove it in an exercise in Chapter 10.) You
may find it useful from time to time. For instance, you can apply it if you
ever need to expand an expression such as (x+ y)
7
. To do this, look at Row
7 of Pascal’s triangle in Figure 3.2 and apply the binomial theorem to get
(x+ y)
7 = x
7 +7x
6
y+21x
5
y
2 +35x
4
y
3 +35x
3
y
4 +21x
2
y
5 +7x y6 + y
7
.
For another example,
(2a− b)
4 = ((2a)+(−b))4
= (2a)
4 +4(2a)
3
(−b)+6(2a)
2
(−b)
2 +4(2a)(−b)
3 +(−b)
4
= 16a
4 −32a
3
b +24a
2
b
2 −8ab3 + b
4
.
Exercises for Section 3.4
1. Write out Row 11 of Pascal’s triangle.
2. Use the binomial theorem to find the coefficient of x
8
y
5
in (x+ y)
13
.
3. Use the binomial theorem to find the coefficient of x
8
in (x+2)13
.
4. Use the binomial theorem to find the coefficient of x
6
y
3
in (3x−2y)
9
.
5. Use the binomial theorem to show Pn
k=0
¡
n
k
¢
= 2
n
.
6. Use Definition 3.2 (page 74) and Fact 1.3 (page 12) to show Pn
k=0
¡
n
k
¢
= 2
n
.
7. Use the binomial theorem to show Pn
k=0
3
k
¡
n
k
¢
= 4
n
.
8. Use Fact 3.3 (page 76) to derive Equation 3.2 (page 78).
9. Use the binomial theorem to show ¡
n
0
¢
−
¡
n
1
¢
+
¡
n
2
¢
−
¡
n
3
¢
+
¡
n
4
¢
−··· +(−1)n
¡
n
n
¢
= 0.
10. Show that the formula k
¡
n
k
¢
= n
¡
n−1
k−1
¢
is true for all integers n,k with 0 ≤ k ≤ n.
11. Use the binomial theorem to show 9
n =
Pn
k=0
(−1)k
¡
n
k
¢
10n−k
.
12. Show that ¡
n
k
¢¡ k
m
¢
=
¡
n
m
¢¡n−m
k−m
¢
.
13. Show that ¡
n
3
¢
=
¡
2
2
¢
+
¡
3
2
¢
+
¡
4
2
¢
+
¡
5
2
¢
+··· +¡
n−1
2
¢
.
14. The first five rows of Pascal’s triangle appear in the digits of powers of 11:
110 = 1, 111 = 11, 112 = 121, 113 = 1331 and 114 = 14641. Why is this so? Why
does the pattern not continue with 115
?
Inclusion-Exclusion 81
3.5 Inclusion-Exclusion
Many counting problems involve computing the cardinality of a union A∪B
of two finite sets. We examine this kind of problem now.
First we develop a formula for |A ∪B|. It is tempting to say that |A ∪B|
must equal |A| +|B|, but that is not quite right. If we count the elements
of A and then count the elements of B and add the two figures together,
we get |A| +|B|. But if A and B have some elements in common, then we
have counted each element in A ∩B twice.
A B
Therefore |A| + |B| exceeds |A ∪ B| by |A ∩ B|, and consequently |A ∪ B| =
|A| +|B| −|A ∩B|. This can be a useful equation.
|A ∪B| = |A| +|B| −|A ∩B| (3.3)
Notice that the sets A, B and A ∩B are all generally smaller than A ∪B, so
Equation (3.3) has the potential of reducing the problem of determining
|A ∪ B| to three simpler counting problems. It is sometimes called an
inclusion-exclusion formula because elements in A ∩B are included (twice)
in |A|+|B|, then excluded when |A∩B| is subtracted. Notice that if A∩B = ;,
then we do in fact get |A ∪B| = |A| +|B|; conversely if |A ∪B| = |A| +|B|, then
it must be that A ∩B = ;.
Example 3.8 A 3-card hand is dealt off of a standard 52-card deck. How
many different such hands are there for which all 3 cards are red or all
three cards are face cards?
Solution: Let A be the set of 3-card hands where all three cards are
red (i.e., either ♥ or ♦). Let B be the set of 3-card hands in which all three
cards are face cards (i.e., J,K or Q of any suit). These sets are illustrated
below.
A =
(( 5
♥
,
K
♦
,
2
♥
)
,
(
K
♥
,
J
♥
,
Q
♥
)
,
(
A
♦
,
6
♦
,
6
♥
)
,...)
(Red cards)
B =
(( K
♠
,
K
♦
,
J
♣
)
,
(
K
♥
,
J
♥
,
Q
♥
)
,
(
Q
♦
,
Q
♣
,
Q
♥
)
,...)
(Face cards)
82 Counting
We seek the number of 3-card hands that are all red or all face cards,
and this number is |A ∪ B|. By Formula (3.3), |A ∪ B| = |A| + |B| − |A ∩ B|.
Let’s examine |A|,|B| and |A ∩ B| separately. Any hand in A is formed
by selecting three cards from the 26 red cards in the deck, so |A| = ¡
26
3
¢
.
Similarly, any hand in B is formed by selecting three cards from the 12
face cards in the deck, so |B| = ¡
12
3
¢
. Now think about A ∩B. It contains all
the 3-card hands made up of cards that are red face cards.
A ∩B =
(( K
♥
,
K
♦
,
J
♥
)
,
(
K
♥
,
J
♥
,
Q
♥
)
,
(
,
Q
♦
,
J
♦
,
Q
♥
)
,...)
(Red face
cards)
The deck has only 6 red face cards, so |A ∩B| = ¡
6
3
¢
.
Now we can answer our question. The number of 3-card hands that
are all red or all face cards is |A ∪ B| = |A| + |B| − |A ∩ B| = ¡
26
3
¢
+
¡
12
3
¢
−
¡
6
3
¢
=
2600+220−20 = 2800.
There is an analogue to Equation (3.3) that involves three sets. Consider
three sets A, B and C, as represented in the following Venn Diagram.
A B
C
Using the same kind of reasoning that resulted in Equation (3.3), you can
convince yourself that
|A ∪B ∪C| = |A| +|B| +|C| −|A ∩B| −|A ∩C| −|B ∩C| +|A ∩B ∩C|. (3.4)
There’s probably not much harm in ignoring this one for now, but if you
find this kind of thing intriguing you should definitely take a course in
combinatorics. (Ask your instructor!)
As we’ve noted, Equation (3.3) becomes |A ∪B| = |A| +|B| if it happens
that A ∩B = ;. Also, in Equation (3.4), note that if A ∩B = ;, A ∩C = ; and
B ∩C = ;, we get the simple formula |A ∪B ∪C| = |A| +|B| +|C|. In general,
we have the following formula for n sets, none of which overlap. It is
sometimes called the addition principle.
Fact 3.4 (Addition Principle) If A1, A2,..., An are sets with Ai ∩ A j = ;
whenever i 6= j, then |A1 ∪ A2 ∪··· ∪ An| = |A1| +|A2| +··· +|An|.
Inclusion-Exclusion 83
Example 3.9 How many 7-digit binary strings (0010100, 1101011, etc.)
have an odd number of 1’s?
Solution: Let A be the set of all 7-digit binary strings with an odd
number of 1’s, so the answer to the question will be |A|. To compute |A|,
we break A up into smaller parts. Notice any string in A will have either
one, three, five or seven 1’s. Let A1 be the set of 7-digit binary strings
with only one 1. Let A3 be the set of 7-digit binary strings with three 1’s.
Let A5 be the set of 7-digit binary strings with five 1’s, and let A7 be the
set of 7-digit binary strings with seven 1’s. Therefore A = A1 ∪ A3 ∪ A5 ∪ A7.
Notice that any two of the sets Ai have empty intersection, so Fact 3.4
gives |A| = |A1| +|A3| +|A5| +|A7|.
Now the problem is to find the values of the individual terms of this
sum. For instance take A3, the set of 7-digit binary strings with three 1’s.
Such a string can be formed by selecting three out of seven positions for
the 1’s and putting 0’s in the other spaces. Therefore |A3| = ¡
7
3
¢
. Similarly
|A1| = ¡
7
1
¢
, |A5| = ¡
7
5
¢
, and |A7| = ¡
7
7
¢
. Finally the answer to our question is
|A| = |A1| + |A3| + |A5| + |A7| = ¡
7
1
¢
+
¡
7
3
¢
+
¡
7
5
¢
+
¡
7
7
¢
= 7+35 +21 +1 = 64. There
are 64 seven-digit binary strings with an odd number of 1’s.
You may already have been using the Addition Principle intuitively,
without thinking of it as a free-standing result. For instance, we used it
in Example 3.2(c) when we divided lists into four types and computed the
number of lists of each type.
Exercises for Section 3.5
1. At a certain university 523 of the seniors are history majors or math majors
(or both). There are 100 senior math majors, and 33 seniors are majoring in
both history and math. How many seniors are majoring in history?
2. How many 4-digit positive integers are there for which there are no repeated
digits, or for which there may be repeated digits, but all are odd?
3. How many 4-digit positive integers are there that are even or contain no 0’s?
4. This problem involves lists made from the letters T,H,E,O,R,Y, with repetition
allowed.
(a) How many 4-letter lists are there that don’t begin with T, or don’t end in
Y?
(b) How many 4-letter lists are there in which the sequence of letters T,H,E
appears consecutively?
(c) How many 5-letter lists are there in which the sequence of letters T,H,E
appears consecutively?
84 Counting
5. How many 7-digit binary strings begin in 1 or end in 1 or have exactly four 1’s?
6. Is the following statement true or false? Explain. If A1 ∩ A2 ∩ A3 = ;, then
|A1 ∪ A2 ∪ A3| = |A1| +|A2| +|A3|.
7. This problem concerns 4-card hands dealt off of a standard 52-card deck. How
many 4-card hands are there for which all 4 cards are of the same suit or all 4
cards are red?
8. This problem concerns 4-card hands dealt off of a standard 52-card deck. How
many 4-card hands are there for which all 4 cards are of different suits or all 4
cards are red?
9. A 4-letter list is made from the letters L,I,S,T,E,D according to the following
rule: Repetition is allowed, and the first two letters on the list are vowels or
the list ends in D. How many such lists are possible?
10. A 5-card poker hand is called a flush if all cards are the same suit. How many
different flushes are there?
Part II
How to Prove Conditional
Statements

CHAPTER 4
Direct Proof
I
t is time to prove some theorems. There are various strategies for doing
this; we now examine the most straightforward approach, a technique
called direct proof. As we begin, it is important to keep in mind the
meanings of three key terms: Theorem, proof and definition.
A theorem is a mathematical statement that is true and can be (and
has been) verified as true. A proof of a theorem is a written verification
that shows that the theorem is definitely and unequivocally true. A proof
should be understandable and convincing to anyone who has the requisite
background and knowledge. This knowledge includes an understanding of
the meanings of the mathematical words, phrases and symbols that occur
in the theorem and its proof. It is crucial that both the writer of the proof
and the readers of the proof agree on the exact meanings of all the words,
for otherwise there is an intolerable level of ambiguity. A definition is an
exact, unambiguous explanation of the meaning of a mathematical word or
phrase. We will elaborate on the terms theorem and definition in the next
two sections, and then finally we will be ready to begin writing proofs.
4.1 Theorems
A theorem is a statement that is true and has been proved to be true.
You have encountered many theorems in your mathematical education.
Here are some theorems taken from an undergraduate calculus text. They
will be familiar to you, though you may not have read all the proofs.
Theorem: Let f be differentiable on an open interval I and let c ∈ I.
If f (c) is the maximum or minimum value of f on I, then f
0
(c) = 0.
Theorem: If P∞
k=1
ak converges, then limk→∞ ak = 0.
Theorem: Suppose f is continuous on the interval [a,b]. Then f is
integrable on [a,b].
Theorem: Every absolutely convergent series converges.
88 Direct Proof
Observe that each of these theorems either has the conditional form “If
P, then Q,” or can be put into that form. The first theorem has an initial
sentence “Let f be differentiable on an open interval I, and let c ∈ I,” which
sets up some notation, but a conditional statement follows it. The third
theorem has form “Suppose P. Then Q,” but this means the same thing
as “If P, then Q.” The last theorem can be re-expressed as “If a series is
absolutely convergent, then it is convergent.”
A theorem of form “If P, then Q,” can be regarded as a device that
produces new information from P. Whenever we are dealing with a
situation in which P is true, then the theorem guarantees that, in addition,
Q is true. Since this kind of expansion of information is useful, theorems
of form “If P, then Q,” are very common.
But not every theorem is a conditional statement. Some have the form
of the biconditional P ⇔ Q, but, as we know, that can be expressed as two
conditional statements. Other theorems simply state facts about specific
things. For example, here is another theorem from your study of calculus.
Theorem: The series 1+
1
2
+
1
3
+
1
4
+
1
5
+··· diverges.
It would be difficult (or at least awkward) to restate this as a conditional
statement. Still, it is true that most theorems are conditional statements,
so much of this book will concentrate on that type of theorem.
It is important to be aware that there are a number of words that
mean essentially the same thing as the word “theorem,” but are used in
slightly different ways. In general the word “theorem” is reserved for a
statement that is considered important or significant (the Pythagorean
theorem, for example). A statement that is true but not as significant
is sometimes called a proposition. A lemma is a theorem whose main
purpose is to help prove another theorem. A corollary is a result that is
an immediate consequence of a theorem or proposition. It is not important
that you remember all these words now, for their meanings will become
clear with usage.
Our main task is to learn how to prove theorems. As the above examples
suggest, proving theorems requires a clear understanding of the meaning
of the conditional statement, and that is the primary reason we studied it
so extensively in Chapter 2. In addition, it is also crucial to understand
the role of definitions.
Definitions 89
4.2 Definitions
A proof of a theorem should be absolutely convincing. Ambiguity must be
avoided. Everyone must agree on the exact meaning of each mathematical
term. In Chapter 1 we defined the meanings of the sets N, Z, R, Q and
;, as well as the meanings of the symbols ∈ and ⊆, and we shall make
frequent use of these things. Here is another definition that we use often.
Definition 4.1 An integer n is even if n = 2a for some integer a ∈ Z.
Thus, for example, 10 is even because 10 = 2·5. Also, according to the
definition, 7 is not even because there is no integer a for which 7 = 2a.
While there would be nothing wrong with defining an integer to be odd if
it’s not even, the following definition is more concrete.
Definition 4.2 An integer n is odd if n = 2a+1 for some integer a ∈ Z.
Thus 7 is odd because 7 = 2·3+1. We will use these definitions whenever
the concept of even or odd numbers arises. If in a proof a certain number
turns out to be even, the definition allows us to write it as 2a for an
appropriate integer a. If some quantity has form 2b + 1 where b is an
integer, then the definition tells us the quantity is odd.
Definition 4.3 Two integers have the same parity if they are both even
or they are both odd. Otherwise they have opposite parity.
Thus 5 and −17 have the same parity, as do 8 and 0; but 3 and 4 have
opposite parity.
Two points about definitions are in order. First, in this book the word
or term being defined appears in boldface type. Second, it is common to
express definitions as conditional statements even though the biconditional
would more appropriately convey the meaning. Consider the definition
of an even integer. You understand full well that if n is even then n = 2a
(for a ∈ Z), and if n = 2a, then n is even. Thus, technically the definition
should read “An integer n is even if and only if n = 2a for some a ∈ Z.”
However, it is an almost-universal convention that definitions are phrased
in the conditional form, even though they are interpreted as being in the
biconditional form. There is really no good reason for this, other than
economy of words. It is the standard way of writing definitions, and we
have to get used to it.
Here is another definition that we will use often.
90 Direct Proof
Definition 4.4 Suppose a and b are integers. We say that a divides b,
written a | b, if b = ac for some c ∈ Z. In this case we also say that a is a
divisor of b, and that b is a multiple of a.
For example, 5 divides 15 because 15 = 5 · 3. We write this as 5 | 15.
Similarly 8 | 32 because 32 = 8 · 4, and −6 | 6 because 6 = −6 · −1. However,
6 does not divide 9 because there is no integer c for which 9 = 6 · c. We
express this as 6 - 9, which we read as “6 does not divide 9.”
Be careful of your interpretation of the symbols. There is a big difference
between the expressions a | b and a/b. The expression a | b is a statement,
while a/b is a fraction. For example, 8 | 16 is true and 8 | 20 is false. By
contrast, 8/16 = 0.5 and 8/20 = 0.4 are numbers, not statements. Be careful
not to write one when you mean the other.
Every integer has a set of integers that divide it. For example, the set
of divisors of 6 is ©
a ∈ Z : a | 6
ª
=
©
−6,−3,−2,−1,1,2,3,6
ª
. The set of divisors
of 5 is ©
− 5,−1,1,5
ª
. The set of divisors of 0 is Z. This brings us to the
following definition, with which you are already familiar.
Definition 4.5 A natural number n is prime if it has exactly two positive
divisors, 1 and n.
For example, 2 is prime, as are 5 and 17. The definition implies that 1
is not prime, as it only has one (not two) positive divisor, namely 1. An
integer n is composite if it factors as n = ab where a,b > 1.
Definition 4.6 The greatest common divisor of integers a and b,
denoted gcd(a,b), is the largest integer that divides both a and b. The
least common multiple of non-zero integers a and b, denoted lcm(a,b),
is smallest positive integer that is a multiple of both a and b.
So gcd(18,24) = 6, gcd(5,5) = 5 and gcd(32,−8) = 8. Also gcd(50,18) = 2,
but gcd(50,9) = 1. Note that gcd(0,6) = 6, because, although every integer
divides 0, the largest divisor of 6 is 6.
The expression gcd(0,0) is problematic. Every integer divides 0, so the
only conclusion is that gcd(0,0) = ∞. We circumvent this irregularity by
simply agreeing to consider gcd(a,b) only when a and b are not both zero.
Continuing our examples, lcm(4,6) = 12, and lcm(7,7) = 7.
Of course not all terms can be defined. If every word in a definition were
defined, there would be separate definitions for the words that appeared
in those definitions, and so on, until the chain of defined terms became
circular. Thus we accept some ideas as being so intuitively clear that
they require no definitions or verifications. For example, we will not find
Definitions 91
it necessary to define what an integer (or a real number) is. Nor will
we define addition, multiplication, subtraction and division, though we
will use these operations freely. We accept and use such things as the
distributive and commutative properties of addition and multiplication, as
well as other standard properties of arithmetic and algebra.
As mentioned in Section 1.9, we accept as fact the natural ordering
of the elements of N,Z,Q and R, so that (for example) statements such as
“5 < 7,” and “x < y implies −x > −y,” do not need to be justified.
In addition, we accept the following fact without justification or proof.
Fact 4.1 Suppose a and b are integers. Then:
• a+ b ∈ Z
• a− b ∈ Z
• ab ∈ Z
These three statements can be combined. For example, we see that if a,b
and c are integers, then a
2b − ca+ b is also an integer.
We will also accept as obvious the fact that any integer a can be divided
by a non-zero integer b, resulting in a unique quotient q and remainder r.
For example, b = 3 goes into a = 17 q = 5 times with remainder r = 2. In
symbols, 17 = 5·3+2, or a = qb + r. This fact, called the division algorithm,
was mentioned on page 29.
(The Division Algorithm) Given integers a and b with b > 0, there exist
unique integers q and r for which a = qb + r and 0 ≤ r < b.
Another fact that we will accept without proof (at least for now) is
that every natural number greater than 1 has a unique factorization into
primes. For example, the number 1176 can be factored into primes as
1176 = 2·2·2·3·7·7 = 2
3
·3·7
2
. By unique we mean that any factorization
of 1176 into primes will have exactly the same factors (i.e., three 2’s, one 3
and two 7’s). Thus, for example, there is no valid factorization of 1176 that
has a factor of 5. You may be so used to factoring numbers into primes
that it seems obvious that there cannot be different prime factorizations
of the same number, but in fact this is a fundamental result whose proof
is not transparent. Nonetheless, we will be content to assume that every
natural number greater than 1 has a unique factorization into primes.
(We will revisit the issue of a proof in Section 10.2.)
We will introduce other accepted facts, as well as definitions, as needed.
92 Direct Proof
4.3 Direct Proof
This section explains a simple way to prove theorems or propositions
that have the form of conditional statements. The technique is called
direct proof. To simplify the discussion, our first examples will involve
proving statements that are almost obviously true. Thus we will call the
statements propositions rather than theorems. (Remember, a proposition
is a statement that, although true, is not as significant as a theorem.)
To understand how the technique of direct proof works, suppose we
have some proposition of the following form.
Proposition If P, then Q.
This proposition is a conditional statement of form P ⇒ Q. Our goal
is to show that this conditional statement is true. To see how to proceed,
look at the truth table.
P Q P ⇒ Q
T T T
T F F
F T T
F F T
The table shows that if P is false, the statement P ⇒ Q is automatically
true. This means that if we are concerned with showing P ⇒ Q is true, we
don’t have to worry about the situations where P is false (as in the last
two lines of the table) because the statement P ⇒ Q will be automatically
true in those cases. But we must be very careful about the situations
where P is true (as in the first two lines of the table). We must show that
the condition of P being true forces Q to be true also, for that means the
second line of the table cannot happen.
This gives a fundamental outline for proving statements of the form
P ⇒ Q. Begin by assuming that P is true (remember, we don’t need to worry
about P being false) and show this forces Q to be true. We summarize this
as follows.
Outline for Direct Proof
Proposition If P, then Q.
Proof. Suppose P.
.
.
.
Therefore Q. ■
Direct Proof 93
So the setup for direct proof is remarkably simple. The first line
of the proof is the sentence “Suppose P.” The last line is the sentence
“Therefore Q.” Between the first and last line we use logic, definitions and
standard math facts to transform the statement P to the statement Q. It
is common to use the word “Proof” to indicate the beginning of a proof,
and the symbol to indicate the end.
As our first example, let’s prove that if x is odd then x
2
is also odd.
(Granted, this is not a terribly impressive result, but we will move on to
more significant things in due time.) The first step in the proof is to fill
in the outline for direct proof. This is a lot like painting a picture, where
the basic structure is sketched in first. We leave some space between the
first and last line of the proof. The following series of frames indicates the
steps you might take to fill in this space with a logical chain of reasoning.
Proposition If x is odd, then x
2
is odd.
Proof. Suppose x is odd.
Therefore x
2
is odd. ■
Now that we have written the first and last lines, we need to fill in the
space with a chain of reasoning that shows that x being odd forces x
2
to
be odd.
In doing this it’s always advisable to use any definitions that apply.
The first line says x is odd, and by Definition 4.2 it must be that x = 2a+1
for some a ∈ Z, so we write this in as our second line.
Proposition If x is odd, then x
2
is odd.
Proof. Suppose x is odd.
Then x = 2a+1 for some a ∈ Z, by definition of an odd number.
Therefore x
2
is odd. ■
Now jump down to the last line, which says x
2
is odd. Think about what
the line immediately above it would have to be in order for us to conclude
that x
2
is odd. By the definition of an odd number, we would have to have
x
2 = 2a +1 for some a ∈ Z. However, the symbol a now appears earlier in
the proof in a different context, so we should use a different symbol, say b.
94 Direct Proof
Proposition If x is odd, then x
2
is odd.
Proof. Suppose x is odd.
Then x = 2a+1 for some a ∈ Z, by definition of an odd number.
Thus x
2 = 2b +1 for an integer b.
Therefore x
2
is odd, by definition of an odd number. ■
We are almost there. We can bridge the gap as follows.
Proposition If x is odd, then x
2
is odd.
Proof. Suppose x is odd.
Then x = 2a+1 for some a ∈ Z, by definition of an odd number.
Thus x
2 = (2a+1)2 = 4a
2 +4a+1 = 2(2a
2 +2a)+1.
So x
2 = 2b +1 where b is the integer b = 2a
2 +2a.
Thus x
2 = 2b +1 for an integer b.
Therefore x
2
is odd, by definition of an odd number. ■
Finally, we may wish to clean up our work and write the proof in paragraph
form. Here is our final version.
Proposition If x is odd, then x
2
is odd.
Proof. Suppose x is odd. Then x = 2a+1 for some a ∈ Z, by definition
of an odd number. Thus x
2 = (2a+1)2 = 4a
2+4a+1 = 2(2a
2+2a)+1, so
x
2 = 2b +1 where b = 2a
2 +2a ∈ Z. Therefore x
2
is odd, by definition
of an odd number. ■
At least initially, it’s generally a good idea to write the first and last line
of your proof first, and then fill in the gap, sometimes jumping alternately
between top and bottom until you meet in the middle, as we did above. This
way you are constantly reminded that you are aiming for the statement
at the bottom. Sometimes you will leave too much space, sometimes not
enough. Sometimes you will get stuck before figuring out what to do. This
is normal. Mathematicians do scratch work just as artists do sketches for
their paintings.
Direct Proof 95
Here is another example. Consider proving following proposition.
Proposition Let a,b and c be integers. If a | b and b | c, then a | c.
Let’s apply the basic outline for direct proof. To clarify the procedure
we will write the proof in stages again.
Proposition Let a,b and c be integers. If a | b and b | c, then a | c.
Proof. Suppose a | b and b | c.
Therefore a | c. ■
Our first step is to apply Definition 4.4 to the first line. The definition
says a | b means b = ac for some integer c, but since c already appears in
a different context on the first line, we must use a different letter, say d.
Similarly let’s use a new letter e in the definition of b | c.
Proposition Let a,b and c be integers. If a | b and b | c, then a | c.
Proof. Suppose a | b and b | c.
By Definition 4.4, we know a | b means there is an integer d with b = ad.
Likewise, b | c means there is an integer e for which c = be.
Therefore a | c. ■
We have almost bridged the gap. The line immediately above the last line
should show that a | c. According to Definition 4.4, this line should say
that c = ax for some integer x. We can get this equation from the lines at
the top, as follows.
Proposition Let a,b and c be integers. If a | b and b | c, then a | c.
Proof. Suppose a | b and b | c.
By Definition 4.4, we know a | b means there is an integer d with b = ad.
Likewise, b | c means there is an integer e for which c = be.
Thus c = be = (ad)e = a(de), so c = ax for the integer x = de.
Therefore a | c. ■
The next example is presented all at once rather than in stages.
96 Direct Proof
Proposition If x is an even integer, then x
2 −6x+5 is odd.
Proof. Suppose x is an even integer.
Then x = 2a for some a ∈ Z, by definition of an even integer.
So x
2−6x+5 = (2a)
2−6(2a)+5 = 4a
2−12a+5 = 4a
2−12a+4+1 = 2(2a
2−6a+2)+1.
Therefore we have x
2 −6x+5 = 2b +1, where b = 2a
2 −6a+2 ∈ Z.
Consequently x
2 −6x+5 is odd, by definition of an odd number. ■
One doesn’t normally use a separate line for each sentence in a proof,
but for clarity we will often do this in the first few chapters of this book.
Our next example illustrates a standard technique for showing two
quantities are equal. If we can show m ≤ n and n ≤ m then it follows that
m = n. In general, the reasoning involved in showing m ≤ n can be quite
different from that of showing n ≤ m.
Recall Definition 4.6 of a least common multiple on page 90.
Proposition If a,b, c ∈ N, then lcm(ca, cb) = c ·lcm(a,b).
Proof. Assume a,b, c ∈ N. Let m = lcm(ca, cb) and n = c ·lcm(a,b). We will
show m = n. By definition, lcm(a,b) is a multiple of both a and b, so
lcm(a,b) = ax = b y for some x, y ∈ Z. From this we see that n = c ·lcm(a,b) =
cax = cb y is a multiple of both ca and cb. But m = lcm(ca, cb) is the smallest
multiple of both ca and cb. Thus m ≤ n.
On the other hand, as m = lcm(ca, cb) is a multiple of both ca and cb,
we have m = cax = cb y for some x, y ∈ Z. Then 1
c m = ax = b y is a multiple of
both a and b. Therefore lcm(a,b) ≤
1
c m, so c ·lcm(a,b) ≤ m, that is, n ≤ m.
We’ve shown m ≤ n and n ≤ m, so m = n. The proof is complete. ■
The examples we’ve looked at so far have all been proofs of statements
about integers. In our next example, we are going to prove that if x and y
are positive real numbers for which x ≤ y, then p
x ≤
p
y. You may feel that
the proof is not as “automatic” as the proofs we have done so far. Finding
the right steps in a proof can be challenging, and that is part of the fun.
Proposition Let x and y be positive numbers. If x ≤ y, then p
x ≤
p
y.
Proof. Suppose x ≤ y. Subtracting y from both sides gives x− y ≤ 0.
This can be written as p
x
2
−
p
y
2 ≤ 0.
Factor this to get (
p
x−
p
y)(p
x+
p
y) ≤ 0.
Dividing both sides by the positive number p
x+
p
y produces p
x−
p
y ≤ 0.
Adding p
y to both sides gives p
x ≤
p
y. ■
Direct Proof 97
This proposition tells us that whenever x ≤ y, we can take the square
root of both sides and be assured that p
x ≤
p
y. This can be useful, as we
will see in our next proposition.
That proposition will concern the expression 2
p
x y ≤ x + y. Notice when
you substitute random positive values for the variables, the expression is
true. For example, for x = 6 and y = 4, the left side is 2
p
6·4 = 4
p
6 ≈ 9.79,
which is less than the right side 6+4 = 10. Is it true that 2
p
x y ≤ x + y for
any positive x and y? How could we prove it?
To see how, let’s first cast this into the form of a conditional statement:
If x and y are positive real numbers, then 2
p
x y ≤ x+ y. The proof begins
with the assumption that x and y are positive, and ends with 2
p
x y ≤ x+ y.
In mapping out a strategy, it can be helpful to work backwards, working
from 2
p
x y ≤ x+ y to something that is obviously true. Then the steps can
be reversed in the proof. In this case, squaring both sides of 2
p
x y ≤ x+ y
gives us
4x y ≤ x
2 +2x y+ y
2
.
Now subtract 4x y from both sides and factor.
0 ≤ x
2 −2x y+ y
2
0 ≤ (x− y)
2
But this last line is clearly true, since the square of x−y cannot be negative!
This gives us a strategy for the proof, which follows.
Proposition If x and y are positive real numbers, then 2
p
x y ≤ x+ y.
Proof. Suppose x and y are positive real numbers.
Then 0 ≤ (x− y)
2
, that is, 0 ≤ x
2 −2x y+ y
2
.
Adding 4x y to both sides gives 4x y ≤ x
2 +2x y+ y
2
.
Factoring yields 4x y ≤ (x+ y)
2
.
Previously we proved that such an inequality still holds after taking the
square root of both sides; doing so produces 2
p
x y ≤ x+ y. ■
Notice that in the last step of the proof we took the square root of both
sides of 4x y ≤ (x + y)
2 and got p
4x y ≤
p
(x+ y)
2, and the fact that this did
not reverse the symbol ≤ followed from our previous proposition. This
is an important point. Often the proof of a proposition or theorem uses
another proposition or theorem (that has already been proved).
98 Direct Proof
4.4 Using Cases
In proving a statement is true, we sometimes have to examine multiple
cases before showing the statement is true in all possible scenarios. This
section illustrates a few examples.
Our examples will concern the expression 1+(−1)n
(2n −1). Here is a
table showing its value for various integers for n. Notice that 1+(−1)n
(2n−1)
is a multiple of 4 in every line.
n 1+(−1)n
(2n−1)
1 0
2 4
3 −4
4 8
5 −8
6 12
Is 1+(−1)n
(2n−1) always a multiple of 4? We prove the answer is “yes”
in our next example. Notice, however, that the expression 1+(−1)n
(2n−1)
behaves differently depending on whether n is even or odd, for in the first
case (−1)n = 1, and in the second (−1)n = −1. Thus the proof must examine
these two possibilities separately.
Proposition If n ∈ N, then 1+(−1)n
(2n−1) is a multiple of 4.
Proof. Suppose n ∈ N.
Then n is either even or odd. Let’s consider these two cases separately.
Case 1. Suppose n is even. Then n = 2k for some k ∈ Z, and (−1)n = 1.
Thus 1+(−1)n
(2n−1) = 1+(1)(2·2k −1) = 4k, which is a multiple of 4.
Case 2. Suppose n is odd. Then n = 2k +1 for some k ∈ Z, and (−1)n = −1.
Thus 1+(−1)n
(2n−1) = 1−(2(2k +1)−1) = −4k, which is a multiple of 4.
These cases show that 1+(−1)n
(2n−1) is always a multiple of 4. ■
Now let’s examine the flip side of the question. We just proved that
1+(−1)n
(2n−1) is always a multiple of 4, but can we get every multiple of 4
this way? The following proposition and proof give an affirmative answer.
Treating Similar Cases 99
Proposition Every multiple of 4 equals 1+(−1)n
(2n−1) for some n ∈ N.
Proof. In conditional form, the proposition is as follows:
If k is a multiple of 4, then there is an n ∈ N for which 1+(−1)n
(2n−1) = k.
What follows is a proof of this conditional statement.
Suppose k is a multiple of 4.
This means k = 4a for some integer a.
We must produce an n ∈ N for which 1+(−1)n
(2n−1) = k.
This is done by cases, depending on whether a is zero, positive or negative.
Case 1. Suppose a = 0. Let n = 1. Then 1+(−1)n
(2n−1) = 1+(−1)1
(2−1) = 0
= 4·0 = 4a = k.
Case 2. Suppose a > 0. Let n = 2a, which is in N because a is positive. Also
n is even, so (−1)n = 1. Thus 1+(−1)n
(2n−1) = 1+(2n−1) = 2n = 2(2a) = 4a = k.
Case 3. Suppose a < 0. Let n = 1−2a, which is an element of N because
a is negative, making 1−2a positive. Also n is odd, so (−1)n = −1. Thus
1+(−1)n
(2n−1) = 1−(2n−1) = 1−(2(1−2a)−1) = 4a = k.
The above cases show that no matter whether a multiple k = 4a of 4 is
zero, positive or negative, k = 1+(−1)n
(2n−1) for some n ∈ N. ■
4.5 Treating Similar Cases
Occasionally two or more cases in a proof will be so similar that writing
them separately seems tedious or unnecessary. Here is an example.
Proposition If two integers have opposite parity, then their sum is odd.
Proof. Suppose m and n are two integers with opposite parity.
We need to show that m+ n is odd. This is done in two cases, as follows.
Case 1. Suppose m is even and n is odd. Thus m = 2a and n = 2b +1 for
some integers a and b. Therefore m+ n = 2a+2b +1 = 2(a+ b)+1, which is
odd (by Definition 4.2).
Case 2. Suppose m is odd and n is even. Thus m = 2a +1 and n = 2b for
some integers a and b. Therefore m+ n = 2a+1+2b = 2(a+ b)+1, which is
odd (by Definition 4.2).
In either case, m+ n is odd. ■
The two cases in this proof are entirely alike except for the order in
which the even and odd terms occur. It is entirely appropriate to just do
one case and indicate that the other case is nearly identical. The phrase
“Without loss of generality...” is a common way of signaling that the proof is
treating just one of several nearly identical cases. Here is a second version
of the above example.
100 Direct Proof
Proposition If two integers have opposite parity, then their sum is odd.
Proof. Suppose m and n are two integers with opposite parity.
We need to show that m+ n is odd.
Without loss of generality, suppose m is even and n is odd.
Thus m = 2a and n = 2b +1 for some integers a and b.
Therefore m+n = 2a+2b+1 = 2(a+b)+1, which is odd (by Definition 4.2). ■
In reading proofs in other texts, you may sometimes see the phrase
“Without loss of generality” abbreviated as “WLOG.” However, in the
interest of transparency we will avoid writing it this way. In a similar
spirit, it is advisable—at least until you become more experienced in proof
writing—that you write out all cases, no matter how similar they appear
to be.
Please check your understanding by doing the following exercises. The
odd numbered problems have complete proofs in the Solutions section in
the back of the text.
Exercises for Chapter 4
Use the method of direct proof to prove the following statements.
1. If x is an even integer, then x
2
is even.
2. If x is an odd integer, then x
3
is odd.
3. If a is an odd integer, then a
2 +3a+5 is odd.
4. Suppose x, y ∈ Z. If x and y are odd, then x y is odd.
5. Suppose x, y ∈ Z. If x is even, then x y is even.
6. Suppose a,b, c ∈ Z. If a | b and a | c, then a | (b + c).
7. Suppose a,b ∈ Z. If a | b, then a
2
| b
2
.
8. Suppose a is an integer. If 5 | 2a, then 5 | a.
9. Suppose a is an integer. If 7 | 4a, then 7 | a.
10. Suppose a and b are integers. If a | b, then a | (3b
3 − b
2 +5b).
11. Suppose a,b, c,d ∈ Z. If a | b and c | d, then ac | bd.
12. If x ∈ R and 0 < x < 4, then 4
x(4−x)
≥ 1.
13. Suppose x, y ∈ R. If x
2 +5y = y
2 +5x, then x = y or x+ y = 5.
14. If n ∈ Z, then 5n
2 +3n+7 is odd. (Try cases.)
15. If n ∈ Z, then n
2 +3n+4 is even. (Try cases.)
16. If two integers have the same parity, then their sum is even. (Try cases.)
17. If two integers have opposite parity, then their product is even.
18. Suppose x and y are positive real numbers. If x < y, then x
2 < y
2
.
Treating Similar Cases 101
19. Suppose a,b and c are integers. If a
2
| b and b
3
| c, then a
6
| c.
20. If a is an integer and a
2
| a, then a ∈
©
−1,0,1
ª
.
21. If p is prime and k is an integer for which 0 < k < p, then p divides ¡
p
k
¢
.
22. If n ∈ N, then n
2 = 2
¡
n
2
¢
+
¡
n
1
¢
. (You may need a separate case for n = 1.)
23. If n ∈ N, then ¡
2n
n
¢
is even.
24. If n ∈ N and n ≥ 2, then the numbers n!+2, n!+3, n!+4, n!+5, ..., n!+ n are all
composite. (Thus for any n ≥ 2, one can find n consecutive composite numbers.
This means there are arbitrarily large “gaps” between prime numbers.)
25. If a,b, c ∈ N and c ≤ b ≤ a, then ¡
a
b
¢¡b
c
¢
=
¡
a
b−c
¢¡a−b+c
c
¢
.
26. Every odd integer is a difference of two squares. (Example 7 = 4
2 −3
2
, etc.)
27. Suppose a,b ∈ N. If gcd(a,b) > 1, then b | a or b is not prime.
28. If a,b, c ∈ Z, then c ·gcd(a,b) ≤ gcd(ca, cb).
CHAPTER 5
Contrapositive Proof
We now examine an alternative to direct proof called contrapositive
proof. Like direct proof, the technique of contrapositive proof is
used to prove conditional statements of the form “If P, then Q.” Although
it is possible to use direct proof exclusively, there are occasions where
contrapositive proof is much easier.
5.1 Contrapositive Proof
To understand how contrapositive proof works, imagine that you need to
prove a proposition of the following form.
Proposition If P, then Q.
This is a conditional statement of form P ⇒ Q. Our goal is to show
that this conditional statement is true. Recall that in Section 2.6 we
observed that P ⇒ Q is logically equivalent to ∼ Q ⇒∼ P. For convenience,
we duplicate the truth table that verifies this fact.
P Q ∼ Q ∼ P P ⇒ Q ∼ Q ⇒∼ P
T T F F T T
T F T F F F
F T F T T T
F F T T T T
According to the table, statements P ⇒ Q and ∼ Q ⇒∼ P are different
ways of expressing exactly the same thing. The expression ∼ Q ⇒∼ P is
called the contrapositive form of P ⇒ Q.
1
1Do not confuse the words contrapositive and converse. Recall from Section 2.4 that the
converse of P ⇒ Q is the statement Q ⇒ P, which is not logically equivalent to P ⇒ Q.
Contrapositive Proof 103
Since P ⇒ Q is logically equivalent to ∼ Q ⇒∼ P, it follows that to prove
P ⇒ Q is true, it suffices to instead prove that ∼ Q ⇒∼ P is true. If we were
to use direct proof to show ∼ Q ⇒∼ P is true, we would assume ∼ Q is true
use this to deduce that ∼ P is true. This in fact is the basic approach of
contrapositive proof, summarized as follows.
Outline for Contrapositive Proof
Proposition If P, then Q.
Proof. Suppose ∼ Q.
.
.
.
Therefore ∼ P. ■
So the setup for contrapositive proof is very simple. The first line of the
proof is the sentence “Suppose Q is not true.” (Or something to that effect.)
The last line is the sentence “Therefore P is not true.” Between the first
and last line we use logic and definitions to transform the statement ∼ Q
to the statement ∼ P.
To illustrate this new technique, and to contrast it with direct proof,
we now prove a proposition in two ways: first with direct proof and then
with contrapositive proof.
Proposition Suppose x ∈ Z. If 7x+9 is even, then x is odd.
Proof. (Direct) Suppose 7x+9 is even.
Thus 7x+9 = 2a for some integer a.
Subtracting 6x+9 from both sides, we get x = 2a−6x−9.
Thus x = 2a−6x−9 = 2a−6x−10+1 = 2(a−3x−5)+1.
Consequently x = 2b +1, where b = a−3x−5 ∈ Z.
Therefore x is odd. ■
Here is a contrapositive proof of the same statement:
Proposition Suppose x ∈ Z. If 7x+9 is even, then x is odd.
Proof. (Contrapositive) Suppose x is not odd.
Thus x is even, so x = 2a for some integer a.
Then 7x+9 = 7(2a)+9 = 14a+8+1 = 2(7a+4)+1.
Therefore 7x+9 = 2b +1, where b is the integer 7a+4.
Consequently 7x+9 is odd.
Therefore 7x+9 is not even. ■
104 Contrapositive Proof
Though the proofs are of equal length, you may feel that the contrapositive
proof flowed more smoothly. This is because it is easier to
transform information about x into information about 7x+9 than the other
way around. For our next example, consider the following proposition
concerning an integer x:
Proposition If x
2 −6x+5 is even, then x is odd.
A direct proof would be problematic. We would begin by assuming that
x
2 −6x+5 is even, so x
2 −6x+5 = 2a. Then we would need to transform this
into x = 2b +1 for b ∈ Z. But it is not quite clear how that could be done,
for it would involve isolating an x from the quadratic expression. However
the proof becomes very simple if we use contrapositive proof.
Proposition Suppose x ∈ Z. If x
2 −6x+5 is even, then x is odd.
Proof. (Contrapositive) Suppose x is not odd.
Thus x is even, so x = 2a for some integer a.
So x
2−6x+5 = (2a)
2−6(2a)+5 = 4a
2−12a+5 = 4a
2−12a+4+1 = 2(2a
2−6a+2)+1.
Therefore x
2 −6x+5 = 2b +1, where b is the integer 2a
2 −6a+2.
Consequently x
2 −6x+5 is odd.
Therefore x
2 −6x+5 is not even. ■
In summary, since x being not odd (∼ Q) resulted in x
2−6x+5 being not
even (∼ P), then x
2 −6x +5 being even (P) means that x is odd (Q). Thus
we have proved P ⇒ Q by proving ∼ Q ⇒∼ P. Here is another example:
Proposition Suppose x, y ∈ R. If y
3 + yx2 ≤ x
3 + x y2
, then y ≤ x.
Proof. (Contrapositive) Suppose it is not true that y ≤ x, so y > x.
Then y− x > 0. Multiply both sides of y− x > 0 by the positive value x
2 + y
2
.
(y− x)(x
2 + y
2
) > 0(x
2 + y
2
)
yx2 + y
3 − x
3 − x y2 > 0
y
3 + yx2 > x
3 + x y2
Therefore y
3 + yx2 > x
3 + x y2
, so it is not true that y
3 + yx2 ≤ x
3 + x y2
. ■
Proving “If P, then Q,” with the contrapositive approach necessarily
involves the negated statements ∼ P and ∼ Q. In working with these we
may have to use the techniques for negating statements (e.g., DeMorgan’s
laws) discussed in Section 2.10. We consider such an example next.
Congruence of Integers 105
Proposition Suppose x, y ∈ Z. If 5 - x y, then 5 - x and 5 - y.
Proof. (Contrapositive) Suppose it is not true that 5 - x and 5 - y.
By DeMorgan’s law, it is not true that 5 - x or it is not true that 5 - y.
Therefore 5 | x or 5 | y. We consider these possibilities separately.
Case 1. Suppose 5 | x. Then x = 5a for some a ∈ Z.
From this we get x y = 5(a y), and that means 5 | x y.
Case 2. Suppose 5 | y. Then y = 5a for some a ∈ Z.
From this we get x y = 5(ax), and that means 5 | x y.
The above cases show that 5 | x y, so it is not true that 5 - x y. ■
5.2 Congruence of Integers
This is a good time to introduce a new definition. It is not necessarily
related to contrapositive proof, but introducing it now ensures that we
have a sufficient variety of exercises to practice all our proof techniques on.
This new definition occurs in many branches of mathematics, and it will
surely play a role in some of your later courses. But our primary reason
for introducing it is that it will give us more practice in writing proofs.
Definition 5.1 Given integers a and b and an n ∈ N, we say that a and b
are congruent modulo n if n | (a − b). We express this as a ≡ b (mod n).
If a and b are not congruent modulo n, we write this as a 6≡ b (mod n).
Example 5.1 Here are some examples:
1. 9 ≡ 1 (mod 4) because 4 | (9−1).
2. 6 ≡ 10 (mod 4) because 4 | (6−10).
3. 14 6≡ 8 (mod 4) because 4 - (14−8).
4. 20 ≡ 4 (mod 8) because 8 | (20−4).
5. 17 ≡ −4 (mod 3) because 3 | (17−(−4)).
In practical terms, a ≡ b (mod n) means that a and b have the same
remainder when divided by n. For example, we saw above that 6 ≡ 10
(mod 4) and indeed 6 and 10 both have remainder 2 when divided by 4.
Also we saw 14 6≡ 8 (mod 4), and sure enough 14 has remainder 2 when
divided by 4, while 8 has remainder 0.
To see that this is true in general, note that if a and b both have the
same remainder r when divided by n, then it follows that a = kn + r and
b = `n + r for some k,` ∈ Z. Then a − b = (kn + r) − (`n + r) = n(k − `). But
a − b = n(k − `) means n | (a − b), so a ≡ b (mod n). Conversely, one of the
exercises for this chapter asks you to show that if a ≡ b (mod n), then a
and b have the same remainder when divided by n.
106 Contrapositive Proof
We conclude this section with several proofs involving congruence of
integers, but you will also test your skills with other proofs in the exercises.
Proposition Let a,b ∈ Z and n ∈ N. If a ≡ b (mod n), then a
2 ≡ b
2
(mod n).
Proof. We will use direct proof. Suppose a ≡ b (mod n).
By definition of congruence of integers, this means n | (a− b).
Then by definition of divisibility, there is an integer c for which a− b = nc.
Now multiply both sides of this equation by a+ b.
a− b = nc
(a− b)(a+ b) = nc(a+ b)
a
2 − b
2 = nc(a+ b)
Since c(a+ b) ∈ Z, the above equation tells us n | (a
2 − b
2
).
According to Definition 5.1, this gives a
2 ≡ b
2
(mod n). ■
Let’s pause to consider this proposition’s meaning. It says a ≡ b (mod n)
implies a
2 ≡ b
2
(mod n). In other words, it says that if integers a and b
have the same remainder when divided by n, then a
2 and b
2 also have
the same remainder when divided by n. As an example of this, 6 and 10
have the same remainder (2) when divided by n = 4, and their squares
36 and 100 also have the same remainder (0) when divided by n = 4. The
proposition promises this will happen for all a, b and n. In our examples
we tend to concentrate more on how to prove propositions than on what
the propositions mean. This is reasonable since our main goal is to learn
how to prove statements. But it is helpful to sometimes also think about
the meaning of what we prove.
Proposition Let a,b, c ∈ Z and n ∈ N. If a ≡ b (mod n), then ac ≡ bc (mod n).
Proof. We employ direct proof. Suppose a ≡ b (mod n). By Definition 5.1,
it follows that n | (a− b). Therefore, by definition of divisibility, there exists
an integer k for which a − b = nk. Multiply both sides of this equation
by c to get ac − bc = nkc. Thus ac − bc = n(kc) where kc ∈ Z, which means
n | (ac − bc). By Definition 5.1, we have ac ≡ bc (mod n). ■
Contrapositive proof seems to be the best approach in the next example,
since it will eliminate the symbols - and 6≡.
Mathematical Writing 107
Proposition Suppose a,b ∈ Z and n ∈ N. If 12a 6≡ 12b (mod n), then n - 12.
Proof. (Contrapositive) Suppose n | 12, so there is an integer c for which
12 = nc. Now reason as follows.
12 = nc
12(a− b) = nc(a− b)
12a−12b = n(ca − cb)
Since ca − cb ∈ Z, the equation 12a −12b = n(ca − cb) implies n | (12a −12b).
This in turn means 12a ≡ 12b (mod n). ■
5.3 Mathematical Writing
Now that we have begun writing proofs, it is a good time to contemplate the
craft of writing. Unlike logic and mathematics, where there is a clear-cut
distinction between what is right or wrong, the difference between good
and bad writing is sometimes a matter of opinion. But there are some
standard guidelines that will make your writing clearer. Some of these
are listed below.
1. Begin each sentence with a word, not a mathematical symbol.
The reason is that sentences begin with capital letters, but mathematical
symbols are case sensitive. Because x and X can have entirely different
meanings, putting such symbols at the beginning of a sentence can lead
to ambiguity. Here are some examples of bad usage (marked with ×)
and good usage (marked with X).
A is a subset of B. ×
The set A is a subset of B. X
x is an integer, so 2x+5 is an integer. ×
Because x is an integer, 2x+5 is an integer. X
x
2 − x+2 = 0 has two solutions. ×
X
2 − x+2 = 0 has two solutions. × (and silly too)
The equation x
2 − x+2 = 0 has two solutions. X
2. End each sentence with a period, even when the sentence ends
with a mathematical symbol or expression.
Euler proved that X∞
k=1
1
k
s
=
Y
p∈P
1
1−
1
p
s
×
Euler proved that X∞
k=1
1
k
s
=
Y
p∈P
1
1−
1
p
s
. X
108 Contrapositive Proof
Mathematical statements (equations, etc.) are like English phrases that
happen to contain special symbols, so use normal punctuation.
3. Separate mathematical symbols and expressions with words.
Not doing this can cause confusion by making distinct expressions
appear to merge into one. Compare the clarity of the following examples.
Because x
2 −1 = 0, x = 1 or x = −1. ×
Because x
2 −1 = 0, it follows that x = 1 or x = −1. X
Unlike A ∪B, A ∩B equals ;. ×
Unlike A ∪B, the set A ∩B equals ;. X
4. Avoid misuse of symbols. Symbols such as =, ≤, ⊆, ∈, etc., are not
words. While it is appropriate to use them in mathematical expressions,
they are out of place in other contexts.
Since the two sets are =, one is a subset of the other. ×
Since the two sets are equal, one is a subset of the other. X
The empty set is a ⊆ of every set. ×
The empty set is a subset of every set. X
Since a is odd and x odd ⇒ x
2 odd, a
2
is odd. ×
Since a is odd and any odd number squared is odd, then a
2
is odd.X
5. Avoid using unnecessary symbols. Mathematics is confusing enough
without them. Don’t muddy the water even more.
No set X has negative cardinality. ×
No set has negative cardinality. X
6. Use the first person plural. In mathematical writing, it is common
to use the words “we” and “us” rather than “I,” “you” or “me.” It is as if
the reader and writer are having a conversation, with the writer guiding
the reader through the details of the proof.
7. Use the active voice. This is just a suggestion, but the active voice
makes your writing more lively.
The value x = 3 is obtained through the division of both sides by 5.×
Dividing both sides by 5, we get the value x = 3. X
8. Explain each new symbol. In writing a proof, you must explain the
meaning of every new symbol you introduce. Failure to do this can lead
to ambiguity, misunderstanding and mistakes. For example, consider
the following two possibilities for a sentence in a proof, where a and b
have been introduced on a previous line.
Mathematical Writing 109
Since a | b, it follows that b = ac. ×
Since a | b, it follows that b = ac for some integer c. X
If you use the first form, then a reader who has been carefully following
your proof may momentarily scan backwards looking for where the c
entered into the picture, not realizing at first that it came from the
definition of divides.
9. Watch out for “it.” The pronoun “it” can cause confusion when it is
unclear what it refers to. If there is any possibility of confusion, you
should avoid the word “it.” Here is an example:
Since X ⊆ Y, and 0 < |X|, we see that it is not empty. ×
Is “it” X or Y? Either one would make sense, but which do we mean?
Since X ⊆ Y, and 0 < |X|, we see that Y is not empty. X
10. Since, because, as, for, so. In proofs, it is common to use these
words as conjunctions joining two statements, and meaning that one
statement is true and as a consequence the other true. The following
statements all mean that P is true (or assumed to be true) and as a
consequence Q is true also.
Q since P Q because P Q, as P Q, for P P, so Q
Since P, Q Because P, Q as P, Q
Notice that the meaning of these constructions is different from that of
“If P, then Q,” for they are asserting not only that P implies Q, but also
that P is true. Exercise care in using them. It must be the case that P
and Q are both statements and that Q really does follow from P.
x ∈ N, so Z ×
x ∈ N, so x ∈ Z X
11. Thus, hence, therefore consequently. These adverbs precede a
statement that follows logically from previous sentences or clauses. Be
sure that a statement follows them.
Therefore 2k +1. ×
Therefore a = 2k +1. X
12. Clarity is the gold standard of mathematical writing. If you
believe breaking a rule makes your writing clearer, then break the rule.
Your mathematical writing will evolve with practice useage. One of the
best ways to develop a good mathematical writing style is to read other
people’s proofs. Adopt what works and avoid what doesn’t.
110 Contrapositive Proof
Exercises for Chapter 5
A. Use the method of contrapositive proof to prove the following statements. (In
each case you should also think about how a direct proof would work. You will
find in most cases that contrapositive is easier.)
1. Suppose n ∈ Z. If n
2
is even, then n is even.
2. Suppose n ∈ Z. If n
2
is odd, then n is odd.
3. Suppose a,b ∈ Z. If a
2
(b
2 −2b) is odd, then a and b are odd.
4. Suppose a,b, c ∈ Z. If a does not divide bc, then a does not divide b.
5. Suppose x ∈ R. If x
2 +5x < 0 then x < 0.
6. Suppose x ∈ R. If x
3 − x > 0 then x > −1.
7. Suppose a,b ∈ Z. If both ab and a+ b are even, then both a and b are even.
8. Suppose x ∈ R. If x
5 −4x
4 +3x
3 − x
2 +3x−4 ≥ 0, then x ≥ 0.
9. Suppose n ∈ Z. If 3 - n
2
, then 3 - n.
10. Suppose x, y, z ∈ Z and x 6= 0. If x - yz, then x - y and x - z.
11. Suppose x, y ∈ Z. If x
2
(y+3) is even, then x is even or y is odd.
12. Suppose a ∈ Z. If a
2
is not divisible by 4, then a is odd.
13. Suppose x ∈ R. If x
5 +7x
3 +5x ≥ x
4 + x
2 +8, then x ≥ 0.
B. Prove the following statements using either direct or contrapositive proof.
Sometimes one approach will be much easier than the other.
14. If a,b ∈ Z and a and b have the same parity, then 3a+7 and 7b −4 do not.
15. Suppose x ∈ Z. If x
3 −1 is even, then x is odd.
16. Suppose x ∈ Z. If x+ y is even, then x and y have the same parity.
17. If n is odd, then 8 | (n
2 −1).
18. For any a,b ∈ Z, it follows that (a+ b)
3 ≡ a
3 + b
3
(mod 3).
19. Let a,b ∈ Z and n ∈ N. If a ≡ b (mod n) and a ≡ c (mod n), then c ≡ b (mod n).
20. If a ∈ Z and a ≡ 1 (mod 5), then a
2 ≡ 1 (mod 5).
21. Let a,b ∈ Z and n ∈ N. If a ≡ b (mod n), then a
3 ≡ b
3
(mod n)
22. Let a ∈ Z, n ∈ N. If a has remainder r when divided by n, then a ≡ r (mod n).
23. Let a,b, c ∈ Z and n ∈ N. If a ≡ b (mod n), then ca ≡ cb (mod n).
24. If a ≡ b (mod n) and c ≡ d (mod n), then ac ≡ bd (mod n).
25. If n ∈ N and 2
n −1 is prime, then n is prime.
26. If n = 2
k −1 for k ∈ N, then every entry in Row n of Pascal’s Triangle is odd.
27. If a ≡ 0 (mod 4) or a ≡ 1 (mod 4), then ¡
a
2
¢
is even.
28. If n ∈ Z, then 4 - (n
2 −3).
29. If integers a and b are not both zero, then gcd(a,b) = gcd(a− b,b).
30. If a ≡ b (mod n), then gcd(a,n) = gcd(b,n).
31. Suppose the division algorithm applied to a and b yields a = qb + r. Then
gcd(a,b) = gcd(r,b).
CHAPTER 6
Proof by Contradiction
We now explore a third method of proof: proof by contradiction.
This method is not limited to proving just conditional statements—
it can be used to prove any kind of statement whatsoever. The basic idea
is to assume that the statement we want to prove is false, and then show
that this assumption leads to nonsense. We are then led to conclude that
we were wrong to assume the statement was false, so the statement must
be true. As an example, consider the following proposition and its proof.
Proposition If a,b ∈ Z, then a
2 −4b 6= 2.
Proof. Suppose this proposition is false.
This conditional statement being false means there exist numbers a and b
for which a,b ∈ Z is true, but a
2 −4b 6= 2 is false.
In other words, there exist integers a,b ∈ Z for which a
2 −4b = 2.
From this equation we get a
2 = 4b +2 = 2(2b +1), so a
2
is even.
Because a
2
is even, it follows that a is even, so a = 2c for some integer c.
Now plug a = 2c back into the boxed equation to get (2c)
2 −4b = 2,
so 4c
2 −4b = 2. Dividing by 2, we get 2c
2 −2b = 1.
Therefore 1 = 2(c
2 − b), and because c
2 − b ∈ Z, it follows that 1 is even.
We know 1 is not even, so something went wrong.
But all the logic after the first line of the proof is correct, so it must be
that the first line was incorrect. In other words, we were wrong to assume
the proposition was false. Thus the proposition is true. ■
You may be a bit suspicious of this line of reasoning, but in the next
section we will see that it is logically sound. For now, notice that at
the end of the proof we deduced that 1 is even, which conflicts with our
knowledge that 1 is odd. In essence, we have obtained the statement
(1 is odd)∧ ∼ (1 is odd), which has the form C∧ ∼ C. Notice that no matter
what statement C is, and whether or not it is true, the statement C∧ ∼ C
is false. A statement—like this one—that cannot be true is called a
contradiction. Contradictions play a key role in our new technique.
112 Proof by Contradiction
6.1 Proving Statements with Contradiction
Let’s now see why the proof on the previous page is logically valid. In
that proof we needed to show that a statement P : (a,b ∈ Z) ⇒ (a
2 −4b 6= 2)
was true. The proof began with the assumption that P was false, that is
that ∼ P was true, and from this we deduced C∧ ∼ C. In other words we
proved that ∼ P being true forces C∧ ∼ C to be true, and this means that
we proved that the conditional statement (∼ P) ⇒ (C ∧ ∼ C) is true. To see
that this is the same as proving P is true, look at the following truth table
for (∼ P) ⇒ (C ∧ ∼ C). Notice that the columns for P and (∼ P) ⇒ (C ∧ ∼ C)
are exactly the same, so P is logically equivalent to (∼ P) ⇒ (C ∧ ∼ C).
P C ∼ P C ∧ ∼ C (∼ P) ⇒ (C ∧ ∼ C)
T T F F T
T F F F T
F T T F F
F F T F F
Therefore to prove a statement P, it suffices to instead prove the conditional
statement (∼ P) ⇒ (C ∧ ∼ C). This can be done with direct proof: Assume
∼ P and deduce C ∧ ∼ C. Here is the outline:
Outline for Proof by Contradiction
Proposition P.
Proof. Suppose ∼ P.
.
.
.
Therefore C ∧ ∼ C. ■
One slightly unsettling feature of this method is that we may not know
at the beginning of the proof what the statement C is going to be. In
doing the scratch work for the proof, you assume that ∼ P is true, then
deduce new statements until you have deduced some statement C and its
negation ∼ C.
If this method seems confusing, look at it this way. In the first line of
the proof we suppose ∼ P is true, that is we assume P is false. But if P is
really true then this contradicts our assumption that P is false. But we
haven’t yet proved P to be true, so the contradiction is not obvious. We
use logic and reasoning to transform the non-obvious contradiction ∼ P to
an obvious contradiction C∧ ∼ C.
Proving Statements with Contradiction 113
The idea of proof by contradiction is quite ancient, and goes back at
least as far as the Pythagoreans, who used it to prove that certain numbers
are irrational. Our next example follows their logic to prove that p
2 is
irrational. Recall that a number is rational if it equals a fraction of two
integers, and it is irrational if it cannot be expressed as a fraction of two
integers. Here is the exact definition.
Definition 6.1 A real number x is rational if x =
a
b
for some a,b ∈ Z.
Also, x is irrational if it is not rational, that is if x 6=
a
b
for every a,b ∈ Z.
We are now ready to use contradiction to prove that p
2 is irrational.
According to the outline, the first line of the proof should be “Suppose that
it is not true that p
2 is irrational.” But it is helpful (though not mandatory)
to tip our reader off to the fact that we are using proof by contradiction.
One standard way of doing this is to make the first line “Suppose for the
sake of contradiction that it is not true that p
2 is irrational."
Proposition The number p
2 is irrational.
Proof. Suppose for the sake of contradiction that it is not true that p
2 is
irrational. Then p
2 is rational, so there are integers a and b for which
p
2 =
a
b
. (6.1)
Let this fraction be fully reduced; in particular, this means that a and b are
not both even. (If they were both even, the fraction could be further reduced
by factoring 2’s from the numerator and denominator and canceling.)
Squaring both sides of Equation 6.1 gives 2 =
a
2
b
2
, and therefore
a
2 = 2b
2
. (6.2)
From this it follows that a
2
is even. But we proved earlier (Exercise 1
on page 110) that a
2 being even implies a is even. Thus, as we know
that a and b are not both even, it follows that b is odd. Now, since a is
even there is an integer c for which a = 2c. Plugging this value for a into
Equation (6.2), we get (2c)
2 = 2b
2
, so 4c
2 = 2b
2
, and hence b
2 = 2c
2
. This
means b
2
is even, so b is even also. But previously we deduced that b is
odd. Thus we have the contradiction b is even and b is odd. ■
To appreciate the power of proof by contradiction, imagine trying to
prove that p
2 is irrational without it. Where would we begin? What would
be our initial assumption? There are no clear answers to these questions.
114 Proof by Contradiction
Proof by contradiction gives us a starting point: Assume p
2 is rational,
and work from there.
In the above proof we got the contradiction (b is even) ∧ ∼(b is even)
which has the form C∧ ∼ C. In general, your contradiction need not
necessarily be of this form. Any statement that is clearly false is sufficient.
For example 2 6= 2 would be a fine contradiction, as would be 4 | 2, provided
that you could deduce them.
Here is another ancient example, dating back at least as far as Euclid:
Proposition There are infinitely many prime numbers.
Proof. For the sake of contradiction, suppose there are only finitely many
prime numbers. Then we can list all the prime numbers as p1, p2, p3,... pn,
where p1 = 2, p2 = 3, p3 = 5, p4 = 7 and so on. Thus pn is the nth and largest
prime number. Now consider the number a = (p1p2p3 ··· pn)+1, that is, a is
the product of all prime numbers, plus 1. Now a, like any natural number
greater than 1, has at least one prime divisor, and that means pk | a for at
least one of our n prime numbers pk. Thus there is an integer c for which
a = cpk, which is to say
(p1p2p3 ··· pk−1pk pk+1 ··· pn)+1 = cpk.
Dividing both sides of this by pk gives us
(p1p2p3 ··· pk−1pk+1 ··· pn)+
1
pk
= c,
so
1
pk
= c −(p1p2p3 ··· pk−1pk+1 ··· pn).
The expression on the right is an integer, while the expression on the left
is not an integer. This is a contradiction. ■
Proof by contradiction often works well in proving statements of the
form ∀x,P(x). The reason is that the proof set-up involves assuming
∼ ∀x,P(x), which as we know from Section 2.10 is equivalent to ∃ x,∼ P(x).
This gives us a specific x for which ∼ P(x) is true, and often that is enough
to produce a contradiction. Here is an example:
Proposition For every real number x ∈ [0,π/2], we have sinx+cos x ≥ 1.
Proof. Suppose for the sake of contradiction that this is not true.
Then there exists an x ∈ [0,π/2] for which sinx+cos x < 1.
Proving Conditional Statements by Contradiction 115
Since x ∈ [0,π/2], neither sinx nor cos x is negative, so 0 ≤ sinx+cos x < 1.
Thus 0
2 ≤ (sinx+cos x)
2 < 1
2
, which gives 0
2 ≤ sin2
x+2sinxcos x+cos2
x < 1
2
.
As sin2
x+cos2
x = 1, this becomes 0 ≤ 1+2sinxcos x < 1, so 1+2sinxcos x < 1.
Subtracting 1 from both sides gives 2sinxcos x < 0.
But this contradicts the fact that neither sinx nor cos x is negative. ■
6.2 Proving Conditional Statements by Contradiction
Since the previous two chapters dealt exclusively with proving conditional
statements, we now formalize the procedure in which contradiction is used
to prove a conditional statement. Suppose we want to prove a proposition
of the following form.
Proposition If P, then Q.
Thus we need to prove that P ⇒ Q is a true statement. Proof by
contradiction begins with the assumption that ∼ (P ⇒ Q) is true, that is,
that P ⇒ Q is false. But we know that P ⇒ Q being false means that it is
possible that P can be true while Q is false. Thus the first step in the
proof is to assume P and ∼ Q. Here is an outline:
Outline for Proving a Conditional
Statement with Contradiction
Proposition If P, then Q.
Proof. Suppose P and ∼ Q.
.
.
.
Therefore C ∧ ∼ C. ■
To illustrate this new technique, we revisit a familiar result: If a
2
is
even, then a is even. According to the outline, the first line of the proof
should be “For the sake of contradiction, suppose a
2
is even and a is not
even.”
Proposition Suppose a ∈ Z. If a
2
is even, then a is even.
Proof. For the sake of contradiction, suppose a
2
is even and a is not even.
Then a
2
is even, and a is odd.
Since a is odd, there is an integer c for which a = 2c +1.
Then a
2 = (2c +1)2 = 4c
2 +4c +1 = 2(2c
2 +2c)+1, so a
2
is odd.
Thus a
2
is even and a
2
is not even, a contradiction. ■
116 Proof by Contradiction
Here is another example.
Proposition If a,b ∈ Z and a ≥ 2, then a - b or a - (b +1).
Proof. Suppose for the sake of contradiction there exist a,b ∈ Z with a ≥ 2,
and for which it is not true that a - b or a - (b +1).
By DeMorgan’s law, we have a | b and a | (b +1).
The definition of divisibility says there are c,d ∈ Z with b = ac and b+1 = ad.
Subtracting one equation from the other gives ad − ac = 1, so a(d − c) = 1.
Since a is positive, d−c is also positive (otherwise a(d−c) would be negative).
Then d − c is a positive integer and a(d − c) = 1, so a = 1/(d − c) < 2.
Thus we have a ≥ 2 and a < 2, a contradiction. ■
6.3 Combining Techniques
Often, especially in more complex proofs, several proof techniques are
combined within a single proof. For example, in proving a conditional
statement P ⇒ Q, we might begin with direct proof and thus assume P to
be true with the aim of ultimately showing Q is true. But the truth of Q
might hinge on the truth of some other statement R which—together with
P—would imply Q. We would then need to prove R, and we would use
whichever proof technique seems most appropriate. This can lead to “proofs
inside of proofs.” Consider the following example. The overall approach is
direct, but inside the direct proof is a separate proof by contradiction.
Proposition Every non-zero rational number can be expressed as a
product of two irrational numbers.
Proof. This proposition can be reworded as follows: If r is a non-zero
rational number, then r is a product of two irrational numbers. In what
follows, we prove this with direct proof.
Suppose r is a non-zero rational number. Then r =
a
b
for integers a
and b. Also, r can be written as a product of two numbers as follows:
r =
p
2·
r
p
2
.
We know p
2 is irrational, so to complete the proof we must show r/
p
2 is
also irrational.
To show this, assume for the sake of contradiction that r/
p
2 is rational.
This means
r
p
2
=
c
d
Some Words of Advice 117
for integers c and d, so
p
2 = r
d
c
.
But we know r = a/b, which combines with the above equation to give
p
2 = r
d
c
=
a
b
d
c
=
ad
bc
.
This means p
2 is rational, which is a contradiction because we know it is
irrational. Therefore r/
p
2 is irrational.
Consequently r =
p
2· r/
p
2 is a product of two irrational numbers. ■
For another example of a proof-within-a-proof, try Exercise 5 at the
end of this chapter (or see its solution). Exercise 5 asks you to prove that
p
3 is irrational. This turns out to be slightly trickier than proving that
p
2 is irrational.
6.4 Some Words of Advice
Despite the power of proof by contradiction, it’s best to use it only when the
direct and contrapositive approaches do not seem to work. The reason for
this is that a proof by contradiction can often have hidden in it a simpler
contrapositive proof, and if this is the case it’s better to go with the simpler
approach. Consider the following example.
Proposition Suppose a ∈ Z. If a
2 −2a+7 is even, then a is odd.
Proof. To the contrary, suppose a
2 −2a+7 is even and a is not odd.
That is, suppose a
2 −2a+7 is even and a is even.
Since a is even, there is an integer c for which a = 2c.
Then a
2 −2a+7 = (2c)
2 −2(2c)+7 = 2(2c
2 −2c +3)+1, so a
2 −2a+7 is odd.
Thus a
2 −2a+7 is both even and odd, a contradiction. ■
Though there is nothing really wrong with this proof, notice that part
of it assumes a is not odd and deduces that a
2 −2a +7 is not even. That is
the contrapositive approach! Thus it would be more efficient to proceed as
follows, using contrapositive proof.
Proposition Suppose a ∈ Z. If a
2 −2a+7 is even, then a is odd.
Proof. (Contrapositive) Suppose a is not odd.
Then a is even, so there is an integer c for which a = 2c.
Then a
2 −2a+7 = (2c)
2 −2(2c)+7 = 2(2c
2 −2c +3)+1, so a
2 −2a+7 is odd.
Thus a
2 −2a+7 is not even. ■
118 Proof by Contradiction
Exercises for Chapter 6
A. Use the method of proof by contradiction to prove the following statements.
(In each case, you should also think about how a direct or contrapositive proof
would work. You will find in most cases that proof by contradiction is easier.)
1. Suppose n ∈ Z. If n is odd, then n
2
is odd.
2. Suppose n ∈ Z. If n
2
is odd, then n is odd.
3. Prove that p3
2 is irrational.
4. Prove that p
6 is irrational.
5. Prove that p
3 is irrational.
6. If a,b ∈ Z, then a
2 −4b −2 6= 0.
7. If a,b ∈ Z, then a
2 −4b −3 6= 0.
8. Suppose a,b, c ∈ Z. If a
2 + b
2 = c
2
, then a or b is even.
9. Suppose a,b ∈ R. If a is rational and ab is irrational, then b is irrational.
10. There exist no integers a and b for which 21a+30b = 1.
11. There exist no integers a and b for which 18a+6b = 1.
12. For every positive x ∈ Q, there is a positive y ∈ Q for which y < x.
13. For every x ∈ [π/2,π], sinx−cos x ≥ 1.
14. If A and B are sets, then A ∩(B − A) = ;.
15. If b ∈ Z and b - k for every k ∈ N, then b = 0.
16. If a and b are positive real numbers, then a+ b ≥ 2
p
ab.
17. For every n ∈ Z, 4 - (n
2 +2).
18. Suppose a,b ∈ Z. If 4 | (a
2 + b
2
), then a and b are not both odd.
B. Prove the following statements using any method from Chapters 4, 5 or 6.
19. The product of any five consecutive integers is divisible by 120. (For
example, the product of 3,4,5,6 and 7 is 2520, and 2520 = 120·21.)
20. We say that a point P = (x, y) in R
2
is rational if both x and y are rational.
More precisely, P is rational if P = (x, y) ∈ Q
2
. An equation F(x, y) = 0 is said
to have a rational point if there exists x0, y0 ∈ Q such that F(x0, y0) = 0. For
example, the curve x
2 + y
2 −1 = 0 has rational point (x0, y0) = (1,0). Show that
the curve x
2 + y
2 −3 = 0 has no rational points.
21. Exercise 20 (above) involved showing that there are no rational points on
the curve x
2 + y
2 −3 = 0. Use this fact to show that p
3 is irrational.
22. Explain why x
2 + y
2 −3 = 0 not having any rational solutions (Exercise 20)
implies x
2 + y
2 −3
k = 0 has no rational solutions for k an odd, positive integer.
23. Use the above result to prove that p
3
k is irrational for all odd, positive k.
24. The number log2 3 is irrational.
Part III
More on Proof

CHAPTER 7
Proving Non-Conditional Statements
T
he last three chapters introduced three major proof techniques: direct,
contrapositive and contradiction. These three techniques are used to
prove statements of the form “If P, then Q.” As we know, most theorems
and propositions have this conditional form, or they can be reworded to
have this form. Thus the three main techniques are quite important. But
some theorems and propositions cannot be put into conditional form. For
example, some theorems have form “P if and only if Q.” Such theorems
are biconditional statements, not conditional statements. In this chapter
we examine ways to prove them. In addition to learning how to prove
if-and-only-if theorems, we will also look at two other types of theorems.
7.1 If-and-Only-If Proof
Some propositions have the form
P if and only if Q.
We know from Section 2.4 that this statement asserts that both of the
following conditional statements are true:
If P, then Q.
If Q, then P.
So to prove “P if and only if Q,” we must prove two conditional statements.
Recall from Section 2.4 that Q ⇒ P is called the converse of P ⇒ Q. Thus
we need to prove both P ⇒ Q and its converse. Since these are both conditional
statements we may prove them with either direct, contrapositive or
contradiction proof. Here is an outline:
Outline for If-and-Only-If Proof
Proposition P if and only if Q.
Proof.
[Prove P ⇒ Q using direct, contrapositive or contradiction proof.]
[Prove Q ⇒ P using direct, contrapositive or contradiction proof.]
122 Proving Non-Conditional Statements
Let’s start with a very simple example. You already know that an
integer n is odd if and only if n
2
is odd, but let’s prove it anyway, just
to illustrate the outline. In this example we prove (n is odd)⇒(n
2
is odd)
using direct proof and (n
2
is odd)⇒(n is odd) using contrapositive proof.
Proposition The integer n is odd if and only if n
2
is odd.
Proof. First we show that n being odd implies that n
2
is odd. Suppose n
is odd. Then, by definition of an odd number, n = 2a+1 for some integer a.
Thus n
2 = (2a+1)2 = 4a
2 +4a+1 = 2(2a
2 +2a)+1. This expresses n
2 as twice
an integer, plus 1, so n
2
is odd.
Conversely, we need to prove that n
2 being odd implies that n is odd.
We use contrapositive proof. Suppose n is not odd. Then n is even, so
n = 2a for some integer a (by definition of an even number). Thus n
2 =
(2a)
2 = 2(2a
2
), so n
2
is even because it’s twice an integer. Thus n
2
is not
odd. We’ve now proved that if n is not odd, then n
2
is not odd, and this is
a contrapositive proof that if n
2
is odd then n is odd. ■
In proving “P if and only if Q,” you should begin a new paragraph
when starting the proof of Q ⇒ P. Since this is the converse of P ⇒ Q, it’s
a good idea to begin the paragraph with the word “Conversely” (as we did
above) to remind the reader that you’ve finished the first part of the proof
and are moving on to the second. Likewise, it’s a good idea to remind the
reader of exactly what statement that paragraph is proving.
The next example uses direct proof in both parts of the proof.
Proposition Suppose a and b are integers. Then a ≡ b (mod 6) if and
only if a ≡ b (mod 2) and a ≡ b (mod 3).
Proof. First we prove that if a ≡ b (mod 6), then a ≡ b (mod 2) and a ≡ b
(mod 3). Suppose a ≡ b (mod 6). This means 6 | (a−b), so there is an integer
n for which
a− b = 6n.
From this we get a−b = 2(3n), which implies 2 | (a−b), so a ≡ b (mod 2). But
we also get a−b = 3(2n), which implies 3 | (a−b), so a ≡ b (mod 3). Therefore
a ≡ b (mod 2) and a ≡ b (mod 3).
Conversely, suppose a ≡ b (mod 2) and a ≡ b (mod 3). Since a ≡ b (mod 2)
we get 2 | (a−b), so there is an integer k for which a−b = 2k. Therefore a−b
is even. Also, from a ≡ b (mod 3) we get 3 | (a − b), so there is an integer `
for which
a− b = 3`.
Equivalent Statements 123
But since we know a − b is even, it follows that ` must be even also, for
if it were odd then a − b = 3` would be odd (because a − b would be the
product of two odd integers). Hence ` = 2m for some integer m. Thus
a− b = 3` = 3·2m = 6m. This means 6 | (a− b), so a ≡ b (mod 6). ■
Since if-and-only-if proofs simply combine methods with which we are
already familiar, we will not do any further examples in this section.
However, it is of utmost importance that you practice your skill on some
of this chapter’s exercises.
7.2 Equivalent Statements
In other courses you will sometimes encounter a certain kind of theorem
that is neither a conditional nor a biconditional statement. Instead, it
asserts that a list of statements is “equivalent.” You saw this (or will see
it) in your linear algebra textbook, which featured the following theorem:
Theorem Suppose A is an n × n matrix. The following statements are
equivalent:
(a) The matrix A is invertible.
(b) The equation Ax = b has a unique solution for every b ∈ R
n
.
(c) The equation Ax = 0 has only the trivial solution.
(d) The reduced row echelon form of A is In.
(e) det(A) 6= 0.
(f) The matrix A does not have 0 as an eigenvalue.
When a theorem asserts that a list of statements is “equivalent,” it is
asserting that either the statements are all true, or they are all false.
Thus the above theorem tells us that whenever we are dealing with a
particular n×n matrix A, then either the statements (a) through (f) are all
true for A, or statements (a) through (f) are all false for A. For example, if
we happen to know that det(A) 6= 0, the theorem assures us that in addition
to statement (e) being true, all the statements (a) through (f) are true. On
the other hand, if it happens that det(A) = 0, the theorem tells us that all
statements (a) through (f) are false. In this way, the theorem multiplies
our knowledge of A by a factor of six. Obviously that can be very useful.
What method would we use to prove such a theorem? In a certain
sense, the above theorem is like an if-and-only-if theorem. An if-and-only-if
theorem of form P ⇔ Q asserts that P and Q are either both true or both
false, that is, that P and Q are equivalent. To prove P ⇔ Q we prove P ⇒ Q
followed by Q ⇒ P, essentially making a “cycle” of implications from P to Q
124 Proving Non-Conditional Statements
and back to P. Similarly, one approach to proving the theorem about the
n × n matrix would be to prove the conditional statement (a) ⇒ (b), then
(b) ⇒ (c), then (c) ⇒ (d), then (d) ⇒ (e), then (e) ⇒ (f ) and finally (f ) ⇒ (a).
This pattern is illustrated below.
(a) =⇒ (b) =⇒ (c)
⇑ ⇓
(f ) ⇐= (e) ⇐= (d)
Notice that if these six implications have been proved, then it really does
follow that the statements (a) through (f) are either all true or all false. If
one of them is true, then the circular chain of implications forces them
all to be true. On the other hand, if one of them (say (c)) is false, the fact
that (b) ⇒ (c) is true forces (b) to be false. This combined with the truth of
(a) ⇒ (b) makes (a) false, and so on counterclockwise around the circle.
Thus to prove that n statements are equivalent, it suffices to prove n
conditional statements showing each statement implies another, in circular
pattern. But it is not necessary that the pattern be circular. The following
schemes would also do the job:
(a) =⇒ (b) ⇐⇒ (c)
⇑ ⇓
(f ) ⇐= (e) ⇐⇒ (d)
(a) ⇐⇒ (b) ⇐⇒ (c)
m
(f ) ⇐⇒ (e) ⇐⇒ (d)
But a circular pattern yields the fewest conditional statements that
must be proved. Whatever the pattern, each conditional statement can be
proved with either direct, contrapositive or contradiction proof.
Though we shall not do any of these proofs in this text, you are sure to
encounter them in subsequent courses.
7.3 Existence Proofs; Existence and Uniqueness Proofs
Up until this point, we have dealt with proving conditional statements
or with statements that can be expressed with two or more conditional
statements. Generally, these conditional statements have form P(x) ⇒ Q(x).
(Possibly with more than one variable.) We saw in Section 2.8 that this
can be interpreted as a universally quantified statement ∀ x,P(x) ⇒ Q(x).
Existence Proofs; Existence and Uniqueness Proofs 125
Thus, conditional statements are universally quantified statements, so
in proving a conditional statement—whether we use direct, contrapositive
or contradiction proof—we are really proving a universally quantified
statement.
But how would we prove an existentially quantified statement? What
technique would we employ to prove a theorem of the following form?
∃ x,R(x)
This statement asserts that there exists some specific object x for which
R(x) is true. To prove ∃ x,R(x) is true, all we would have to do is find and
display an example of a specific x that makes R(x) true.
Though most theorems and propositions are conditional (or if-andonly-if)
statements, a few have the form ∃ x,R(x). Such statements are
called existence statements, and theorems that have this form are called
existence theorems. To prove an existence theorem, all you have to do
is provide a particular example that shows it is true. This is often quite
simple. (But not always!) Here are some examples:
Proposition There exists an even prime number.
Proof. Observe that 2 is an even prime number. ■
Admittedly, this last proposition was a bit of an oversimplification. The
next one is slightly more challenging.
Proposition There exists an integer that can be expressed as the sum
of two perfect cubes in two different ways.
Proof. Consider the number 1729. Note that 1
3 +123 = 1729 and 9
3 +103 =
1729. Thus the number 1729 can be expressed as the sum of two perfect
cubes in two different ways. ■
Sometimes in the proof of an existence statement, a little verification is
needed to show that the example really does work. For example, the above
proof would be incomplete if we just asserted that 1729 can be written as
a sum of two cubes in two ways without showing how this is possible.
WARNING: Although an example suffices to prove an existence statement,
a single example does not prove a conditional statement.
126 Proving Non-Conditional Statements
Often an existence statement will be embedded inside of a conditional
statement. Consider the following. (Recall the definition of gcd on page 90.)
If a,b ∈ N, then there exist integers k and ` for which gcd(a,b) = ak+b`.
This is a conditional statement that has the form
a,b ∈ N =⇒ ∃ k,` ∈ Z, gcd(a,b) = ak + b`.
To prove it with direct proof, we would first assume that a,b ∈ N, then
prove the existence statement ∃ k,` ∈ Z, gcd(a,b) = ak + b`. That is, we
would produce two integers k and ` (which depend on a and b) for which
gcd(a,b) = ak+b`. Let’s carry out this plan. (We will use this fundamental
proposition several times later, so it is given a number.)
Proposition 7.1 If a,b ∈ N, then there exist integers k and ` for which
gcd(a,b) = ak + b`.
Proof. (Direct) Suppose a,b ∈ N. Consider the set A =
©
ax + b y : x, y ∈ Z
ª
.
This set contains both positive and negative integers, as well as 0. (Reason:
Let y = 0 and let x range over all integers. Then ax+ b y = ax ranges over
all multiples of a, both positive, negative and zero.) Let d be the smallest
positive element of A. Then, because d is in A, it must have the form
d = ak + b` for some specific k,` ∈ Z.
To finish, we will show d = gcd(a,b). We will first argue that d is a
common divisor of a and b, and then that it is the greatest common divisor.
To see that d | a, use the division algorithm (page 29) to write a = qd+r
for integers q and r with 0 ≤ r < d. The equation a = qd + r yields
r = a− qd
= a− q(ak + b`)
= a(1− qk)+ b(−q`).
Therefore r has form r = ax + b y, so it belongs to A. But 0 ≤ r < d and d
is the smallest positive number in A, so r can’t be positive; hence r = 0.
Updating our equation a = qd + r, we get a = qd, so d | a. Repeating this
argument with b = qd + r shows d | b. Thus d is indeed a common divisor
of a and b. It remains to show that it is the greatest common divisor.
As gcd(a,b) divides a and b, we have a = gcd(a,b)·m and b = gcd(a,b)·n for
some m,n ∈ Z. So d = ak+b` = gcd(a,b)·mk+gcd(a,b)·n` = gcd(a,b)
¡
mk+n`
¢
,
and thus d is a multiple of gcd(a,b). Therefore d ≥ gcd(a,b). But d can’t
be a larger common divisor of a and b than gcd(a,b), so d = gcd(a,b). ■
Existence Proofs; Existence and Uniqueness Proofs 127
We conclude this section with a discussion of so-called uniqueness
proofs. Some existence statements have form “There is a unique x for
which P(x).” Such a statement asserts that there is exactly one example x
for which P(x) is true. To prove it, you must produce an example x = d for
which P(d) is true, and you must show that d is the only such example.
The next proposition illustrates this. In essence, it asserts that the set
©
ax+ b y : x, y ∈ Z
ª
consists precisely of all the multiples of gcd(a,b).
Proposition Suppose a,b ∈ N. Then there exists a unique d ∈ N for which:
An integer m is a multiple of d if and only if m = ax+ b y for some x, y ∈ Z.
Proof. Suppose a,b ∈ N. Let d = gcd(a,b). We now show that an integer m
is a multiple of d if and only if m = ax+b y for some x, y ∈ Z. Let m = dn be a
multiple of d. By Proposition 7.1 (on the previous page), there are integers
k and ` for which d = ak + b`. Then m = dn = (ak + b`)n = a(kn)+ b(`n), so
m = ax+ b y for integers x = kn and y = `n.
Conversely, suppose m = ax+ b y for some x, y ∈ Z. Since d = gcd(a,b) is
a divisor of both a and b, we have a = dc and b = de for some c, e ∈ Z. Then
m = ax+ b y = dcx+ de y = d(cx+ e y), and this is a multiple of d.
We have now shown that there is a natural number d with the property
that m is a multiple of d if and only if m = ax + b y for some x, y ∈ Z. It
remains to show that d is the unique such natural number. To do this,
suppose d
0
is any natural number with the property that d has:
m is a multiple of d
0 ⇐⇒ m = ax+ b y for some x, y ∈ Z. (7.1)
We next argue that d
0 = d; that is, d is the unique natural number with
the stated property. Because of (7.1), m = a·1+ b ·0 = a is a multiple of d
0
.
Likewise m = a · 0 + b · 1 = b is a multiple of d
0
. Hence a and b are both
multiples of d
0
, so d
0
is a common divisor of a and b, and therefore
d
0 ≤ gcd(a,b) = d.
But also, by (7.1), the multiple m = d
0
· 1 = d
0 of d
0
can be expressed as
d
0 = ax+b y for some x, y ∈ Z. As noted in the second paragraph of the proof,
a = dc and b = de for some c, e ∈ Z. Thus d
0 = ax+ b y = dcx+ de y = d(cx+ e y),
so d
0
is a multiple d. As d
0 and d are both positive, it follows that
d ≤ d
0
.
We’ve now shown that d
0 ≤ d and d ≤ d
0
, so d = d
0
. The proof is complete. ■
128 Proving Non-Conditional Statements
7.4 Constructive Versus Non-Constructive Proofs
Existence proofs fall into two categories: constructive and non-constructive.
Constructive proofs display an explicit example that proves the theorem;
non-constructive proofs prove an example exists without actually giving it.
We illustrate the difference with two proofs of the same fact: There exist
irrational numbers x and y (possibly equal) for which x
y
is rational.
Proposition There exist irrational numbers x, y for which x
y
is rational.
Proof. Let x =
p
2
p
2
and y =
p
2. We know y is irrational, but it is not clear
whether x is rational or irrational. On one hand, if x is irrational, then
we have an irrational number to an irrational power that is rational:
x
y =
µp
2
p
2
¶
p
2
=
p
2
p
2
p
2
=
p
2
2
= 2.
On the other hand, if x is rational, then y
y =
p
2
p
2
= x is rational. Either
way, we have a irrational number to an irrational power that is rational. ■
The above is a classic example of a non-constructive proof. It shows
that there exist irrational numbers x and y for which x
y
is rational without
actually producing (or constructing) an example. It convinces us that one
of ¡p
2
p
2¢
p
2
or p
2
p
2
is an irrational number to an irrational power that
is rational, but it does not say which one is the correct example. It thus
proves that an example exists without explicitly stating one.
Next comes a constructive proof of this statement, one that produces
(or constructs) two explicit irrational numbers x, y for which x
y
is rational.
Proposition There exist irrational numbers x, y for which x
y
is rational.
Proof. Let x =
p
2 and y = log2 9. Then
x
y =
p
2
log2 9
=
p
2
log2 3
2
=
p
2
2 log2 3
=
³p
2
2
´log2 3
= 2
log2 3 = 3.
As 3 is rational, we have shown that x
y = 3 is rational.
We know that x =
p
2 is irrational. The proof will be complete if we
can show that y = log2 9 is irrational. Suppose for the sake of contradiction
that log2 9 is rational, so there are integers a and b for which a
b
= log2 9.
This means 2
a/b = 9, so ¡
2
a/b
¢b
= 9
b
, which reduces to 2
a = 9
b
. But 2
a
is even,
while 9
b
is odd (because it is the product of the odd number 9 with itself
b times). This is a contradiction; the proof is complete. ■
Constructive Versus Non-Constructive Proofs 129
This existence proof has inside of it a separate proof (by contradiction)
that log2 9 is irrational. Such combinations of proof techniques are, of
course, typical.
Be alert to constructive and non-constructive proofs as you read proofs
in other books and articles, as well as to the possibility of crafting such
proofs of your own.
Exercises for Chapter 7
Prove the following statements. These exercises are cumulative, covering all
techniques addressed in Chapters 4–7.
1. Suppose x ∈ Z. Then x is even if and only if 3x+5 is odd.
2. Suppose x ∈ Z. Then x is odd if and only if 3x+6 is odd.
3. Given an integer a, then a
3 + a
2 + a is even if and only if a is even.
4. Given an integer a, then a
2 +4a+5 is odd if and only if a is even.
5. An integer a is odd if and only if a
3
is odd.
6. Suppose x, y ∈ R. Then x
3 + x
2
y = y
2 + x y if and only if y = x
2 or y = −x.
7. Suppose x, y ∈ R. Then (x+ y)
2 = x
2 + y
2
if and only if x = 0 or y = 0.
8. Suppose a,b ∈ Z. Prove that a ≡ b (mod 10) if and only if a ≡ b (mod 2) and a ≡ b
(mod 5).
9. Suppose a ∈ Z. Prove that 14 | a if and only if 7 | a and 2 | a.
10. If a ∈ Z, then a
3 ≡ a (mod 3).
11. Suppose a,b ∈ Z. Prove that (a−3)b
2
is even if and only if a is odd or b is even.
12. There exist a positive real number x for which x
2 <
p
x.
13. Suppose a,b ∈ Z. If a+ b is odd, then a
2 + b
2
is odd.
14. Suppose a ∈ Z. Then a
2
| a if and only if a ∈
©
−1,0,1
ª
.
15. Suppose a,b ∈ Z. Prove that a+ b is even if and only if a and b have the same
parity.
16. Suppose a,b ∈ Z. If ab is odd, then a
2 + b
2
is even.
17. There is a prime number between 90 and 100.
18. There is a set X for which N ∈ X and N ⊆ X.
19. If n ∈ N, then 2
0 +2
1 +2
2 +2
3 +2
4 +··· +2
n = 2
n+1 −1.
20. There exists an n ∈ N for which 11 | (2n −1).
21. Every real solution of x
3 + x+3 = 0 is irrational.
22. If n ∈ Z, then 4 | n
2 or 4 | (n
2 −1).
23. Suppose a,b and c are integers. If a | b and a | (b
2 − c), then a | c.
24. If a ∈ Z, then 4 - (a
2 −3).
130 Proving Non-Conditional Statements
25. If p > 1 is an integer and n - p for each integer n for which 2 ≤ n ≤
p
p, then p is
prime.
26. The product of any n consecutive positive integers is divisible by n!.
27. Suppose a,b ∈ Z. If a
2 + b
2
is a perfect square, then a and b are not both odd.
28. Prove the division algorithm: If a,b ∈ N, there exist unique integers q, r for
which a = bq + r, and 0 ≤ r < b. (A proof of existence is given in Section 1.9, but
uniqueness needs to be established too.)
29. If a | bc and gcd(a,b) = 1, then a | c.
(Suggestion: Use the proposition on page 126.)
30. Suppose a,b, p ∈ Z and p is prime. Prove that if p | ab then p | a or p | b.
(Suggestion: Use the proposition on page 126.)
31. If n ∈ Z, then gcd(n,n+1) = 1.
32. If n ∈ Z, then gcd(n,n+2) ∈
©
1,2
ª
.
33. If n ∈ Z, then gcd(2n+1,4n
2 +1) = 1.
34. If gcd(a, c) = gcd(b, c) = 1, then gcd(ab, c) = 1.
(Suggestion: Use the proposition on page 126.)
35. Suppose a,b ∈ N. Then a = gcd(a,b) if and only if a | b.
36. Suppose a,b ∈ N. Then a = lcm(a,b) if and only if b | a.
CHAPTER 8
Proofs Involving Sets
S
tudents in their first advanced mathematics classes are often surprised
by the extensive role that sets play and by the fact that most of the
proofs they encounter are proofs about sets. Perhaps you’ve already seen
such proofs in your linear algebra course, where a vector space was
defined to be a set of objects (called vectors) that obey certain properties.
Your text proved many things about vector spaces, such as the fact that
the intersection of two vector spaces is also a vector space, and the proofs
used ideas from set theory. As you go deeper into mathematics, you will
encounter more and more ideas, theorems and proofs that involve sets.
The purpose of this chapter is to give you a foundation that will prepare
you for this new outlook.
We will discuss how to show that an object is an element of a set, how
to prove one set is a subset of another and how to prove two sets are
equal. As you read this chapter, you may need to occasionally refer back
to Chapter 1 to refresh your memory. For your convenience, the main
definitions from Chapter 1 are summarized below. If A and B are sets,
then:
A ×B =
©
(x, y) : x ∈ A, y ∈ B
ª
,
A ∪B =
©
x : (x ∈ A)∨(x ∈ B)
ª
,
A ∩B =
©
x : (x ∈ A)∧(x ∈ B)
ª
,
A −B =
©
x : (x ∈ A)∧(x ∉ B)
ª
,
A = U − A.
Recall that A ⊆ B means that every element of A is also an element of B.
8.1 How to Prove a ∈ A
We will begin with a review of set-builder notation, and then review how
to show that a given object a is an element of some set A.
132 Proofs Involving Sets
Generally, a set A will be expressed in set-builder notation A =
©
x : P(x)
ª
,
where P(x) is some statement (or open sentence) about x. The set A is
understood to have as elements all those things x for which P(x) is true.
For example,
©
x : x is an odd integerª
=
©
...,−5,−3,−1,1,3,5,...ª
.
A common variation of this notation is to express a set as A =
©
x ∈ S : P(x)
ª
.
Here it is understood that A consists of all elements x of the (predetermined)
set S for which P(x) is true. Keep in mind that, depending on context, x
could be any kind of object (integer, ordered pair, set, function, etc.). There
is also nothing special about the particular variable x; any reasonable
symbol x, y, k, etc., would do. Some examples follow.
©
n ∈ Z : n is oddª
=
©
...,−5,−3,−1,1,3,5,...ª
©
x ∈ N : 6| x
ª
=
©
6,12,18,24,30,...ª
©
(a,b) ∈ Z×Z : b = a+5
ª
=
©
...,(−2,3),(−1,4),(0,5),(1,6),...ª
©
X ∈ P(Z) : |X| = 1
ª
=
©
...,©
−1
ª
,
©
0
ª
,
©
1
ª
,
©
2
ª
,
©
3
ª
,
©
4
ª
,...ª
Now it should be clear how to prove that an object a belongs to a set
©
x : P(x)
ª
. Since ©
x : P(x)
ª
consists of all things x for which P(x) is true, to
show that a ∈
©
x : P(x)
ª
we just need to show that P(a) is true. Likewise, to
show a ∈
©
x ∈ S : P(x)
ª
, we need to confirm that a ∈ S and that P(a) is true.
These ideas are summarized below. However, you should not memorize
these methods, you should understand them. With contemplation and
practice, using them becomes natural and intuitive.
How to show a ∈
©
x : P(x)
ª
How to show a ∈
©
x ∈ S : P(x)
ª
Show that P(a) is true. 1. Verify that a ∈ S.
2. Show that P(a) is true.
Example 8.1 Let’s investigate elements of A =
©
x : x ∈ N and 7| x
ª
. This set
has form A =
©
x : P(x)
ª
where P(x) is the open sentence (x ∈ N)∧(7| x). Thus
21 ∈ A because P(21) is true. Similarly, 7,14,28,35, etc., are all elements of
A. But 8 ∉ A (for example) because P(8) is false. Likewise −14 ∉ A because
P(−14) is false.
Example 8.2 Consider the set A =
©
X ∈ P(N) : |X| = 3
ª
. We know that
©
4,13,45ª
∈ A because ©
4,13,45ª
∈ P(N) and
¯
¯
©
4,13,45ª¯
¯ = 3. Also ©
1,2,3
ª
∈ A,
©
10,854,3
ª
∈ A, etc. However ©
1,2,3,4
ª
∉ A because
¯
¯
©
1,2,3,4
ª¯
¯ 6= 3. Further,
©
−1,2,3
ª
∉ A because ©
−1,2,3
ª
∉ P(N).
How to Prove A ⊆ B 133
Example 8.3 Consider the set B =
©
(x, y) ∈ Z × Z : x ≡ y (mod 5)ª
. Notice
(8,23) ∈ B because (8,23) ∈ Z×Z and 8 ≡ 23 (mod 5). Likewise, (100,75) ∈ B,
(102,77) ∈ B, etc., but (6,10) ∉ B.
Now suppose n ∈ Z and consider the ordered pair (4n+3,9n−2). Does
this ordered pair belong to B? To answer this, we first observe that
(4n+3,9n−2) ∈ Z×Z. Next, we observe that (4n+3)−(9n−2) = −5n+5 = 5(1−n),
so 5|
¡
(4n+3)−(9n−2)¢
, which means (4n+3) ≡ (9n−2) (mod 5). Therefore we
have established that (4n+3,9n−2) meets the requirements for belonging
to B, so (4n+3,9n−2) ∈ B for every n ∈ Z.
Example 8.4 This illustrates another common way of defining a set.
Consider the set C =
©
3x
3 +2 : x ∈ Z
ª
. Elements of this set consist of all the
values 3x
3 +2 where x is an integer. Thus −22 ∈ C because −22 = 3(−2)3 +2.
You can confirm −1 ∈ C and 5 ∈ C, etc. Also 0 ∉ C and 1
2
∉ C, etc.
8.2 How to Prove A ⊆ B
In this course (and more importantly, beyond it) you will encounter many
circumstances where it is necessary to prove that one set is a subset of another.
This section explains how to do this. The methods we discuss should
improve your skills in both writing your own proofs and in comprehending
the proofs that you read.
Recall (Definition 1.3) that if A and B are sets, then A ⊆ B means that
every element of A is also an element of B. In other words, it means if
a ∈ A, then a ∈ B. Therefore to prove that A ⊆ B, we just need to prove that
the conditional statement
“If a ∈ A, then a ∈ B”
is true. This can be proved directly, by assuming a ∈ A and deducing a ∈ B.
The contrapositive approach is another option: Assume a ∉ B and deduce
a ∉ A. Each of these two approaches is outlined below.
How to Prove A ⊆ B How to Prove A ⊆ B
(Direct approach) (Contrapositive approach)
Proof. Suppose a ∈ A.
.
.
.
Therefore a ∈ B.
Thus a ∈ A implies a ∈ B,
so it follows that A ⊆ B. ■
Proof. Suppose a ∉ B.
.
.
.
Therefore a ∉ A.
Thus a ∉ B implies a ∉ A,
so it follows that A ⊆ B. ■
134 Proofs Involving Sets
In practice, the direct approach usually results in the most straightforward
and easy proof, though occasionally the contrapositive is the
most expedient. (You can even prove A ⊆ B by contradiction: Assume
(a ∈ A)∧(a ∉ B), and deduce a contradiction.) The remainder of this section
consists of examples with occasional commentary. Unless stated otherwise,
we will use the direct approach in all proofs; pay special attention to how
the above outline for the direct approach is used.
Example 8.5 Prove that ©
x ∈ Z : 18| x
ª
⊆
©
x ∈ Z : 6| x
ª
.
Proof. Suppose a ∈
©
x ∈ Z : 18| x
ª
.
This means that a ∈ Z and 18|a.
By definition of divisibility, there is an integer c for which a = 18c.
Consequently a = 6(3c), and from this we deduce that 6|a.
Therefore a is one of the integers that 6 divides, so a ∈
©
x ∈ Z : 6| x
ª
.
We’ve shown a ∈
©
x ∈ Z : 18| x
ª
implies a ∈
©
n ∈ Z : 6| x
ª
, so it follows that
©
x ∈ Z : 18| x
ª
⊆
©
x ∈ Z : 6| x
ª
. ■
Example 8.6 Prove that ©
x ∈ Z : 2| x
ª
∩
©
x ∈ Z : 9| x
ª
⊆
©
x ∈ Z : 6| x
ª
.
Proof. Suppose a ∈
©
x ∈ Z : 2| x
ª
∩
©
x ∈ Z : 9| x
ª
.
By definition of intersection, this means a ∈
©
x ∈ Z : 2| x
ª
and a ∈
©
x ∈ Z : 9| x
ª
.
Since a ∈
©
x ∈ Z : 2| x
ª
we know 2|a, so a = 2c for some c ∈ Z. Thus a is even.
Since a ∈
©
x ∈ Z : 9| x
ª
we know 9|a, so a = 9d for some d ∈ Z.
As a is even, a = 9d implies d is even. (Otherwise a = 9d would be odd.)
Then d = 2e for some integer e, and we have a = 9d = 9(2e) = 6(3e).
From a = 6(3e), we conclude 6|a, and this means a ∈
©
x ∈ Z : 6| x
ª
.
We have shown that a ∈
©
x ∈ Z : 2| x
ª
∩
©
x ∈ Z : 9| x
ª
implies a ∈
©
x ∈ Z : 6| x
ª
,
so it follows that ©
x ∈ Z : 2| x
ª
∩
©
x ∈ Z : 9| x
ª
⊆
©
x ∈ Z : 6| x
ª
. ■
Example 8.7 Show ©
(x, y) ∈ Z×Z : x ≡ y (mod 6)ª
⊆
©
(x, y) ∈ Z×Z : x ≡ y (mod 3)ª
.
Proof. Suppose (a,b) ∈
©
(x, y) ∈ Z×Z : x ≡ y (mod 6)ª
.
This means (a,b) ∈ Z×Z and a ≡ b (mod 6).
Consequently 6|(a− b), so a− b = 6c for some integer c.
It follows that a− b = 3(2c), and this means 3|(a− b), so a ≡ b (mod 3).
Thus (a,b) ∈
©
(x, y) ∈ Z×Z : x ≡ y (mod 3)ª
.
We’ve now seen that (a,b) ∈
©
(x, y) ∈ Z×Z : x ≡ y (mod 6)ª
implies (a,b) ∈
©
(x, y) ∈ Z×Z : x ≡ y (mod 3)ª
, so it follows that ©
(x, y) ∈ Z×Z : x ≡ y (mod 6)ª
⊆
©
(x, y) ∈ Z×Z : x ≡ y (mod 3)ª
. ■
How to Prove A ⊆ B 135
Some statements involving subsets are transparent enough that we
often accept (and use) them without proof. For example, if A and B are any
sets, then it’s very easy to confirm A ∩B ⊆ A. (Reason: Suppose x ∈ A ∩B.
Then x ∈ A and x ∈ B by definition of intersection, so in particular x ∈ A.
Thus x ∈ A ∩B implies x ∈ A, so A ∩B ⊆ A.) Other statements of this nature
include A ⊆ A ∪B and A −B ⊆ A, as well as conditional statements such as
¡
(A ⊆ B)∧(B ⊆ C)
¢
⇒ (A ⊆ C) and (X ⊆ A) ⇒ (X ⊆ A ∪B). Our point of view in
this text is that we do not need to prove such obvious statements unless we
are explicitly asked to do so in an exercise. (Still, you should do some quick
mental proofs to convince yourself that the above statements are true. If
you don’t see that A ∩ B ⊆ A is true but that A ⊆ A ∩ B is not necessarily
true, then you need to spend more time on this topic.)
The next example will show that if A and B are sets, then P(A)∪P(B) ⊆
P(A ∪ B). Before beginning our proof, let’s look at an example to see if
this statement really makes sense. Suppose A =
©
1,2
ª
and B =
©
2,3
ª
. Then
P(A)∪P(B) =
©
;,
©
1
ª
,
©
2
ª
,
©
1,2
ªª∪
©
;,
©
2
ª
,
©
3
ª
,
©
2,3
ªª
=
©
;,
©
1
ª
,
©
2
ª
,
©
3
ª
,
©
1,2
ª
,
©
2,3
ªª.
Also P(A∪B) = P(
©
1,2,3
ª
) =
©
;,
©
1
ª
,
©
2
ª
,
©
3
ª
,
©
1,2
ª
,
©
2,3
ª
,
©
1,3
ª
,
©
1,2,3
ªª. Thus,
even though P(A)∪P(B) 6= P(A∪B), it is true that P(A)∪P(B) ⊆ P(A∪B)
for this particular A and B. Now let’s prove P(A) ∪P(B) ⊆ P(A ∪ B) no
matter what sets A and B are.
Example 8.8 Prove that if A and B are sets, then P(A)∪P(B) ⊆ P(A∪B).
Proof. Suppose X ∈ P(A)∪P(B).
By definition of union, this means X ∈ P(A) or X ∈ P(B).
Therefore X ⊆ A or X ⊆ B (by definition of power sets). We consider cases.
Case 1. Suppose X ⊆ A. Then X ⊆ A ∪B, and this means X ∈ P(A ∪B).
Case 2. Suppose X ⊆ B. Then X ⊆ A ∪B, and this means X ∈ P(A ∪B).
(We do not need to consider the case where X ⊆ A and X ⊆ B because that
is taken care of by either of cases 1 or 2.) The above cases show that
X ∈ P(A ∪B).
Thus we’ve shown that X ∈ P(A)∪P(B) implies X ∈ P(A ∪B), and this
completes the proof that P(A)∪P(B) ⊆ P(A ∪B). ■
In our next example, we prove a conditional statement. Direct proof is
used, and in the process we use our new technique for showing A ⊆ B.
136 Proofs Involving Sets
Example 8.9 Suppose A and B are sets. If P(A) ⊆ P(B), then A ⊆ B.
Proof. We use direct proof. Assume P(A) ⊆ P(B).
Based on this assumption, we must now show that A ⊆ B.
To show A ⊆ B, suppose that a ∈ A.
Then the one-element set ©
a
ª
is a subset of A, so ©
a
ª
∈ P(A).
But then, since P(A) ⊆ P(B), it follows that ©
a
ª
∈ P(B).
This means that ©
a
ª
⊆ B, hence a ∈ B.
We’ve shown that a ∈ A implies a ∈ B, so therefore A ⊆ B. ■
8.3 How to Prove A = B
In proofs it is often necessary to show that two sets are equal. There is a
standard way of doing this. Suppose we want to show A = B. If we show
A ⊆ B, then every element of A is also in B, but there is still a possibility
that B could have some elements that are not in A, so we can’t conclude
A = B. But if in addition we also show B ⊆ A, then B can’t contain anything
that is not in A, so A = B. This is the standard procedure for proving A = B:
Prove both A ⊆ B and B ⊆ A.
How to Prove A = B
Proof.
[Prove that A ⊆ B.]
[Prove that B ⊆ A.]
Therefore, since A ⊆ B and B ⊆ A,
it follows that A = B. ■
Example 8.10 Prove that ©
n ∈ Z : 35|n
ª
=
©
n ∈ Z : 5|n
ª
∩
©
n ∈ Z : 7|n
ª
.
Proof. First we show ©
n ∈ Z : 35|n
ª
⊆
©
n ∈ Z : 5|n
ª
∩
©
n ∈ Z : 7|n
ª
. Suppose
a ∈
©
n ∈ Z : 35|n
ª
. This means 35|a, so a = 35c for some c ∈ Z. Thus a = 5(7c)
and a = 7(5c). From a = 5(7c) it follows that 5|a, so a ∈
©
n ∈ Z : 5|n
ª
. From
a = 7(5c) it follows that 7|a, which means a ∈
©
n ∈ Z : 7|n
ª
. As a belongs
to both ©
n ∈ Z : 5|n
ª
and ©
n ∈ Z : 7|n
ª
, we get a ∈
©
n ∈ Z : 5|n
ª
∩
©
n ∈ Z : 7|n
ª
.
Thus we’ve shown that ©
n ∈ Z : 35|n
ª
⊆
©
n ∈ Z : 5|n
ª
∩
©
n ∈ Z : 7|n
ª
.
Next we show ©
n ∈ Z : 5|n
ª
∩
©
n ∈ Z : 7|n
ª
⊆
©
n ∈ Z : 35|n
ª
. Suppose that
a ∈
©
n ∈ Z : 5|n
ª
∩
©
n ∈ Z : 7|n
ª
. By definition of intersection, this means that
a ∈
©
n ∈ Z : 5|n
ª
and a ∈
©
n ∈ Z : 7|n
ª
. Therefore it follows that 5|a and 7|a.
By definition of divisibility, there are integers c and d with a = 5c and a = 7d.
Then a has both 5 and 7 as prime factors, so the prime factorization of a
How to Prove A = B 137
must include factors of 5 and 7. Hence 5·7 = 35 divides a, so a ∈
©
n ∈ Z : 35|n
ª
.
We’ve now shown that ©
n ∈ Z : 5|n
ª
∩
©
n ∈ Z : 7|n
ª
⊆
©
n ∈ Z : 35|n
ª
.
At this point we’ve shown that ©
n ∈ Z : 35|n
ª
⊆
©
n ∈ Z : 5|n
ª
∩
©
n ∈ Z : 7|n
ª
and ©
n ∈ Z : 5|n
ª
∩
©
n ∈ Z : 7|n
ª
⊆
©
n ∈ Z : 35|n
ª
, so we’ve proved ©
n ∈ Z : 35|n
ª
=
©
n ∈ Z : 5|n
ª
∩
©
n ∈ Z : 7|n
ª
. ■
You know from algebra that if c 6= 0 and ac = bc, then a = b. The next
example shows that an analogous statement holds for sets A,B and C. The
example asks us to prove a conditional statement. We will prove it with
direct proof. In carrying out the process of direct proof, we will have to
use the new techniques from this section.
Example 8.11 Suppose A, B, and C are sets, and C 6= ;. Prove that if
A ×C = B ×C, then A = B.
Proof. Suppose A ×C = B ×C. We must now show A = B.
First we will show A ⊆ B. Suppose a ∈ A. Since C 6= ;, there exists
an element c ∈ C. Thus, since a ∈ A and c ∈ C, we have (a, c) ∈ A × C, by
definition of the Cartesian product. But then, since A×C = B×C, it follows
that (a, c) ∈ B ×C. Again by definition of the Cartesian product, it follows
that a ∈ B. We have shown a ∈ A implies a ∈ B, so A ⊆ B.
Next we show B ⊆ A. We use the same argument as above, with the
roles of A and B reversed. Suppose a ∈ B. Since C 6= ;, there exists an
element c ∈ C. Thus, since a ∈ B and c ∈ C, we have (a, c) ∈ B ×C. But then,
since B × C = A × C, we have (a, c) ∈ A × C. It follows that a ∈ A. We have
shown a ∈ B implies a ∈ A, so B ⊆ A.
The previous two paragraphs have shown A ⊆ B and B ⊆ A, so A = B. In
summary, we have shown that if A ×C = B ×C, then A = B. This completes
the proof. ■
Now we’ll look at another way that set operations are similar to operations
on numbers. From algebra you are familiar with the distributive
property a·(b + c) = a· b + a· c. Replace the numbers a,b, c with sets A,B,C,
and replace · with × and + with ∩. We get A × (B ∩ C) = (A × B) ∩ (A × C).
This statement turns out to be true, as we now prove.
Example 8.12 Given sets A, B, and C, prove A ×(B∩C) = (A ×B)∩(A ×C).
Proof. First we will show that A ×(B ∩C) ⊆ (A ×B)∩(A ×C).
Suppose (a,b) ∈ A ×(B ∩C).
By definition of the Cartesian product, this means a ∈ A and b ∈ B ∩C.
By definition of intersection, it follows that b ∈ B and b ∈ C.
138 Proofs Involving Sets
Thus, since a ∈ A and b ∈ B, it follows that (a,b) ∈ A ×B (by definition of ×).
Also, since a ∈ A and b ∈ C, it follows that (a,b) ∈ A ×C (by definition of ×).
Now we have (a,b) ∈ A ×B and (a,b) ∈ A ×C, so (a,b) ∈ (A ×B)∩(A ×C).
We’ve shown that (a,b) ∈ A × (B ∩ C) implies (a,b) ∈ (A × B) ∩ (A × C) so we
have A ×(B ∩C) ⊆ (A ×B)∩(A ×C).
Next we will show that (A ×B)∩(A ×C) ⊆ A ×(B ∩C).
Suppose (a,b) ∈ (A ×B)∩(A ×C).
By definition of intersection, this means (a,b) ∈ A ×B and (a,b) ∈ A ×C.
By definition of the Cartesian product, (a,b) ∈ A ×B means a ∈ A and b ∈ B.
By definition of the Cartesian product, (a,b) ∈ A ×C means a ∈ A and b ∈ C.
We now have b ∈ B and b ∈ C, so b ∈ B ∩C, by definition of intersection.
Thus we’ve deduced that a ∈ A and b ∈ B ∩C, so (a,b) ∈ A ×(B ∩C).
In summary, we’ve shown that (a,b) ∈ (A×B)∩(A×C) implies (a,b) ∈ A×(B∩C)
so we have (A ×B)∩(A ×C) ⊆ A ×(B ∩C).
The previous two paragraphs show that A×(B∩C) ⊆ (A×B)∩(A×C) and
(A×B)∩(A×C) ⊆ A×(B∩C), so it follows that (A×B)∩(A×C) = A×(B∩C). ■
Occasionally you can prove two sets are equal by working out a series of
equalities leading from one set to the other. This is analogous to showing
two algebraic expressions are equal by manipulating one until you obtain
the other. We illustrate this in the following example, which gives an
alternate solution to the previous example. You are cautioned that this
approach is sometimes difficult to apply, but when it works it can shorten
a proof dramatically.
Before beginning the example, a note is in order. Notice that any
statement P is logically equivalent to P ∧P. (Write out a truth table if you
are in doubt.) At one point in the following example we will replace the
expression x ∈ A with the logically equivalent statement (x ∈ A)∧(x ∈ A).
Example 8.13 Given sets A, B, and C, prove A ×(B∩C) = (A ×B)∩(A ×C).
Proof. Just observe the following sequence of equalities.
A ×(B ∩C) =
©
(x, y) : (x ∈ A)∧(y ∈ B ∩C)
ª
(def. of ×)
=
©
(x, y) : (x ∈ A)∧(y ∈ B)∧(y ∈ C)
ª
(def. of ∩)
=
©
(x, y) : (x ∈ A)∧(x ∈ A)∧(y ∈ B)∧(y ∈ C)
ª
(P = P ∧ P)
=
©
(x, y) : ((x ∈ A)∧(y ∈ B))∧((x ∈ A)∧(y ∈ C))ª
(rearrange)
=
©
(x, y) : (x ∈ A)∧(y ∈ B)
ª
∩
©
(x, y) : (x ∈ A)∧(y ∈ C)
ª
(def. of ∩)
= (A ×B)∩(A ×C) (def. of ×)
The proof is complete. ■
Examples: Perfect Numbers 139
The equation A×(B∩C) = (A×B)∩(A×C) just obtained is a fundamental
law that you may actually use fairly often as you continue with mathematics.
Some similar equations are listed below. Each of these can be proved with
this section’s techniques, and the exercises will ask that you do so.
A ∩B = A ∪B
A ∪B = A ∩B
)
DeMorgan’s laws for sets
A ∩(B ∪C) = (A ∩B)∪(A ∩C)
A ∪(B ∩C) = (A ∪B)∩(A ∪C)
¾
Distributive laws for sets
A ×(B ∪C) = (A ×B)∪(A ×C)
A ×(B ∩C) = (A ×B)∩(A ×C)
¾
Distributive laws for sets
It is very good practice to prove these equations. Depending on your
learning style, it is probably not necessary to commit them to memory.
But don’t forget them entirely. They may well be useful later in your
mathematical education. If so, you can look them up or re-derive them on
the spot. If you go on to study mathematics deeply, you will at some point
realize that you’ve internalized them without even being cognizant of it.
8.4 Examples: Perfect Numbers
Sometimes it takes a good bit of work and creativity to show that one set
is a subset of another or that they are equal. We illustrate this now with
examples from number theory involving what are called perfect numbers.
Even though this topic is quite old, dating back more than 2000 years, it
leads to some questions that are unanswered even today.
The problem involves adding up the positive divisors of a natural
number. To begin the discussion, consider the number 12. If we add up the
positive divisors of 12 that are less than 12, we obtain 1+2+3+4+6 = 16,
which is greater than 12. Doing the same thing for 15, we get 1+3+5 = 9
which is less than 15. For the most part, given a natural number p, the
sum of its positive divisors less than itself will either be greater than p
or less than p. But occasionally the divisors add up to exactly p. If this
happens, then p is said to be a perfect number.
Definition 8.1 A number p ∈ N is perfect if it equals the sum of its
positive divisors less than itself. Some examples follow.
• The number 6 is perfect since 6 = 1+2+3.
• The number 28 is perfect since 28 = 1+2+4+7+14.
• The number 496 is perfect since 496 = 1+2+4+8+16+31+62+124+248.
140 Proofs Involving Sets
Though it would take a while to find it by trial-and-error, the next
perfect number after 496 is 8128. You can check that 8128 is perfect. Its
divisors are 1, 2, 4, 8, 16, 32, 64, 127, 254, 508, 1016, 2032, 4064 and indeed
8128 = 1+2+4+8+16+32+64+127+254+508+1016+2032+4064.
Are there other perfect numbers? How can they be found? Do they obey any
patterns? These questions fascinated the ancient Greek mathematicians.
In what follows we will develop an idea—recorded by Euclid—that partially
answers these questions. Although Euclid did not use sets,1 we will
nonetheless phrase his idea using the language of sets.
Since our goal is to understand what numbers are perfect, let’s define
the following set:
P =
©
p ∈ N : p is perfectª
.
Therefore P =
©
6,28,496,8128,...ª
, but it is unclear what numbers are in
P other than the ones listed. Our goal is to gain a better understanding
of just which numbers the set P includes. To do this, we will examine
the following set A. It looks more complicated than P, but it will be very
helpful for understanding P, as we will soon see.
A =
©
2
n−1
(2n −1) : n ∈ N, and 2
n −1 is primeª
In words, A consists of every natural number of form 2
n−1
(2n −1), where
2
n −1 is prime. To get a feel for what numbers belong to A, look at the
following table. For each natural number n, it tallies the corresponding
numbers 2
n−1 and 2
n −1. If 2
n −1 happens to be prime, then the product
2
n−1
(2n −1) is given; otherwise that entry is labeled with an ∗.
n 2
n−1 2
n −1 2
n−1
(2n −1)
1 1 1 ∗
2 2 3 6
3 4 7 28
4 8 15 ∗
5 16 31 496
6 32 63 ∗
7 64 127 8128
8 128 255 ∗
9 256 511 ∗
10 512 1023 ∗
11 1024 2047 ∗
12 2048 4095 ∗
13 4096 8191 33,550,336
1Set theory was invented over 2000 years after Euclid died.
Examples: Perfect Numbers 141
Notice that the first four entries of A are the perfect numbers 6, 28,
496 and 8128. At this point you may want to jump to the conclusion that
A = P. But it is a shocking fact that in over 2000 years no one has ever
been able to determine whether or not A = P. But it is known that A ⊆ P,
and we will now prove it. In other words, we are going to show that every
element of A is perfect. (But by itself, that leaves open the possibility that
there may be some perfect numbers in P that are not in A.)
The main ingredient for the proof will be the formula for the sum of a
geometric series with common ratio r. You probably saw this most recently
in Calculus II. The formula is
Xn
k=0
r
k =
r
n+1 −1
r −1
.
We will need this for the case r = 2, which is
Xn
k=0
2
k = 2
n+1 −1. (8.1)
(See the solution for Exercise 19 in Section 7.4 for a proof of this formula.)
Now we are ready to prove our result. Let’s draw attention to its
significance by calling it a theorem rather than a proposition.
Theorem 8.1 If A =
©
2
n−1
(2n −1) : n ∈ N, and 2
n −1 is primeª
and P =
©
p ∈ N : p is perfectª
, then A ⊆ P.
Proof. Assume A and P are as stated. To show A ⊆ P, we must show that
p ∈ A implies p ∈ P. Thus suppose p ∈ A. By definition of A, this means
p = 2
n−1
(2n −1) (8.2)
for some n ∈ N for which 2
n −1 is prime. We want to show that p ∈ P, that
is, we want to show p is perfect. Thus, we need to show that the sum of
the positive divisors of p that are less than p add up to p. Notice that
since 2
n −1 is prime, any divisor of p = 2
n−1
(2n −1) must have the form 2
k
or 2
k
(2n −1) for 0 ≤ k ≤ n−1. Thus the positive divisors of p are as follows:
2
0
, 2
1
, 2
2
, ... 2
n−2
, 2
n−1
,
2
0
(2n −1), 2
1
(2n −1), 2
2
(2n −1), ... 2
n−2
(2n −1), 2
n−1
(2n −1).
Notice that this list starts with 2
0 = 1 and ends with 2
n−1
(2n −1) = p.
142 Proofs Involving Sets
If we add up all these divisors except for the last one (which equals p)
we get the following:
nX−1
k=0
2
k +
nX−2
k=0
2
k
(2n −1) =
nX−1
k=0
2
k +(2n −1)
nX−2
k=0
2
k
= (2n −1)+(2n −1)(2n−1 −1) (by Equation (8.1))
= [1+(2n−1 −1)](2n −1)
= 2
n−1
(2n −1)
= p (by Equation (8.2)).
This shows that the positive divisors of p that are less than p add up to p.
Therefore p is perfect, by definition of a perfect number. Thus p ∈ P, by
definition of P.
We have shown that p ∈ A implies p ∈ P, which means A ⊆ P. ■
Combined with the chart on the previous page, this theorem gives us
a new perfect number! The element p = 2
13−1
(213 −1) = 33,550,336 in A is
perfect.
Observe also that every element of A is a multiple of a power of 2, and
therefore even. But this does not necessarily mean every perfect number
is even, because we’ve only shown A ⊆ P, not A = P. For all we know there
may be odd perfect numbers in P − A that are not in A.
Are there any odd perfect numbers? No one knows.
In over 2000 years, no one has ever found an odd perfect number, nor
has anyone been able to prove that there are none. But it is known that the
set A does contain every even perfect number. This fact was first proved by
Euler, and we duplicate his reasoning in the next theorem, which proves
that A = E, where E is the set of all even perfect numbers. It is a good
example of how to prove two sets are equal.
For convenience, we are going to use a slightly different definition of a
perfect number. A number p ∈ N is perfect if its positive divisors add up
to 2p. For example, the number 6 is perfect since the sum of its divisors
is 1+2+3+6 = 2·6. This definition is simpler than the first one because
we do not have to stipulate that we are adding up the divisors that are
less than p. Instead we add in the last divisor p, and that has the effect
of adding an additional p, thereby doubling the answer.
Examples: Perfect Numbers 143
Theorem 8.2 If A =
©
2
n−1
(2n −1) : n ∈ N, and 2
n −1 is primeª
and E =
©
p ∈ N : p is perfect and evenª
, then A = E.
Proof. To show that A = E, we need to show A ⊆ E and E ⊆ A.
First we will show that A ⊆ E. Suppose p ∈ A. This means p is even,
because the definition of A shows that every element of A is a multiple of
a power of 2. Also, p is a perfect number because Theorem 8.1 states that
every element of A is also an element of P, hence perfect. Thus p is an
even perfect number, so p ∈ E. Therefore A ⊆ E.
Next we show that E ⊆ A. Suppose p ∈ E. This means p is an even
perfect number. Write the prime factorization of p as p = 2
k3
n1 5
n2 7
n2
...,
where some of the powers n1, n2, n3 ... may be zero. But, as p is even, the
power k must be greater than zero. It follows p = 2
k q for some positive
integer k and an odd integer q. Now, our aim is to show that p ∈ A, which
means we must show p has form p = 2
n−1
(2n−1). To get our current p = 2
k q
closer to this form, let n = k +1, so we now have
p = 2
n−1
q. (8.3)
List the positive divisors of q as d1,d2,d3,...,dm. (Where d1 = 1 and dm = q.)
Then the divisors of p are:
2
0d1 2
0d2 2
0d3 ... 2
0dm
2
1d1 2
1d2 2
1d3 ... 2
1dm
2
2d1 2
2d2 2
2d3 ... 2
2dm
2
3d1 2
3d2 2
3d3 ... 2
3dm
.
.
.
.
.
.
.
.
.
.
.
.
2
n−1d1 2
n−1d2 2
n−1d3 ... 2
n−1dm
Since p is perfect, these divisors add up to 2p. By Equation (8.3), their
sum is 2p = 2(2n−1q) = 2
nq. Adding the divisors column-by-column, we get
nX−1
k=0
2
kd1 +
nX−1
k=0
2
kd2 +
nX−1
k=0
2
kd3 +··· +
nX−1
k=0
2
kdm = 2
n
q.
Applying Equation (8.1), this becomes
(2n −1)d1 +(2n −1)d2 +(2n −1)d3 +··· +(2n −1)dm = 2
n
q
(2n −1)(d1 + d2 + d3 +··· + dm) = 2
n
q
d1 + d2 + d3 +··· + dm =
2
nq
2
n −1
,
144 Proofs Involving Sets
so that
d1 + d2 + d3 +··· + dm =
(2n −1+1)q
2
n −1
=
(2n −1)q + q
2
n −1
= q +
q
2
n −1
.
From this we see that q
2
n−1
is an integer. It follows that both q and q
2
n−1
are positive divisors of q. Since their sum equals the sum of all positive
divisors of q, it follows that q has only two positive divisors, q and q
2
n−1
.
Since one of its divisors must be 1, it must be that q
2
n−1
= 1, which means
q = 2
n − 1. Now a number with just two positive divisors is prime, so
q = 2
n −1 is prime. Plugging this into Equation (8.3) gives p = 2
n−1
(2n −1),
where 2
n −1 is prime. This means p ∈ A, by definition of A. We have now
shown that p ∈ E implies p ∈ A, so E ⊆ A.
Since A ⊆ E and E ⊆ A, it follows that A = E. ■
Do not be alarmed if you feel that you wouldn’t have thought of this
proof. It took the genius of Euler to discover this approach.
We’ll conclude this chapter with some facts about perfect numbers.
• The sixth perfect number is p = 2
17−1
(217 −1) = 8589869056.
• The seventh perfect number is p = 2
19−1
(219 −1) = 137438691328.
• The eighth perfect number is p = 2
31−1
(231 −1) = 2305843008139952128.
• The twentieth perfect number is p = 2
4423−1
(24423 −1). It has 2663 digits.
• The twenty-third perfect number is p = 2
11213−1
(211213 −1). It has 6957
digits.
As mentioned earlier, no one knows whether or not there are any odd
perfect numbers. It is not even known whether there are finitely many or
infinitely many perfect numbers. It is known that the last digit of every
even perfect number is either a 6 or an 8. Perhaps this is something you’d
enjoy proving.
We’ve seen that perfect numbers are closely related to prime numbers
that have the form 2
n − 1. Such prime numbers are called Mersenne
primes, after the French scholar Marin Mersenne (1588–1648), who
popularized them. The first several Mersenne primes are 2
2 − 1 = 3,
2
3 − 1 = 7, 2
5 − 1 = 31, 2
7 − 1 = 127 and 2
13 − 1 = 8191. To date, only
48 Mersenne primes are known, the largest of which is 2
57,885,161 − 1.
There is a substantial cash prize for anyone who finds a 49th. (See
http://www.mersenne.org/prime.htm.) You will probably have better luck
with the exercises.
Examples: Perfect Numbers 145
Exercises for Chapter 8
Use the methods introduced in this chapter to prove the following statements.
1. Prove that ©
12n : n ∈ Z
ª
⊆
©
2n : n ∈ Z
ª
∩
©
3n : n ∈ Z
ª
.
2. Prove that ©
6n : n ∈ Z
ª
=
©
2n : n ∈ Z
ª
∩
©
3n : n ∈ Z
ª
.
3. If k ∈ Z, then ©
n ∈ Z : n|k
ª
⊆
©
n ∈ Z : n|k
2
ª
.
4. If m,n ∈ Z, then ©
x ∈ Z : mn| x
ª
⊆
©
x ∈ Z : m| x
ª
∩
©
x ∈ Z : n| x
ª
.
5. If p and q are positive integers, then ©
pn : n ∈ N
ª
∩
©
qn : n ∈ N
ª
6= ;.
6. Suppose A,B and C are sets. Prove that if A ⊆ B, then A −C ⊆ B −C.
7. Suppose A,B and C are sets. If B ⊆ C, then A ×B ⊆ A ×C.
8. If A,B and C are sets, then A ∪(B ∩C) = (A ∪B)∩(A ∪C).
9. If A,B and C are sets, then A ∩(B ∪C) = (A ∩B)∪(A ∩C).
10. If A and B are sets in a universal set U, then A ∩B = A ∪B.
11. If A and B are sets in a universal set U, then A ∪B = A ∩B.
12. If A,B and C are sets, then A −(B ∩C) = (A −B)∪(A −C).
13. If A,B and C are sets, then A −(B ∪C) = (A −B)∩(A −C).
14. If A,B and C are sets, then (A ∪B)−C = (A −C)∪(B −C).
15. If A,B and C are sets, then (A ∩B)−C = (A −C)∩(B −C).
16. If A,B and C are sets, then A ×(B ∪C) = (A ×B)∪(A ×C).
17. If A,B and C are sets, then A ×(B ∩C) = (A ×B)∩(A ×C).
18. If A,B and C are sets, then A ×(B −C) = (A ×B)−(A ×C).
19. Prove that ©
9
n
: n ∈ Z
ª
⊆
©
3
n
: n ∈ Z
ª
, but ©
9
n
: n ∈ Z
ª
6=
©
3
n
: n ∈ Z
ª
20. Prove that ©
9
n
: n ∈ Q
ª
=
©
3
n
: n ∈ Q
ª
.
21. Suppose A and B are sets. Prove A ⊆ B if and only if A −B = ;.
22. Let A and B be sets. Prove that A ⊆ B if and only if A ∩B = A.
23. For each a ∈ R, let Aa =
©
(x,a(x
2−1)) ∈ R
2
: x ∈ R
ª
. Prove that \
a∈R
Aa =
©
(−1,0),(1,0)ª
.
24. Prove that \
x∈R
[3− x
2
,5+ x
2
] = [3,5].
25. Suppose A,B,C and D are sets. Prove that (A ×B)∪(C ×D) ⊆ (A ∪C)×(B ∪D).
26. Prove ©
4k +5 : k ∈ Z
ª
=
©
4k +1 : k ∈ Z
ª
.
27. Prove ©
12a+4b : a,b ∈ Z
ª
=
©
4c : c ∈ Z
ª
.
28. Prove ©
12a+25b : a,b ∈ Z
ª
= Z.
29. Suppose A 6= ;. Prove that A ×B ⊆ A ×C, if and only if B ⊆ C.
30. Prove that (Z×N)∩(N×Z) = N×N.
31. Suppose B 6= ; and A ×B ⊆ B ×C. Prove A ⊆ C.
CHAPTER 9
Disproof
E
ver since Chapter 4 we have dealt with one major theme: Given a
statement, prove that is it true. In every example and exercise we
were handed a true statement and charged with the task of proving it.
Have you ever wondered what would happen if you were given a false
statement to prove? The answer is that no (correct) proof would be possible,
for if it were, the statement would be true, not false.
But how would you convince someone that a statement is false? The
mere fact that you could not produce a proof does not automatically mean
the statement is false, for you know (perhaps all too well) that proofs
can be difficult to construct. It turns out that there is a very simple and
utterly convincing procedure that proves a statement is false. The process
of carrying out this procedure is called disproof. Thus, this chapter is
concerned with disproving statements.
Before describing the new method, we will set the stage with some
relevant background information. First, we point out that mathematical
statements can be divided into three categories, described below.
One category consists of all those statements that have been proved to be
true. For the most part we regard these statements as significant enough
to be designated with special names such as “theorem,” “proposition,”
“lemma” and “corollary.” Some examples of statements in this category are
listed in the left-hand box in the diagram on the following page. There are
also some wholly uninteresting statements (such as 2 = 2) in this category,
and although we acknowledge their existence we certainly do not dignify
them with terms such as “theorem” or “proposition.”
At the other extreme is a category consisting of statements that are
known to be false. Examples are listed in the box on the right. Since
mathematicians are not very interested in them, these types of statements
do not get any special names, other than the blanket term “false statement.”
But there is a third (and quite interesting) category between these
two extremes. It consists of statements whose truth or falsity has not
been determined. Examples include things like “Every perfect number
147
is even,” or “Every even integer greater than 2 is the sum of two primes.”
(The latter statement is called the Goldbach conjecture. See Section 2.1.)
Mathematicians have a special name for the statements in this category
that they suspect (but haven’t yet proved) are true. Such statements are
called conjectures.
Three Types of Statements:
Known to be true Truth unknown Known to be false
(Theorems & propositions) (Conjectures)
Examples:
• Pythagorean theorem
• Fermat’s last theorem
(Section 2.1)
• The square of an odd
number is odd.
• The series X∞
k=1
1
k
diverges.
Examples:
• All perfect numbers are
even.
• Any even number greater
than 2 is the sum of two
primes. (Goldbach’s
conjecture, Section 2.1)
• There are infinitely many
prime numbers of form
2
n −1, with n ∈ N.
Examples:
• All prime numbers are
odd.
• Some quadratic equations
have three solutions.
• 0 = 1
• There exist natural
numbers a,b and c
for which a
3 + b
3 = c
3
.
Mathematicians spend much of their time and energy attempting
to prove or disprove conjectures. (They also expend considerable mental
energy in creating new conjectures based on collected evidence or intuition.)
When a conjecture is proved (or disproved) the proof or disproof will
typically appear in a published paper, provided the conjecture is of sufficient
interest. If it is proved, the conjecture attains the status of a theorem or
proposition. If it is disproved, then no one is really very interested in it
anymore—mathematicians do not care much for false statements.
Most conjectures that mathematicians are interested in are quite
difficult to prove or disprove. We are not at that level yet. In this text, the
“conjectures” that you will encounter are the kinds of statements that an
experienced mathematician would immediately spot as true or false, but
you may have to do some work before figuring out a proof or disproof. But
in keeping with the cloud of uncertainty that surrounds conjectures at the
advanced levels of mathematics, most exercises in this chapter (and many
beyond it) will ask you to prove or disprove statements without giving any
hint as to whether they are true or false. Your job will be to decide whether
or not they are true and to either prove or disprove them. The examples
in this chapter will illustrate the processes one typically goes through in
148 Disproof
deciding whether a statement is true or false, and then verifying that it’s
true or false.
You know the three major methods of proving a statement: direct proof,
contrapositive proof and proof by contradiction. Now we are ready to
understand the method of disproving a statement. Suppose you want to
disprove a statement P. In other words you want to prove that P is false.
The way to do this is to prove that ∼ P is true, for if ∼ P is true, it follows
immediately that P has to be false.
How to disprove P: Prove ∼ P.
Our approach is incredibly simple. To disprove P, prove ∼ P. In theory,
this proof can be carried out by direct, contrapositive or contradiction
approaches. However, in practice things can be even easier than that
if we are disproving a universally quantified statement or a conditional
statement. That is our next topic.
9.1 Disproving Universal Statements: Counterexamples
A conjecture may be described as a statement that we hope is a theorem.
As we know, many theorems (hence many conjectures) are universally
quantified statements. Thus it seems reasonable to begin our discussion
by investigating how to disprove a universally quantified statement such as
∀x ∈ S,P(x).
To disprove this statement, we must prove its negation. Its negation is
∼ (∀x ∈ S,P(x)) = ∃ x ∈ S,∼ P(x).
The negation is an existence statement. To prove the negation is true,
we just need to produce an example of an x ∈ S that makes ∼ P(x) true,
that is, an x that makes P(x) false. This leads to the following outline for
disproving a universally quantified statement.
How to disprove ∀x ∈ S,P(x).
Produce an example of an x ∈ S
that makes P(x) false.
Counterexamples 149
Things are even simpler if we want to disprove a conditional statement
P(x) ⇒ Q(x). This statement asserts that for every x that makes P(x) true,
Q(x) will also be true. The statement can only be false if there is an x that
makes P(x) true and Q(x) false. This leads to our next outline for disproof.
How to disprove P(x) ⇒ Q(x).
Produce an example of an x that
makes P(x) true and Q(x) false.
In both of the above outlines, the statement is disproved simply by
exhibiting an example that shows the statement is not always true. (Think
of it as an example that proves the statement is a promise that can be
broken.) There is a special name for an example that disproves a statement:
It is called a counterexample.
Example 9.1 As our first example, we will work through the process of
deciding whether or not the following conjecture is true.
Conjecture: For every n ∈ Z, the integer f (n) = n
2 − n+11 is prime.
In resolving the truth or falsity of a conjecture, it’s a good idea to gather
as much information about the conjecture as possible. In this case let’s
start by making a table that tallies the values of f (n) for some integers n.
n −3 −2 −1 0 1 2 3 4 5 6 7 8 9 10
f (n) 23 17 13 11 11 13 17 23 31 41 53 67 83 101
In every case, f (n) is prime, so you may begin to suspect that the conjecture
is true. Before attempting a proof, let’s try one more n. Unfortunately,
f (11) = 112−11+11 = 112
is not prime. The conjecture is false because n = 11
is a counterexample. We summarize our disproof as follows:
Disproof. The statement “For every n ∈ Z, the integer f (n) = n
2 − n + 11 is
prime,” is false. For a counterexample, note that for n = 11, the integer
f (11) = 121 = 11·11 is not prime. ■
In disproving a statement with a counterexample, it is important to explain
exactly how the counterexample makes the statement false. Our work
would not have been complete if we had just said “for a counterexample,
consider n = 11,” and left it at that. We need to show that the answer f (11)
is not prime. Showing the factorization f (11) = 11·11 suffices for this.
150 Disproof
Example 9.2 Either prove or disprove the following conjecture.
Conjecture If A, B and C are sets, then A −(B ∩C) = (A −B)∩(A −C).
Disproof. This conjecture is false because of the following counterexample.
Let A =
©
1,2,3
ª
, B =
©
1,2
ª
and C =
©
2,3
ª
. Notice that A −(B ∩C) =
©
1,3
ª
and
(A −B)∩(A −C) = ;, so A −(B ∩C) 6= (A −B)∩(A −C). ■
(To see where this counterexample came from, draw Venn diagrams for
A−(B∩C) and (A−B)∩(A−C). You will see that the diagrams are different.
The numbers 1, 2 and 3 can then be inserted into the regions of the
diagrams in such a way as to create the above counterexample.)
9.2 Disproving Existence Statements
We have seen that we can disprove a universally quantified statement or a
conditional statement simply by finding a counterexample. Now let’s turn
to the problem of disproving an existence statement such as
∃ x ∈ S,P(x).
Proving this would involve simply finding an example of an x that makes
P(x) true. To disprove it, we have to prove its negation ∼ (∃ x ∈ S,P(x)) =
∀x ∈ S,∼ P(x). But this negation is universally quantified. Proving it
involves showing that ∼ P(x) is true for all x ∈ S, and for this an example
does not suffice. Instead we must use direct, contrapositive or contradiction
proof to prove the conditional statement “If x ∈ S, then ∼ P(x).” As an
example, here is a conjecture to either prove or disprove.
Example 9.3 Either prove or disprove the following conjecture.
Conjecture: There is a real number x for which x
4 < x < x
2
.
This may not seem like an unreasonable statement at first glance. After
all, if the statement were asserting the existence of a real number for
which x
3 < x < x
2
, then it would be true: just take x = −2. But it asserts
there is an x for which x
4 < x < x
2
. When we apply some intelligent guessing
to locate such an x we run into trouble. If x =
1
2
, then x
4 < x, but we don’t
have x < x
2
; similarly if x = 2, we have x < x
2 but not x
4 < x. Since finding
an x with x
4 < x < x
2
seems problematic, we may begin to suspect that the
given statement is false.
Let’s see if we can disprove it. According to our strategy for disproof,
to disprove it we must prove its negation. Symbolically, the statement is
Disproving Existence Statements 151
∃ x ∈ R, x
4 < x < x
2
, so its negation is
∼ (∃ x ∈ R, x
4 < x < x
2
) = ∀x ∈ R,∼ (x
4 < x < x
2
).
Thus, in words the negation is:
For every real number x, it is not the case that x
4 < x < x
2
.
This can be proved with contradiction, as follows. Suppose for the
sake of contradiction that there is an x for which x
4 < x < x
2
. Then x must
be positive since it’s greater than the non-negative number x
4
. Dividing
all parts of x
4 < x < x
2 by the positive number x produces x
3 < 1 < x. Now
subtract 1 from all parts of x
3 < 1 < x to obtain x
3 −1 < 0 < x−1 and reason
as follows:
x
3 −1 < 0 < x−1
(x−1)(x
2 + x+1) < 0 < (x−1)
x
2 + x+1 < 0 < 1
(Division by x−1 did not reverse the inequality < because the second line
above shows 0 < x−1, that is, x−1 is positive.) Now we have x
2 + x+1 < 0,
which is a contradiction because x being positive forces x
2 + x+1 > 0
We summarize our work as follows.
The statement “There is a real number x for which x
4 < x < x
2
” is false
because we have proved its negation “For every real number x, it is not the
case that x
4 < x < x
2
.”
As you work the exercises, keep in mind that not every conjecture will be
false. If one is true, then a disproof is impossible and you must produce a
proof. Here is an example:
Example 9.4 Either prove or disprove the following conjecture.
Conjecture There exist three integers x, y, z, all greater than 1 and no
two equal, for which x
y = y
z
.
This conjecture is true. It is an existence statement, so to prove it we
just need to give an example of three integers x, y, z, all greater than 1 and
no two equal, so that x
y = y
z
. A proof follows.
Proof. Note that if x = 2, y = 16 and z = 4, then x
y = 2
16 = (24
)
4 = 164 = y
z
. ■
152 Disproof
9.3 Disproof by Contradiction
Contradiction can be a very useful way to disprove a statement. To see
how this works, suppose we wish to disprove a statement P. We know
that to disprove P, we must prove ∼ P. To prove ∼ P with contradiction,
we assume ∼∼ P is true and deduce a contradiction. But since ∼∼ P = P,
this boils down to assuming P is true and deducing a contradiction. Here
is an outline:
How to disprove P with contradiction:
Assume P is true, and deduce a contradiction.
To illustrate this, let’s revisit Example 9.3 but do the disproof with
contradiction. You will notice that the work duplicates much of what we
did in Example 9.3, but is it much more streamlined because here we do
not have to negate the conjecture.
Example 9.5 Disprove the following conjecture.
Conjecture: There is a real number x for which x
4 < x < x
2
.
Disproof. Suppose for the sake of contradiction that this conjecture is true.
Let x be a real number for which x
4 < x < x
2
. Then x is positive, since it is
greater than the non-negative number x
4
. Dividing all parts of x
4 < x < x
2
by the positive number x produces x
3 < 1 < x. Now subtract 1 from all parts
of x
3 < 1 < x to obtain x
3 −1 < 0 < x−1 and reason as follows:
x
3 −1 < 0 < x−1
(x−1)(x
2 + x+1) < 0 < (x−1)
x
2 + x+1 < 0 < 1
Now we have x
2 + x +1 < 0, which is a contradiction because x is positive.
Thus the conjecture must be false. ■
Exercises for Chapter 9
Each of the following statements is either true or false. If a statement is true,
prove it. If a statement is false, disprove it. These exercises are cumulative,
covering all topics addressed in Chapters 1–9.
1. If x, y ∈ R, then |x+ y| = |x| +|y|.
2. For every natural number n, the integer 2n
2 −4n+31 is prime.
Disproof by Contradiction 153
3. If n ∈ Z and n
5 − n is even, then n is even.
4. For every natural number n, the integer n
2 +17n+17 is prime.
5. If A, B,C and D are sets, then (A ×B)∪(C ×D) = (A ∪C)×(B ∪D).
6. If A, B,C and D are sets, then (A ×B)∩(C ×D) = (A ∩C)×(B ∩D).
7. If A, B and C are sets, and A ×C = B ×C, then A = B.
8. If A, B and C are sets, then A −(B ∪C) = (A −B)∪(A −C).
9. If A and B are sets, then P(A)−P(B) ⊆ P(A −B).
10. If A and B are sets and A ∩B = ;, then P(A)−P(B) ⊆ P(A −B).
11. If a,b ∈ N, then a+ b < ab.
12. If a,b, c ∈ N and ab, bc and ac all have the same parity, then a,b and c all have
the same parity.
13. There exists a set X for which R ⊆ X and ; ∈ X.
14. If A and B are sets, then P(A)∩P(B) = P(A ∩B).
15. Every odd integer is the sum of three odd integers.
16. If A and B are finite sets, then |A ∪B| = |A| +|B|.
17. For all sets A and B, if A −B = ;, then B 6= ;.
18. If a,b, c ∈ N, then at least one of a− b, a+ c and b − c is even.
19. For every r, s ∈ Q with r < s, there is an irrational number u for which r < u < s.
20. There exist prime numbers p and q for which p − q = 1000.
21. There exist prime numbers p and q for which p − q = 97.
22. If p and q are prime numbers for which p < q, then 2p + q
2
is odd.
23. If x, y ∈ R and x
3 < y
3
, then x < y.
24. The inequality 2
x ≥ x+1 is true for all positive real numbers x.
25. For all a,b, c ∈ Z, if a|bc, then a|b or a| c.
26. Suppose A, B and C are sets. If A = B −C, then B = A ∪C.
27. The equation x
2 = 2
x has three real solutions.
28. Suppose a,b ∈ Z. If a|b and b|a, then a = b.
29. If x, y ∈ R and |x+ y| = |x− y|, then y = 0.
30. There exist integers a and b for which 42a+7b = 1.
31. No number (other than 1) appears in Pascal’s triangle more than four times.
32. If n,k ∈ N and ¡
n
k
¢
is a prime number, then k = 1 or k = n−1.
33. Suppose f (x) = a0 + a1x+ a2x
2 +··· + anx
n
is a polynomial of degree 1 or greater,
and for which each coefficient ai is in N. Then there is an n ∈ N for which the
integer f (n) is not prime.
34. If X ⊆ A ∪B, then X ⊆ A or X ⊆ B.
CHAPTER 10
Mathematical Induction
T
his chapter explains a powerful proof technique called mathematical
induction (or just induction for short). To motivate the discussion,
let’s first examine the kinds of statements that induction is used to prove.
Consider the following statement.
Conjecture. The sum of the first n odd natural numbers equals n
2
.
The following table illustrates what this conjecture says. Each row
is headed by a natural number n, followed by the sum of the first n odd
natural numbers, followed by n
2
.
n sum of the first n odd natural numbers n
2
1 1 = . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1
2 1+3 = . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
3 1+3+5 = . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
4 1+3+5+7 = . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
5 1+3+5+7+9 = . . . . . . . . . . . . . . . . . . . . . . . . 25
.
.
.
.
.
.
.
.
.
n 1+3+5+7+9+11+··· +(2n−1) = . . . . . . . n
2
.
.
.
.
.
.
.
.
.
Note that in the first five lines of the table, the sum of the first n odd
numbers really does add up to n
2
. Notice also that these first five lines
indicate that the nth odd natural number (the last number in each sum)
is 2n − 1. (For instance, when n = 2, the second odd natural number is
2·2−1 = 3; when n = 3, the third odd natural number is 2·3−1 = 5, etc.)
The table raises a question. Does the sum 1+3+5+7+···+(2n−1) really
always equal n
2
? In other words, is the conjecture true?
Let’s rephrase this as follows. For each natural number n (i.e., for each
line of the table), we have a statement Sn, as follows:
155
S1 : 1 = 1
2
S2 : 1+3 = 2
2
S3 : 1+3+5 = 3
2
.
.
.
Sn : 1+3+5+7+··· +(2n−1) = n
2
.
.
.
Our question is: Are all of these statements true?
Mathematical induction is designed to answer just this kind of question.
It is used when we have a set of statements S1,S2,S3,...,Sn,..., and we
need to prove that they are all true. The method is really quite simple.
To visualize it, think of the statements as dominoes, lined up in a row.
Imagine you can prove the first statement S1, and symbolize this as
domino S1 being knocked down. Additionally, imagine that you can prove
that any statement Sk being true (falling) forces the next statement Sk+1
to be true (to fall). Then S1 falls, and knocks down S2. Next S2 falls and
knocks down S3, then S3 knocks down S4, and so on. The inescapable
conclusion is that all the statements are knocked down (proved true).
The Simple Idea Behind Mathematical Induction
Statements are lined up like dominoes.
(1) Suppose the first statement falls (i.e. is proved true);
(2) Suppose the k
th falling always causes the (k +1)th to fall;
Then all must fall (i.e. all statements are proved true).
S1
S2
S3
S4
S5
S6
S1
S2
S3
S4
S5
S6
Sk
Sk+1
Sk+2
Sk+3
Sk
Sk
Sk+1
Sk+1
Sk+2
Sk+2
Sk+2
Sk+3
Sk+3
Sk+3
Sk+4
Sk+4
Sk+4
···
···
···
···
···
···
···
···
S1
Sk
Sk+1
S1 S2 S3 S4 S5 S6
S2 S3 S4 S5 S6
156 Mathematical Induction
This picture gives our outline for proof by mathematical induction.
Outline for Proof by Induction
Proposition The statements S1,S2,S3,S4,... are all true.
Proof. (Induction)
(1) Prove that the first statement S1 is true.
(2) Given any integer k ≥ 1, prove that the statement Sk ⇒ Sk+1 is true.
It follows by mathematical induction that every Sn is true. ■
In this setup, the first step (1) is called the basis step. Because S1 is
usually a very simple statement, the basis step is often quite easy to do.
The second step (2) is called the inductive step. In the inductive step
direct proof is most often used to prove Sk ⇒ Sk+1, so this step is usually
carried out by assuming Sk is true and showing this forces Sk+1 to be true.
The assumption that Sk is true is called the inductive hypothesis.
Now let’s apply this technique to our original conjecture that the sum
of the first n odd natural numbers equals n
2
. Our goal is to show that for
each n ∈ N, the statement Sn : 1+3+5+7+··· +(2n−1) = n
2
is true. Before
getting started, observe that Sk is obtained from Sn by plugging k in for n.
Thus Sk is the statement Sk : 1+3+5+7+···+(2k−1) = k
2
. Also, we get Sk+1
by plugging in k+1 for n, so that Sk+1 : 1+3+5+7+···+(2(k+1)−1) = (k+1)2
.
Proposition If n ∈ N, then 1+3+5+7+··· +(2n−1) = n
2
.
Proof. We will prove this with mathematical induction.
(1) Observe that if n = 1, this statement is 1 = 1
2
, which is obviously true.
(2) We must now prove Sk ⇒ Sk+1 for any k ≥ 1. That is, we must show
that if 1+3+5+7+···+(2k−1) = k
2
, then 1+3+5+7+···+(2(k+1)−1) = (k+1)2
.
We use direct proof. Suppose 1+3+5+7+··· +(2k −1) = k
2
. Then
1+3+5+7+··············· +(2(k +1)−1) =
1+3+5+7+··· + (2k −1) +(2(k +1)−1) =
¡
1+3+5+7+··· +(2k −1)¢
+(2(k +1)−1) =
k
2 +(2(k +1)−1) = k
2 +2k +1
= (k +1)2
.
Thus 1+3+5+7+··· +(2(k +1)−1) = (k +1)2
. This proves that Sk ⇒ Sk+1.
It follows by induction that 1+3+5+7+···+(2n−1) = n
2
for every n ∈ N. ■
157
In induction proofs it is usually the case that the first statement
S1 is indexed by the natural number 1, but this need not always be so.
Depending on the problem, the first statement could be S0, or Sm for any
other integer m. In the next example the statements are S0,S1,S2,S3,...
The same outline is used, except that the basis step verifies S0, not S1.
Proposition If n is a non-negative integer, then 5 | (n
5 − n).
Proof. We will prove this with mathematical induction. Observe that the
first non-negative integer is 0, so the basis step involves n = 0.
(1) If n = 0, this statement is 5 | (05 −0) or 5 | 0, which is obviously true.
(2) Let k ≥ 0. We need to prove that if 5 | (k
5 − k), then 5 | ((k +1)5 −(k +1)).
We use direct proof. Suppose 5 | (k
5 − k). Thus k
5 − k = 5a for some a ∈ Z.
Observe that
(k +1)5 −(k +1) = k
5 +5k
4 +10k
3 +10k
2 +5k +1− k −1
= (k
5 − k)+5k
4 +10k
3 +10k
2 +5k
= 5a+5k
4 +10k
3 +10k
2 +5k
= 5(a+ k
4 +2k
3 +2k
2 + k).
This shows (k+1)5−(k+1) is an integer multiple of 5, so 5 | ((k+1)5−(k+1)).
We have now shown that 5 | (k
5 − k) implies 5 | ((k +1)5 −(k +1)).
It follows by induction that 5 | (n
5 − n) for all non-negative integers n. ■
As noted, induction is used to prove statements of the form ∀n ∈ N,Sn.
But notice the outline does not work for statements of form ∀n ∈ Z,Sn
(where n is in Z, not N). The reason is that if you are trying to prove
∀n ∈ Z,Sn by induction, and you’ve shown S1 is true and Sk ⇒ Sk+1, then
it only follows from this that Sn is true for n ≥ 1. You haven’t proved
that any of the statements S0,S−1,S−2,... are true. If you ever want to
prove ∀n ∈ Z,Sn by induction, you have to show that some Sa is true and
Sk ⇒ Sk+1 and Sk ⇒ Sk−1.
Unfortunately, the term mathematical induction is sometimes confused
with inductive reasoning, that is, the process of reaching the conclusion
that something is likely to be true based on prior observations of similar
circumstances. Please note that that mathematical induction, as introduced
here, is a rigorous method that proves statements with absolute
certainty.
158 Mathematical Induction
To round out this section, we present four additional induction proofs.
Proposition If n ∈ Z and n ≥ 0, then Xn
i=0
i· i! = (n+1)!−1.
Proof. We will prove this with mathematical induction.
(1) If n = 0, this statement is
X
0
i=0
i· i! = (0+1)!−1.
Since the left-hand side is 0·0! = 0, and the right-hand side is 1!−1 = 0,
the equation P0
i=0
i· i! = (0+1)!−1 holds, as both sides are zero.
(2) Consider any integer k ≥ 0. We must show that Sk implies Sk+1. That
is, we must show that
X
k
i=0
i· i! = (k +1)!−1
implies
k
X+1
i=0
i· i! = ((k +1)+1)!−1.
We use direct proof. Suppose X
k
i=0
i· i! = (k +1)!−1. Observe that
k
X+1
i=0
i· i! =
Ã
X
k
i=0
i· i!
!
+(k +1)(k +1)!
=
³
(k +1)!−1
´
+(k +1)(k +1)!
= (k +1)!+(k +1)(k +1)!−1
=
¡
1+(k +1)¢
(k +1)!−1
= (k +2)(k +1)!−1
= (k +2)!−1
= ((k +1)+1)!−1.
Therefore
k
X+1
i=0
i· i! = ((k +1)+1)!−1.
It follows by induction that Xn
i=0
i· i! = (n+1)!−1 for every integer n ≥ 0. ■
159
The next example illustrates a trick that is occasionally useful. You
know that you can add equal quantities to both sides of an equation without
violating equality. But don’t forget that you can add unequal quantities to
both sides of an inequality, as long as the quantity added to the bigger
side is bigger than the quantity added to the smaller side. For example, if
x ≤ y and a ≤ b, then x+ a ≤ y+ b. Similarly, if x ≤ y and b is positive, then
x ≤ y+ b. This oft-forgotten fact is used in the next proof.
Proposition For each n ∈ N, it follows that 2
n ≤ 2
n+1 −2
n−1 −1.
Proof. We will prove this with mathematical induction.
(1) If n = 1, this statement is 2
1 ≤ 2
1+1 − 2
1−1 − 1, which simplifies to
2 ≤ 4−1−1, which is obviously true.
(2) Suppose k ≥ 1. We need to show that 2
k ≤ 2
k+1 − 2
k−1 − 1 implies
2
k+1 ≤ 2
(k+1)+1−2
(k+1)−1−1. We use direct proof. Suppose 2
k ≤ 2
k+1−2
k−1−1,
and reason as follows:
2
k ≤ 2
k+1 −2
k−1 −1
2(2k
) ≤ 2(2k+1 −2
k−1 −1) (multiply both sides by 2)
2
k+1 ≤ 2
k+2 −2
k −2
2
k+1 ≤ 2
k+2 −2
k −2+1 (add 1 to the bigger side)
2
k+1 ≤ 2
k+2 −2
k −1
2
k+1 ≤ 2
(k+1)+1 −2
(k+1)−1 −1.
It follows by induction that 2
n ≤ 2
n+1 −2
n−1 −1 for each n ∈ N. ■
We next prove that if n ∈ N, then the inequality (1+ x)
n ≥ 1+ nx holds
for all x ∈ R with x > −1. Thus we will need to prove that the statement
Sn : (1+ x)
n ≥ 1+ nx for every x ∈ R with x > −1
is true for every natural number n. This is (only) slightly different from
our other examples, which proved statements of the form ∀n ∈ N, P(n),
where P(n) is a statement about the number n. This time we are proving
something of form
∀n ∈ N, P(n, x),
where the statement P(n, x) involves not only n, but also a second variable x.
(For the record, the inequality (1 + x)
n ≥ 1 + nx is known as Bernoulli’s
inequality.)
160 Mathematical Induction
Proposition If n ∈ N, then (1+ x)
n ≥ 1+ nx for all x ∈ R with x > −1.
Proof. We will prove this with mathematical induction.
(1) For the basis step, notice that when n = 1 the statement is (1+ x)
1 ≥
1+1· x , and this is true because both sides equal 1+ x.
(2) Assume that for some k ≥ 1, the statement (1+ x)
k ≥ 1+ kx is true for
all x ∈ R with x > −1. From this we need to prove (1+ x)
k+1 ≥ 1+(k +1)x.
Now, 1+ x is positive because x > −1, so we can multiply both sides of
(1+ x)
k ≥ 1+ kx by (1+ x) without changing the direction of the ≥.
(1+ x)
k
(1+ x) ≥ (1+ kx)(1+ x)
(1+ x)
k+1 ≥ 1+ x+ kx + kx2
(1+ x)
k+1 ≥ 1+(k +1)x+ kx2
The above term kx2
is positive, so removing it from the right-hand side
will only make that side smaller. Thus we get (1+ x)
k+1 ≥ 1+(k +1)x. ■
Next, an example where the basis step involves more than routine
checking. (It will be used later, so it is numbered for reference.)
Proposition 10.1 Suppose a1,a2,...,an are n integers, where n ≥ 2. If p
is prime and p | (a1 · a2 · a3 ···an), then p | ai for at least one of the ai
.
Proof. The proof is induction on n.
(1) The basis step involves n = 2. Let p be prime and suppose p | (a1a2).
We need to show that p | a1 or p | a2, or equivalently, if p - a1, then
p | a2. Thus suppose p - a1. Since p is prime, it follows that gcd(p,a1) = 1.
By Proposition 7.1 (on page 126), there are integers k and ` for which
1 = pk + a1`. Multiplying this by a2 gives
a2 = pka2 + a1a2`.
As we are assuming that p divides a1a2, it is clear that p divides the
expression pka2+a1a2` on the right; hence p | a2. We’ve now proved that
if p | (a1a2), then p | a1 or p | a2. This completes the basis step.
(2) Suppose that k ≥ 2, and p | (a1 · a2 ···ak) implies then p | ai for some ai
.
Now let p | (a1 · a2 ···ak · ak+1). Then p |
¡
(a1 · a2 ···ak)· ak+1
¢
. By what we
proved in the basis step, it follows that p | (a1 · a2 ···ak) or p | ak+1. This
and the inductive hypothesis imply that p divides one of the ai
. ■
Please test your understanding now by working a few exercises.
Proof by Strong Induction 161
10.1 Proof by Strong Induction
This section describes a useful variation on induction.
Occasionally it happens in induction proofs that it is difficult to show
that Sk forces Sk+1 to be true. Instead you may find that you need to use
the fact that some “lower” statements Sm (with m < k) force Sk+1 to be true.
For these situations you can use a slight variant of induction called strong
induction. Strong induction works just like regular induction, except that
in Step (2) instead of assuming Sk is true and showing this forces Sk+1
to be true, we assume that all the statements S1,S2,...,Sk are true and
show this forces Sk+1 to be true. The idea is that if it always happens that
the first k dominoes falling makes the (k +1)th domino fall, then all the
dominoes must fall. Here is the outline.
Outline for Proof by Strong Induction
Proposition The statements S1,S2,S3,S4,... are all true.
Proof. (Strong induction)
(1) Prove the first statement S1. (Or the first several Sn.)
(2) Given any integer k ≥ 1, prove (S1 ∧ S2 ∧ S3 ∧··· ∧ Sk) ⇒ Sk+1. ■
Strong induction can be useful in situations where assuming Sk is true
does not neatly lend itself to forcing Sk+1 to be true. You might be better
served by showing some other statement (Sk−1 or Sk−2 for instance) forces
Sk to be true. Strong induction says you are allowed to use any (or all) of
the statements S1,S2,...,Sk to prove Sk+1.
As our first example of strong induction, we are going to prove that
12 | (n
4 − n
2
) for any n ∈ N. But first, let’s look at how regular induction
would be problematic. In regular induction we would start by showing
12 | (n
4 − n
2
) is true for n = 1. This part is easy because it reduces to 12 | 0,
which is clearly true. Next we would assume that 12 | (k
4 − k
2
) and try to
show this implies 12 | ((k+1)4−(k+1)2
). Now, 12 | (k
4−k
2
) means k
4−k
2 = 12a
for some a ∈ Z. Next we use this to try to show (k +1)4 −(k +1)2 = 12b for
some integer b. Working out (k +1)4 −(k +1)2
, we get
(k +1)4 −(k +1)2 = (k
4 +4k
3 +6k
2 +4k +1)−(k
2 +2k +1)
= (k
4 − k
2
)+4k
3 +6k
2 +6k
= 12a+4k
3 +6k
2 +6k.
At this point we’re stuck because we can’t factor out a 12. Now let’s see
how strong induction can get us out of this bind.
162 Mathematical Induction
Strong induction involves assuming each of statements S1,S2,...,Sk is
true, and showing that this forces Sk+1 to be true. In particular, if S1
through Sk are true, then certainly Sk−5 is true, provided that 1 ≤ k−5 < k.
The idea is then to show Sk−5 ⇒ Sk+1 instead of Sk ⇒ Sk+1. For this to
make sense, our basis step must involve checking that S1,S2,S3,S4,S5,S6
are all true. Once this is established, Sk−5 ⇒ Sk+1 will imply that the other
Sk are all true. For example, if k = 6, then Sk−5 ⇒ Sk+1 is S1 ⇒ S7, so S7 is
true; for k = 7, then Sk−5 ⇒ Sk+1 is S2 ⇒ S8, so S8 is true, etc.
Proposition If n ∈ N, then 12 | (n
4 − n
2
).
Proof. We will prove this with strong induction.
(1) First note that the statement is true for the first six positive integers:
If n = 1, 12 divides n
4 − n
2 = 1
4 −1
2 = 0.
If n = 2, 12 divides n
4 − n
2 = 2
4 −2
2 = 12.
If n = 3, 12 divides n
4 − n
2 = 3
4 −3
2 = 72.
If n = 4, 12 divides n
4 − n
2 = 4
4 −4
2 = 240.
If n = 5, 12 divides n
4 − n
2 = 5
4 −5
2 = 600.
If n = 6, 12 divides n
4 − n
2 = 6
4 −6
2 = 1260.
(2) Let k ≥ 6 and assume 12 | (m4 − m2
) for 1 ≤ m ≤ k. (That is, assume
statements S1,S2,...,Sk are all true.) We must show 12 |
¡
(k+1)4−(k+1)2
¢
.
(That is, we must show that Sk+1 is true.) Since Sk−5 is true, we have
12 | ((k − 5)4 − (k − 5)2
). For simplicity, let’s set m = k −5, so we know
12 | (m4−m2
), meaning m4 − m2 = 12a for some integer a. Observe that:
(k +1)4 −(k +1)2 = (m+6)4 −(m+6)2
= m4 +24m3 +216m2 +864m+1296−(m2 +12m+36)
= (m4 − m2
)+24m3 +216m2 +852m+1260
= 12a+24m3 +216m2 +852m+1260
= 12¡
a+2m3 +18m2 +71m+105¢
.
As (a+2m3 +18m2 +71m+105) is an integer, we get 12 | ((k+1)4 −(k+1)2
).
This shows by strong induction that 12 | (n
4 − n
2
) for every n ∈ N. ■
Proof by Strong Induction 163
Our next example involves mathematical objects called graphs. In
mathematics, the word graph is used in two contexts. One context involves
the graphs of equations and functions from algebra and calculus. In
the other context, a graph is a configuration consisting of points (called
vertices) and edges which are lines connecting the vertices. Following
are some pictures of graphs. Let’s agree that all of our graphs will be in
“one piece,” that is, you can travel from any vertex of a graph to any other
vertex by traversing a route of edges from one vertex to the other.
v0
v1
v2 v3
v4
Figure 10.1. Examples of Graphs
A cycle in a graph is a sequence of distinct edges in the graph that
form a route that ends where it began. For example, the graph on the
far left of Figure 10.1 has a cycle that starts at vertex v1, then goes to v2,
then to v3, then v4 and finally back to its starting point v1. You can find
cycles in both of the graphs on the left, but the two graphs on the right do
not have cycles. There is a special name for a graph that has no cycles;
it is called a tree. Thus the two graphs on the right of Figure 10.1 are
trees, but the two graphs on the left are not trees.
Figure 10.2. A tree
Note that the trees in Figure 10.1 both have one fewer edge than vertex.
The tree on the far right has 5 vertices and 4 edges. The one next to it
has 6 vertices and 5 edges. Draw any tree; you will find that if it has n
vertices, then it has n−1 edges. We now prove that this is always true.
164 Mathematical Induction
Proposition If a tree has n vertices, then it has n−1 edges.
Proof. Notice that this theorem asserts that for any n ∈ N, the following
statement is true: Sn : A tree with n vertices has n−1 edges. We use strong
induction to prove this.
(1) Observe that if a tree has n = 1 vertex then it has no edges. Thus it
has n−1 = 0 edges, so the theorem is true when n = 1.
(2) Now take an integer k ≥ 1. We must show (S1 ∧ S2 ∧ ··· ∧ Sk) ⇒ Sk+1.
In words, we must show that if it is true that any tree with m vertices
has m−1 edges, where 1 ≤ m ≤ k, then any tree with k +1 vertices has
(k +1)−1 = k edges. We will use direct proof.
Suppose that for each integer m with 1 ≤ m ≤ k, any tree with m vertices
has m−1 edges. Now let T be a tree with k +1 vertices. Single out an
edge of T and label it e, as illustrated below.
···
··· ···
···
T1 T2
T
e
···
··· ···
···
Now remove the edge e from T, but leave the two endpoints of e. This
leaves two smaller trees that we call T1 and T2. Let’s say T1 has x
vertices and T2 has y vertices. As each of these two smaller trees has
fewer than k +1 vertices, our inductive hypothesis guarantees that T1
has x−1 edges, and T2 has y−1 edges. Think about our original tree T.
It has x + y vertices. It has x −1 edges that belong to T1 and y−1 edges
that belong to T2, plus it has the additional edge e that belongs to
neither T1 nor T2. Thus, all together, the number of edges that T has is
(x−1)+(y−1)+1 = (x+ y)−1. In other words, T has one fewer edges than
it has vertices. Thus it has (k +1)−1 = k edges.
It follows by strong induction that a tree with n vertices has n−1 edges. ■
Notice that it was absolutely essential that we used strong induction
in the above proof because the two trees T1 and T2 will not both have k
vertices. At least one will have fewer than k vertices. Thus the statement
Sk is not enough to imply Sk+1. We need to use the assumption that Sm
will be true whenever m ≤ k, and strong induction allows us to do this.
Proof by Smallest Counterexample 165
10.2 Proof by Smallest Counterexample
This section introduces yet another proof technique, called proof by smallest
counterexample. It is a hybrid of induction and proof by contradiction.
It has the nice feature that it leads you straight to a contradiction. It
is therefore more “automatic” than the proof by contradiction that was
introduced in Chapter 6. Here is the outline:
Outline for Proof by Smallest Counterexample
Proposition The statements S1,S2,S3,S4,... are all true.
Proof. (Smallest counterexample)
(1) Check that the first statement S1 is true.
(2) For the sake of contradiction, suppose not every Sn is true.
(3) Let k > 1 be the smallest integer for which Sk is false.
(4) Then Sk−1 is true and Sk is false. Use this to get a contradiction. ■
Notice that this setup leads you to a point where Sk−1 is true and
Sk is false. It is here, where true and false collide, that you will find a
contradiction. Let’s do an example.
Proposition If n ∈ N, then 4 | (5n −1).
Proof. We use proof by smallest counterexample. (We will number the
steps to match the outline, but that is not usually done in practice.)
(1) If n = 1, then the statement is 4 | (51 −1), or 4 | 4, which is true.
(2) For sake of contradiction, suppose it’s not true that 4 | (5n −1) for all n.
(3) Let k > 1 be the smallest integer for which 4 - (5k −1).
(4) Then 4 | (5k−1−1), so there is an integer a for which 5
k−1−1 = 4a. Then:
5
k−1 −1 = 4a
5(5k−1 −1) = 5·4a
5
k −5 = 20a
5
k −1 = 20a+4
5
k −1 = 4(5a+1)
This means 4 | (5k −1), a contradiction, because 4 - (5k −1) in Step 3. Thus,
we were wrong in Step 2 to assume that it is untrue that 4 | (5n −1) for
every n. Therefore 4 | (5n −1) is true for every n. ■
166 Mathematical Induction
We next prove the fundamental theorem of arithmetic, which says
any integer greater than 1 has a unique prime factorization. For example,
12 factors into primes as 12 = 2·2·3, and moreover any factorization of 12
into primes uses exactly the primes 2, 2 and 3. Our proof combines the
techniques of induction, cases, minimum counterexample and the idea of
uniqueness of existence outlined at the end of Section 7.3. We dignify this
fundamental result with the label of “Theorem.”
Theorem 10.1 (Fundamental Theorem of Arithmetic) Any integer n > 1
has a unique prime factorization. That is, if n = p1 · p2 · p3 ··· pk and n =
a1 ·a2 ·a3 ···a` are two prime factorizations of n, then k = `, and the primes
pi and ai are the same, except that they may be in a different order.
Proof. Suppose n > 1. We first use strong induction to show that n has a
prime factorization. For the basis step, if n = 2, it is prime, so it is already
its own prime factorization. Let n ≥ 2 and assume every integer between 2
and n (inclusive) has a prime factorization. Consider n +1. If it is prime,
then it is its own prime factorization. If it is not prime, then it factors as
n +1 = ab with a,b > 1. Because a and b are both less than n +1 they have
prime factorizations a = p1 · p2 · p3 ··· pk and b = p
0
1
· p
0
2
· p
0
3
··· p
0
`
. Then
n+1 = ab = (p1 · p2 · p3 ··· pk)(p
0
1
· p
0
2
· p
0
3
··· p
0
`
)
is a prime factorization of n+1. This competes the proof by strong induction
that every integer greater than 1 has a prime factorization.
Next we use proof by smallest counterexample to prove that the prime
factorization of any n ≥ 2 is unique. If n = 2, then n clearly has only one
prime factorization, namely itself. Assume for the sake of contradiction that
there is an n > 2 that has different prime factorizations n = p1 · p2 · p3 ··· pk
and n = a1 ·a2 ·a3 ···a`. Assume n is the smallest number with this property.
From n = p1 · p2 · p3 ··· pk, we see that p1 | n, so p1 | (a1 · a2 · a3 ···a`). By
Proposition 10.1 (page 160), p1 divides one of the primes ai
. As ai is prime,
we have p1 = ai
. Dividing n = p1 · p2 · p3 ··· pk = a1 · a2 · a3 ···a` by p1 = ai
yields
p2 · p3 ··· pk = a1 · a2 · a3 ···ai−1 · ai+1 ···a`.
These two factorizations are different, because the two prime factorizations
of n were different. (Remember: the primes p1 and ai are equal, so the
difference appears in the remaining factors, displayed above.) But also the
above number p2 · p3 ··· pk is smaller than n, and this contradicts the fact
that n was the smallest number with two different prime factorizations. ■
Fibonacci Numbers 167
One word of warning about proof by smallest counterexample. In proofs
in other textbooks or in mathematical papers, it often happens that the
writer doesn’t tell you up front that proof by smallest counterexample
is being used. Instead, you will have to read through the proof to glean
from context that this technique is being used. In fact, the same warning
applies to all of our proof techniques. If you continue with mathematics,
you will gradually gain through experience the ability to analyze a proof
and understand exactly what approach is being used when it is not stated
explicitly. Frustrations await you, but do not be discouraged by them.
Frustration is a natural part of anything that’s worth doing.
10.3 Fibonacci Numbers
Leonardo Pisano, now known as Fibonacci, was a mathematician born
around 1175 in what is now Italy. His most significant work was a book
Liber Abaci, which is recognized as a catalyst in medieval Europe’s slow
transition from Roman numbers to the Hindu-Arabic number system. But
he is best known today for a number sequence that he described in his
book and that bears his name. The Fibonacci sequence is
1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144, 233, 377,...
The numbers that appear in this sequence are called Fibonacci numbers.
The first two numbers are 1 and 1, and thereafter any entry is the sum
of the previous two entries. For example 3+5 = 8, and 5+8 = 13, etc. We
denote the nth term of this sequence as Fn. Thus F1 = 1, F2 = 1, F3 = 2,
F4 = 3, F7 = 13 and so on. Notice that the Fibonacci Sequence is entirely
determined by the rules F1 = 1, F2 = 1, and Fn = Fn−1 + Fn−2.
We introduce Fibonacci’s sequence here partly because it is something
everyone should know about, but also because it is a great source of induction
problems. This sequence, which appears with surprising frequency in
nature, is filled with mysterious patterns and hidden structures. Some of
these structures will be revealed to you in the examples and exercises.
We emphasize that the condition Fn = Fn−1+Fn−2 (or equivalently Fn+1 =
Fn + Fn−1) is the perfect setup for induction. It suggests that we can
determine something about Fn by looking at earlier terms of the sequence.
In using induction to prove something about the Fibonacci sequence, you
should expect to use the equation Fn = Fn−1 + Fn−2 somewhere.
For our first example we will prove that F
2
n+1
− Fn+1Fn − F
2
n = (−1)n
for
any natural number n. For example, if n = 5 we have F
2
6
− F6F5 − F
2
5
=
8
2 −8·5−5
2 = 64−40−25 = −1 = (−1)5
.
168 Mathematical Induction
Proposition The Fibonacci sequence obeys F
2
n+1
− Fn+1Fn − F
2
n = (−1)n
.
Proof. We will prove this with mathematical induction.
(1) If n = 1 we have F
2
n+1
−Fn+1Fn−F
2
n = F
2
2
−F2F1−F
2
1
= 1
2−1·1−1
2 = −1 =
(−1)1 = (−1)n
, so indeed F
2
n+1
− Fn+1Fn − F
2
n = (−1)n
is true when n = 1.
(2) Take any integer k ≥ 1. We must show that if F
2
k+1
−Fk+1Fk−F
2
k
= (−1)k
,
then F
2
k+2
− Fk+2Fk+1 − F
2
k+1
= (−1)k+1
. We use direct proof. Suppose
F
2
k+1
− Fk+1Fk − F
2
k
= (−1)k
. Now we are going to carefully work out the
expression F
2
k+2
− Fk+2Fk+1 − F
2
k+1
and show that it really does equal
(−1)k+1
. In so doing, we will use the fact that Fk+2 = Fk+1 + Fk.
F
2
k+2 − Fk+2Fk+1 − F
2
k+1 = (Fk+1 + Fk)
2 −(Fk+1 + Fk)Fk+1 − F
2
k+1
= F
2
k+1 +2Fk+1Fk + F
2
k − F
2
k+1 − FkFk+1 − F
2
k+1
= −F
2
k+1 + Fk+1Fk + F
2
k
= −(F
2
k+1 − Fk+1Fk − F
2
k
)
= −(−1)k
(inductive hypothesis)
= (−1)1
(−1)k
= (−1)k+1
Therefore F
2
k+2
− Fk+2Fk+1 − F
2
k+1
= (−1)k+1
.
It follows by induction that F
2
n+1
− Fn+1Fn − F
2
n = (−1)n
for every n ∈ N. ■
Let’s pause for a moment and think about what the result we just
proved means. Dividing both sides of F
2
n+1
−Fn+1Fn −F
2
n = (−1)n by F
2
n gives
µ
Fn+1
Fn
¶2
−
Fn+1
Fn
−1 =
(−1)n
F
2
n
.
For large values of n, the right-hand side is very close to zero, and the
left-hand side is Fn+1/Fn plugged into the polynomial x
2 − x−1. Thus, as
n increases, the ratio Fn+1/Fn approaches a root of x
2 − x − 1 = 0. By the
quadratic formula, the roots of x
2 −x−1 are 1±
p
5
2
. As Fn+1/Fn > 1, this ratio
must be approaching the positive root 1+
p
5
2
. Therefore
limn→∞
Fn+1
Fn
=
1+
p
5
2
. (10.1)
For a quick spot check, note that F13/F12 ≈ 1.618025, while 1+
p
5
2
≈ 1.618033.
Even for the small value n = 12, the numbers match to four decimal places.
Fibonacci Numbers 169
The number Φ =
1+
p
5
2
is sometimes called the golden ratio, and there
has been much speculation about its occurrence in nature as well as
in classical art and architecture. One theory holds that the Parthenon
and the Great Pyramids of Egypt were designed in accordance with this
number.
But we are here concerned with things that can be proved. We close by
observing how the Fibonacci sequence in many ways resembles a geometric
sequence. Recall that a geometric sequence with first term a and
common ratio r has the form
a, ar, ar2
, ar3
, ar4
, ar5
, ar6
, ar7
, ar8
,...
where any term is obtained by multiplying the previous term by r. In
general its nth term is Gn = arn
, and Gn+1/Gn = r. Equation (10.1) tells
us that Fn+1/Fn ≈ Φ. Thus even though it is not a geometric sequence,
the Fibonacci sequence tends to behave like a geometric sequence with
common ratio Φ, and the further “out” you go, the higher the resemblance.
Exercises for Chapter 10
Prove the following statements with either induction, strong induction or proof
by smallest counterexample.
1. For every integer n ∈ N, it follows that 1+2+3+4+··· + n =
n
2 + n
2
.
2. For every integer n ∈ N, it follows that 1
2 +2
2 +3
2 +4
2 +··· + n
2 =
n(n+1)(2n+1)
6
.
3. For every integer n ∈ N, it follows that 1
3 +2
3 +3
3 +4
3 +··· + n
3 =
n
2
(n+1)2
4
.
4. If n ∈ N, then 1·2+2·3+3·4+4·5+··· + n(n+1) =
n(n+1)(n+2)
3
.
5. If n ∈ N, then 2
1 +2
2 +2
3 +··· +2
n = 2
n+1 −2.
6. For every natural number n, it follows that Xn
i=1
(8i −5) = 4n
2 − n.
7. If n ∈ N, then 1·3+2·4+3·5+4·6+··· + n(n+2) =
n(n+1)(2n+7)
6
.
8. If n ∈ N, then 1
2!
+
2
3!
+
3
4!
+··· +
n
(n+1)!
= 1−
1
(n+1)!
9. For any integer n ≥ 0, it follows that 24 | (52n −1).
10. For any integer n ≥ 0, it follows that 3 | (52n −1).
11. For any integer n ≥ 0, it follows that 3 | (n
3 +5n+6).
12. For any integer n ≥ 0, it follows that 9 | (43n +8).
170 Mathematical Induction
13. For any integer n ≥ 0, it follows that 6 | (n
3 − n).
14. Suppose a ∈ Z. Prove that 5 | 2
na implies 5 | a for any n ∈ N.
15. If n ∈ N, then 1
1·2
+
1
2·3
+
1
3·4
+
1
4·5
+··· +
1
n(n+1)
= 1−
1
n+1
.
16. For every natural number n, it follows that 2
n +1 ≤ 3
n
.
17. Suppose A1, A2,... An are sets in some universal set U, and n ≥ 2. Prove that
A1 ∩ A2 ∩··· ∩ An = A1 ∪ A2 ∪··· ∪ An.
18. Suppose A1, A2,... An are sets in some universal set U, and n ≥ 2. Prove that
A1 ∪ A2 ∪··· ∪ An = A1 ∩ A2 ∩··· ∩ An.
19. Prove that 1
1
+
1
4
+
1
9
+··· +
1
n
2
≤ 2−
1
n
.
20. Prove that (1+2+3+··· + n)
2 = 1
3 +2
3 +3
3 +··· + n
3
for every n ∈ N.
21. If n ∈ N, then 1
1
+
1
2
+
1
3
+
1
4
+
1
5
+··· +
1
2
n −1
+
1
2
n
≥ 1+
n
2
.
(Note: This problem asserts that the sum of the first 2
n
terms of the harmonic
series is at least 1+ n/2. It thus implies that the harmonic series diverges.)
22. If n ∈ N, then µ
1−
1
2
¶µ1−
1
4
¶µ1−
1
8
¶µ1−
1
16¶
···µ
1−
1
2
n
¶
≥
1
4
+
1
2
n+1
.
23. Use mathematical induction to prove the binomial theorem (Theorem 3.1 on
page 80). You may find that you need Equation (3.2) on page 78.
24. Prove that Xn
k=1
k
¡
n
k
¢
= n2
n−1
for each natural number n.
25. Concerning the Fibonacci sequence, prove that F1+F2+F3+F4+...+Fn = Fn+2−1.
26. Concerning the Fibonacci sequence, prove that Xn
k=1
F
2
k = FnFn+1.
27. Concerning the Fibonacci sequence, prove that F1 +F3 +F5 +F7 +...+F2n−1 = F2n.
28. Concerning the Fibonacci sequence, prove that F2 + F4 + F6 + F8 + ... + F2n =
F2n+1 −1.
29. In this problem n ∈ N and Fn is the nth Fibonacci number. Prove that
¡
n
0
¢
+
¡
n−1
1
¢
+
¡
n−2
2
¢
+
¡
n−3
3
¢
+··· +¡
0
n
¢
= Fn+1.
(For example, ¡
6
0
¢
+
¡
5
1
¢
+
¡
4
2
¢
+
¡
3
3
¢
+
¡
2
4
¢
+
¡
1
5
¢
+
¡
0
6
¢
= 1+5+6+1+0+0+0 = 13 = F6+1.)
30. Here Fn is the nth Fibonacci number. Prove that
Fn =
³
1+
p
5
2
´n
−
³
1−
p
5
2
´n
p
5
.
31. Prove that Xn
k=0
¡
k
r
¢
=
¡
n+1
r+1
¢
, where 1 ≤ r ≤ n.
32. Prove that the number of n-digit binary numbers that have no consecutive
1’s is the Fibonacci number Fn+2. For example, for n = 2 there are three such
Fibonacci Numbers 171
numbers (00, 01, and 10), and 3 = F2+2 = F4. Also, for n = 3 there are five such
numbers (000, 001, 010, 100, 101), and 5 = F3+2 = F5.
33. Suppose n (infinitely long) straight lines lie on a plane in such a way that no
two of the lines are parallel, and no three of the lines intersect at a single
point. Show that this arrangement divides the plane into n
2+n+2
2
regions.
34. Prove that 3
1 +3
2 +3
3 +3
4 +··· +3
n =
3
n+1 −3
2
for every n ∈ N.
35. Prove that if n,k ∈ N, and n is even and k is odd, then ¡
n
k
¢
is even.
36. Prove that if n = 2
k−1 for some k ∈ N, then every entry in the nth row of Pascal’s
triangle is odd.
The remaining odd-numbered exercises below are not solved in the back of the
book.
37. Prove that if m,n ∈ N, then Pn
k=0
k
¡m+k
m
¢
= n
¡m+n+1
m+1
¢
−
¡m+n+1
m+2
¢
.
38. Prove that if n is a positive integer, then ¡
n
0
¢2
+
¡
n
1
¢2
+
¡
n
2
¢2
+··· +¡
n
n
¢2
=
¡
2n
n
¢
.
39. Prove that if n is a positive integer, then ¡
n+0
0
¢
+
¡
n+1
1
¢
+
¡
n+2
2
¢
+··· +¡
n+k
k
¢
=
¡
n+k+1
k
¢
.
40. Prove that P
p
k=0
¡m
k
¢¡ n
p−k
¢
=
¡m+n
p
¢
for positive integers m,n and p.
41. Prove that Pm
k=0
¡m
k
¢¡ n
p+k
¢
=
¡m+n
m+p
¢
for positive integers m,n and p.

Part IV
Relations, Functions and
Cardinality

CHAPTER 11
Relations
I
n mathematics there are endless ways that two entities can be related
to each other. Consider the following mathematical statements.
5 < 10 5 ≤ 5 6 =
30
5
5 | 80 7 > 4 x 6= y 8 - 3
a ≡ b ( mod n) 6 ∈ Z X ⊆ Y π ≈ 3.14 0 ≥ −1
p
2 ∉ Z Z 6⊆ N
In each case two entities appear on either side of a symbol, and we
interpret the symbol as expressing some relationship between the two
entities. Symbols such as <,≤,=,|,-,≥,>, ∈ and ⊂, etc., are called relations
because they convey relationships among things.
Relations are significant. In fact, you would have to admit that there
would be precious little left of mathematics if we took away all the relations.
Therefore it is important to have a firm understanding of them, and this
chapter is intended to develop that understanding.
Rather than focusing on each relation individually (an impossible task
anyway since there are infinitely many different relations), we will develop
a general theory that encompasses all relations. Understanding this
general theory will give us the conceptual framework and language needed
to understand and discuss any specific relation.
Before stating the theoretical definition of a relation, let’s look at a
motivational example. This example will lead naturally to our definition.
Consider the set A =
©
1,2,3,4,5
ª
. (There’s nothing special about this
particular set; any set of numbers would do for this example.) Elements of
A can be compared to each other by the symbol “<.” For example, 1 < 4,
2 < 3, 2 < 4, and so on. You have no trouble understanding this because the
notion of numeric order is so ingrained. But imagine you had to explain
it to an idiot savant, one with an obsession for detail but absolutely no
understanding of the meaning of (or relationships between) integers. You
might consider writing down for your student the following set:
R =
©
(1,2),(1,3),(1,4),(1,5),(2,3),(2,4),(2,5),(3,4),(3,5),(4,5) ª
.
176 Relations
The set R encodes the meaning of the < relation for elements in A. An
ordered pair (a,b) appears in the set if and only if a < b. If asked whether
or not it is true that 3 < 4, your student could look through R until he
found the ordered pair (3,4); then he would know 3 < 4 is true. If asked
about 5 < 2, he would see that (5,2) does not appear in R, so 5 6< 2. The set
R, which is a subset of A × A, completely describes the relation < for A.
Though it may seem simple-minded at first, this is exactly the idea
we will use for our main definition. This definition is general enough to
describe not just the relation < for the set A =
©
1,2,3,4,5
ª
, but any relation
for any set A.
Definition 11.1 A relation on a set A is a subset R ⊆ A × A. We often
abbreviate the statement (x, y) ∈ R as xR y. The statement (x, y) ∉ R is
abbreviated as xR y 6 .
Notice that a relation is a set, so we can use what we know about sets
to understand and explore relations. But before getting deeper into the
theory of relations, let’s look at some examples of Definition 11.1.
Example 11.1 Let A =
©
1,2,3,4
ª
, and consider the following set:
R =
©
(1,1),(2,1),(2,2),(3,3),(3,2),(3,1),(4,4),(4,3),(4,2),(4,1)ª
⊆ A × A.
The set R is a relation on A, by Definition 11.1. Since (1,1) ∈ R, we have
1R1. Similarly 2R1 and 2R2, and so on. However, notice that (for example)
(3,4) ∉ R, so 3R6 4. Observe that R is the familiar relation ≥ for the set A.
Chapter 1 proclaimed that all of mathematics can be described with
sets. Just look at how successful this program has been! The greaterthan-or-equal-to
relation is now a set R. (We might even express this in
the rather cryptic form ≥= R.)
Example 11.2 Let A =
©
1,2,3,4
ª
, and consider the following set:
S =
©
(1,1),(1,3),(3,1),(3,3),(2,2),(2,4),(4,2),(4,4)ª
⊆ A × A.
Here we have 1S1, 1S3, 4S2, etc., but 3 S6 4 and 2 S6 1. What does S mean?
Think of it as meaning “has the same parity as.” Thus 1S1 reads “1 has
the same parity as 1,” and 4S2 reads “4 has the same parity as 2.”
Example 11.3 Consider relations R and S of the previous two examples.
Note that R∩S =
©
(1,1),(2,2),(3,3),(3,1),(4,4),(4,2)ª
⊆ A×A is a relation on A.
The expression x(R ∩ S)y means “x ≥ y, and x, y have the same parity.”
177
Example 11.4 Let B =
©
0,1,2,3,4,5
ª
, and consider the following set:
U =
©
(1,3),(3,3),(5,2),(2,5),(4,2)ª
⊆ B ×B.
Then U is a relation on B because U ⊆ B × B. You may be hard-pressed
to invent any “meaning” for this particular relation. A relation does not
have to have any meaning. Any random subset of B ×B is a relation on B,
whether or not it describes anything familiar.
Some relations can be described with pictures. For example, we can
depict the above relation U on B by drawing points labeled by elements of
B. The statement (x, y) ∈ U is then represented by an arrow pointing from
x to y, a graphic symbol meaning “x relates to y.” Here’s a picture of U:
0 1 2
3 4 5
The next picture illustrates the relation R on the set A =
©
a,b, c,d
ª
, where
xR y means x comes before y in the alphabet. According to Definition 11.1,
as a set this relation is R =
©
(a,b),(a, c),(a,d),(b, c),(b,d),(c,d)
ª
. You may
feel that the picture conveys the relation better than the set does. They
are two different ways of expressing the same thing. In some instances
pictures are more convenient than sets for discussing relations.
d
b c
a
Although such diagrams can help us visualize relations, they do have
their limitations. If A and R were infinite, then the diagram would be
impossible to draw, but the set R might be easily expressed in set-builder
notation. Here are some examples.
Example 11.5 Consider the set R =
©
(x, y) ∈ Z×Z : x − y ∈ N
ª
⊆ Z×Z. This
is the > relation on the set A = Z. It is infinite because there are infinitely
many ways to have x > y where x and y are integers.
Example 11.6 The set R =
©
(x, x) : x ∈ R
ª
⊆ R×R is the relation = on the
set R, because xR y means the same thing as x = y. Thus R is a set that
expresses the notion of equality of real numbers.
178 Relations
Exercises for Section 11.0
1. Let A =
©
0,1,2,3,4,5
ª
. Write out the relation R that expresses > on A. Then
illustrate it with a diagram.
2. Let A =
©
1,2,3,4,5,6
ª
. Write out the relation R that expresses | (divides) on A.
Then illustrate it with a diagram.
3. Let A =
©
0,1,2,3,4,5
ª
. Write out the relation R that expresses ≥ on A. Then
illustrate it with a diagram.
4. Here is a diagram for a relation R on a set A. Write the sets A and R.
0 1 2
3 4 5
5. Here is a diagram for a relation R on a set A. Write the sets A and R.
0 1 2
3 4 5
6. Congruence modulo 5 is a relation on the set A = Z. In this relation xR y means
x ≡ y (mod 5). Write out the set R in set-builder notation.
7. Write the relation < on the set A = Z as a subset R of Z×Z. This is an infinite
set, so you will have to use set-builder notation.
8. Let A =
©
1,2,3,4,5,6
ª
. Observe that ; ⊆ A × A, so R = ; is a relation on A. Draw
a diagram for this relation.
9. Let A =
©
1,2,3,4,5,6
ª
. How many different relations are there on the set A?
10. Consider the subset R = (R×R)−
©
(x, x) : x ∈ R
ª
⊆ R×R. What familiar relation on
R is this? Explain.
11. Given a finite set A, how many different relations are there on A?
In the following exercises, subsets R of R
2 = R×R or Z
2 = Z×Z are indicated by
gray shading. In each case, R is a familiar relation on R or Z. State it.
12.
R
13.
R
14. 15.
Properties of Relations 179
11.1 Properties of Relations
A relational expression xR y is a statement (or an open sentence); it is either
true or false. For example, 5 < 10 is true, and 10 < 5 is false. (Thus an
operation like + is not a relation, because, for instance, 5+10 has a numeric
value, not a T/F value.) Since relational expressions have T/F values, we
can combine them with logical operators; for example, xR y ⇒ yRx is a
statement or open sentence whose truth or falsity may depend on x and y.
With this in mind, note that some relations have properties that others
don’t have. For example, the relation ≤ on Z satisfies x ≤ x for every x ∈ Z.
But this is not so for < because x < x is never true. The next definition
lays out three particularly significant properties that relations may have.
Definition 11.2 Suppose R is a relation on a set A.
1. Relation R is reflexive if xRx for every x ∈ A.
That is, R is reflexive if ∀x ∈ A, xRx.
2. Relation R is symmetric if xR y implies yRx for all x, y ∈ A
That is, R is symmetric if ∀x, y ∈ A, xR y ⇒ yRx.
3. Relation R is transitive if whenever xR y and yRz, then also xRz.
That is, R is transitive if ∀x, y, z ∈ A,
¡
(xR y)∧(yRz)
¢
⇒ xRz.
To illustrate this, let’s consider the set A = Z. Examples of reflexive
relations on Z include ≤, =, and |, because x ≤ x, x = x and x| x are all true
for any x ∈ Z. On the other hand, >, <, 6= and - are not reflexive for none
of the statements x < x, x > x, x 6= x and x - x is ever true.
The relation 6= is symmetric, for if x 6= y, then surely y 6= x also. Also,
the relation = is symmetric because x = y always implies y = x.
The relation ≤ is not symmetric, as x ≤ y does not necessarily imply
y ≤ x. For instance 5 ≤ 6 is true, but 6 ≤ 5 is false. Notice (x ≤ y) ⇒ (y ≤ x)
is true for some x and y (for example, it is true when x = 2 and y = 2), but
still ≤ is not symmetric because it is not the case that (x ≤ y) ⇒ (y ≤ x) is
true for all integers x and y.
The relation ≤ is transitive because whenever x ≤ y and y ≤ z, it also
is true that x ≤ z. Likewise <,≥,> and = are all transitive. Examine the
following table and be sure you understand why it is labeled as it is.
Relations on Z: < ≤ = | - 6=
Reflexive no yes yes yes no no
Symmetric no no yes no no yes
Transitive yes yes yes yes no no
180 Relations
Example 11.7 Here A =
©
b, c,d, e
ª
, and R is the following relation on A:
R =
©
(b,b),(b, c),(c,b),(c, c),(d,d),(b,d),(d,b),(c,d),(d, c)
ª
.
This relation is not reflexive, for although bRb, cRc and dRd, it is not
true that eRe. For a relation to be reflexive, xRx must be true for all x ∈ A.
The relation R is symmetric, because whenever we have xR y, it follows
that yRx too. Observe that bRc and cRb; bRd and dRb; dRc and cRd.
Take away the ordered pair (c,b) from R, and R is no longer symmetric.
The relation R is transitive, but it takes some work to check it. We
must check that the statement (xR y∧ yRz) ⇒ xRz is true for all x, y, z ∈ A.
For example, taking x = b, y = c and z = d, we have (bRc ∧ cRd) ⇒ bRd,
which is the true statement (T ∧ T) ⇒ T. Likewise, (bRd ∧ dRc) ⇒ bRc is
the true statement (T ∧ T) ⇒ T. Take note that if x = b, y = e and z = c,
then (bRe ∧ eRc) ⇒ bRc becomes (F ∧ F) ⇒ T, which is still true. It’s not
much fun, but going through all the combinations, you can verify that
(xR y∧ yRz) ⇒ xRz is true for all choices x, y, z ∈ A. (Try at least a few of
them.)
The relation R from Example 11.7 has a meaning. You can think of
xR y as meaning that x and y are both consonants. Thus bRc because b
and c are both consonants; but bRe 6 because it’s not true that b and e are
both consonants. Once we look at it this way, it’s immediately clear that R
has to be transitive. If x and y are both consonants and y and z are both
consonants, then surely x and z are both consonants. This illustrates a
point that we will see again later in this section: Knowing the meaning of
a relation can help us understand it and prove things about it.
Here is a picture of R. Notice that we can immediately spot several
properties of R that may not have been so clear from its set description.
For instance, we see that R is not reflexive because it lacks a loop at e,
hence eRe 6 .
b
c
d
e
Figure 11.1. The relation R from Example 11.7
Properties of Relations 181
In what follows, we summarize how to spot the various properties of a
relation from its diagram. Compare these with Figure 11.1.
1.
A relation is
reflexive if
for each point x ...
x
...there is a
loop at x:
x
2.
A relation is
symmetric if
whenever there is an
arrow from x to y ...
x y
...there is also
an arrow from
y back to x:
x y
3.
A relation is
transitive if
whenever there are
arrows from x to y
and y to z ...
x
y
z
...there is also
an arrow from
x to z:
x
y
z
(If x = z, this means
that if there are
arrows from x to y
and from y to x ... x
y ...there is also
a loop from
x back to x.)
x
y
Consider the bottom diagram in Box 3, above. The transitive property
demands (xR y∧ yRx) ⇒ xRx. Thus, if xR y and yRx in a transitive relation,
then also xRx, so there is a loop at x. In this case (yRx ∧ xR y) ⇒ yR y, so
there will be a loop at y too.
Although these visual aids can be illuminating, their use is limited because
many relations are too large and complex to be adequately described
as diagrams. For example, it would be impossible to draw a diagram
for the relation ≡ (mod n), where n ∈ N. Such a relation would best be
explained in a more theoretical (and less visual) way.
We next prove that ≡ (mod n) is reflexive, symmetric and transitive.
Obviously we will not glean this from a drawing. Instead we will prove it
from the properties of ≡ (mod n) and Definition 11.2. Pay attention to this
example. It illustrates how to prove things about relations.
182 Relations
Example 11.8 Prove the following proposition.
Proposition Let n ∈ N. The relation ≡ (mod n) on the set Z is reflexive,
symmetric and transitive.
Proof. First we will show that ≡ (mod n) is reflexive. Take any integer x ∈ Z,
and observe that n|0, so n | (x− x). By definition of congruence modulo n,
we have x ≡ x (mod n). This shows x ≡ x (mod n) for every x ∈ Z, so ≡ (mod n)
is reflexive.
Next, we will show that ≡ (mod n) is symmetric. For this, we must show
that for all x, y ∈ Z, the condition x ≡ y (mod n) implies that y ≡ x (mod n).
We use direct proof. Suppose x ≡ y (mod n). Thus n | (x − y) by definition
of congruence modulo n. Then x − y = na for some a ∈ Z by definition of
divisibility. Multiplying both sides by −1 gives y − x = n(−a). Therefore
n | (y− x), and this means y ≡ x (mod n). We’ve shown that x ≡ y (mod n)
implies that y ≡ x (mod n), and this means ≡ (mod n) is symmetric.
Finally we will show that ≡ (mod n) is transitive. For this we must
show that if x ≡ y (mod n) and y ≡ z (mod n), then x ≡ z (mod n). Again
we use direct proof. Suppose x ≡ y (mod n) and y ≡ z (mod n). This
means n | (x − y) and n | (y − z). Therefore there are integers a and b for
which x − y = na and y − z = nb. Adding these two equations, we obtain
x−z = na+nb. Consequently, x−z = n(a+b), so n | (x−z), hence x ≡ z (mod n).
This completes the proof that ≡ (mod n) is transitive.
The past three paragraphs have shown that ≡ (mod n) is reflexive,
symmetric and transitive, so the proof is complete. ■
As you continue with mathematics you will find that the reflexive,
symmetric and transitive properties take on special significance in a
variety of settings. In preparation for this, the next section explores
further consequences of these properties. But first work some of the
following exercises.
Exercises for Section 11.1
1. Consider the relation R =
©
(a,a),(b,b),(c, c),(d,d),(a,b),(b,a)
ª
on set A =
©
a,b, c,d
ª
.
Is R reflexive? Symmetric? Transitive? If a property does not hold, say why.
2. Consider the relation R =
©
(a,b),(a, c),(c, c),(b,b),(c,b),(b, c)
ª
on the set A =
©
a,b, c
ª
.
Is R reflexive? Symmetric? Transitive? If a property does not hold, say why.
3. Consider the relation R =
©
(a,b),(a, c),(c,b),(b, c)
ª
on the set A =
©
a,b, c
ª
. Is R
reflexive? Symmetric? Transitive? If a property does not hold, say why.
Properties of Relations 183
4. Let A =
©
a,b, c,d
ª
. Suppose R is the relation
R =
©
(a,a),(b,b),(c, c),(d,d),(a,b),(b,a),(a, c),(c,a),
(a,d),(d,a),(b, c),(c,b),(b,d),(d,b),(c,d),(d, c)
ª
.
Is R reflexive? Symmetric? Transitive? If a property does not hold, say why.
5. Consider the relation R =
©
(0,0),(
p
2,0),(0,
p
2),(
p
2,
p
2)ª
on R. Is R reflexive?
Symmetric? Transitive? If a property does not hold, say why.
6. Consider the relation R =
©
(x, x) : x ∈ Z
ª
on Z. Is R reflexive? Symmetric?
Transitive? If a property does not hold, say why. What familiar relation is
this?
7. There are 16 possible different relations R on the set A =
©
a,b
ª
. Describe all of
them. (A picture for each one will suffice, but don’t forget to label the nodes.)
Which ones are reflexive? Symmetric? Transitive?
8. Define a relation on Z as xR y if |x−y| < 1. Is R reflexive? Symmetric? Transitive?
If a property does not hold, say why. What familiar relation is this?
9. Define a relation on Z by declaring xR y if and only if x and y have the same
parity. Is R reflexive? Symmetric? Transitive? If a property does not hold, say
why. What familiar relation is this?
10. Suppose A 6= ;. Since ; ⊆ A × A, the set R = ; is a relation on A. Is R reflexive?
Symmetric? Transitive? If a property does not hold, say why.
11. Suppose A =
©
a,b, c,d
ª
and R =
©
(a,a),(b,b),(c, c),(d,d)
ª
. Is R reflexive? Symmetric?
Transitive? If a property does not hold, say why.
12. Prove that the relation | (divides) on the set Z is reflexive and transitive. (Use
Example 11.8 as a guide if you are unsure of how to proceed.)
13. Consider the relation R =
©
(x, y) ∈ R×R : x− y ∈ Z
ª
on R. Prove that this relation
is reflexive, symmetric and transitive.
14. Suppose R is a symmetric and transitive relation on a set A, and there is an
element a ∈ A for which aRx for every x ∈ A. Prove that R is reflexive.
15. Prove or disprove: If a relation is symmetric and transitive, then it is also
reflexive.
16. Define a relation R on Z by declaring that xR y if and only if x
2 ≡ y
2
(mod 4).
Prove that R is reflexive, symmetric and transitive.
17. Modifying the above Exercise 8 (above) slightly, define a relation ∼ on Z as x ∼ y
if and only if |x− y| ≤ 1. Say whether ∼ is reflexive. Is it symmetric? Transitive?
18. The table on page 179 shows that relations on Z may obey various combinations
of the reflexive, symmetric and transitive properties. In all, there are 2
3 =
8 possible combinations, and the table shows 5 of them. (There is some
redundancy, as ≤ and | have the same type.) Complete the table by finding
examples of relations on Z for the three missing combinations.
184 Relations
11.2 Equivalence Relations
The relation = on the set Z (or on any set A) is reflexive, symmetric
and transitive. There are many other relations that are also reflexive,
symmetric and transitive. Relations that have all three of these properties
occur very frequently in mathematics and often play quite significant roles.
(For instance, this is certainly true of the relation =.) Such relations are
given a special name. They are called equivalence relations.
Definition 11.3 A relation R on a set A is an equivalence relation if
it is reflexive, symmetric and transitive.
As an example, Figure 11.2 shows four different equivalence relations
R1,R2,R3 and R4 on the set A =
©
−1,1,2,3,4
ª
. Each one has its own meaning,
as labeled. For example, in the second row the relation R2 literally means
“has the same parity as.” So 1R2 3 means “1 has the same parity as 3,” etc.
Relation R Diagram Equivalence classes
(see next page)
“is equal to” (=)
R1 =
©
(−1,−1),(1,1),(2,2),(3,3),(4,4)ª
−1
3
1
4
2 ©
−1
ª
,
©
1
ª
,
©
2
ª
,
©
3
ª
,
©
4
ª
“has same parity as”
R2 =
©
(−1,−1),(1,1),(2,2),(3,3),(4,4),
(−1,1),(1,−1),(−1,3),(3,−1),
(1,3),(3,1),(2,4),(4,2)ª
−1
3
1
4
2 ©
−1,1,3
ª
,
©
2,4
ª
“has same sign as”
R3 =
©
(−1,−1),(1,1),(2,2),(3,3),(4,4),
(1,2),(2,1),(1,3),(3,1),(1,4),(4,1),(3,4),
(4,3),(2,3),(3,2),(2,4),(4,2),(1,3),(3,1)ª
−1
3
1
4
2
©
−1
ª
,
©
1,2,3,4
ª
“has same parity and sign as”
R4 =
©
(−1,−1),(1,1),(2,2),(3,3),(4,4),
(1,3),(3,1),(2,4),(4,2)ª
−1
3
1
4
2 ©
−1
ª
,
©
1,3
ª
,
©
2,4
ª
Figure 11.2. Examples of equivalence relations on the set A =
©
−1,1,2,3,4
ª
Equivalence Relations 185
The above diagrams make it easy to check that each relation is reflexive,
symmetric and transitive, i.e., that each is an equivalence relation. For
example, R1 is symmetric because xR1 y ⇒ yR1x is always true: When x = y
it becomes T ⇒ T (true), and when x 6= y it becomes F ⇒ F (also true). In
a similar fashion, R1 is transitive because (xR1 y∧ yR1z) ⇒ xR1z is always
true: It always works out to one of T ⇒ T, F ⇒ T or F ⇒ F. (Check this.)
As you can see from the examples in Figure 11.2, equivalence relations
on a set tend to express some measure of “sameness” among the elements
of the set, whether it is true equality or something weaker (like having
the same parity).
It’s time to introduce an important definition. Whenever you have
an equivalence relation R on a set A, it divides A into subsets called
equivalence classes. Here is the definition:
Definition 11.4 Suppose R is an equivalence relation on a set A. Given
any element a ∈ A, the equivalence class containing a is the subset
©
x ∈ A : xRaª
of A consisting of all the elements of A that relate to a. This
set is denoted as [a]. Thus the equivalence class containing a is the set
[a] =
©
x ∈ A : xRaª
.
Example 11.9 Consider the relation R1 in Figure 11.2. The equivalence
class containing 2 is the set [2] =
©
x ∈ A : xR12
ª
. Because in this relation
the only element that relates to 2 is 2 itself, we have [2] =
©
2
ª
. Other
equivalence classes for R1 are [−1] =
©
−1
ª
, [1] =
©
1
ª
, [3] =
©
3
ª
and [4] =
©
4
ª
.
Thus this relation has five separate equivalence classes.
Example 11.10 Consider the relation R2 in Figure 11.2. The equivalence
class containing 2 is the set [2] =
©
x ∈ A : xR22
ª
. Because only 2 and 4 relate
to 2, we have [2] =
©
2,4
ª
. Observe that we also have [4] =
©
x ∈ A : xR24
ª
=
©
2,4
ª
, so [2] = [4]. Another equivalence class for R2 is [1] =
©
x ∈ A : xR21
ª
=
©
− 1,1,3
ª
. In addition, note that [1] = [−1] = [3] =
©
− 1,1,3
ª
. Thus this
relation has just two equivalence classes, namely ©
2,4
ª
and ©
−1,1,3
ª
.
Example 11.11 The relation R4 in Figure 11.2 has three equivalence
classes. They are [−1] =
©
−1
ª
and [1] = [3] =
©
1,3
ª
and [2] = [4] =
©
2,4
ª
.
Don’t be misled by Figure 11.2. It’s important to realize that not
every equivalence relation can be drawn as a diagram involving nodes
and arrows. Even the simple relation R =
©
(x, x) : x ∈ R
ª
, which expresses
equality in the set R, is too big to be drawn. Its picture would involve a
point for every real number and a loop at each point. Clearly that’s too
many points and loops to draw.
186 Relations
We close this section with several other examples of equivalence relations
on infinite sets.
Example 11.12 Let P be the set of all polynomials with real coefficients.
Define a relation R on P as follows. Given f (x), g(x) ∈ P, let f (x)R g(x) mean
that f (x) and g(x) have the same degree. Thus (x
2 +3x−4)R (3x
2 −2) and
(x
3 +3x
2 −4) R6 (3x
2 −2), for example. It takes just a quick mental check to
see that R is an equivalence relation. (Do it.) It’s easy to describe the
equivalence classes of R. For example, [3x
2 +2] is the set of all polynomials
that have the same degree as 3x
2 +2, that is, the set of all polynomials of
degree 2. We can write this as [3x
2 +2] =
©
ax2 + bx + c : a,b, c ∈ R,a 6= 0
ª
.
Example 11.8 proved that for a given n ∈ N the relation ≡ (mod n) is
reflexive, symmetric and transitive. Thus, in our new parlance, ≡ (mod n)
is an equivalence relation on Z. Consider the case n = 3. Let’s find the
equivalence classes of the equivalence relation ≡ (mod 3). The equivalence
class containing 0 seems like a reasonable place to start. Observe that
[0] =
©
x ∈ Z : x ≡ 0 (mod 3)ª
=
©
x ∈ Z : 3 | (x−0)ª
=
©
x ∈ Z : 3 | x
ª
=
©
...,−3,0,3,6,9,...ª
.
Thus the class [0] consists of all the multiples of 3. (Or, said differently,
[0] consists of all integers that have a remainder of 0 when divided by 3).
Note that [0] = [3] = [6] = [9], etc. The number 1 does not show up in the
set [0] so let’s next look at the equivalence class [1]:
[1] =
©
x ∈ Z : x ≡ 1 (mod 3)ª
=
©
x ∈ Z : 3 | (x−1)ª
=
©
...,−5,−2,1,4,7,10,...ª
.
The equivalence class [1] consists of all integers that give a remainder of
1 when divided by 3. The number 2 is in neither of the sets [0] or [1], so
we next look at the equivalence class [2]:
[2] =
©
x ∈ Z : x ≡ 2 (mod 3)ª
=
©
x ∈ Z : 3 | (x−2)ª
=
©
...,−4,−1,2,5,8,11,...ª
.
The equivalence class [2] consists of all integers that give a remainder of
2 when divided by 3. Observe that any integer is in one of the sets [0], [1]
or [2], so we have listed all of the equivalence classes. Thus ≡ (mod 3) has
exactly three equivalence classes, as described above.
Similarly, you can show that the equivalence relation ≡ (mod n) has n
equivalence classes [0],[1],[2],..., [n−1].
Equivalence Relations 187
Exercises for Section 11.2
1. Let A =
©
1,2,3,4,5,6
ª
, and consider the following equivalence relation on A:
R =
©
(1,1),(2,2),(3,3),(4,4),(5,5),(6,6),(2,3),(3,2),(4,5),(5,4),(4,6),(6,4),(5,6),(6,5)ª
List the equivalence classes of R.
2. Let A =
©
a,b, c,d, e
ª
. Suppose R is an equivalence relation on A. Suppose R has
two equivalence classes. Also aRd, bRc and eRd. Write out R as a set.
3. Let A =
©
a,b, c,d, e
ª
. Suppose R is an equivalence relation on A. Suppose R has
three equivalence classes. Also aRd and bRc. Write out R as a set.
4. Let A =
©
a,b, c,d, e
ª
. Suppose R is an equivalence relation on A. Suppose also
that aRd and bRc, eRa and cRe. How many equivalence classes does R have?
5. There are two different equivalence relations on the set A =
©
a,b
ª
. Describe
them. Diagrams will suffice.
6. There are five different equivalence relations on the set A =
©
a,b, c
ª
. Describe
them all. Diagrams will suffice.
7. Define a relation R on Z as xR y if and only if 3x −5y is even. Prove R is an
equivalence relation. Describe its equivalence classes.
8. Define a relation R on Z as xR y if and only if x
2 + y
2
is even. Prove R is an
equivalence relation. Describe its equivalence classes.
9. Define a relation R on Z as xR y if and only if 4|(x+3y). Prove R is an equivalence
relation. Describe its equivalence classes.
10. Suppose R and S are two equivalence relations on a set A. Prove that R ∩ S
is also an equivalence relation. (For an example of this, look at Figure 11.2.
Observe that for the equivalence relations R2,R3 and R4, we have R2 ∩R3 = R4.)
11. Prove or disprove: If R is an equivalence relation on an infinite set A, then R
has infinitely many equivalence classes.
12. Prove or disprove: If R and S are two equivalence relations on a set A, then
R ∪ S is also an equivalence relation on A.
13. Suppose R is an equivalence relation on a finite set A, and every equivalence
class has the same cardinality m. Express |R| in terms of m and |A|.
14. Suppose R is a reflexive and symmetric relation on a finite set A. Define
a relation S on A by declaring xS y if and only if for some n ∈ N there are
elements x1, x2,..., xn ∈ A satisfying xRx1, x1Rx2, x2Rx3, x3Rx4,..., xn−1Rxn, and
xnR y. Show that S is an equivalence relation and R ⊆ S. Prove that S is the
unique smallest equivalence relation on A containing R.
15. Suppose R is an equivalence relation on a set A, with four equivalence classes.
How many different equivalence relations S on A are there for which R ⊆ S?
188 Relations
11.3 Equivalence Classes and Partitions
This section collects several properties of equivalence classes.
Our first result proves that [a] = [b] if and only if aRb. This is useful
because it assures us that whenever we are in a situation where [a] = [b],
we also have aRb, and vice versa. Being able to switch back and forth
between these two pieces of information can be helpful in a variety of
situations, and you may find yourself using this result a lot. Be sure to
notice that the proof uses all three properties (reflexive, symmetric and
transitive) of equivalence relations. Notice also that we have to use some
Chapter 8 techniques in dealing with the sets [a] and [b].
Theorem 11.1 Suppose R is an equivalence relation on a set A. Suppose
also that a,b ∈ A. Then [a] = [b] if and only if aRb.
Proof. Suppose [a] = [b]. Note that aRa by the reflexive property of R, so
a ∈
©
x ∈ A : xRaª
= [a] = [b] =
©
x ∈ A : xRbª
. But a belonging to ©
x ∈ A : xRbª
means aRb. This completes the first part of the if-and-only-if proof.
Conversely, suppose aRb. We need to show [a] = [b]. We will do this by
showing [a] ⊆ [b] and [b] ⊆ [a].
First we show [a] ⊆ [b]. Suppose c ∈ [a]. As c ∈ [a] =
©
x ∈ A : xRaª
, we get
cRa. Now we have cRa and aRb, so cRb because R is transitive. But cRb
implies c ∈
©
x ∈ A : xRbª
= [b]. This demonstrates that c ∈ [a] implies c ∈ [b],
so [a] ⊆ [b].
Next we show [b] ⊆ [a]. Suppose c ∈ [b]. As c ∈ [b] =
©
x ∈ A : xRbª
, we get
cRb. Remember that we are assuming aRb, so bRa because R is symmetric.
Now we have cRb and bRa, so cRa because R is transitive. But cRa implies
c ∈
©
x ∈ A : xRaª
= [a]. This demonstrates that c ∈ [b] implies c ∈ [a]; hence
[b] ⊆ [a].
The previous two paragraphs imply that [a] = [b]. ■
To illustrate Theorem 11.1, recall how we worked out the equivalence
classes of ≡ (mod 3) at the end of Section 11.2. We observed that
[−3] = [9] =
©
...,−3,0,3,6,9,...ª
.
Note that [−3] = [9] and −3 ≡ 9 (mod 3), just as Theorem 11.1 predicts. The
theorem assures us that this will work for any equivalence relation. In the
future you may find yourself using the result of Theorem 11.1 often. Over
time it may become natural and familiar; you will use it automatically,
without even thinking of it as a theorem.
Equivalence Classes and Partitions 189
Our next topic addresses the fact that an equivalence relation on a set
A divides A into various equivalence classes. There is a special word for
this kind of situation. We address it now, as you are likely to encounter it
in subsequent mathematics classes.
Definition 11.5 A partition of a set A is a set of non-empty subsets of
A, such that the union of all the subsets equals A, and the intersection of
any two different subsets is ;.
Example 11.13 Let A =
©
a,b, c,d
ª
. One partition of A is ©©a,b
ª
,
©
c
ª
,
©
d
ªª.
This is a set of three subsets ©
a,b
ª
,
©
c
ª
and ©
d
ª
of A. The union of the
three subsets equals A; the intersection of any two subsets is ;.
Other partitions of A are
©©a,b
ª
,
©
c,d
ªª,
©©a, c
ª
,
©
b
ª
,
©
d
ªª,
©©a
ª
,
©
b
ª
,
©
c
ª©d
ªª,
©©a,b, c,d
ªª,
to name a few. Intuitively, a partition is just a dividing up of A into pieces.
Example 11.14 Consider the equivalence relations in Figure 11.2. Each
of these is a relation on the set A =
©
−1,1,2,3,4
ª
. The equivalence classes
of each relation are listed on the right side of the figure. Observe that,
in each case, the set of equivalence classes forms a partition of A. For
example, the relation R1 yields the partition ©©−1
ª
,
©
1
ª
,
©
2
ª
,
©
3
ª
,
©
4
ªª of A.
Likewise the equivalence classes of R2 form the partition ©©−1,1,3
ª
,
©
2,4
ªª.
Example 11.15 Recall that we worked out the equivalence classes of the
equivalence relation ≡ (mod 3) on the set Z. These equivalence classes
give the following partition of Z:
©©...,−3,0,3,6,9,...ª
,
©
...,−2,1,4,7,10,...ª
,
©
...,−1,2,5,8,11,...ªª.
We can write it more compactly as ©
[0],[1],[2]ª
.
Our examples and experience suggest that the equivalence classes of
an equivalence relation on a set form a partition of that set. This is indeed
the case, and we now prove it.
Theorem 11.2 Suppose R is an equivalence relation on a set A. Then
the set ©
[a] : a ∈ A
ª
of equivalence classes of R forms a partition of A.
Proof. To show that ©
[a] : a ∈ A
ª
is a partition of A we need to show two
things: We need to show that the union of all the sets [a] equals A, and
we need to show that if [a] 6= [b], then [a]∩[b] = ;.
190 Relations
Notationally, the union of all the sets [a] is S
a∈A[a], so we need to prove
S
a∈A[a] = A. Suppose x ∈
S
a∈A[a]. This means x ∈ [a] for some a ∈ A. Since
[a] ⊆ A, it then follows that x ∈ A. Thus S
a∈A[a] ⊆ A. On the other hand,
suppose x ∈ A. As x ∈ [x], we know x ∈ [a] for some a ∈ A (namely a = x).
Therefore x ∈
S
a∈A[a], and this shows A ⊆
S
a∈A[a]. Since S
a∈A[a] ⊆ A and
A ⊆
S
a∈A[a], it follows that S
a∈A[a] = A.
Next we need to show that if [a] 6= [b] then [a] ∩ [b] = ;. Let’s use
contrapositive proof. Suppose it’s not the case that [a]∩[b] = ;, so there is
some element c with c ∈ [a]∩[b]. Thus c ∈ [a] and c ∈ [b]. Now, c ∈ [a] means
cRa, and then aRc since R is symmetric. Also c ∈ [b] means cRb. Now
we have aRc and cRb, so aRb (because R is transitive). By Theorem 11.1,
aRb implies [a] = [b]. Thus [a] 6= [b] is not true.
We’ve now shown that the union of all the equivalence classes is A,
and the intersection of two different equivalence classes is ;. Therefore
the set of equivalence classes is a partition of A. ■
Theorem 11.2 says the equivalence classes of any equivalence relation
on a set A form a partition of A. Conversely, any partition of A describes
an equivalence relation R where xR y if and only if x and y belong to the
same set in the partition. (See Exercise 4 for this section, below.) Thus
equivalence relations and partitions are really just two different ways of
looking at the same thing. In your future mathematical studies, you may
find yourself easily switching between these two points of view.
Exercises for Section 11.3
1. List all the partitions of the set A =
©
a,b
ª
. Compare your answer to the answer
to Exercise 5 of Section 11.2.
2. List all the partitions of the set A =
©
a,b, c
ª
. Compare your answer to the
answer to Exercise 6 of Section 11.2.
3. Describe the partition of Z resulting from the equivalence relation ≡ (mod 4).
4. Suppose P is a partition of a set A. Define a relation R on A by declaring xR y
if and only if x, y ∈ X for some X ∈ P. Prove R is an equivalence relation on A.
Then prove that P is the set of equivalence classes of R.
5. Consider the partition P =
©©...,−4,−2,0,2,4,...ª
,
©
...,−5,−3,−1,1,3,5,...ªª of Z.
Let R be the equivalence relation whose equivalence classes are the two elements
of P. What familiar equivalence relation is R?
The Integers Modulo n 191
11.4 The Integers Modulo n
Example 11.8 proved that for a given n ∈ N, the relation ≡ (mod n) is
reflexive, symmetric and transitive, so it is an equivalence relation. This
is a particularly significant equivalence relation in mathematics, and in
the present section we deduce some of its properties.
To make matters simpler, let’s pick a concrete n, say n = 5. Let’s begin
by looking at the equivalence classes of the relation ≡ (mod 5). There are
five equivalence classes, as follows:
[0] =
©
x ∈ Z : x ≡ 0 (mod 5))
ª
=
©
x ∈ Z : 5 | (x−0)ª
=
©
...,−10,−5,0,5,10,15,...ª
,
[1] =
©
x ∈ Z : x ≡ 1 (mod 5))
ª
=
©
x ∈ Z : 5 | (x−1)ª
=
©
..., −9,−4,1,6,11,16,...ª
,
[2] =
©
x ∈ Z : x ≡ 2 (mod 5))
ª
=
©
x ∈ Z : 5 | (x−2)ª
=
©
..., −8,−3,2,7,12,17,...ª
,
[3] =
©
x ∈ Z : x ≡ 3 (mod 5))
ª
=
©
x ∈ Z : 5 | (x−3)ª
=
©
..., −7,−2,3,8,13,18,...ª
,
[4] =
©
x ∈ Z : x ≡ 4 (mod 5))
ª
=
©
x ∈ Z : 5 | (x−4)ª
=
©
..., −6,−1,4,9,14,19,...ª
.
Notice how these equivalence classes form a partition of the set Z.
We label the five equivalence classes as [0],[1],[2],[3] and [4], but you
know of course that there are other ways to label them. For example,
[0] = [5] = [10] = [15], and so on; and [1] = [6] = [−4], etc. Still, for this
discussion we denote the five classes as [0],[1],[2],[3] and [4].
These five classes form a set, which we shall denote as Z5. Thus
Z5 =
©
[0],[1],[2],[3],[4]ª
is a set of five sets. The interesting thing about Z5 is that even though its
elements are sets (and not numbers), it is possible to add and multiply
them. In fact, we can define the following rules that tell how elements of
Z5 can be added and multiplied.
[a]+[b] = [a+ b]
[a]·[b] = [a· b]
For example, [2] + [1] = [2 + 1] = [3], and [2]·[2] = [2 · 2] = [4]. We stress
that in doing this we are adding and multiplying sets (more precisely
equivalence classes), not numbers. We added (or multiplied) two elements
of Z5 and obtained another element of Z5.
Here is a trickier example. Observe that [2] + [3] = [5]. This time we
added elements [2],[3] ∈ Z5, and got the element [5] ∈ Z5. That was easy,
except where is our answer [5] in the set Z5 =
©
[0],[1],[2],[3],[4]ª
? Since
[5] = [0], it is more appropriate to write [2]+[3] = [0].
192 Relations
In a similar vein, [2]·[3] = [6] would be written as [2]·[3] = [1] because
[6] = [1]. Test your skill with this by verifying the following addition and
multiplication tables for Z5.
+ [0] [1] [2] [3] [4]
[0] [0] [1] [2] [3] [4]
[1] [1] [2] [3] [4] [0]
[2] [2] [3] [4] [0] [1]
[3] [3] [4] [0] [1] [2]
[4] [4] [0] [1] [2] [3]
· [0] [1] [2] [3] [4]
[0] [0] [0] [0] [0] [0]
[1] [0] [1] [2] [3] [4]
[2] [0] [2] [4] [1] [3]
[3] [0] [3] [1] [4] [2]
[4] [0] [4] [3] [2] [1]
We call the set Z5 =
©
[0],[1],[2],[3],[4]ª
the integers modulo 5. As our
tables suggest, Z5 is more than just a set: It is a little number system
with its own addition and multiplication. In this way it is like the familiar
set Z which also comes equipped with an addition and a multiplication.
Of course, there is nothing special about the number 5. We can also
define Zn for any natural number n. Here is the definition:
Definition 11.6 Let n ∈ N. The equivalence classes of the equivalence
relation ≡ (mod n) are [0],[1],[2],...,[n−1]. The integers modulo n is the
set Zn =
©
[0],[1],[2],...,[n − 1]ª
. Elements of Zn can be added by the rule
[a]+[b] = [a+ b] and multiplied by the rule [a]·[b] = [ab].
Given a natural number n, the set Zn is a number system containing n
elements. It has many of the algebraic properties that Z,R and Q possess.
For example, it is probably obvious to you already that elements of Zn obey
the commutative laws [a]+[b] = [b]+[a] and [a]·[b] = [b]·[a]. You can also
verify the distributive law [a]·([b]+[c]) = [a]·[b]+[a]·[c], as follows:
[a]·([b]+[c]) = [a]·[b + c]
= [a(b + c)]
= [ab + ac]
= [ab]+[ac]
= [a]·[b]+[a]·[c].
The integers modulo n are significant because they more closely fit certain
applications than do other number systems such as Z or R. If you go on to
The Integers Modulo n 193
take a course in abstract algebra, then you will work extensively with Zn
as well as other, more exotic, number systems. (In such a course you will
also use all of the proof techniques that we have discussed, as well as the
ideas of equivalence relations.)
To close this section we take up an issue that may have bothered
you earlier. It has to do with our definitions of addition [a]+[b] = [a + b]
and multiplication [a]·[b] = [ab]. These definitions define addition and
multiplication of equivalence classes in terms of representatives a and b
in the equivalence classes. Since there are many different ways to choose
such representatives, we may well wonder if addition and multiplication
are consistently defined. For example, suppose two people, Alice and Bob,
want to multiply the elements [2] and [3] in Z5. Alice does the calculation
as [2] · [3] = [6] = [1], so her final answer is [1]. Bob does it differently.
Since [2] = [7] and [3] = [8], he works out [2]·[3] as [7]·[8] = [56]. Since 56 ≡ 1
(mod 5), Bob’s answer is [56] = [1], and that agrees with Alice’s answer. Will
their answers always agree or did they just get lucky (with the arithmetic)?
The fact is that no matter how they do the multiplication in Zn, their
answers will agree. To see why, suppose Alice and Bob want to multiply
the elements [a],[b] ∈ Zn, and suppose [a] = [a
0
] and [b] = [b
0
]. Alice and Bob
do the multiplication as follows:
Alice: [a]·[b] = [ab],
Bob: [a
0
]·[b
0
] = [a
0b
0
].
We need to show that their answers agree, that is, we need to show
[ab] = [a
0b
0
]. Since [a] = [a
0
], we know by Theorem 11.1 that a ≡ a
0
(mod n).
Thus n | (a − a
0
), so a− a
0 = nk for some integer k. Likewise, as [b] = [b
0
], we
know b ≡ b
0
(mod n), or n | (b − b
0
), so b − b
0 = n` for some integer `. Thus
we get a = a
0 + nk and b = b
0 + n`. Therefore:
ab = (a
0 + nk)(b
0 + n`)
= a
0
b
0 + a
0n`+ nkb0 + n
2
k`,
hence ab − a
0
b
0 = n(a
0
`+ kb0 + nk`).
This shows n | (ab − a
0b
0
), so ab ≡ a
0b
0
(mod n), and from that we conclude
[ab] = [a
0b
0
]. Consequently Alice and Bob really do get the same answer, so
we can be assured that the definition of multiplication in Zn is consistent.
Exercise 8 below asks you to show that addition in Zn is similarly
consistent.
194 Relations
Exercises for Section 11.4
1. Write the addition and multiplication tables for Z2.
2. Write the addition and multiplication tables for Z3.
3. Write the addition and multiplication tables for Z4.
4. Write the addition and multiplication tables for Z6.
5. Suppose [a],[b] ∈ Z5 and [a]·[b] = [0]. Is it necessarily true that either [a] = [0]
or [b] = [0]?
6. Suppose [a],[b] ∈ Z6 and [a]·[b] = [0]. Is it necessarily true that either [a] = [0]
or [b] = [0]?
7. Do the following calculations in Z9, in each case expressing your answer as [a]
with 0 ≤ a ≤ 8.
(a) [8]+[8] (b) [24]+[11] (c) [21]·[15] (d) [8]·[8]
8. Suppose [a],[b] ∈ Zn, and [a] = [a
0
] and [b] = [b
0
]. Alice adds [a] and [b] as
[a]+[b] = [a+ b]. Bob adds them as [a
0
]+[b
0
] = [a
0 + b
0
]. Show that their answers
[a+ b] and [a
0 + b
0
] are the same.
11.5 Relations Between Sets
In the beginning of this chapter, we defined a relation on a set A to
be a subset R ⊆ A × A. This created a framework that could model any
situation in which elements of A are compared to themselves. In this
setting, the statement xR y has elements x and y from A on either side
of the R because R compares elements from A. But there are other
relational symbols that don’t work this way. Consider ∈. The statement
5 ∈ Z expresses a relationship between 5 and Z (namely that the element 5
is in the set Z) but 5 and Z are not in any way naturally regarded as both
elements of some set A. To overcome this difficulty, we generalize the idea
of a relation on A to a relation from A to B.
Definition 11.7 A relation from a set A to a set B is a subset R ⊆ A ×B.
We often abbreviate the statement (x, y) ∈ R as xR y. The statement (x, y) ∉ R
is abbreviated as xR y 6 .
Example 11.16 Suppose A =
©
1,2
ª
and B = P(A) =
©
;,
©
1
ª
,
©
2
ª
,
©
1,2
ªª. Then
R =
©
(1,
©
1
ª
),(2,
©
2
ª
),(1,
©
1,2
ª
),(2,
©
1,2
ª
)
ª
⊆ A×B is a relation from A to B. Note
that we have 1R
©
1
ª
, 2R
©
2
ª
, 1R
©
1,2
ª
and 2R
©
1,2
ª
. The relation R is the
familiar relation ∈ for the set A, that is, x R X means exactly the same
thing as x ∈ X.
Relations Between Sets 195
Diagrams for relations from A to B differ from diagrams for relations
on A. Since there are two sets A and B in a relation from A to B, we have
to draw labeled nodes for each of the two sets. Then we draw arrows from x
to y whenever xR y. The following figure illustrates this for Example 11.16.
;
©
1
ª
©
2
ª
©
1,2
ª
1
2
A B
Figure 11.3. A relation from A to B
The ideas from this chapter show that any relation (whether it is a
familiar one like ≥, ≤, =, |, ∈ or ⊆, or a more exotic one) is really just a
set. Therefore the theory of relations is a part of the theory of sets. In
the next chapter, we will see that this idea touches on another important
mathematical construction, namely functions. We will define a function to
be a special kind of relation from one set to another, and in this context
we will see that any function is really just a set.
CHAPTER 12
Functions
Y
ou know from calculus that functions play a fundamental role in mathematics.
You likely view a function as a kind of formula that describes
a relationship between two (or more) quantities. You certainly understand
and appreciate the fact that relationships between quantities are important
in all scientific disciplines, so you do not need to be convinced that
functions are important. Still, you may not be aware of the full significance
of functions. Functions are more than merely descriptions of numeric
relationships. In a more general sense, functions can compare and relate
different kinds of mathematical structures. You will see this as your
understanding of mathematics deepens. In preparation of this deepening,
we will now explore a more general and versatile view of functions.
The concept of a relation between sets (Definition 11.7) plays a big role
here, so you may want to quickly review it.
12.1 Functions
Let’s start on familiar ground. Consider the function f (x) = x
2
from R to R.
Its graph is the set of points R =
©
(x, x
2
) : x ∈ R
ª
⊆ R×R.
R
R
(x, x
2
)
x
Figure 12.1. A familiar function
Having read Chapter 11, you may see f in a new light. Its graph
R ⊆ R × R is a relation on the set R. In fact, as we shall see, functions
are just special kinds of relations. Before stating the exact definition, we
Functions 197
look at another example. Consider the function f (n) = |n| +2 that converts
integers n into natural numbers |n| +2. Its graph is R =
©
(n,|n| +2) : n ∈ Z
ª
⊆ Z×N.
N
Z
−4 −3 −2 −1 0 1 2 3 4
1
2
3
4
5
6
Figure 12.2. The function f : Z → N, where f (n) = |n| +2
Figure 12.2 shows the graph R as darkened dots in the grid of points Z×N.
Notice that in this example R is not a relation on a single set. The set of
input values Z is different from the set N of output values, so the graph
R ⊆ Z×N is a relation from Z to N.
This example illustrates three things. First, a function can be viewed
as sending elements from one set A to another set B. (In the case of f ,
A = Z and B = N.) Second, such a function can be regarded as a relation
from A to B. Third, for every input value n, there is exactly one output
value f (n). In your high school algebra course, this was expressed by the
vertical line test: Any vertical line intersects a function’s graph at most
once. It means that for any input value x, the graph contains exactly one
point of form (x, f (x)). Our main definition, given below, incorporates all of
these ideas.
Definition 12.1 Suppose A and B are sets. A function f from A to B
(denoted as f : A → B) is a relation f ⊆ A × B from A to B, satisfying the
property that for each a ∈ A the relation f contains exactly one ordered
pair of form (a,b). The statement (a,b) ∈ f is abbreviated f (a) = b.
Example 12.1 Consider the function f graphed in Figure 12.2. According
to Definition 12.1, we regard f as the set of points in its graph, that is,
f =
©
(n,|n| +2) : n ∈ Z
ª
⊆ Z×N. This is a relation from Z to N, and indeed
given any a ∈ Z the set f contains exactly one ordered pair (a,|a|+2) whose
first coordinate is a. Since (1,3) ∈ f , we write f (1) = 3; and since (−3,5) ∈ f
we write f (−3) = 5, etc. In general, (a,b) ∈ f means that f sends the input
198 Functions
value a to the output value b, and we express this as f (a) = b. This function
can be expressed by a formula: For each input value n, the output value
is |n| +2, so we may write f (n) = |n| +2. All this agrees with the way we
thought of functions in algebra and calculus; the only difference is that
now we also think of a function as a relation.
Definition 12.2 For a function f : A → B, the set A is called the domain
of f . (Think of the domain as the set of possible “input values” for f .) The
set B is called the codomain of f . The range of f is the set ©
f (a) : a ∈ A
ª
=
©
b : (a,b) ∈ f
ª
. (Think of the range as the set of all possible “output values”
for f . Think of the codomain as a sort of “target” for the outputs.)
Continuing Example 12.1, the domain of f is Z and its codomain is
N. Its range is ©
f (a) : a ∈ Z
ª
=
©
|a| +2 : a ∈ Z
ª
=
©
2,3,4,5,...ª
. Notice that the
range is a subset of the codomain, but it does not (in this case) equal the
codomain.
In our examples so far, the domains and codomains are sets of numbers,
but this needn’t be the case in general, as the next example indicates.
Example 12.2 Let A =
©
p, q, r, s
ª
and B =
©
0,1,2
ª
, and
f =
©
(p,0),(q,1),(r,2),(s,2)ª
⊆ A ×B.
This is a function f : A → B because each element of A occurs exactly once
as a first coordinate of an ordered pair in f . We have f (p) = 0, f (q) = 1,
f (r) = 2 and f (s) = 2. The domain of f is ©
p, q, r, s
ª
, and the codomain and
range are both ©
0,1,2
ª
.
(p,0)
(p,1)
(p,2)
(q,0)
(q,1)
(q,2)
(r,0)
(r,1)
(r,2)
(s,0)
(s,1)
(s,2)
0
1
2
p q r s A
B
(a)
0
1
p 2
q
r
s
(b)
A B
Figure 12.3. Two ways of drawing the function f =
©
(p,0),(q,1),(r,2),(s,2)ª
Functions 199
If A and B are not both sets of numbers it can be difficult to draw
a graph of f : A → B in the traditional sense. Figure 12.3(a) shows an
attempt at a graph of f from Example 12.2. The sets A and B are aligned
roughly as x- and y-axes, and the Cartesian product A × B is filled in
accordingly. The subset f ⊆ A ×B is indicated with dashed lines, and this
can be regarded as a “graph” of f . A more natural visual description of f
is shown in 12.3(b). The sets A and B are drawn side-by-side, and arrows
point from a to b whenever f (a) = b.
In general, if f : A → B is the kind of function you may have encountered
in algebra or calculus, then conventional graphing techniques offer the
best visual description of it. On the other hand, if A and B are finite or if
we are thinking of them as generic sets, then describing f with arrows is
often a more appropriate way of visualizing it.
We emphasize that, according to Definition 12.1, a function is really
just a special kind of set. Any function f : A → B is a subset of A ×B. By
contrast, your calculus text probably defined a function as a certain kind of
“rule.” While that intuitive outlook is adequate for the first few semesters
of calculus, it does not hold up well to the rigorous mathematical standards
necessary for further progress. The problem is that words like “rule” are
too vague. Defining a function as a set removes the ambiguity. It makes a
function into a concrete mathematical object.
Still, in practice we tend to think of functions as rules. Given f : Z → N
where f (x) = |x| +2, we think of this as a rule that associates any number
n ∈ Z to the number |n| + 2 in N, rather than a set containing ordered
pairs (n,|n| +2). It is only when we have to understand or interpret the
theoretical nature of functions (as we do in this text) that Definition 12.1
comes to bear. The definition is a foundation that gives us license to think
about functions in a more informal way.
The next example brings up a point about notation. Consider a function
such as f : Z
2 → Z, whose domain is a Cartesian product. This function
takes as input an ordered pair (m,n) ∈ Z
2 and sends it to a number f ((m,n)) ∈
Z. To simplify the notation, it is common to write f (m,n) instead of f ((m,n)),
even though this is like writing f x instead of f (x). We also remark that
although we’ve been using the letters f , g and h to denote functions, any
other reasonable symbol could be used. Greek letters such as ϕ and θ are
common.
Example 12.3 Say a function ϕ : Z
2 → Z is defined as ϕ(m,n) = 6m −9n.
Note that as a set, this function is ϕ =
© ¡(m,n),6m−9n
¢
: (m,n) ∈ Z
2
ª
⊆ Z
2×Z.
What is the range of ϕ?
200 Functions
To answer this, first observe that for any (m,n) ∈ Z
2
, the value f (m,n) =
6m−9n = 3(2m−3n) is a multiple of 3. Thus every number in the range is
a multiple of 3, so the range is a subset of the set of all multiples of 3. On
the other hand if b = 3k is a multiple of 3 we have ϕ(−k,−k) = 6(−k)−9(−k) =
3k = b, which means any multiple of 3 is in the range of ϕ. Therefore the
range of ϕ is the set ©
3k : k ∈ Z
ª
of all multiples of 3.
To conclude this section, let’s use Definition 12.1 to help us understand
what it means for two functions f : A → B and g : C → D to be equal.
According to our definition, functions f and g are subsets f ⊆ A × B and
g ⊆ C ×D. It makes sense to say that f and g are equal if f = g, that is, if
they are equal as sets.
Thus the two functions f =
©
(1,a),(2,a),(3,b)
ª
and g =
©
(3,b),(2,a),(1,a)
ª
are equal because the sets f and g are equal. Notice that the domain of
both functions is A =
©
1,2,3
ª
, the set of first elements x in the ordered pairs
(x, y) ∈ f = g. In general, equal functions must have equal domains.
Observe also that the equality f = g means f (x) = g(x) for every x ∈ A.
We repackage these ideas in the following definition.
Definition 12.3 Two functions f : A → B and g : A → D are equal if
f (x) = g(x) for every x ∈ A.
Observe that f and g can have different codomains and still be equal.
Consider the functions f : Z → N and g : Z → Z defined as f (x) = |x| +2 and
g(x) = |x| +2. Even though their codomains are different, the functions are
equal because f (x) = g(x) for every x in the domain.
Exercises for Section 12.1
1. Suppose A =
©
0,1,2,3,4
ª
, B =
©
2,3,4,5
ª
and f =
©
(0,3),(1,3),(2,4),(3,2),(4,2)ª
. State
the domain and range of f . Find f (2) and f (1).
2. Suppose A =
©
a,b, c,d
ª
, B =
©
2,3,4,5,6
ª
and f =
©
(a,2),(b,3),(c,4),(d,5)ª
. State the
domain and range of f . Find f (b) and f (d).
3. There are four different functions f :
©
a,b
ª
→
©
0,1
ª
. List them all. Diagrams
will suffice.
4. There are eight different functions f :
©
a,b, c
ª
→
©
0,1
ª
. List them all. Diagrams
will suffice.
5. Give an example of a relation from ©
a,b, c,d
ª
to ©
d, e
ª
that is not a function.
6. Suppose f : Z → Z is defined as f =
©
(x,4x+5) : x ∈ Z
ª
. State the domain, codomain
and range of f . Find f (10).
Injective and Surjective Functions 201
7. Consider the set f =
©
(x, y) ∈ Z × Z : 3x + y = 4
ª
. Is this a function from Z to Z?
Explain.
8. Consider the set f =
©
(x, y) ∈ Z × Z : x + 3y = 4
ª
. Is this a function from Z to Z?
Explain.
9. Consider the set f =
©
(x
2
, x) : x ∈ R
ª
. Is this a function from R to R? Explain.
10. Consider the set f =
©
(x
3
, x) : x ∈ R
ª
. Is this a function from R to R? Explain.
11. Is the set θ =
©
(X,|X|) : X ⊆ Z5
ª
a function? If so, what is its domain and range?
12. Is the set θ =
©¡(x, y),(3y,2x, x+ y)
¢
: x, y ∈ R
ª
a function? If so, what is its domain,
codomain and range?
12.2 Injective and Surjective Functions
You may recall from algebra and calculus that a function may be oneto-one
and onto, and these properties are related to whether or not the
function is invertible. We now review these important ideas. In advanced
mathematics, the word injective is often used instead of one-to-one, and
surjective is used instead of onto. Here are the exact definitions:
Definition 12.4 A function f : A → B is:
1. injective (or one-to-one) if for every x, y ∈ A, x 6= y implies f (x) 6= f (y);
2. surjective (or onto) if for every b ∈ B there is an a ∈ A with f (a) = b;
3. bijective if f is both injective and surjective.
Below is a visual description of Definition 12.4. In essence, injective
means that unequal elements in A always get sent to unequal elements in
B. Surjective means that every element of B has an arrow pointing to it,
that is, it equals f (a) for some a in the domain of f .
A
A
A
A
B
B
B
B
b a b
x x
y y
Injective means that for any
two x, y ∈ A, this happens... ...and not this:
Surjective means that for
any b ∈ B...
...this happens:
202 Functions
For more concrete examples, consider the following functions from R
to R. The function f (x) = x
2
is not injective because −2 6= 2, but f (−2) = f (2).
Nor is it surjective, for if b = −1 (or if b is any negative number), then
there is no a ∈ R with f (a) = b. On the other hand, g(x) = x
3
is both injective
and surjective, so it is also bijective.
There are four possible injective/surjective combinations that a function
may possess. This is illustrated in the following figure showing four
functions from A to B. Functions in the first column are injective, those
in the second column are not injective. Functions in the first row are
surjective, those in the second row are not.
A
A
A
A
B
B
B
B
a
a
a
a
b
b
b
b
c c
c
1
1
1
1
2
2
2
2
3 3
3
(bijective)
Injective Not injective
Surjective
Not surjective
We note in passing that, according to the definitions, a function is
surjective if and only if its codomain equals its range.
Often it is necessary to prove that a particular function f : A → B
is injective. For this we must prove that for any two elements x, y ∈ A,
the conditional statement (x 6= y) ⇒
¡
f (x) 6= f (y)
¢
is true. The two main
approaches for this are summarized below.
How to show a function f : A → B is injective:
Direct approach:
Suppose x, y ∈ A and x 6= y.
.
.
.
Therefore f (x) 6= f (y).
Contrapositive approach:
Suppose x, y ∈ A and f (x) = f (y).
.
.
.
Therefore x = y.
Of these two approaches, the contrapositive is often the easiest to use,
especially if f is defined by an algebraic formula. This is because the
contrapositive approach starts with the equation f (x) = f (y) and proceeds
Injective and Surjective Functions 203
to the equation x = y. In algebra, as you know, it is usually easier to work
with equations than inequalities.
To prove that a function is not injective, you must disprove the statement
(x 6= y) ⇒
¡
f (x) 6= f (y)
¢
. For this it suffices to find example of two elements
x, y ∈ A for which x 6= y and f (x) = f (y).
Next we examine how to prove that f : A → B is surjective. According
to Definition 12.4, we must prove the statement ∀b ∈ B,∃a ∈ A, f (a) = b. In
words, we must show that for any b ∈ B, there is at least one a ∈ A (which
may depend on b) having the property that f (a) = b. Here is an outline.
How to show a function f : A → B is surjective:
Suppose b ∈ B.
[Prove there exists a ∈ A for which f (a) = b.]
In the second step, we have to prove the existence of an a for which
f (a) = b. For this, just finding an example of such an a would suffice. (How
to find such an example depends on how f is defined. If f is given as a
formula, we may be able to find a by solving the equation f (a) = b for a.
Sometimes you can find a by just plain common sense.) To show f is not
surjective, we must prove the negation of ∀b ∈ B,∃a ∈ A, f (a) = b, that is,
we must prove ∃ b ∈ B,∀a ∈ A, f (a) 6= b.
The following examples illustrate these ideas. (For the first example,
note that the set R−
©
0
ª
is R with the number 0 removed.)
Example 12.4 Show that the function f : R−
©
0
ª
→ R defined as f (x) =
1
x
+1
is injective but not surjective.
We will use the contrapositive approach to show that f is injective.
Suppose x, y ∈ R−
©
0
ª
and f (x) = f (y). This means 1
x
+1 =
1
y
+1. Subtracting
1 from both sides and inverting produces x = y. Therefore f is injective.
Function f is not surjective because there exists an element b = 1 ∈ R
for which f (x) =
1
x
+1 6= 1 for every x ∈ R−
©
0
ª
.
Example 12.5 Show that the function g : Z × Z → Z × Z defined by the
formula g(m,n) = (m+ n,m+2n), is both injective and surjective.
We will use the contrapositive approach to show that g is injective.
Thus we need to show that g(m,n) = g(k,`) implies (m,n) = (k,`). Suppose
(m,n),(k,`) ∈ Z×Z and g(m,n) = g(k,`). Then (m+n,m+2n) = (k+`,k+2`). It
follows that m+ n = k+` and m+2n = k+2`. Subtracting the first equation
from the second gives n = `. Next, subtract n = ` from m+ n = k +` to get
m = k. Since m = k and n = `, it follows that (m,n) = (k,`). Therefore g is
injective.
204 Functions
To see that g is surjective, consider an arbitrary element (b, c) ∈ Z×Z.
We need to show that there is some (x, y) ∈ Z×Z for which g(x, y) = (b, c). To
find (x, y), note that g(x, y) = (b, c) means (x+ y, x+2y) = (b, c). This leads to
the following system of equations:
x + y = b
x + 2y = c.
Solving gives x = 2b − c and y = c − b. Then (x, y) = (2b − c, c − b). We now
have g(2b − c, c − b) = (b, c), and it follows that g is surjective.
Example 12.6 Consider function h : Z×Z → Q defined as h(m,n) =
m
|n| +1
.
Determine whether this is injective and whether it is surjective.
This function is not injective because of the unequal elements (1,2) and
(1,−2) in Z×Z for which h(1,2) = h(1,−2) =
1
3
. However, h is surjective: Take
any element b ∈ Q. Then b =
c
d
for some c,d ∈ Z. Notice we may assume d is
positive by making c negative, if necessary. Then h(c,d−1) =
c
|d−1|+1
=
c
d
= b.
Exercises for Section 12.2
1. Let A =
©
1,2,3,4
ª
and B =
©
a,b, c
ª
. Give an example of a function f : A → B that
is neither injective nor surjective.
2. Consider the logarithm function ln : (0,∞) → R. Decide whether this function is
injective and whether it is surjective.
3. Consider the cosine function cos : R → R. Decide whether this function is injective
and whether it is surjective. What if it had been defined as cos : R → [−1,1]?
4. A function f : Z → Z × Z is defined as f (n) = (2n,n + 3). Verify whether this
function is injective and whether it is surjective.
5. A function f : Z → Z is defined as f (n) = 2n+1. Verify whether this function is
injective and whether it is surjective.
6. A function f : Z × Z → Z is defined as f (m,n) = 3n − 4m. Verify whether this
function is injective and whether it is surjective.
7. A function f : Z × Z → Z is defined as f (m,n) = 2n − 4m. Verify whether this
function is injective and whether it is surjective.
8. A function f : Z×Z → Z×Z is defined as f (m,n) = (m+ n,2m+ n). Verify whether
this function is injective and whether it is surjective.
9. Prove that the function f : R−
©
2
ª
→ R−
©
5
ª
defined by f (x) =
5x+1
x−2
is bijective.
10. Prove the function f : R−
©
1
ª
→ R−
©
1
ª
defined by f (x) =
µ
x+1
x−1
¶3
is bijective.
11. Consider the function θ :
©
0,1
ª
×N → Z defined as θ(a,b) = (−1)ab. Is θ injective?
Is it surjective? Bijective? Explain.
The Pigeonhole Principle 205
12. Consider the function θ :
©
0,1
ª
×N → Z defined as θ(a,b) = a−2ab+b. Is θ injective?
Is it surjective? Bijective? Explain.
13. Consider the function f : R
2 → R
2 defined by the formula f (x, y) = (x y, x
3
). Is f
injective? Is it surjective? Bijective? Explain.
14. Consider the function θ : P(Z) → P(Z) defined as θ(X) = X. Is θ injective? Is it
surjective? Bijective? Explain.
15. This question concerns functions f :
©
A,B,C,D,E,F,G
ª
→
©
1,2,3,4,5,6,7
ª
. How
many such functions are there? How many of these functions are injective?
How many are surjective? How many are bijective?
16. This question concerns functions f :
©
A,B,C,D,E
ª
→
©
1,2,3,4,5,6,7
ª
. How many
such functions are there? How many of these functions are injective? How
many are surjective? How many are bijective?
17. This question concerns functions f :
©
A,B,C,D,E,F,G
ª
→
©
1,2
ª
. How many such
functions are there? How many of these functions are injective? How many
are surjective? How many are bijective?
18. Prove that the function f : N → Z defined as f (n) =
(−1)n
(2n−1)+1
4
is bijective.
12.3 The Pigeonhole Principle
Here is a simple but useful idea. Imagine there is a set A of pigeons and
a set B of pigeon-holes, and all the pigeons fly into the pigeon-holes. You
can think of this as describing a function f : A → B, where pigeon X flies
into pigeon-hole f (X). Figure 12.4 illustrates this.
Pigeons Pigeon-holes
(a)
f
Pigeons Pigeon-holes
(b)
f
Figure 12.4. The pigeonhole principle
In Figure 12.4(a) there are more pigeons than pigeon-holes, and it
is obvious that in such a case at least two pigeons have to fly into the
same pigeon-hole, meaning that f is not injective. In Figure 12.4(b) there
are fewer pigeons than pigeon-holes, so clearly at least one pigeon-hole
remains empty, meaning that f is not surjective.
206 Functions
Although the underlying idea expressed by these figures has little to
do with pigeons, it is nonetheless called the pigeonhole principle:
The Pigeonhole Principle
Suppose A and B are finite sets and f : A → B is any function. Then:
1. If |A| > |B|, then f is not injective.
2. If |A| < |B|, then f is not surjective.
Though the pigeonhole principle is obvious, it can be used to prove
some things that are not so obvious.
Example 12.7 Prove the following proposition.
Proposition If A is any set of 10 integers between 1 and 100, then there
exist two different subsets X ⊆ A and Y ⊆ A for which the sum of elements
in X equals the sum of elements in Y.
To illustrate what this proposition is saying, consider the random set
A =
©
5,7,12,11,17,50,51,80,90,100ª
of 10 integers between 1 and 100. Notice that A has subsets X =
©
5,80ª
and
Y =
©
7,11,17,50ª
for which the sum of the elements in X equals the sum of
those in Y. If we tried to “mess up” A by changing the 5 to a 6, we get
A =
©
6,7,12,11,17,50,51,80,90,100ª
which has subsets X =
©
7,12,17,50ª
and Y =
©
6,80ª
both of whose elements
add up to the same number (86). The proposition asserts that this is always
possible, no matter what A is. Here is a proof:
Proof. Suppose A ⊆
©
1,2,3,4,...,99,100ª
and |A| = 10, as stated. Notice that
if X ⊆ A, then X has no more than 10 elements, each between 1 and 100,
and therefore the sum of all the elements of X is less than 100·10 = 1000.
Consider the function
f : P(A) →
©
0,1,2,3,4,...,1000ª
where f (X) is the sum of the elements in X. (Examples: f
¡©3,7,50ª¢ = 60;
f
¡©1,70,80,95ª¢ = 246.) As |P(A)| = 2
10 = 1024 > 1001 =
¯
¯
©
0,1,2,3,...,1000ª¯
¯
,
it follows from the pigeonhole principle that f is not injective. Therefore
there are two unequal sets X,Y ∈ P(A) for which f (X) = f (Y). In other
words, there are subsets X ⊆ A and Y ⊆ A for which the sum of elements
in X equals the sum of elements in Y. ■
The Pigeonhole Principle 207
Example 12.8 Prove the following proposition.
Proposition There are at least two Texans with the same number of
hairs on their heads.
Proof. We will use two facts. First, the population of Texas is more than
twenty million. Second, it is a biological fact that every human head
has fewer than one million hairs. Let A be the set of all Texans, and
let B =
©
0,1,2,3,4,...,1000000ª
. Let f : A → B be the function for which f (x)
equals the number of hairs on the head of x. Since |A| > |B|, the pigeonhole
principle asserts that f is not injective. Thus there are two Texans x and
y for whom f (x) = f (y), meaning that they have the same number of hairs
on their heads. ■
Proofs that use the pigeonhole principle tend to be inherently nonconstructive,
in the sense discussed in Section 7.4. For example, the above
proof does not explicitly give us of two Texans with the same number of
hairs on their heads; it only shows that two such people exist. If we were
to make a constructive proof, we could find examples of two bald Texans.
Then they have the same number of head hairs, namely zero.
Exercises for Section 12.3
1. Prove that if six numbers are chosen at random, then at least two of them will
have the same remainder when divided by 5.
2. Prove that if a is a natural number, then there exist two unequal natural
numbers k and ` for which a
k − a
`
is divisible by 10.
3. Prove that if six natural numbers are chosen at random, then the sum or
difference of two of them is divisible by 9.
4. Consider a square whose side-length is one unit. Select any five points from
inside this square. Prove that at least two of these points are within
p
2
2
units
of each other.
5. Prove that any set of seven distinct natural numbers contains a pair of numbers
whose sum or difference is divisible by 10.
6. Given a sphere S, a great circle of S is the intersection of S with a plane
through its center. Every great circle divides S into two parts. A hemisphere
is the union of the great circle and one of these two parts. Prove that if five
points are placed arbitrarily on S, then there is a hemisphere that contains
four of them.
7. Prove or disprove: Any subset X ⊆
©
1,2,3,...,2n
ª
with |X| > n contains two
(unequal) elements a,b ∈ X for which a | b or b | a.
208 Functions
12.4 Composition
You should be familiar with the notion of function composition from algebra
and calculus. Still, it is worthwhile to revisit it now with our more
sophisticated ideas about functions.
Definition 12.5 Suppose f : A → B and g : B → C are functions with the
property that the codomain of f equals the domain of g. The composition
of f with g is another function, denoted as g◦ f and defined as follows: If
x ∈ A, then g◦f (x) = g(f (x)). Therefore g◦f sends elements of A to elements
of C, so g◦ f : A → C.
The following figure illustrates the definition. Here f : A → B, g : B → C,
and g◦ f : A → C. We have, for example, g◦ f (0) = g(f (0)) = g(2) = 4. Be very
careful with the order of the symbols. Even though g comes first in the
symbol g◦f , we work out g◦f (x) as g(f (x)), with f acting on x first, followed
by g acting on f (x).
A
A
C
B C
3
2
1
0
3
2
1
0
7
6
5
4
7
6
5
4
3
2
1
f g
g ◦ f
Figure 12.5. Composition of two functions
Notice that the composition g ◦ f also makes sense if the range of f
is a subset of the domain of g. You should take note of this fact, but to
keep matters simple we will continue to emphasize situations where the
codomain of f equals the domain of g.
Example 12.9 Suppose A =
©
a,b, c
ª
, B =
©
0,1
ª
, C =
©
1,2,3
ª
. Let f : A → B
be the function f =
©
(a,0),(b,1),(c,0)ª
, and let g : B → C be the function
g =
©
(0,3),(1,1)ª
. Then g ◦ f =
©
(a,3),(b,1),(c,3)ª
.
Example 12.10 Suppose A =
©
a,b, c
ª
, B =
©
0,1
ª
, C =
©
1,2,3
ª
. Let f : A → B
be the function f =
©
(a,0),(b,1),(c,0)ª
, and let g : C → B be the function
g =
©
(1,0),(2,1),(3,1)ª
. In this situation the composition g ◦ f is not defined
because the codomain B of f is not the same set as the domain C of g.
Composition 209
Remember: In order for g ◦ f to make sense, the codomain of f must equal
the domain of g. (Or at least be a subset of it.)
Example 12.11 Let f : R → R be defined as f (x) = x
2 + x, and g : R → R be
defined as g(x) = x + 1. Then g ◦ f : R → R is the function defined by the
formula g ◦ f (x) = g(f (x)) = g(x
2 + x) = x
2 + x+1.
Since the domains and codomains of g and f are the same, we can in
this case do a composition in the other order. Note that f ◦ g : R → R is the
function defined as f ◦ g(x) = f (g(x)) = f (x+1) = (x+1)2 +(x+1) = x
2 +3x+2.
This example illustrates that even when g◦ f and f ◦ g are both defined,
they are not necessarily equal. We can express this fact by saying function
composition is not commutative.
We close this section by proving several facts about function composition
that you are likely to encounter in your future study of mathematics. First,
we note that, although it is not commutative, function composition is
associative.
Theorem 12.1 Composition of functions is associative. That is if f : A → B,
g : B → C and h : C → D, then (h◦ g) ◦ f = h◦ (g ◦ f ).
Proof. Suppose f , g,h are as stated. It follows from Definition 12.5 that
both (h◦ g) ◦ f and h◦ (g ◦ f ) are functions from A to D. To show that they
are equal, we just need to show
³
(h◦ g) ◦ f
´
(x) =
³
h◦ (g ◦ f )
´
(x)
for every x ∈ A. Note that Definition 12.5 yields
³
(h◦ g) ◦ f
´
(x) = (h◦ g)(f (x)) = h(g(f (x)).
Also
³
h◦ (g ◦ f )
´
(x) = h(g ◦ f (x)) = h(g(f (x))).
Thus
³
(h◦ g) ◦ f
´
(x) =
³
h◦ (g ◦ f )
´
(x),
as both sides equal h(g(f (x))). ■
Theorem 12.2 Suppose f : A → B and g : B → C. If both f and g are
injective, then g ◦ f is injective. If both f and g are surjective, then g ◦ f is
surjective.
210 Functions
Proof. First suppose both f and g are injective. To see that g◦ f is injective,
we must show that g◦ f (x) = g◦ f (y) implies x = y. Suppose g◦ f (x) = g◦ f (y).
This means g(f (x)) = g(f (y)). It follows that f (x) = f (y). (For otherwise g
wouldn’t be injective.) But since f (x) = f (y) and f is injective, it must be
that x = y. Therefore g ◦ f is injective.
Next suppose both f and g are surjective. To see that g◦ f is surjective,
we must show that for any element c ∈ C, there is a corresponding element
a ∈ A for which g ◦ f (a) = c. Thus consider an arbitrary c ∈ C. Because g
is surjective, there is an element b ∈ B for which g(b) = c. And because
f is surjective, there is an element a ∈ A for which f (a) = b. Therefore
g(f (a)) = g(b) = c, which means g ◦ f (a) = c. Thus g ◦ f is surjective. ■
Exercises for Section 12.4
1. Suppose A =
©
5,6,8
ª
, B =
©
0,1
ª
, C =
©
1,2,3
ª
. Let f : A → B be the function f = ©
(5,1),(6,0),(8,1)ª
, and g : B → C be g =
©
(0,1),(1,1)ª
. Find g ◦ f .
2. Suppose A =
©
1,2,3,4
ª
, B =
©
0,1,2
ª
, C =
©
1,2,3
ª
. Let f : A → B be
f =
©
(1,0),(2,1),(3,2),(4,0)ª
,
and g : B → C be g =
©
(0,1),(1,1),(2,3)ª
. Find g ◦ f .
3. Suppose A =
©
1,2,3
ª
. Let f : A → A be the function f =
©
(1,2),(2,2),(3,1)ª
, and let
g : A → A be the function g =
©
(1,3),(2,1),(3,2)ª
. Find g ◦ f and f ◦ g.
4. Suppose A =
©
a,b, c
ª
. Let f : A → A be the function f =
©
(a, c),(b, c),(c, c)
ª
, and let
g : A → A be the function g =
©
(a,a),(b,b),(c,a)
ª
. Find g ◦ f and f ◦ g.
5. Consider the functions f , g : R → R defined as f (x) =
p3
x+1 and g(x) = x
3
. Find
the formulas for g ◦ f and f ◦ g.
6. Consider the functions f , g : R → R defined as f (x) =
1
x
2+1
and g(x) = 3x+2. Find
the formulas for g ◦ f and f ◦ g.
7. Consider the functions f , g : Z × Z → Z × Z defined as f (m,n) = (mn,m2
) and
g(m,n) = (m+1,m+ n). Find the formulas for g ◦ f and f ◦ g.
8. Consider the functions f , g : Z × Z → Z × Z defined as f (m,n) = (3m − 4n,2m + n)
and g(m,n) = (5m+ n,m). Find the formulas for g ◦ f and f ◦ g.
9. Consider the functions f : Z×Z → Z defined as f (m,n) = m+ n and g : Z → Z×Z
defined as g(m) = (m,m). Find the formulas for g ◦ f and f ◦ g.
10. Consider the function f : R
2 → R
2 defined by the formula f (x, y) = (x y, x
3
). Find
a formula for f ◦ f .
Inverse Functions 211
12.5 Inverse Functions
You may recall from calculus that if a function f is injective and surjective,
then it has an inverse function f
−1
that “undoes” the effect of f in the
sense that f
−1
(f (x)) = x for every x in the domain. (For example, if f (x) = x
3
,
then f
−1
(x) =
p3
x.) We now review these ideas. Our approach uses two
ingredients, outlined in the following definitions.
Definition 12.6 Given a set A, the identity function on A is the function
i A : A → A defined as i A(x) = x for every x ∈ A.
Example: If A =
©
1,2,3
ª
, then i A =
©
(1,1),(2,2),(3,3)ª
. Also iZ =
©
(n,n) : n ∈ Z
ª
.
The identity function on a set is the function that sends any element of
the set to itself.
Notice that for any set A, the identity function i A is bijective: It is
injective because i A(x) = i A(y) immediately reduces to x = y. It is surjective
because if we take any element b in the codomain A, then b is also in the
domain A, and i A(b) = b.
Definition 12.7 Given a relation R from A to B, the inverse relation
of R is the relation from B to A defined as R
−1 =
©
(y, x) : (x, y) ∈ R
ª
. In other
words, the inverse of R is the relation R
−1 obtained by interchanging the
elements in every ordered pair in R.
For example, let A =
©
a,b, c
ª
and B =
©
1,2,3
ª
, and suppose f is the
relation f =
©
(a,2),(b,3),(c,1)ª
from A to B. Then f
−1 =
©
(2,a),(3,b),(1, c)
ª
and this is a relation from B to A. Notice that f is actually a function
from A to B, and f
−1
is a function from B to A. These two relations are
drawn below. Notice the drawing for relation f
−1
is just the drawing for f
with arrows reversed.
A B A B
c
b
a
c
b
a
3
2
1
3
2
1
f =
©
(a,2),(b,3),(c,1)ª
f
−1 =
©
(2,a),(3,b),(1, c)
ª
For another example, let A and B be the same sets as above, but consider
the relation g =
©
(a,2),(b,3),(c,3)ª
from A to B. Then g
−1 =
©
(2,a),(3,b),(3, c)
ª
is a relation from B to A. These two relations are sketched below.
212 Functions
A B A B
c
b
a
c
b
a
3
2
1
3
2
1
g =
©
(a,2),(b,3),(c,3)ª
g
−1 =
©
(2,a),(3,b),(3, c)
ª
This time, even though the relation g is a function, its inverse g
−1
is
not a function because the element 3 occurs twice as a first coordinate of
an ordered pair in g
−1
.
In the above examples, relations f and g are both functions, and f
−1
is
a function and g
−1
is not. This raises a question: What properties does f
have and g lack that makes f
−1 a function and g
−1 not a function? The
answer is not hard to see. Function g is not injective because g(b) = g(c) = 3,
and thus (b,3) and (c,3) are both in g. This causes a problem with g
−1
because it means (3,b) and (3, c) are both in g
−1
, so g
−1
can’t be a function.
Thus, in order for g
−1
to be a function, it would be necessary that g be
injective.
But that is not enough. Function g also fails to be surjective because
no element of A is sent to the element 1 ∈ B. This means g
−1
contains no
ordered pair whose first coordinate is 1, so it can’t be a function from B to
A. If g
−1 were to be a function it would be necessary that g be surjective.
The previous two paragraphs suggest that if g is a function, then it
must be bijective in order for its inverse relation g
−1
to be a function.
Indeed, this is easy to verify. Conversely, if a function is bijective, then its
inverse relation is easily seen to be a function. We summarize this in the
following theorem.
Theorem 12.3 Let f : A → B be a function. Then f is bijective if and only
if the inverse relation f
−1
is a function from B to A.
Suppose f : A → B is bijective, so according to the theorem f
−1
is a
function. Observe that the relation f contains all the pairs (x, f (x)) for x ∈ A,
so f
−1
contains all the pairs (f (x), x). But (f (x), x) ∈ f
−1 means f
−1
(f (x)) = x.
Therefore f
−1
◦ f (x) = x for every x ∈ A. From this we get f
−1
◦ f = i A. Similar
reasoning produces f ◦ f
−1 = iB. This leads to the following definitions.
Definition 12.8 If f : A → B is bijective then its inverse is the function
f
−1
: B → A. Functions f and f
−1 obey the equations f
−1
◦ f = i A and
f ◦ f
−1 = iB.
Inverse Functions 213
You probably recall from algebra and calculus at least one technique
for computing the inverse of a bijective function f : to find f
−1
, start with
the equation y = f (x). Then interchange variables to get x = f (y). Solving
this equation for y (if possible) produces y = f
−1
(x). The next two examples
illustrate this.
Example 12.12 The function f : R → R defined as f (x) = x
3 +1 is bijective.
Find its inverse.
We begin by writing y = x
3 + 1. Now interchange variables to obtain
x = y
3 +1. Solving for y produces y =
p3
x−1. Thus
f
−1
(x) =
p3
x−1.
(You can check your answer by computing
f
−1
(f (x)) =
p3
f (x)−1 =
p3
x
3 +1−1 = x.
Therefore f
−1
(f (x)) = x. Any answer other than x indicates a mistake.)
We close with one final example. Example 12.5 showed that the function
g : Z×Z → Z×Z defined by the formula g(m,n) = (m+ n,m+2n) is bijective.
Let’s find its inverse. The approach outlined above should work, but we
need to be careful to keep track of coordinates in Z × Z. We begin by
writing (x, y) = g(m,n), then interchanging the variables (x, y) and (m,n) to
get (m,n) = g(x, y). This gives
(m,n) = (x+ y, x+2y),
from which we get the following system of equations:
x + y = m
x + 2y = n.
Solving this system using techniques from algebra with which you are
familiar, we get
x = 2m− n
y = n− m.
Then (x, y) = (2m− n,n− m), so g
−1
(m,n) = (2m− n,n− m).
214 Functions
We can check our work by confirming that g
−1
(g(m,n)) = (m,n). Doing
the math,
g
−1
(g(m,n)) = g
−1
(m+ n,m+2n)
=
¡
2(m+ n)−(m+2n),(m+2n)−(m+ n)
¢
= (m,n).
Exercises for Section 12.5
1. Check that the function f : Z → Z defined by f (n) = 6 − n is bijective. Then
compute f
−1
.
2. In Exercise 9 of Section 12.2 you proved that f : R −
©
2
ª
→ R −
©
5
ª
defined by
f (x) =
5x+1
x−2
is bijective. Now find its inverse.
3. Let B =
©
2
n
: n ∈ Z
ª
=
©
...,
1
4
,
1
2
,1,2,4,8,...ª
. Show that the function f : Z → B
defined as f (n) = 2
n
is bijective. Then find f
−1
.
4. The function f : R → (0,∞) defined as f (x) = e
x
3+1
is bijective. Find its inverse.
5. The function f : R → R defined as f (x) = πx− e is bijective. Find its inverse.
6. The function f : Z×Z → Z×Z defined by the formula f (m,n) = (5m+4n,4m+3n)
is bijective. Find its inverse.
7. Show that the function f : R
2 → R
2 defined by the formula f (x, y) = ((x
2 +1)y, x
3
)
is bijective. Then find its inverse.
8. Is the function θ : P(Z) → P(Z) defined as θ(X) = X bijective? If so, what is its
inverse?
9. Consider the function f : R×N → N×R defined as f (x, y) = (y,3x y). Check that
this is bijective; find its inverse.
10. Consider f : N → Z defined as f (n) =
(−1)n
(2n−1)+1
4
. This function is bijective
by Exercise 18 in Section 12.2. Find its inverse.
12.6 Image and Preimage
It is time to take up a matter of notation that you will encounter in future
mathematics classes. Suppose we have a function f : A → B. If X ⊆ A, the
expression f (X) has a special meaning. It stands for the set ©
f (x) : x ∈ X
ª
.
Similarly, if Y ⊆ B then f
−1
(Y) has a meaning even if f is not invertible.
The expression f
−1
(Y) stands for the set ©
x ∈ A : f (x) ∈ Y
ª
. Here are the
precise definitions.
Image and Preimage 215
Definition 12.9 Suppose f : A → B is a function.
1. If X ⊆ A, the image of X is the set f (X) =
©
f (x) : x ∈ X
ª
⊆ B.
2. If Y ⊆ B, the preimage of Y is the set f
−1
(Y) =
©
x ∈ A : f (x) ∈ Y
ª
⊆ A.
In words, the image f (X) of X is the set of all things in B that f sends
elements of X to. (Roughly speaking, you might think of f (X) as a kind of
distorted “copy” or “image” of X in B.) The preimage f
−1
(Y) of Y is the set
of all things in A that f sends into Y.
Maybe you have already encountered these ideas in linear algebra, in
a setting involving a linear transformation T : V → W between two vector
spaces. If X ⊆ V is a subspace of V, then its image T(X) is a subspace of W.
If Y ⊆ W is a subspace of W, then its preimage T
−1
(Y) is a subspace of V.
(If this does not sound familiar, then ignore it.)
Example 12.13 Let f :
©
s,t,u,v,w, x, y, z
ª
→
©
0,1,2,3,4,5,6,7,8,9
ª
, where
f =
©
(s,4),(t,8),(u,8),(v,1),(w,2),(x,4),(y,6),(z,4)ª
.
Notice that f is neither injective nor surjective, so it certainly is not
invertible. Be sure you understand the following statements.
1. f
¡©s,t,u, z
ª¢ =
©
8,4
ª
2. f
¡©s, x, z
ª¢ =
©
4
ª
3. f
¡©s,v,w, y
ª¢ =
©
1,2,4,6
ª
4. f
−1
¡©4
ª¢ =
©
s, x, z
ª
5. f
−1
¡©4,9
ª¢ =
©
s, x, z
ª
6. f
−1
¡©9
ª¢ = ;
7. f
−1
¡©1,4,8
ª¢ =
©
s,t,u,v, x, z
ª
It is important to realize that the X and Y in Definition 12.9 are
subsets (not elements!) of A and B. Note that in the above example we
had f
−1
¡©4
ª¢ =
©
s, x, z
ª
, while f
−1
(4) has absolutely no meaning because the
inverse function f
−1 does not exist. Likewise, there is a subtle difference
between f
¡©s
ª¢ =
©
4
ª
and f (s) = 4. Be careful.
Example 12.14 Consider the function f : R → R defined as f (x) = x
2
.
Note that f
¡©0,1,2
ª¢ =
©
0,1,4
ª
and f
−1
¡©0,1,4
ª¢ =
©
−2,−1,0,1,2
ª
. This shows
f
−1
(f (X)) 6= X in general.
Using the same f , now check your understanding of the following
statements involving images and preimages of intervals: f ([−2,3]) = [0,9],
and f
−1
([0,9]) = [−3,3]. Also f (R) = [0,∞) and f
−1
([−2,−1]) = ;.
216 Functions
If you continue with mathematics you are likely to encounter the
following results. For now, you are asked to prove them in the exercises.
Theorem 12.4 Suppose f : A → B is a function. Let W, X ⊆ A, and Y,Z ⊆ B.
Then:
1. f (W ∩ X) ⊆ f (W)∩ f (X)
2. f (W ∪ X) = f (W)∪ f (X)
3. f
−1
(Y ∩ Z) = f
−1
(Y)∩ f
−1
(Z)
4. f
−1
(Y ∪ Z) = f
−1
(Y)∪ f
−1
(Z)
5. X ⊆ f
−1
(f (X))
6. f (f
−1
(Y)) ⊆ Y.
Exercises for Section 12.6
1. Consider the function f : R → R defined as f (x) = x
2 + 3. Find f ([−3,5]) and
f
−1
([12,19]).
2. Consider the function f :
©
1,2,3,4,5,6,7
ª
→
©
0,1,2,3,4,5,6,7,8,9
ª
given as
f =
©
(1,3),(2,8),(3,3),(4,1),(5,2),(6,4),(7,6)ª
.
Find: f
¡©1,2,3
ª¢, f
¡©4,5,6,7
ª¢, f (;), f
−1
¡©0,5,9
ª¢ and f
−1
¡©0,3,5,9
ª¢.
3. This problem concerns functions f :
©
1,2,3,4,5,6,7
ª
→
©
0,1,2,3,4
ª
. How many
such functions have the property that
¯
¯
f
−1
¡©3
ª¢¯
¯ = 3?
4. This problem concerns functions f :
©
1,2,3,4,5,6,7,8
ª
→
©
0,1,2,3,4,5,6
ª
. How
many such functions have the property that
¯
¯
f
−1
¡©2
ª¢¯
¯ = 4?
5. Consider a function f : A → B and a subset X ⊆ A. We observed in Section 12.6
that f
−1
(f (X)) 6= X in general. However X ⊆ f
−1
(f (X)) is always true. Prove this.
6. Given a function f : A → B and a subset Y ⊆ B, is f (f
−1
(Y)) = Y always true?
Prove or give a counterexample.
7. Given a function f : A → B and subsets W, X ⊆ A, prove f (W ∩ X) ⊆ f (W)∩ f (X).
8. Given a function f : A → B and subsets W, X ⊆ A, then f (W ∩ X) = f (W)∩ f (X) is
false in general. Produce a counterexample.
9. Given a function f : A → B and subsets W, X ⊆ A, prove f (W ∪ X) = f (W)∪ f (X).
10. Given f : A → B and subsets Y,Z ⊆ B, prove f
−1
(Y ∩ Z) = f
−1
(Y)∩ f
−1
(Z).
11. Given f : A → B and subsets Y,Z ⊆ B, prove f
−1
(Y ∪ Z) = f
−1
(Y)∪ f
−1
(Z).
12. Consider f : A → B. Prove that f is injective if and only if X = f
−1
(f (X)) for all
X ⊆ A. Prove that f is surjective if and only if f (f
−1
(Y)) = Y for all Y ⊆ B.
13. Let f : A → B be a function, and X ⊆ A. Prove or disprove: f
¡
f
−1
(f (X))¢
= f (X).
14. Letf : A → B be a function, and Y ⊆ B. Prove or disprove: f
−1
¡
f (f
−1
(Y))¢
= f
−1
(Y).
CHAPTER 13
Cardinality of Sets
T
his chapter is all about cardinality of sets. At first this looks like a
very simple concept. To find the cardinality of a set, just count its
elements. If A =
©
a,b, c,d
ª
, then |A| = 4; if B =
©
n ∈ Z : −5 ≤ n ≤ 5
ª
, then
|B| = 11. In this case |A| < |B|. What could be simpler than that?
Actually, the idea of cardinality becomes quite subtle when the sets
are infinite. The main point of this chapter is to explain how there are
numerous different kinds of infinity, and some infinities are bigger than
others. Two sets A and B can both have infinite cardinality, yet |A| < |B|.
13.1 Sets with Equal Cardinalities
We begin with a discussion of what it means for two sets to have the
same cardinality. Up until this point we’ve said |A| = |B| if A and B have
the same number of elements: Count the elements of A, then count the
elements of B. If you get the same number, then |A| = |B|.
Although this is a fine strategy if the sets are finite (and not too big!),
it doesn’t apply to infinite sets because we’d never be done counting their
elements. We need a new approach that applies to both finite and infinite
sets. Here it is:
Definition 13.1 Two sets A and B have the same cardinality, written
|A| = |B|, if there exists a bijective function f : A → B. If no such bijective
function exists, then the sets have unequal cardinalities, that is, |A| 6= |B|.
e
d
c
b
a
4
3
2
1
0
A B
f
The above picture illustrates our definition. There is a bijective function
f : A → B, so |A| = |B|. The function f matches up A with B. Think of f as
describing how to overlay A onto B so that they fit together perfectly.
218 Cardinality of Sets
On the other hand, if A and B are as indicated in either of the following
figures, then there can be no bijection f : A → B. (The best we can do is a
function that is either injective or surjective, but not both). Therefore the
definition says |A| 6= |B| in these cases.
d
c
b
a
4
3
2
1
0
A B
f
d
c
b
a
e
3
2
1
0
A B
f
Example 13.1 The sets A =
©
n ∈ Z : 0 ≤ n ≤ 5
ª
and B =
©
n ∈ Z : −5 ≤ n ≤ 0
ª
have the same cardinality because there is a bijective function f : A → B
given by the rule f (n) = −n.
Several comments are in order. First, if |A| = |B|, there can be lots of
bijective functions from A to B. We only need to find one of them in order to
conclude |A| = |B|. Second, as bijective functions play such a big role here,
we use the word bijection to mean bijective function. Thus the function
f (n) = −n from Example 13.1 is a bijection. Also, an injective function is
called an injection and a surjective function is called a surjection.
We emphasize and reiterate that Definition 13.1 applies to finite as
well as infinite sets. If A and B are infinite, then |A| = |B| provided there
exists a bijection f : A → B. If no such bijection exists, then |A| 6= |B|.
Example 13.2 This example shows that |N| = |Z|. To see why this is true,
notice that the following table describes a bijection f : N → Z.
n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 . . .
f (n) 0 1 −1 2 −2 3 −3 4 −4 5 −5 6 −6 7 −7 . . .
Notice that f is described in such a way that it is both injective and
surjective. Every integer appears exactly once on the infinitely long second
row. Thus, according to the table, given any b ∈ Z there is some natural
number n with f (n) = b, so f is surjective. It is injective because the way
the table is constructed forces f (m) 6= f (n) whenever m 6= n. Because of this
bijection f : N → Z, we must conclude from Definition 13.1 that |N| = |Z|.
Example 13.2 may seem slightly unsettling. On one hand it makes
sense that |N| = |Z| because N and Z are both infinite, so their cardinalities
are both “infinity.” On the other hand, Z may seem twice as large as
Sets with Equal Cardinalities 219
N because Z has all the negative integers as well as the positive ones.
Definition 13.1 settles the issue. Because the bijection f : N → Z matches
up N with Z, it follows that |N| = |Z|. We summarize this with a theorem.
Theorem 13.1 There exists a bijection f : N → Z. Therefore |N| = |Z|.
The fact that N and Z have the same cardinality might prompt us
compare the cardinalities of other infinite sets. How, for example, do N
and R compare? Let’s turn our attention to this.
In fact, |N| 6= |R|. This was first recognized by Georg Cantor (1845–1918),
who devised an ingenious argument to show that there are no surjective
functions f : N → R. (This in turn implies that there can be no bijections
f : N → R, so |N| 6= |R| by Definition 13.1.)
We now describe Cantor’s argument for why there are no surjections
f : N → R. We will reason informally, rather than writing out an exact proof.
Take any arbitrary function f : N → R. Here’s why f can’t be surjective:
Imagine making a table for f , where values of n in N are in the lefthand
column and the corresponding values f (n) are on the right. The
first few entries might look something as follows. In this table, the real
numbers f (n) are written with all their decimal places trailing off to the
right. Thus, even though f (1) happens to be the real number 0.4, we write
it as 0.40000000...., etc.
n f (n)
1 0 . 4 0 0 0 0 0 0 0 0 0 0 0 0 0. . .
2 8 . 5 0 0 6 0 7 0 8 6 6 6 9 0 0. . .
3 7 . 5 0 5 0 0 9 4 0 0 4 4 1 0 1. . .
4 5 . 5 0 7 0 4 0 0 8 0 4 8 0 5 0. . .
5 6 . 9 0 0 2 6 0 0 0 0 0 0 5 0 6. . .
6 6 . 8 2 8 0 9 5 8 2 0 5 0 0 2 0. . .
7 6 . 5 0 5 0 5 5 5 0 6 5 5 8 0 8. . .
8 8 . 7 2 0 8 0 6 4 0 0 0 0 4 4 8. . .
9 0 . 5 5 0 0 0 0 8 8 8 8 0 0 7 7. . .
10 0 . 5 0 0 2 0 7 2 2 0 7 8 0 5 1. . .
11 2 . 9 0 0 0 0 8 8 0 0 0 0 9 0 0. . .
12 6 . 5 0 2 8 0 0 0 8 0 0 9 6 7 1. . .
13 8 . 8 9 0 0 8 0 2 4 0 0 8 0 5 0. . .
14 8 . 5 0 0 0 8 7 4 2 0 8 0 2 2 6. . .
.
.
.
.
.
.
220 Cardinality of Sets
There is a diagonal shaded band in the table. For each n ∈ N, this band
covers the n
th decimal place of f (n):
The 1st decimal place of f (1) is the 1st entry on the diagonal.
The 2nd decimal place of f (2) is the 2nd entry on the diagonal.
The 3rd decimal place of f (3) is the 3rd entry on the diagonal.
The 4th decimal place of f (4) is the 4th entry on the diagonal, etc.
The diagonal helps us construct a number b ∈ R that is unequal to any f (n).
Just let the nth decimal place of b differ from the nth entry of the diagonal.
Then the nth decimal place of b differs from the nth decimal place of f (n).
In order to be definite, define b to be the positive number less than 1 whose
nth decimal place is 0 if the nth decimal place of f (n) is not 0, and whose
nth decimal place is 1 if the nth decimal place of f (n) equals 0. Thus, for
the function f illustrated in the above table, we have
b = 0.01010001001000...
and b has been defined so that, for any n ∈ N, its nth decimal place is
unequal to the nth decimal place of f (n). Therefore f (n) 6= b for every
natural number n, meaning f is not surjective.
Since this argument applies to any function f : N → R (not just the one
in the above example) we conclude that there exist no bijections f : N → R,
so |N| 6= |R| by Definition 13.1. We summarize this as a theorem.
Theorem 13.2 There exists no bijection f : N → R. Therefore |N| 6= |R|.
This is our first indication of how there are different kinds of infinities.
Both N and R are infinite sets, yet |N| 6= |R|. We will continue to develop
this theme throughout this chapter. The next example shows that the
intervals (0,∞) and (0,1) on R have the same cardinality.
∞
1



1
x
P
−1 0
f (x)
Figure 13.1. A bijection f : (0,∞) → (0,1)
Sets with Equal Cardinalities 221
Example 13.3 Show that |(0,∞)| = |(0,1)|.
To accomplish this, we need to show that there is a bijection f : (0,∞) → (0,1).
We describe this function geometrically. Consider the interval (0,∞) as the
positive x-axis of R
2
. Let the interval (0,1) be on the y-axis as illustrated
in Figure 13.1, so that (0,∞) and (0,1) are perpendicular to each other.
The figure also shows a point P = (−1,1). Define f (x) to be the point on
(0,1) where the line from P to x ∈ (0,∞) intersects the y-axis. By similar
triangles, we have
1
x+1
=
f (x)
x
,
and therefore
f (x) =
x
x+1
.
If it is not clear from the figure that f : (0,∞) → (0,1) is bijective, then you
can verify it using the techniques from Section 12.2. (Exercise 16, below.)
It is important to note that equality of cardinalities is an equivalence
relation on sets: it is reflexive, symmetric and transitive. Let us confirm
this. Given a set A, the identity function A → A is a bijection, so |A| = |A|.
(This is the reflexive property.) For the symmetric property, if |A| = |B|,
then there is a bijection f : A → B, and its inverse is a bijection f
−1
: B → A,
so |B| = |A|. For transitivity, suppose |A| = |B| and |B| = |C|. Then there
are bijections f : A → B and g : B → C. The composition g ◦ f : A → C is a
bijection (Theorem 12.2), so |A| = |C|.
The transitive property can be useful. If, in trying to show two sets A
and C have the same cardinality, we can produce a third set B for which
|A| = |B| and |B| = |C|, then transitivity assures us that indeed |A| = |C|.
The next example uses this idea.
Example 13.4 Show that |R| = |(0,1)|.
Because of the bijection g : R → (0,∞) where g(x) = 2
x
, we have |R| = |(0,∞)|.
Also, Example 13.3 shows that |(0,∞)| = |(0,1)|. Therefore |R| = |(0,1)|.
So far in this chapter we have declared that two sets have “the same
cardinality” if there is a bijection between them. They have “different
cardinalities” if there exists no bijection between them. Using this idea,
we showed that |Z| = |N| 6= |R| = |(0,∞)| = |(0,1)|. So, we have a means of
determining when two sets have the same or different cardinalities. But
we have neatly avoided saying exactly what cardinality is. For example,
we can say that |Z| = |N|, but what exactly is |Z|, or |N|? What exactly are
these things that are equal? Certainly not numbers, for they are too big.
222 Cardinality of Sets
And saying they are “infinity” is not accurate, because we now know that
there are different types of infinity. So just what kind of mathematical
entity is |Z|? In general, given a set X, exactly what is its cardinality |X|?
This is a lot like asking what a number is. A number, say 5, is an
abstraction, not a physical thing. Early in life we instinctively grouped
together certain sets of things (five apples, five oranges, etc.) and conceived
of 5 as the thing common to all such sets. In a very real sense, the number
5 is an abstraction of the fact that any two of these sets can be matched
up via a bijection. That is, it can be identified with a certain equivalence
class of sets under the "has the same cardinality as" relation. (Recall that
this is an equivalence relation.) This is easy to grasp because our sense of
numeric quantity is so innate. But in exactly the same way we can say
that the cardinality of a set X is what is common to all sets that can be
matched to X via a bijection. This may be harder to grasp, but it is really
no different from the idea of the magnitude of a (finite) number.
In fact, we could be concrete and define |X| to be the equivalence class of
all sets whose cardinality is the same as that of X. This has the advantage
of giving an explicit meaning to |X|. But there is no harm in taking the
intuitive approach and just interpreting the cardinality |X| of a set X to
be a measure the “size” of X. The point of this section is that we have a
means of deciding whether two sets have the same size or different sizes.
Exercises for Section 13.1
A. Show that the two given sets have equal cardinality by describing a bijection
from one to the other. Describe your bijection with a formula (not as a table).
1. R and (0,∞)
2. R and (
p
2,∞)
3. R and (0,1)
4. The set of even integers and
the set of odd integers
5. A =
©
3k : k ∈ Z
ª
and B =
©
7k : k ∈ Z
ª
6. N and S =
© p
2
n
: n ∈ N
ª
7. Z and S =
©
...,
1
8
,
1
4
,
1
2
,1,2,4,8,16,...ª
8. Z and S =
©
x ∈ R : sinx = 1
ª
9. ©
0,1
ª
×N and N
10. ©
0,1
ª
×N and Z
11. [0,1] and (0,1)
12. N and Z (Suggestion: use Exercise 18 of Section 12.2.)
13. P(N) and P(Z) (Suggestion: use Exercise 12, above.)
14. N×N and ©
(n,m) ∈ N×N : n ≤ m
ª
B. Answer the following questions concerning bijections from this section.
15. Find a formula for the bijection f in Example 13.2 (page 218).
16. Verify that the function f in Example 13.3 is a bijection.
Countable and Uncountable Sets 223
13.2 Countable and Uncountable Sets
Let’s summarize the main points from the previous section.
1. |A| = |B| if and only if there exists a bijection A → B.
2. |N| = |Z| because there exists a bijection N → Z.
3. |N| 6= |R| because there exists no bijection N → R.
Thus, even though N, Z and R are all infinite sets, their cardinalities
are not all the same. The sets N and Z have the same cardinality, but
R’s cardinality is different from that of both the other sets. This means
infinite sets can have different sizes. We now make some definitions to
put words and symbols to this phenomenon.
In a certain sense you can count the elements of N; you can count its
elements off as 1,2,3,4,..., but you’d have to continue this process forever
to count the whole set. Thus we will call N a countably infinite set, and
the same term is used for any set whose cardinality equals that of N.
Definition 13.2 Suppose A is a set. Then A is countably infinite if
|N| = |A|, that is, if there exists a bijection N → A. The set A is uncountable
if A is infinite and |N| 6= |A|, that is, if A is infinite and there exists no
bijection N → A.
Thus Z is countably infinite but R is uncountable. This section deals
mainly with countably infinite sets. Uncountable sets are treated later.
If A is countably infinite, then |N| = |A|, so there is a bijection f : N → A.
You can think of f as “counting” the elements of A. The first element of A
is f (1), followed by f (2), then f (3) and so on. It makes sense to think of a
countably infinite set as the smallest type of infinite set, because if the
counting process stopped, the set would be finite, not infinite; a countably
infinite set has the fewest elements that a set can have and still be infinite.
It is common to reserve the special symbol ℵ0 to stand for the cardinality
of countably infinite sets.
Definition 13.3 The cardinality of the natural numbers is denoted as ℵ0.
That is, |N| = ℵ0. Thus any countably infinite set has cardinality ℵ0.
(The symbol ℵ is the first letter in the Hebrew alphabet, and is pronounced
“aleph.” The symbol ℵ0 is pronounced “aleph naught.”) The summary of
facts at the beginning of this section shows |Z| = ℵ0 and |R| 6= ℵ0.
Example 13.5 Let E =
©
2k : k ∈ Z
ª
be the set of even integers. The function
f : Z → E defined as f (n) = 2n is easily seen to be a bijection, so we have
|Z| = |E|. Thus, as |N| = |Z| = |E|, the set E is countably infinite and |E| = ℵ0.
224 Cardinality of Sets
Here is a significant fact: The elements of any countably infinite set A
can be written in an infinitely long list a1,a2,a3,a4,... that begins with some
element a1 ∈ A and includes every element of A. For example, the set E in
the above example can be written in list form as 0,2,−2,4,−4,6,−6,8,−8,...
The reason that this can be done is as follows. Since A is countably infinite,
Definition 13.2 says there is a bijection f : N → A. This allows us to list
out the set A as an infinite list f (1), f (2), f (3), f (4),... Conversely, if the
elements of A can be written in list form as a1,a2,a3,..., then the function
f : N → A defined as f (n) = an is a bijection, so A is countably infinite. We
summarize this as follows.
Theorem 13.3 A set A is countably infinite if and only if its elements
can be arranged in an infinite list a1,a2,a3,a4,...
As an example of how this theorem might be used, let P denote the set
of all prime numbers. Since we can list its elements as 2,3,5,7,11,13,..., it
follows that the set P is countably infinite.
As another consequence of Theorem 13.3, note that we can interpret the
fact that the set R is not countably infinite as meaning that it is impossible
to write out all the elements of R in an infinite list. (After all, we tried to
do that in the table on page 219, and failed!)
This raises a question. Is it also impossible to write out all the elements
of Q in an infinite list? In other words, is the set Q of rational numbers
countably infinite or uncountable? If you start plotting the rational numbers
on the number line, they seem to mostly fill up R. Sure, some numbers
such as p
2, π and e will not be plotted, but the dots representing rational
numbers seem to predominate. We might thus expect Q to be uncountable.
However, it is a surprising fact that Q is countable. The proof presented
below arranges all the rational numbers in an infinitely long list.
Theorem 13.4 The set Q of rational numbers is countably infinite.
Proof. To prove this, we just need to show how to write the set Q in list
form. Begin by arranging all rational numbers in an infinite array. This is
done by making the following chart. The top row has a list of all integers,
beginning with 0, then alternating signs as they increase. Each column
headed by an integer k contains all the fractions (in reduced form) with
numerator k. For example, the column headed by 2 contains the fractions
2
1
,
2
3
,
2
5
,
2
7
,..., and so on. It does not contain 2
2
,
2
4
,
2
6
, etc., because those are
not reduced, and in fact their reduced forms appear in the column headed
by 1. You should examine this table and convince yourself that it contains
all rational numbers in Q.
Countable and Uncountable Sets 225
0 1 −1 2 −2 3 −3 4 −4 5 −5 ···
0
1
1
1
−1
1
2
1
−2
1
3
1
−3
1
4
1
−4
1
5
1
−5
1
···
1
2
−1
2
2
3
−2
3
3
2
−3
2
4
3
−4
3
5
2
−5
2
···
1
3
−1
3
2
5
−2
5
3
4
−3
4
4
5
−4
5
5
3
−5
3
···
1
4
−1
4
2
7
−2
7
3
5
−3
5
4
7
−4
7
5
4
−5
4
···
1
5
−1
5
2
9
−2
9
3
7
−3
7
4
9
−4
9
5
6
−5
6
···
1
6
−1
6
2
11
−2
11
3
8
−3
8
4
11
−4
11
5
7
−5
7
···
1
7
−1
7
2
13
−2
13
3
10
−3
10
4
13
−4
13
5
8
−5
8
···
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
Next, draw an infinite path in this array, beginning at 0
1
and snaking
back and forth as indicated below. Every rational number is on this path.
0 1 −1 2 −2 3 −3 4 −4 5 −5 ···
0
1
1
1
−1
1
2
1
−2
1
3
1
−3
1
4
1
−4
1
5
1
−5
1
···
1
2
−1
2
2
3
−2
3
3
2
−3
2
4
3
−4
3
5
2
−5
2
···
1
3
−1
3
2
5
−2
5
3
4
−3
4
4
5
−4
5
5
3
−5
3
···
1
4
−1
4
2
7
−2
7
3
5
−3
5
4
7
−4
7
5
4
−5
4
···
1
5
−1
5
2
9
−2
9
3
7
−3
7
4
9
−4
9
5
6
−5
6
···
1
6
−1
6
2
11
−2
11
3
8
−3
8
4
11
−4
11
5
7
−5
7
···
1
7
−1
7
2
13
−2
13
3
10
−3
10
4
13
−4
13
5
8
−5
8
···
1
8
−1
8
2
15
−2
15
3
11
−3
11
4
15
−4
15
5
9
−5
9
···
1
9
−1
9
2
17
−2
17
3
13
−3
13
4
17
−4
17
5
11
−5
11 ···
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
226 Cardinality of Sets
Beginning at 0
1
and following the path, we get an infinite list of all
rational numbers:
0, 1,
1
2
, −
1
2
, −1, 2,
2
3
,
2
5
, −
1
3
,
1
3
,
1
4
, −
1
4
,
2
7
, −
2
7
, −
2
5
, −
2
3
, −
2
3
, −2, 3,
3
2
, ...
By Theorem 13.3, it follows that Q is countably infinite, that is, |Q| = |N|. ■
It is also true that the Cartesian product of two countably infinite sets
is itself countably infinite, as our next theorem states.
Theorem 13.5 If A and B are both countably infinite, then so is A ×B.
Proof. Suppose A and B are both countably infinite. By Theorem 13.3, we
know we can write A and B in list form as
A =
©
a1,a2,a3,a4,...ª
,
B =
©
b1,b2,b3,b4,...ª
.
Figure 13.2 shows how to form an infinite path winding through all of A×B.
Therefore A ×B can be written in list form, so it is countably infinite. ■
(a1,b1)
(a1,b2)
(a1,b3)
(a1,b4)
(a1,b5)
(a1,b6)
(a1,b7)
(a2,b1)
(a2,b2)
(a2,b3)
(a2,b4)
(a2,b5)
(a2,b6)
(a2,b7)
(a3,b1)
(a3,b2)
(a3,b3)
(a3,b4)
(a3,b5)
(a3,b6)
(a3,b7)
(a4,b1)
(a4,b2)
(a4,b3)
(a4,b4)
(a4,b5)
(a4,b6)
(a4,b7)
(a5,b1)
(a5,b2)
(a5,b3)
(a5,b4)
(a5,b5)
(a5,b6)
(a5,b7)
(a6,b1)
(a6,b2)
(a6,b3)
(a6,b4)
(a6,b5)
(a6,b6)
(a6,b7)
(a7,b1)
(a7,b2)
(a7,b3)
(a7,b4)
(a7,b5)
(a7,b6)
(a7,b7)
a1 a2 a3 a4 a5 a6 a7
b1
b2
b3
b4
b5
b6
b7
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
···
···
···
···
···
···
···
···
A
B
Figure 13.2. A product of two countably infinite sets is countably infinite
Countable and Uncountable Sets 227
As an example of a consequence of this theorem, notice that since Q is
countably infinite, the set Q×Q is also countably infinite.
Recall that the word “corollary” means a result that follows easily from
some other result. We have the following corollary of Theorem 13.5.
Corollary 13.1 Given n countably infinite sets A1, A2, A3,..., An, with
n ≥ 2, the Cartesian product A1 × A2 × A3 ×···× An is also countably infinite.
Proof. The proof is by induction on n. For the basis step, notice that when
n = 2 the statement asserts that for countably infinite sets A1 and A2, the
product A1 × A2 is countably infinite, and this is true by Theorem 13.5.
Assume that for k ≥ 2, any product A1 × A2 × A3 ×··· ×Ak of countably
infinite sets is countably infinite. Consider a product A1×A2×A3×···×Ak+1
of k +1 countably infinite sets. It is easily confirmed that the function
f : A1 × A2 × A3 ×··· × Ak × Ak+1 −→ (A1 × A2 × A3 ×··· × Ak)× Ak+1
f (x1, x2,..., xk, xk+1) =
¡
(x1, x2,..., xk), xk+1
¢
is bijective, so |A1 × A2 × A3 ×···× Ak × Ak+1| = |(A1 × A2 × A3 ×···× Ak)× Ak+1|.
By the induction hypothesis, (A1 × A2 × A3 ×··· × Ak)× Ak+1 is a product of
two countably infinite sets, so it is countably infinite by Theorem 13.5. As
noted above, A1 × A2 × A3 ×···× Ak × Ak+1 has the same cardinality, so it too
is countably infinite. ■
Theorem 13.6 If A and B are both countably infinite, then A ∪ B is
countably infinite.
Proof. Suppose A and B are both countably infinite. By Theorem 13.3, we
know we can write A and B in list form as
A =
©
a1,a2,a3,a4,...ª
,
B =
©
b1,b2,b3,b4,...ª
.
We can “shuffle” A and B into one infinite list for A ∪B as follows.
A ∪B =
©
a1,b1,a2,b2,a3,b3,a4,b4,...ª
.
(We agree not to list an element twice if it belongs to both A and B.)
Therefore, by Theorem 13.3, it follows that A ∪B is countably infinite. ■
228 Cardinality of Sets
Exercises for Section 13.2
1. Prove that the set A =
©
ln(n) : n ∈ N
ª
⊆ R is countably infinite.
2. Prove that the set A =
©
(m,n) ∈ N×N : m ≤ n
ª
is countably infinite.
3. Prove that the set A =
©
(5n,−3n) : n ∈ Z
ª
is countably infinite.
4. Prove that the set of all irrational numbers is uncountable. (Suggestion:
Consider proof by contradiction using Theorems 13.4 and 13.6.)
5. Prove or disprove: There exists a countably infinite subset of the set of irrational
numbers.
6. Prove or disprove: There exists a bijective function f : Q → R.
7. Prove or disprove: The set Q
100 is countably infinite.
8. Prove or disprove: The set Z×Q is countably infinite.
9. Prove or disprove: The set ©
0,1
ª
×N is countably infinite.
10. Prove or disprove: The set A =
© p
2
n
: n ∈ N
ª
countably infinite.
11. Describe a partition of N that divides N into eight countably infinite subsets.
12. Describe a partition of N that divides N into ℵ0 countably infinite subsets.
13. Prove or disprove: If A = {X ⊆ N : X is finite}, then |A| = ℵ0.
14. Suppose A =
©
(m,n) ∈ N×R : n = πm
ª
. Is it true that |N| = |A|?
15. Theorem 13.5 implies that N×N is countably infinite. Construct an alternate
proof of this fact by showing that the function ϕ : N×N → N defined as ϕ(m,n) =
2
n−1
(2m−1) is bijective.
13.3 Comparing Cardinalities
At this point we know that there are at least two different kinds of infinity.
On one hand, there are countably infinite sets such as N, of cardinality ℵ0.
Then there is the uncountable set R. Are there other kinds of infinity
beyond these two kinds? The answer is “yes,” but to see why we first need
to introduce some new definitions and theorems.
Our first task will be to formulate a definition for what we mean by
|A| < |B|. Of course if A and B are finite we know exactly what this means:
|A| < |B| means that when the elements of A and B are counted, A is found
to have fewer elements than B. But this process breaks down if A or B is
infinite, for then the elements can’t be counted.
The language of functions helps us overcome this difficulty. Notice
that for finite sets A and B it is intuitively clear that |A| < |B| if and only
if there exists an injective function f : A → B but there are no surjective
functions f : A → B. The following diagram illustrates this:
Comparing Cardinalities 229
d
c
b
a
4
3
2
1
0
A B
f
We will use this idea to define what is meant by |A| < |B| and |A| ≤ |B|. For
emphasis, the following definition also restates what is meant by |A| = |B|.
Definition 13.4 Suppose A and B are sets.
(1) |A| = |B| means there is a bijection A → B.
(2) |A| < |B| means there is an injection A → B, but no surjection A → B.
(3) |A| ≤ |B| means |A| < |B| or |A| = |B|.
For example, consider N and R. The function f : N → R defined as f (n) = n
is clearly injective, but it is not surjective because given the element 1
2
∈ R,
we have f (n) 6=
1
2
for every n ∈ N. In fact, Theorem 13.2 of Section 13.1
asserts that there is no surjection N → R. Definition 13.4 yields
|N| < |R|. (13.1)
Said differently, ℵ0 < |R|.
Is there a set X for which |R| < |X|? The answer is “yes,” and the next
theorem explains why. It implies |R| < |P(R)|. (Recall that P(A) denotes
the power set of A.)
Theorem 13.7 If A is any set, then |A| < |P(A)|.
Proof. Before beginning the proof, we remark that this statement is obvious
if A is finite, for then |A| < 2
|A| = |P(A)|. But our proof must apply to all
sets A, both finite and infinite, so it must use Definition 13.4.
We prove the theorem with direct proof. Let A be an arbitrary set.
According to Definition 13.4, to prove |A| < |P(A)| we must show that there
is an injection f : A → P(A), but no surjection f : A → P(A).
To see that there is an injection f : A → P(A), define f by the rule
f (x) =
©
x
ª
. In words, f sends any element x of A to the one-element set
©
x
ª
∈ P(A). Then f : A → P(A) is injective, as follows. Suppose f (x) = f (y).
Then ©
x
ª
=
©
y
ª
. Now, the only way that ©
x
ª
and ©
y
ª
can be equal is if x = y,
so it follows that x = y. Thus f is injective.
Next we need to show that there exists no surjection f : A → P(A).
Suppose for the sake of contradiction that there does exist a surjection
230 Cardinality of Sets
f : A → P(A). Notice that for any element x ∈ A, we have f (x) ∈ P(A), so
f (x) is a subset of A. Thus f is a function that sends elements of A to
subsets of A. It follows that for any x ∈ A, either x is an element of the
subset f (x) or it is not. Using this idea, define the following subset B of A:
B =
©
x ∈ A : x ∉ f (x)
ª
⊆ A.
Now since B ⊆ A we have B ∈ P(A), and since f is surjective there is an
a ∈ A for which f (a) = B. Now, either a ∈ B or a ∉ B. We will consider these
two cases separately, and show that each leads to a contradiction.
Case 1. If a ∈ B, then the definition of B implies a ∉ f (a), and since f (a) = B
we have a ∉ B, which is a contradiction.
Case 2. If a ∉ B, then the definition of B implies a ∈ f (a), and since f (a) = B
we have a ∈ B, again a contradiction.
Since the assumption that there is a surjection f : A → P(A) leads to a
contradiction, we conclude that there are no such surjective functions.
In conclusion, we have seen that there exists an injection A → P(A) but
no surjection A → P(A), so Definition 13.4 implies that |A| < |P(A)|. ■
Beginning with the set A = N and applying Theorem 13.7 over and over
again, we get the following chain of infinite cardinalities.
ℵ0 = |N| < |P(N)| < |P(P(N))| < |P(P(P(N)))| < ··· (13.2)
Thus there is an infinite sequence of different types of infinity, starting
with ℵ0 and becoming ever larger. The set N is countable, and all the sets
P(N), P(P(N)), etc., are uncountable.
In the next section we will prove that |P(N)| = |R|. Thus |N| and |R|
are the first two entries in the chain (13.2) above. They are are just two
relatively tame infinities in a long list of other wild and exotic infinities.
Unless you plan on studying advanced set theory or the foundations
of mathematics, you are unlikely to ever encounter any types of infinity
beyond ℵ0 and |R|. Still you will in future mathematics courses need to
distinguish between countably infinite and uncountable sets, so we close
with two final theorems that can help you do this.
Theorem 13.8 An infinite subset of a countably infinite set is countably
infinite.
Proof. Suppose A is an infinite subset of the countably infinite set B.
Because B is countably infinite, its elements can be written in a list
Comparing Cardinalities 231
b1,b2,b3,b4,... Then we can also write A’s elements in list form by proceeding
through the elements of B, in order, and selecting those that belong to
A. Thus A can be written in list form, and since A is infinite, its list will
be infinite. Consequently A is countably infinite. ■
Theorem 13.9 If U ⊆ A, and U is uncountable, then A is uncountable.
Proof. Suppose for the sake of contradiction that U ⊆ A, and U is uncountable
but A is not uncountable. Then since U ⊆ A and U is infinite, then A
must be infinite too. Since A is infinite, and not uncountable, it must be
countably infinite. Then U is an infinite subset of a countably infinite set
A, so U is countably infinite by Theorem 13.8. Thus U is both uncountable
and countably infinite, a contradiction. ■
Theorems 13.8 and 13.9 can be useful when we need to decide whether
a set is countably infinite or uncountable. They sometimes allow us to
decide its cardinality by comparing it to a set whose cardinality is known.
For example, suppose we want to decide whether or not the set A = R
2
is uncountable. Since the x-axis U =
©
(x,0) : x ∈ R
ª
⊆ R
2 has the same
cardinality as R, it is uncountable. Theorem 13.9 implies that R
2
is
uncountable. Other examples can be found in the exercises.
Exercises for Section 13.3
1. Suppose B is an uncountable set and A is a set. Given that there is a surjective
function f : A → B, what can be said about the cardinality of A?
2. Prove that the set C of complex numbers is uncountable.
3. Prove or disprove: If A is uncountable, then |A| = |R|.
4. Prove or disprove: If A ⊆ B ⊆ C and A and C are countably infinite, then B is
countably infinite.
5. Prove or disprove: The set ©
0,1
ª
×R is uncountable.
6. Prove or disprove: Every infinite set is a subset of a countably infinite set.
7. Prove or disprove: If A ⊆ B and A is countably infinite and B is uncountable,
then B − A is uncountable.
8. Prove or disprove: The set ©
(a1,a2,a3,...) : ai ∈ Z} of infinite sequences of integers
is countably infinite.
9. Prove that if A and B are finite sets with |A| = |B|, then any injection f : A → B
is also a surjection. Show this is not necessarily true if A and B are not finite.
10. Prove that if A and B are finite sets with |A| = |B|, then any surjection f : A → B
is also an injection. Show this is not necessarily true if A and B are not finite.
232 Cardinality of Sets
13.4 The Cantor-Bernstein-Schröeder Theorem
An often used property of numbers is that if a ≤ b and b ≤ a, then a = b. It
is reasonable to ask if the same property applies to cardinality. If |A| ≤ |B|
and |B| ≤ |A|, is it true that |A| = |B|? This is in fact true, and this section’s
goal is to prove it. This will yield an alternate (and highly effective) method
of proving that two sets have the same cardianlity.
Recall (Definition 13.4) that |A| ≤ |B| means that |A| < |B| or |A| = |B|. If
|A| < |B| then (by Definition 13.4) there is an injection A → B. On the other
hand, if |A| = |B|, then there is a bijection (hence also an injection) A → B.
Thus |A| ≤ |B| implies that there is an injection f : A → B.
Likewise, |B| ≤ |A| implies that there is an injection g : B → A.
Our aim is to show that if |A| ≤ |B| and |B| ≤ |A|, then |A| = |B|. In
other words, we aim to show that if there are injections f : A → B and
g : B → A, then there is a bijection h : A → B. The proof of this fact, though
not particularly difficult, is not entirely trivial, either. The fact that f and
g guarantee that such an h exists is called the the Cantor-BernsteinSchröeder
theorem. This theorem is very useful for proving two sets A
and B have the same cardinality: it says that instead of finding a bijection
A → B, it suffices to find injections A → B and B → A. This is useful because
injections are often easier to find than bijections.
We will prove the Cantor-Bernstein-Schröeder theorem, but before
doing so let’s work through an informal visual argument that will guide
us through (and illustrate) the proof.
Suppose there are injections f : A → B and g : B → A. We want to use
them to produce a bijection h : A → B. Sets A and B are sketched below.
For clarity, each has the shape of the letter that denotes it, and to help
distinguish them the set A is shaded.
A B
Figure 13.3. The sets A and B
The injections f : A → B and g : B → A are illustrated in Figure 13.4.
Think of f as putting a “copy” f (A) =
©
f (x) : x ∈ A
ª
of A into B, as illustrated.
This copy, the range of f , does not fill up all of B (unless f happens to be
surjective). Likewise, g puts a “copy” g(B) of B into A. Because they are
The Cantor-Bernstein-Schröeder Theorem 233
not necessarily bijective, neither f nor g is guaranteed to have an inverse.
But the map g : B → g(B) from B to g(B) = {g(x) : x ∈ B} is bijective, so there
is an inverse g
−1
: g(B) → B. (We will need this inverse soon.)
f
g
g
−1
Figure 13.4. The injections f : A → B and g : B → A
Consider the chain of injections illustrated in Figure 13.5. On the left,
g puts a copy of B into A. Then f puts a copy of A (containing the copy of
B) into B. Next, g puts a copy of this B-containing-A-containing-B into A,
and so on, always alternating g and f .
g g g
f f f
···
Figure 13.5. An infinite chain of injections
The first time A occurs in this sequence, it has a shaded region A− g(B).
In the second occurrence of A, the shaded region is (A−g(B))∪(g◦ f )(A−g(B)).
In the third occurrence of A, the shaded region is
(A − g(B)) ∪ (g ◦ f )(A − g(B)) ∪ (g ◦ f ◦ g ◦ f )(A − g(B)).
To tame the notation, let’s say (g ◦ f )
2 = (g ◦ f ) ◦ (g ◦ f ), and (g ◦ f )
3 =
(g◦ f )◦(g◦ f )◦(g◦ f ), and so on. Let’s also agree that (g◦ f )
0 = ιA, that is, it is
the identity function on A. Then the shaded region of the nth occurrence
of A in the sequence is
n[−1
k=0
(g ◦ f )
k
(A − g(B)).
This process divides A into gray and white regions: the gray region is
G =
[∞
k=0
(g ◦ f )
k
(A − g(B)),
234 Cardinality of Sets
and the white region is A −G. (See Figure 13.6.)
Figure 13.6 suggests our desired bijection h : A → B. The injection f
sends the gray areas on the left bijectively to the gray areas on the right.
The injection g
−1
: g(B) → B sends the white areas on the left bijectively
to the white areas on the right. We can thus define h : A → B so that
h(x) = f (x) if x is a gray point, and h(x) = g
−1
(x) if x is a white point.
f
g
−1
f
g
−1
.
.
.
A B
Figure 13.6. The bijection h : A → B
This informal argument suggests that given injections f : A → B and
g : B → A, there is a bijection h : A → B. But it is not a proof. We now
present this as a theorem and tighten up our reasoning in a careful proof,
with the above diagrams and ideas as a guide.
Theorem 13.10 (The Cantor-Bernstein-Schröeder Theorem)
If |A| ≤ |B| and |B| ≤ |A|, then |A| = |B|. In other words, if there are injections
f : A → B and g : B → A, then there is a bijection h : A → B.
Proof. (Direct) Suppose there are injections f : A → B and g : B → A. Then,
in particular, g : B → g(B) is a bijection from B onto the range of g, so it
has an inverse g
−1
: g(B) → B. (Note that g : B → A itself has no inverse
g
−1
: A → B unless g is surjective.) Consider the subset
G =
[∞
k=0
(g ◦ f )
k
(A − g(B)) ⊆ A.
The Cantor-Bernstein-Schröeder Theorem 235
Let W = A −G, so A = G ∪W is partitioned into two sets G (think gray) and
W (think white). Define a function h : A → B as
h(x) =
(
f (x) if x ∈ G
g
−1
(x) if x ∈ W.
Notice that this makes sense: if x ∈ W, then x ∉ G, so x ∉ A − g(B) ⊆ G, hence
x ∈ g(B), so g
−1
(x) is defined.
To finish the proof, we must show that h is both injective and surjective.
For injective, we assume h(x) = h(y), and deduce x = y. There are three
cases to consider. First, if x and y are both in G, then h(x) = h(y) means
f (x) = f (y), so x = y because f is injective. Second, if x and y are both in W,
then h(x) = h(y) means g
−1
(x) = g
−1
(y), and applying g to both sides gives
x = y. In the third case, one of x and y is in G and the other is in W.
Say x ∈ G and y ∈ W. The definition of G gives x = (g ◦ f )
k
(z) for some
k ≥ 0 and z ∈ A − g(B). Note h(x) = h(y) now implies f (x) = g
−1
(y), that is,
f ((g ◦ f )
k
(z)) = g
−1
(y). Applying g to both sides gives (g ◦ f )
k+1
(z) = y, which
means y ∈ G. But this is impossible, as y ∈ W. Thus this third case cannot
happen. But in the first two cases h(x) = h(y) implies x = y, so h is injective.
To see that h is surjective, take any b ∈ B. We will find an x ∈ A with
h(x) = b. Note that g(b) ∈ A, so either g(b) ∈ W or g(b) ∈ G. In the first case,
h(g(b)) = g
−1
(g(b)) = b, so we have an x = g(b) ∈ A for which h(x) = b. In the
second case, g(b) ∈ G. The definition of G shows
g(b) = (g ◦ f )
k
(z)
for some k > 0, and z ∈ A − g(B). Thus
g(b) = (g ◦ f ) ◦ (g ◦ f )
k−1
(z).
Rewriting this,
g(b) = g
³
f
¡
(g ◦ f )
k−1
(z)
¢
´
.
Because g is injective, this implies
b = f
¡
(g ◦ f )
k−1
(z)
¢
.
Let x = (g ◦ f )
k−1
(z), so x ∈ G by definition of G. Observe that h(x) = f (x) =
f
¡
(g ◦ f )
k−1
(z)
¢
= b. We have now seen that for any b ∈ B, there is an x ∈ A
for which h(x) = b. Thus h is surjective.
Since h : A → B is both injective and surjective, it is also bijective. ■
236 Cardinality of Sets
Here are some examples illustrating how the Cantor-Bernstein-Schröeder
theorem can be used. This includes a proof that |R| = |P(N)|.
Example 13.6 The intervals [0,1) and (0,1) in R have equal cardinalities.
Surely this fact is plausible, for the two intervals are identical except for
the endpoint 0. Yet concocting a bijection [0,1) → (0,1) is tricky. (Though
not particularly difficult: see the solution of Exercise 11 of Section 13.1.)
For a simpler approach, note that f (x) =
1
4
+
1
2
x is an injection [0,1) → (0,1).
Also, g(x) = x is an injection (0,1) → [0,1). The Cantor-Bernstein-Schröeder
theorem guarantees a bijection h : [0,1) → (0,1), so |[0,1)| = |(0,1)|.
Theorem 13.11 The sets R and P(N) have the same cardinality.
Proof. Example 13.4 shows that |R| = |(0,1)|, and Example 13.6 shows
|(0,1)| = |[0,1)|. Thus |R| = |[0,1)|, so to prove the theorem we just need to
show that |[0,1)| = |P(N)|. By the Cantor-Bernstein-Schröeder theorem, it
suffices to find injections f : [0,1) → P(N) and g : P(N) → [0,1).
To define f : [0,1) → P(N), we use the fact that any number in [0,1) has
a unique decimal representation 0.b1b2b3b4 ..., where each bi one of the
digits 0,1,2,...,9, and there is not a repeating sequence of 9’s at the end.
(Recall that, e.g., 0.359999 = 0.360, etc.) Define f : [0,1) → P(N) as
f
¡
0.b1b2b3b4 ...¢
=
©
10b1, 102
b2, 103
b3, ...ª
.
For example, f (0.121212) =
©
10,200,1000,20000,100000,...ª
, and f (0.05) =
©
0,500ª
. Also f (0.5) = f (0.50) =
©
0,50ª
. To see that f is injective, take two
unequal numbers 0.b1b2b3b4 ... and 0.d1d2d3d4 ... in [0,1). Then bi 6= di for
some index i. Hence bi10i ∈ f (0.b1b2b3b4 ...) but bi10i ∉ f (0.d1d2d3d4 ...), so
f (0.b1b2b3b4 ...) 6= f (0.d1d2d3d4 ...). Consequently f is injective.
Next, define g : P(N) → [0,1), where g(X) = 0.b1b2b3b4 ... is the number
for which bi = 1 if i ∈ X and bi = 0 if i ∉ X. For example, g
¡©1,3
ª¢ = 0.101000,
and g
¡©2,4,6,8,...ª¢ = 0.01010101. Also g(;) = 0 and g(N) = 0.1111. To see
that g is injective, suppose X 6= Y. Then there is at least one integer i
that belongs to one of X or Y, but not the other. Consequently g(X) 6= g(Y)
because they differ in the ith decimal place. This shows g is injective.
From the injections f : [0,1) → P(N) and g : P(N) → [0,1), the CantorBernstein-Schröeder
theorem guarantees a bijection h : [0,1) → P(N). Hence
|[0,1)| = |P(N)|. As |R| = |[0,1)|, we conclude |R| = |P(N)|. ■
The Cantor-Bernstein-Schröeder Theorem 237
We know that |R| 6= |N|. But we just proved |R| = |P(N)|. This suggests
that the cardinality of R is not “too far” from |N| = ℵ0. We close with a few
informal remarks on this mysterious relationship between ℵ0 and |R|.
We established earlier in this chapter that ℵ0 < |R|. For nearly a century
after Cantor formulated his theories on infinite sets, mathematicians
struggled with the question of whether or not there exists a set A for which
ℵ0 < |A| < |R|.
It was commonly suspected that no such set exists, but no one was able
to prove or disprove this. The assertion that no such A exists came to be
called the continuum hypothesis.
Theorem 13.11 states that |R| = |P(N)|. Placing this in the context of
the chain (13.2) on page 230, we have the following relationships.
ℵ0 |R|
=
=
|N| < |P(N)| < |P(P(N))| < |P(P(P(N)))| < ···
From this, we can see that the continuum hypothesis asserts that no set
has a cardinality between that of N and its power set.
Although this may seem intuitively plausible, it eluded proof since
Cantor first posed it in the 1880s. In fact, the real state of affairs is
almost paradoxical. In 1931, the logician Kurt Gödel proved that for any
sufficiently strong and consistent axiomatic system, there exist statements
which can neither be proved nor disproved within the system.
Later he proved that the negation of the continuum hypothesis cannot
be proved within the standard axioms of set theory (i.e., the ZermeloFraenkel
axioms, mentioned in Section 1.10). This meant that either the
continuum hypothesis is false and cannot be proven false, or it is true.
In 1964, Paul Cohen discovered another startling truth: Given the laws
of logic and the axioms of set theory, no proof can deduce the continuum
hypothesis. In essence he proved that the continuum hypothesis cannot be
proved.
Taken together, Gödel and Cohens’ results mean that the standard
axioms of mathematics cannot “decide” whether the continuum hypothesis
is true or false; that no logical conflict can arise from either asserting or
denying the continuum hypothesis. We are free to either accept it as true
or accept it as false, and the two choices lead to different—but equally
consistent—versions of set theory.
238 Cardinality of Sets
On the face of it, this seems to undermine the foundation of logic, and
everything we have done in this book. The continuum hypothesis should
be a statement – it should be either true or false. How could it be both?
Here is an analogy that may help make sense of this. Consider the
number systems Zn. What if we asked whether [2] = [0] is true or false? Of
course the answer depends on n. The expression [2] = [0] is true in Z2 and
false in Z3. Moreover, if we assert that [2] = [0] is true, we are logically
forced to the conclusion that this is taking place in the system Z2. If we
assert that [2] = [0] is false, then we are dealing with some other Zn. The
fact that [2] = [0] can be either true or false does not necessarily mean
that there is some inherent inconsistency within the individual number
systems Zn. The equation [2] = [0] is a true statement in the “universe” of
Z2 and a false statement in the universe of (say) Z3.
It is the same with the continuum hypothesis. Saying it’s true leads to
one system of set theory. Saying it’s false leads to some other system of set
theory. Gödel and Cohens’ discoveries mean that these two types of set
theory, although different, are equally consistent and valid mathematical
universes.
So what should you believe? Fortunately, it does not make much
difference, because most important mathematical results do not hinge on
the continuum hypothesis. (They are true in both universes.) Unless you
undertake a deep study of the foundations of mathematics, you will be fine
accepting the continuum hypothesis as true. Most mathematicians are
agnostics on this issue, but they tend to prefer the version of set theory in
which the continuum hypothesis holds.
The situation with the continuum hypothesis is a testament to the
immense complexity of mathematics. It is a reminder of the importance
of rigor and careful, systematic methods of reasoning that begin with the
ideas introduced in this book.
Exercises for Section 13.4
1. Show that if A ⊆ B and there is an injection g : B → A, then |A| = |B|.
2. Show that |R
2
| = |R|. Suggestion: Begin by showing |(0,1)×(0,1)| = |(0,1)|.
3. Let F be the set of all functions N →
©
0,1
ª
. Show that |R| = |F|.
4. Let F be the set of all functions R →
©
0,1
ª
. Show that |R| < |F|.
5. Consider the subset B =
©
(x, y) : x
2 + y
2 ≤ 1
ª
⊆ R
2
. Show that |B| = |R
2
|.
6. Show that |P(N×N)| = |P(N)|.
7. Prove or disprove: If there is a injection f : A → B and a surjection g : A → B,
then there is a bijection h : A → B.
Conclusion
I
f you have internalized the ideas in this book, then you have a set
of rhetorical tools for deciphering and communicating mathematics.
These tools are indispensable at any level. But of course it takes more
than mere tools to build something. Planning, creativity, inspiration, skill,
talent, intuition, passion and persistence are also vitally important. It
is safe to say that if you have come this far, then you probably possess a
sufficient measure of these traits.
The quest to understand mathematics has no end, but you are well
equipped for the journey. It is my hope that the things you have learned
from this book will lead you to a higher plane of understanding, creativity
and expression.
Good luck and best wishes.
R.H.
Solutions
Chapter 1 Exercises
Section 1.1
1. {5x−1 : x ∈ Z} = {...−11,−6,−1,4,9,14,19,24,29,...}
3. {x ∈ Z : −2 ≤ x < 7} = {−2,−1,0,1,2,3,4,5,6}
5. ©
x ∈ R : x
2 = 3
ª
=
©
−
p
3,
p
3
ª
7. ©
x ∈ R : x
2 +5x = −6
ª
= {−2,−3}
9. {x ∈ R : sinπx = 0} = {...,−2,−1,0,1,2,3,4,...} = Z
11. {x ∈ Z : |x| < 5} = {−4,−3,−2,−1,0,1,2,3,4}
13. {x ∈ Z : |6x| < 5} = {0}
15. {5a+2b : a,b ∈ Z} = {...,−2,−1,0,1,2,3,...} = Z
17. {2,4,8,16,32,64...} = {2
x
: x ∈ N}
19. {...,−6,−3,0,3,6,9,12,15,...} = {3x : x ∈ Z}
21. {0,1,4,9,16,25,36,...} =
©
x
2
: x ∈ Z
ª
23. {3,4,5,6,7,8} = {x ∈ Z : 3 ≤ x ≤ 8} = {x ∈ N : 3 ≤ x ≤ 8}
25. ©
...,
1
8
,
1
4
,
1
2
,1,2,4,8,...ª
= {2
n
: n ∈ Z}
27. ©
...,−π,−
π
2
,0,
π
2
,π,
3π
2
,2π,
5π
2
,...ª
=
n
kπ
2
: k ∈ Z
o
29. |{{1},{2,{3,4}},;}| = 3
31. |{{{1},{2,{3,4}},;}}| = 1
33. |{x ∈ Z : |x| < 10}| = 19
35. |
©
x ∈ Z : x
2 < 10ª
| = 7
37. |
©
x ∈ N : x
2 < 0
ª
| = 0
39. {(x, y) : x ∈ [1,2], y ∈ [1,2]}
−3 −2 −1 1 2 3
−2
−1
1
2
41. {(x, y) : x ∈ [−1,1], y = 1}
−3 −2 −1 1 2 3
−2
−1
1
2
43. {(x, y) : |x| = 2, y ∈ [0,1]}
−3 −2 −1 1 2 3
−2
−1
1
2
45. ©
(x, y) : x, y ∈ R, x
2 + y
2 = 1
ª
−3 −2 −1 1 2 3
−2
−1
1
2
241
47. ©
(x, y) : x, y ∈ R, y ≥ x
2 −1
ª
−3 −2 −1 1 2 3
−3
−2
−1
1
2
3
49. {(x, x+ y) : x ∈ R, y ∈ Z}
−3 −2 −1 1 2 3
−3
−2
−1
1
2
3
51. {(x, y) ∈ R
2
: (y− x)(y+ x) = 0}
−3 −2 −1 1 2 3
−3
−2
−1
1
2
3
Section 1.2
1. Suppose A = {1,2,3,4} and B = {a, c}.
(a) A ×B = {(1,a),(1, c),(2,a),(2, c),(3,a),(3, c),(4,a),(4, c)}
(b) B × A = {(a,1),(a,2),(a,3),(a,4),(c,1),(c,2),(c,3),(c,4)}
(c) A × A = {(1,1),(1,2),(1,3),(1,4),(2,1),(2,2),(2,3),(2,4),
(3,1),(3,2),(3,3),(3,4),(4,1),(4,2),(4,3),(4,4)}
(d) B ×B = {(a,a),(a, c),(c,a),(c, c)}
(e) ; ×B = {(a,b) : a ∈ ;,b ∈ B} = ; (There are no ordered pairs (a,b) with a ∈ ;.)
(f) (A ×B)×B =
{((1,a),a),((1, c),a),((2,a),a),((2, c),a),((3,a),a),((3, c),a),((4,a),a),((4, c),a),
((1,a), c),((1, c), c),((2,a), c),((2, c), c),((3,a), c),((3, c), c),((4,a), c),((4, c), c)}
(g) A ×(B ×B) = ©
(1,(a,a)),(1,(a, c)),(1,(c,a)),(1,(c, c)),
(2,(a,a)),(2,(a, c)),(2,(c,a)),(2,(c, c)),
(3,(a,a)),(3,(a, c)),(3,(c,a)),(3,(c, c)),
(4,(a,a)),(4,(a, c)),(4,(c,a)),(4,(c, c))ª
(h) B
3 = {(a,a,a),(a,a, c),(a, c,a),(a, c, c),(c,a,a),(c,a, c),(c, c,a),(c, c, c)}
3. ©
x ∈ R : x
2 = 2
ª
×{a, c, e} =
©
(−
p
2,a),(
p
2,a),(−
p
2, c),(
p
2, c),(−
p
2, e),(
p
2, e)
ª
5. ©
x ∈ R : x
2 = 2
ª
×{x ∈ R : |x| = 2} =
©
(−
p
2,−2),(
p
2,2),(−
p
2,2),(
p
2,−2)ª
7. {;}×{0,;}×{0,1} = {(;,0,0),(;,0,1),(;,;,0),(;,;,1)}
242 Solutions
Sketch the following Cartesian products on the x-y plane.
9. {1,2,3}×{−1,0,1}
−3 −2 −1 1 2 3
−2
−1
1
2
11. [0,1]×[0,1]
−3 −2 −1 1 2 3
−2
−1
1
2
13. {1,1.5,2}×[1,2]
−3 −2 −1 1 2 3
−2
−1
1
2
15. {1}×[0,1]
−3 −2 −1 1 2 3
−2
−1
1
2
17. N×Z
−3 −2 −1 1 2 3
−2
−1
1
2
19. [0,1]×[0,1]×[0,1]
−3 −2 −1 1 2 3
−2
−1
1
2
Section 1.3
A. List all the subsets of the following sets.
1. The subsets of {1,2,3,4} are: {}, {1}, {2}, {3}, {4}, {1,2}, {1,3}, {1,4}, {2,3}, {2,4},
{3,4}, {1,2,3}, {1,2,4}, {1,3,4}, {2,3,4}, {1,2,3,4}.
3. The subsets of {{R}} are: {} and {{R}}.
5. The subsets of {;} are {} and {;}.
7. The subsets of {R,{Q,N}} are {}, {R},{{Q,N}}, {R,{Q,N}}.
B. Write out the following sets by listing their elements between braces.
9. ©
X : X ⊆ {3,2,a} and |X| = 2
ª
= {{3,2},{3,a},{2,a}}
11. ©
X : X ⊆ {3,2,a} and |X| = 4
ª
= {} = ;
C. Decide if the following statements are true or false.
13. R
3 ⊆ R
3
is true because any set is a subset of itself.
15. ©
(x, y) : x −1 = 0
ª
⊆
©
(x, y) : x
2 − x = 0
ª
. This is true. (The even-numbered ones
are both false. You have to explain why.)
243
Section 1.4
A. Find the indicated sets.
1. P({{a,b},{c}}) = {;,{{a,b}},{{c}},{{a,b},{c}}}
3. P({{;},5}) = {;,{{;}},{5},{{;},5}}
5. P(P({2})) = {;,{;},{{2}},{;,{2}}}
7. P({a,b})×P({0,1}) = ©
(;,;), (;,{0}), (;,{1}), (;,{0,1}),
({a},;), ({a},{0}), ({a},{1}), ({a},{0,1}),
({b},;), ({b},{0}), ({b},{1}), ({b},{0,1}),
({a,b},;), ({a,b},{0}), ({a,b},{1}), ({a,b},{0,1})
ª
9. P({a,b}×{0}) = {;,{(a,0)},{(b,0)},{(a,0),(b,0)}}
11. {X ⊆ P({1,2,3}) : |X| ≤ 1} =
{;,{;},{{1}},{{2}},{{3}},{{1,2}},{{1,3}},{{2,3}},{{1,2,3}}}
B. Suppose that |A| = m and |B| = n. Find the following cardinalities.
13. |P(P(P(A)))| = 2
³
2
(2m)
´
15. |P(A ×B)| = 2
mn
17. |{X ∈ P(A) : |X| ≤ 1}| = m+1
19. |P(P(P(A × ;)))| = |P(P(P(;)))| = 4
Section 1.5
1. Suppose A = {4,3,6,7,1,9}, B = {5,6,8,4} and C = {5,8,4}. Find:
(a) A ∪B = {1,3,4,5,6,7,8,9}
(b) A ∩B = {4,6}
(c) A −B = {3,7,1,9}
(d) A −C = {3,6,7,1,9}
(e) B − A = {5,8}
(f) A ∩C = {4}
(g) B ∩C = {5,8,4}
(h) B ∪C = {5,6,8,4}
(i) C −B = ;
3. Suppose A = {0,1} and B = {1,2}. Find:
(a) (A ×B)∩(B ×B) = {(1,1),(1,2)}
(b) (A ×B)∪(B ×B) = {(0,1),(0,2),(1,1),(1,2),(2,1),(2,2)}
(c) (A ×B)−(B ×B) = {(0,1),(0,2)}
(d) (A ∩B)× A = {(1,0),(1,1)}
(e) (A ×B)∩B = ;
(f) P(A)∩P(B) = {;,{1}}
(g) P(A)−P(B) = {{0},{0,1}}
(h) P(A ∩B) = {{},{1}}
(i) ©
;,{(0,1)},{(0,2)},{(1,1)},{(1,2)},{(0,1),(0,2)},{(0,1),(1,1)},{(0,1),(1,2)},{(0,2),(1,1)},
{(0,2),(1,2)},{(1,1),(1,2)},{(0,2),(1,1),(1,2)},{(0,1),(1,1),(1,2)},{(0,1),(0,2),(1,2)},
{(0,1),(0,2),(1,1)},{(0,1),(0,2),(1,1),(1,2)}
ª
244 Solutions
5. Sketch the sets X = [1,3]×[1,3] and Y = [2,4]×[2,4] on the plane R
2
. On separate
drawings, shade in the sets X ∪Y, X ∩Y, X −Y and Y − X. (Hint: X and Y are
Cartesian products of intervals. You may wish to review how you drew sets
like [1,3]×[1,3] in the Section 1.2.)
Y
X
X ∪Y
X ∩Y X −Y
Y − X
1 2 3 4
1
2
3
4
1 2 3 4
1
2
3
4
1 2 3 4
1
2
3
4
1 2 3 4
1
2
3
4
1 2 3 4
1
2
3
4
7. Sketch the sets X =
©
(x, y) ∈ R
2
: x
2 + y
2 ≤ 1
ª
and Y =
©
(x, y) ∈ R
2
: x ≥ 0
ª
on R
2
. On
separate drawings, shade in the sets X ∪Y, X ∩Y, X −Y and Y − X.
X
Y X ∪Y X ∩Y X −Y Y − X
−2 −1 1 2
−2
−1
1
2
−2 −1 1 2
−2
−1
1
2
−2 −1 1 2
−2
−1
1
2
−2 −1 1 2
−2
−1
1
2
−2 −1 1 2
−2
−1
1
2
9. The first statement is true. (A picture should convince you; draw one if
necessary.) The second statement is false: Notice for instance that (0.5,0.5) is
in the right-hand set, but not the left-hand set.
Section 1.6
1. Suppose A = {4,3,6,7,1,9} and B = {5,6,8,4} have universal set U = {n ∈ Z : 0 ≤ n ≤ 10}.
(a) A = {0,2,5,8,10}
(b) B = {0,1,2,3,7,9,10}
(c) A ∩ A = ;
(d) A ∪ A = {0,1,2,3,4,5,6,7,8,9,10} = U
(e) A − A = A
(f) A −B = {4,6}
(g) A −B = {5,8}
(h) A ∩B = {5,8}
(i) A ∩B = {0,1,2,3,4,6,7,9,10}
3. Sketch the set X = [1,3]×[1,2] on the plane R
2
. On separate drawings, shade in
the sets X, and X ∩([0,2]×[0,3]).
X
X
X ∩([0,2]×[0,3])
−1 1 2 3
−1
1
2
3
−1 1 2 3
−1
1
2
3
−1 1 2 3
−1
1
2
3
5. Sketch the set X =
©
(x, y) ∈ R
2
: 1 ≤ x
2 + y
2 ≤ 4
ª
on the plane R
2
. On a separate
drawing, shade in the set X.
245
X
X
1 2 3
1
2
3
1 2 3
1
2
3
Solution of 1.6, #5.
A
A (shaded)
U
Solution of 1.7, #1.
Section 1.7
1. Draw a Venn diagram for A. (Solution above right)
3. Draw a Venn diagram for (A −B)∩C.
Scratch work is shown on the right. The
set A −B is indicated with vertical shading.
The set C is indicated with horizontal shading.
The intersection of A −B and C is thus
the overlapping region that is shaded with
both vertical and horizontal lines. The final
answer is drawn on the far right, where the
set (A −B)∩C is shaded in gray.
A B A B
C C
5. Draw Venn diagrams for A∪(B∩C) and (A∪B)∩(A∪C). Based on your drawings,
do you think A ∪(B ∩C) = (A ∪B)∩(A ∪C)?
If you do the drawings carefully, you will find
that your Venn diagrams are the same for both
A ∪ (B ∩ C) and (A ∪ B) ∩ (A ∪ C). Each looks as
illustrated on the right. Based on this, we are
inclined to say that the equation A ∪(B ∩ C) =
(A ∪B)∩(A ∪C) holds for all sets A, B and C. A B
C
7. Suppose sets A and B are in a universal set U. Draw Venn diagrams for A ∩B
and A ∪B. Based on your drawings, do you think it’s true that A ∩B = A ∪B?
The diagrams for A ∩B and A ∪ B look exactly
alike. In either case the diagram is the shaded
region illustrated on the right. Thus we would
expect that the equation A ∩B = A ∪ B is true
for any sets A and B.
A B
U
9. Draw a Venn diagram for (A ∩B)−C.
A B
C
11. The simplest answer is (B ∩C)− A.
13. One answer is (A ∪B ∪C)−(A ∩B ∩C).
246 Solutions
Section 1.8
1. Suppose A1 = {a,b,d, e, g, f }, A2 = {a,b, c,d}, A3 = {b,d,a} and A4 = {a,b,h}.
(a) [
4
i=1
Ai = {a,b, c,d, e, f , g,h} (b) \
4
i=1
Ai = {a,b}
3. For each n ∈ N, let An = {0,1,2,3,...,n}.
(a) [
i∈N
Ai = {0}∪N (b) \
i∈N
Ai = {0,1}
5. (a) [
i∈N
[i,i +1] =[1,∞) (b) \
i∈N
[i,i +1] =;
7. (a) [
i∈N
R×[i,i +1] = {(x, y) : x, y ∈ R, y ≥ 1} (b) \
i∈N
R×[i,i +1] = ;
9. (a) [
X∈P(N)
X = N (b) \
X∈P(N)
X = ;
11. Yes, this is always true.
13. The first is true, the second is false.
Chapter 2 Exercises
Section 2.1
Decide whether or not the following are statements. In the case of a statement,
say if it is true or false.
1. Every real number is an even integer. (Statement, False)
3. If x and y are real numbers and 5x = 5y, then x = y. (Statement, True)
5. Sets Z and N are infinite. (Statement, True)
7. The derivative of any polynomial of degree 5 is a polynomial of degree 6.
(Statement, False)
9. cos(x) = −1
This is not a statement. It is an open sentence because whether it’s true or
false depends on the value of x.
11. The integer x is a multiple of 7.
This is an open sentence, and not a statement.
13. Either x is a multiple of 7, or it is not.
This is a statement, for the sentence is true no matter what x is.
15. In the beginning God created the heaven and the earth.
This is a statement, for it is either definitely true or definitely false. There is
some controversy over whether it’s true or false, but no one claims that it is
neither true nor false.
247
Section 2.2
Express each statement as one of the forms P ∧Q, P ∨Q, or ∼ P. Be sure to also
state exactly what statements P and Q stand for.
1. The number 8 is both even and a power of 2.
P ∧Q
P: 8 is even
Q: 8 is a power of 2
Note: Do not say “Q: a power of 2,” because that is not a statement.
3. x 6= y ∼ (x = y) (Also ∼ P where P : x = y.)
5. y ≥ x ∼ (y < x) (Also ∼ P where P : y < x.)
7. The number x equals zero, but the number y does not.
P∧ ∼ Q
P : x = 0
Q : y = 0
9. x ∈ A −B
(x ∈ A)∧ ∼ (x ∈ B)
11. A ∈
©
X ∈ P(N) : |X| < ∞ª
(A ⊆ N)∧(|A| < ∞).
13. Human beings want to be good, but not too good, and not all the time.
P∧ ∼ Q∧ ∼ R
P : Human beings want to be good.
Q : Human beings want to be too good.
R : Human beings want to be good all the time.
Section 2.3
Without changing their meanings, convert each of the following sentences into a
sentence having the form “If P, then Q.”
1. A matrix is invertible provided that its determinant is not zero.
Answer: If a matrix has a determinant not equal to zero, then it is invertible.
3. For a function to be integrable, it is necessary that it is continuous.
Answer: If function is integrable, then it is continuous.
5. An integer is divisible by 8 only if it is divisible by 4.
Answer: If an integer is divisible by 8, then it is divisible by 4.
7. A series converges whenever it converges absolutely.
Answer: If a series converges absolutely, then it converges.
9. A function is integrable provided the function is continuous.
Answer: If a function is continuous, then that function is integrable.
11. You fail only if you stop writing.
Answer: If you fail, then you have stopped writing.
13. Whenever people agree with me I feel I must be wrong.
Answer: If people agree with me, then I feel I must be wrong.
248 Solutions
Section 2.4
Without changing their meanings, convert each of the following sentences into a
sentence having the form “P if and only if Q.”
1. For a matrix to be invertible, it is necessary and sufficient that its determinant
is not zero.
Answer: A matrix is invertible if and only if its determinant is not zero.
3. If x y = 0 then x = 0 or y = 0, and conversely.
Answer: x y = 0 if and only if x = 0 or y = 0
5. For an occurrence to become an adventure, it is necessary and sufficient for
one to recount it.
Answer: An occurrence becomes an adventure if and only if one recounts it.
Section 2.5
1. Write a truth table for P ∨(Q ⇒ R)
P Q R Q ⇒ R P ∨(Q ⇒ R)
T T T T T
T T F F T
T F T T T
T F F T T
F T T T T
F T F F F
F F T T T
F F F T T
3. Write a truth table for ∼ (P ⇒ Q)
P Q P ⇒ Q ∼ (P ⇒ Q)
T T T F
T F F T
F T T F
F F T F
5. Write a truth table for (P∧ ∼ P)∨Q
P Q (P∧ ∼ P) (P∧ ∼ P)∨Q
T T F T
T F F F
F T F T
F F F F
7. Write a truth table for (P∧ ∼ P) ⇒ Q
P Q (P∧ ∼ P) (P∧ ∼ P) ⇒ Q
T T F T
T F F T
F T F T
F F F T
9. Write a truth table for ∼ (∼ P∨ ∼ Q).
P Q ∼ P ∼ Q ∼ P∨ ∼ Q ∼ (∼ P∨ ∼ Q)
T T F F F T
T F F T T F
F T T F T F
F F T T T F
249
11. Suppose P is false and that the statement (R ⇒ S) ⇔ (P ∧Q) is true. Find the
truth values of R and S. (This can be done without a truth table.)
Answer: Since P is false, it follows that (P ∧Q) is false also. But then in order
for (R ⇒ S) ⇔ (P ∧Q) to be true, it must be that (R ⇒ S) is false. The only way
for (R ⇒ S) to be false is if R is true and S is false.
Section 2.6
A. Use truth tables to show that the following statements are logically equivalent.
1. P ∧(Q ∨ R) = (P ∧Q)∨(P ∧ R)
P Q R Q ∨ R P ∧Q P ∧ R P ∧(Q ∨ R) (P ∧Q)∨(P ∧ R)
T T T T T T T T
T T F T T F T T
T F T T F T T T
T F F F F F F F
F T T T F F F F
F T F T F F F F
F F T T F F F F
F F F F F F F F
Thus since their columns agree, the two statements are logically equivalent.
3. P ⇒ Q = (∼ P)∨Q
P Q ∼ P (∼ P)∨Q P ⇒ Q
T T F T T
T F F F F
F T T T T
F F T T T
Thus since their columns agree, the two statements are logically equivalent.
5. ∼ (P ∨Q ∨ R) = (∼ P)∧(∼ Q)∧(∼ R)
P Q R P ∨Q ∨ R ∼ P ∼ Q ∼ R ∼ (P ∨Q ∨ R) (∼ P)∧(∼ Q)∧(∼ R)
T T T T F F F F F
T T F T F F T F F
T F T T F T F F F
T F F T F T T F F
F T T T T F F F F
F T F T T F T F F
F F T T T T F F F
F F F F T T T T T
Thus since their columns agree, the two statements are logically equivalent.
250 Solutions
7. P ⇒ Q = (P∧ ∼ Q) ⇒ (Q∧ ∼ Q)
P Q ∼ Q P∧ ∼ Q Q∧ ∼ Q (P∧ ∼ Q) ⇒ (Q∧ ∼ Q) P ⇒ Q
T T F F F T T
T F T T F F F
F T F F F T T
F F T F F T T
Thus since their columns agree, the two statements are logically equivalent.
B. Decide whether or not the following pairs of statements are logically equivalent.
9. By DeMorgan’s law, we have ∼ (∼ P∨ ∼ Q) =∼∼ P∧ ∼∼ Q = P ∧Q. Thus the
two statements are logically equivalent.
11. (∼ P)∧(P ⇒ Q) and ∼ (Q ⇒ P)
P Q ∼ P P ⇒ Q Q ⇒ P (∼ P)∧(P ⇒ Q) ∼ (Q ⇒ P)
T T F T T F F
T F F F T F F
F T T T F T T
F F T T T T F
The columns for the two statements do not quite agree, thus the two statements
are not logically equivalent.
Section 2.7
Write the following as English sentences. Say whether the statements are true
or false.
1. ∀x ∈ R, x
2 > 0
Answer: For every real number x, x
2 > 0.
Also: For every real number x, it follows that x
2 > 0.
Also: The square of any real number is positive. (etc.)
This statement is FALSE. Reason: 0 is a real number, but it’s not true that
0
2 > 0.
3. ∃a ∈ R,∀x ∈ R,ax = x.
Answer: There exists a real number a for which ax = x for every real number x.
This statement is TRUE. Reason: Consider a = 1.
5. ∀n ∈ N,∃ X ∈ P(N),|X| < n
Answer: For every natural number n, there is a subset X of N with |X| < n.
This statement is TRUE. Reason: Suppose n ∈ N. Let X = ;. Then |X| = 0 < n.
251
7. ∀ X ⊆ N,∃n ∈ Z,|X| = n
Answer: For any subset X of N, there exists an integer n for which |X| = n.
This statement is FALSE. For example, the set X = {2,4,6,8,...} of all even
natural numbers is infinite, so there does not exist any integer n for which
|X| = n.
9. ∀n ∈ Z,∃m ∈ Z,m = n+5
Answer: For every integer n there is another integer m such that m = n+5.
This statement is TRUE.
Section 2.9
Translate each of the following sentences into symbolic logic.
1. If f is a polynomial and its degree is greater than 2, then f
0
is not constant.
Translation: (P ∧Q) ⇒ R, where
P : f is a polynomial,
Q : f has degree greater than 2,
R : f
0
is not constant.
3. If x is prime then p
x is not a rational number.
Translation: P ⇒∼ Q, where
P : x is prime,
Q :
p
x is a rational number.
5. For every positive number ε, there is a positive number δ for which |x− a| < δ
implies |f (x)− f (a)| < ε.
Translation: ∀ ε ∈ R,ε > 0,∃ δ ∈ R,δ > 0,(|x− a| < δ) ⇒ (|f (x)− f (a)| < ε)
7. There exists a real number a for which a+ x = x for every real number x.
Translation: ∃a ∈ R,∀x ∈ R,a+ x = x
9. If x is a rational number and x 6= 0, then tan(x) is not a rational number.
Translation: ((x ∈ Q)∧(x 6= 0)) ⇒ (tan(x) ∉ Q)
11. There is a Providence that protects idiots, drunkards, children and the United
States of America.
One translation is as follows. Let R be union of the set of idiots, the set of
drunkards, the set of children, and the set consisting of the USA. Let P be the
open sentence P(x): x is a Providence. Let S be the open sentence S(x, y): x
protects y. Then the translation is ∃ x,∀ y ∈ R,P(x)∧ S(x, y).
(Notice that, although this is mathematically correct, some humor has been
lost in the translation.)
13. Everything is funny as long as it is happening to somebody else.
Translation: ∀x,(∼ M(x)∧ S(x)) ⇒ F(x),
where M(x): x is happening to me, S(x): x is happening to someone, and F(x) : x
is funny.
252 Solutions
Section 2.10
Negate the following sentences.
1. The number x is positive, but the number y is not positive.
The “but” can be interpreted as “and.” Using DeMorgan’s law, the negation is:
The number x is not positive or the number y is positive.
3. For every prime number p there, is another prime number q with q > p.
Negation: There is a prime number p such that for every prime number q,
q ≤ p.
Also: There exists a prime number p for which q ≤ p for every prime number q.
(etc.)
5. For every positive number ε there is a positive number M for which |f (x)−b| < ε
whenever x > M.
To negate this, it may be helpful to first write it in symbolic form. The statement
is ∀ε ∈ (0,∞),∃M ∈ (0,∞),(x > M) ⇒ (|f (x)− b| < ε).
Working out the negation, we have
∼
¡
∀ε ∈ (0,∞),∃M ∈ (0,∞),(x > M) ⇒ (|f (x)− b| < ε)
¢
=
∃ε ∈ (0,∞),∼
¡
∃M ∈ (0,∞),(x > M) ⇒ (|f (x)− b| < ε)
¢
=
∃ε ∈ (0,∞),∀M ∈ (0,∞),∼
¡
(x > M) ⇒ (|f (x)− b| < ε)
¢
.
Finally, using the idea from Example 2.14, we can negate the conditional
statement that appears here to get
∃ε ∈ (0,∞),∀M ∈ (0,∞),∃x, (x > M)∧ ∼ (|f (x)− b| < ε)
¢
.
Negation: There exists a positive number ε with the property that for every
positive number M, there is a number x for which x > M and |f (x)− b| ≥ ε.
7. I don’t eat anything that has a face.
Negation: I will eat some things that have a face.
(Note. If your answer was “I will eat anything that has a face.” then that is
wrong, both morally and mathematically.)
9. If sin(x) < 0, then it is not the case that 0 ≤ x ≤ π.
Negation: There exists a number x for which sin(x) < 0 and 0 ≤ x ≤ π.
11. You can fool all of the people all of the time.
There are several ways to negate this, including:
There is a person that you can’t fool all the time. or
There is a person x and a time y for which x is not fooled at time y.
(But Abraham Lincoln said it better.)
253
Chapter 3 Exercises
Section 3.1
1. Consider lists made from the letters T, H, E, O, R, Y, with repetition allowed.
(a) How many length-4 lists are there? Answer: 6·6·6·6 = 1296.
(b) How many length-4 lists are there that begin with T?
Answer: 1·6·6·6 = 216.
(c) How many length-4 lists are there that do not begin with T?
Answer: 5·6·6·6 = 1080.
3. How many ways can you make a list of length 3 from symbols a,b,c,d,e,f if...
(a) ... repetition is allowed. Answer: 6·6·6 = 216.
(b) ... repetition is not allowed. Answer: 6·5·4 = 120.
(c) ... repetition is not allowed and the list must contain the letter a.
Answer: 5·4+5·4+5·4 = 60.
(d) ... repetition is allowed and the list must contain the letter a.
Answer: 6·6·6−5·5·5 = 91.
(Note: See Example 3.2 if a more detailed explanation is required.)
5. Five cards are dealt off of a standard 52-card deck and lined up in a row. How
many such line-ups are there in which all five cards are of the same color? (i.e.,
all black or all red.)
There are 26·25·24·23·22 = 7,893,600 possible black-card line-ups and 26·25·24·23·
22 = 7,893,600 possible red-card line-ups, so the answer is 7,893,600+7,893,600 =
15,787,200.
7. This problems involves 8-digit binary strings such as 10011011 or 00001010.
(i.e., 8-digit numbers composed of 0’s and 1’s.)
(a) How many such strings are there? Answer: 2·2·2·2·2·2·2·2 = 256.
(b) How many such strings end in 0? Answer: 2·2·2·2·2·2·2·1 = 128.
(c) How many such strings have the property that their second and fourth
digits are 1’s? Answer: 2·1·2·1·2·2·2·2 = 64.
(d) How many such strings are such that their second or fourth digits are 1’s?
Answer: These strings can be divided into three types. Type 1 consists of
those strings of form ∗1∗0∗∗∗∗, Type 2 consist of strings of form ∗0∗1∗∗∗∗,
and Type 3 consists of those of form ∗1 ∗ 1 ∗ ∗ ∗ ∗. By the multiplication
principle there are 2
6 = 64 strings of each type, so there are 3·64 = 192
8-digit binary strings whose second or fourth digits are 1’s.
9. This problem concerns 4-letter codes that can be made from the letters of the
English Alphabet.
(a) How many such codes can be made? Answer: 26·26·26·26 = 456976
254 Solutions
(b) How many such codes have no two consecutive letters the same?
We use the multiplication principle. There are 26 choices for the first letter.
The second letter can’t be the same as the first letter, so there are only 25
choices for it. The third letter can’t be the same as the second letter, so there
are only 25 choices for it. The fourth letter can’t be the same as the third letter,
so there are only 25 choices for it. Thus there are 26·25·25·25 = 406,250
codes with no two consecutive letters the same.
11. This problem concerns lists of length 6 made from the letters A,B,C,D,E,F,G,H.
How many such lists are possible if repetition is not allowed and the list
contains two consecutive vowels?
Answer: There are just two vowels A and E to choose from. The lists we want
to make can be divided into five types. They have one of the forms V V ∗ ∗ ∗ ∗,
or ∗V V ∗ ∗∗, or ∗ ∗ V V ∗ ∗, or ∗ ∗ ∗V V∗, or ∗ ∗ ∗ ∗ V V, where V indicates a
vowel and ∗ indicates a consonant. By the multiplication principle, there are
2·1·6·5·4·3 = 720 lists of form V V ∗∗∗∗. In fact, that for the same reason there
are 720 lists of each form. Thus the answer to the question is 5·720 = 3600
Section 3.2
1. Answer n = 14.
3. Answer: 5! = 120.
5. 120!
118! =
120·119·118!
118! = 120·119 = 14,280.
7. Answer: 5!4! = 2880.
9. The case x = 1 is straightforward. For x = 2,3 and 4, use integration by parts.
For x = π, you are on your own.
Section 3.3
1. Suppose a set A has 37 elements. How many subsets of A have 10 elements?
How many subsets have 30 elements? How many have 0 elements?
Answers: ¡
37
10
¢
= 348,330,136;
¡
37
30
¢
= 10,295,472;
¡
37
0
¢
= 1.
3. A set X has exactly 56 subsets with 3 elements. What is the cardinality of X?
The answer will be n, where ¡
n
3
¢
= 56. After some trial and error, you will
discover ¡
8
3
¢
= 56, so |X| = 8.
5. How many 16-digit binary strings contain exactly seven 1’s?
Answer: Make such a string as follows. Start with a list of 16 blank spots.
Choose 7 of the blank spots for the 1’s and put 0’s in the other spots. There
are ¡
16
7
¢
= 114,40 ways to do this.
7. |{X ∈ P({0,1,2,3,4,5,6,7,8,9}) : |X| < 4}| =
¡
10
0
¢
+
¡
10
1
¢
+
¡
10
2
¢
+
¡
10
3
¢
= 1+10+45+120 =
176.
9. This problem concerns lists of length six made from the letters A,B,C,D,E,F,
without repetition. How many such lists have the property that the D occurs
before the A?
Answer: Make such a list as follows. Begin with six blank spaces and select
two of these spaces. Put the D in the first selected space and the A in the
second. There are ¡
6
2
¢
= 15 ways of doing this. For each of these 15 choices
there are 4! = 24 ways of filling in the remaining spaces. Thus the answer to
the question is 15×24 = 360 such lists.
255
11. How many 10-digit integers contain no 0’s and exactly three 6’s?
Answer: Make such a number as follows: Start with 10 blank spaces and choose
three of these spaces for the 6’s. There are ¡
10
3
¢
= 120 ways of doing this. For
each of these 120 choices we can fill in the remaining seven blanks with choices
from the digits 1,2,3,4,5,7,8,9, and there are 8
7
to do this. Thus the answer to
the question is ¡
10
3
¢
·8
7 = 251,658,240.
13. Assume n,k ∈ Z with 0 ≤ k ≤ n. Then ¡
n
k
¢
=
n!
(n−k)!k!
=
n!
k!(n−k)! =
n!
(n−(n−k))!(n−k)! =
¡
n
n−k
¢
.
Section 3.4
1. Write out Row 11 of Pascal’s triangle.
Answer: 1 11 55 165 330 462 462 330 165 55 11 1
3. Use the binomial theorem to find the coefficient of x
8
in (x+2)13
.
Answer: According to the binomial theorem, the coefficient of x
8
y
5
in (x + y)
13
is ¡
13
8
¢
x
8
y
5 = 1287x
8
y
5
. Now plug in y = 2 to get the final answer of 41184x
8
.
5. Use the binomial theorem to show Pn
k=0
¡
n
k
¢
= 2
n
. Hint: Observe that 2
n = (1+1)n
.
Now use the binomial theorem to work out (x+ y)
n and plug in x = 1 and y = 1.
7. Use the binomial theorem to show Pn
k=0
3
k
¡
n
k
¢
= 4
n
.
Hint: Observe that 4
n = (1+3)n
. Now look at the hint for the previous problem.
9. Use the binomial theorem to show ¡
n
0
¢
−
¡
n
1
¢
+
¡
n
2
¢
−
¡
n
3
¢
+
¡
n
4
¢
−
¡
n
5
¢
+ ... ±
¡
n
n
¢
= 0.
Hint: Observe that 0 = 0
n = (1+(−1))n
. Now use the binomial theorem.
11. Use the binomial theorem to show 9
n =
Pn
k=0
(−1)k
¡
n
k
¢
10n−k
.
Hint: Observe that 9
n = (10+(−1))n
. Now use the binomial theorem.
13. Assume n ≥ 3. Then ¡
n
3
¢
=
¡
n−1
3
¢
+
¡
n−1
2
¢
=
¡
n−2
3
¢
+
¡
n−2
2
¢
+
¡
n−1
2
¢
= ··· = ¡
2
2
¢
+
¡
3
2
¢
+···+¡
n−1
2
¢
.
Section 3.5
1. At a certain university 523 of the seniors are history majors or math majors
(or both). There are 100 senior math majors, and 33 seniors are majoring in
both history and math. How many seniors are majoring in history?
Answer: Let A be the set of senior math majors and B be the set of senior
history majors. From |A ∪ B| = |A| + |B| − |A ∩ B| we get 523 = 100 + |B| − 33, so
|B| = 523+33−100 = 456. There are 456 history majors.
3. How many 4-digit positive integers are there that are even or contain no 0’s?
Answer: Let A be the set of 4-digit even positive integers, and let B be the
set of 4-digit positive integers that contain no 0’s. We seek |A ∪ B|. By the
multiplication principle |A| = 9·10·10·5 = 4500. (Note the first digit cannot be 0
and the last digit must be even.) Also |B| = 9·9·9·9 = 6561. Further, A∩B consists
of all even 4-digit integers that have no 0’s. It follows that |A∩B| = 9·9·9·4 = 2916.
Then the answer to our question is |A∪B| = |A|+|B|−|A∩B| = 4500+6561−2916 =
8145.
256 Solutions
5. How many 7-digit binary strings begin in 1 or end in 1 or have exactly four 1’s?
Answer: Let A be the set of such strings that begin in 1. Let B be the set of such
strings that end in 1. Let C be the set of such strings that have exactly four 1’s.
Then the answer to our question is |A ∪B∪C|. Using Equation (3.4) to compute
this number, we have |A∪B∪C| = |A|+|B|+|C|−|A∩B|−|A∩C|−|B∩C|+|A∩B∩C| =
2
6 +2
6 +
¡
7
4
¢
−2
5 −
¡
6
3
¢
−
¡
6
3
¢
+
¡
5
2
¢
= 64+64+35−32−20−20+10 = 101.
7. This problem concerns 4-card hands dealt off of a standard 52-card deck. How
many 4-card hands are there for which all four cards are of the same suit or
all four cards are red?
Answer: Let A be the set of 4-card hands for which all four cards are of the
same suit. Let B be the set of 4-card hands for which all four cards are red.
Then A ∩ B is the set of 4-card hands for which the four cards are either all
hearts or all diamonds. The answer to our question is |A∪B| = |A|+|B|−|A∩B| =
4
¡
13
4
¢
+
¡
26
4
¢
−2
¡
13
4
¢
= 2
¡
13
4
¢
+
¡
26
4
¢
= 1430+14950 = 16380.
9. A 4-letter list is made from the letters L,I,S,T,E,D according to the following
rule: Repetition is allowed, and the first two letters on the list are vowels or
the list ends in D.
Answer: Let A be the set of such lists for which the first two letters are
vowels, so |A| = 2·2·6·6 = 144. Let B be the set of such lists that end in D, so
|B| = 6·6·6·1 = 216. Then A ∩ B is the set of such lists for which the first two
entries are vowels and the list ends in D. Thus |A ∩ B| = 2 · 2 · 6 · 1 = 24. The
answer to our question is |A ∪B| = |A| +|B| −|A ∩B| = 144+216−24 = 336.
Chapter 4 Exercises
1. If x is an even integer, then x
2
is even.
Proof. Suppose x is even. Thus x = 2a for some a ∈ Z.
Consequently x
2 = (2a)
2 = 4a
2 = 2(2a
2
).
Therefore x
2 = 2b, where b is the integer 2a
2
.
Thus x
2
is even by definition of an even number. ■
3. If a is an odd integer, then a
2 +3a+5 is odd.
Proof. Suppose a is odd.
Thus a = 2c +1 for some integer c, by definition of an odd number.
Then a
2 +3a+5 = (2c +1)2 +3(2c +1)+5 = 4c
2 +4c +1+6c +3+5 = 4c
2 +10c +9
= 4c
2 +10c +8+1 = 2(2c
2 +5c +4)+1.
This shows a
2 +3a+5 = 2b +1, where b = 2c
2 +5c +4 ∈ Z.
Therefore a
2 +3a+5 is odd. ■
5. Suppose x, y ∈ Z. If x is even, then x y is even.
Proof. Suppose x, y ∈ Z and x is even.
Then x = 2a for some integer a, by definition of an even number.
Thus x y = (2a)(y) = 2(a y).
Therefore x y = 2b where b is the integer a y, so x y is even. ■
257
7. Suppose a,b ∈ Z. If a | b, then a
2
| b
2
.
Proof. Suppose a | b.
By definition of divisibility, this means b = ac for some integer c.
Squaring both sides of this equation produces b
2 = a
2
c
2
.
Then b
2 = a
2d, where d = c
2 ∈ Z.
By definition of divisibility, this means a
2
| b
2
. ■
9. Suppose a is an integer. If 7 | 4a, then 7 | a.
Proof. Suppose 7 | 4a.
By definition of divisibility, this means 4a = 7c for some integer c.
Since 4a = 2(2a) it follows that 4a is even, and since 4a = 7c, we know 7c is even.
But then c can’t be odd, because that would make 7c odd, not even.
Thus c is even, so c = 2d for some integer d.
Now go back to the equation 4a = 7c and plug in c = 2d. We get 4a = 14d.
Dividing both sides by 2 gives 2a = 7d.
Now, since 2a = 7d, it follows that 7d is even, and thus d cannot be odd.
Then d is even, so d = 2e for some integer e.
Plugging d = 2e back into 2a = 7d gives 2a = 14e.
Dividing both sides of 2a = 14e by 2 produces a = 7e.
Finally, the equation a = 7e means that 7 | a, by definition of divisibility. ■
11. Suppose a,b, c,d ∈ Z. If a | b and c | d, then ac | bd.
Proof. Suppose a | b and c | d.
As a | b, the definition of divisibility means there is an integer x for which
b = ax.
As c | d, the definition of divisibility means there is an integer y for which
d = c y.
Since b = ax, we can multiply one side of d = c y by b and the other by ax.
This gives bd = axc y, or bd = (ac)(x y).
Since x y ∈ Z, the definition of divisibility applied to bd = (ac)(x y) gives ac | bd. ■
13. Suppose x, y ∈ R. If x
2 +5y = y
2 +5x, then x = y or x+ y = 5.
Proof. Suppose x
2 +5y = y
2 +5x.
Then x
2 − y
2 = 5x−5y, and factoring gives (x− y)(x+ y) = 5(x− y).
Now consider two cases.
Case 1. If x − y 6= 0 we can divide both sides of (x − y)(x + y) = 5(x − y) by the
non-zero quantity x− y to get x+ y = 5.
Case 2. If x− y = 0, then x = y. (By adding y to both sides.)
Thus x = y or x+ y = 5. ■
258 Solutions
15. If n ∈ Z, then n
2 +3n+4 is even.
Proof. Suppose n ∈ Z. We consider two cases.
Case 1. Suppose n is even. Then n = 2a for some a ∈ Z.
Therefore n
2 +3n+4 = (2a)
2 +3(2a)+4 = 4a
2 +6a+4 = 2(2a
2 +3a+2).
So n
2 +3n+4 = 2b where b = 2a
2 +3a+2 ∈ Z, so n
2 +3n+4 is even.
Case 2. Suppose n is odd. Then n = 2a+1 for some a ∈ Z.
Therefore n
2 +3n+4 = (2a+1)2 +3(2a+1)+4 = 4a
2 +4a+1+6a+3+4 = 4a
2 +10a+8
= 2(2a
2+5a+4). So n
2+3n+4 = 2b where b = 2a
2+5a+4 ∈ Z, so n
2+3n+4 is even.
In either case n
2 +3n+4 is even. ■
17. If two integers have opposite parity, then their product is even.
Proof. Suppose a and b are two integers with opposite parity. Thus one is even
and the other is odd. Without loss of generality, suppose a is even and b is
odd. Therefore there are integers c and d for which a = 2c and b = 2d +1. Then
the product of a and b is ab = 2c(2d +1) = 2(2cd + c). Therefore ab = 2k where
k = 2cd + c ∈ Z. Therefore the product ab is even. ■
19. Suppose a,b, c ∈ Z. If a
2
| b and b
3
| c then a
6
| c.
Proof. Since a
2
| b we have b = ka2
for some k ∈ Z. Since b
3
| c we have c = hb3
for some h ∈ Z. Thus c = h(ka2
)
3 = hk3a
6
. Hence a
6
| c. ■
21. If p is prime and 0 < k < p then p |
¡
p
k
¢
.
Proof. From the formula ¡
p
k
¢
=
p!
(p−k)!k!
, we get p! =
¡
p
k
¢
(p − k)!k!. Now, since the
prime number p is a factor of p! on the left, it must also be a factor of ¡
p
k
¢
(p−k)!k!
on the right. Thus the prime number p appears in the prime factorization of
¡
p
k
¢
(p − k)!k!.
Now, k! is a product of numbers smaller than p, so its prime factorization
contains no p’s. Similarly the prime factorization of (p − k)! contains no p’s.
But we noted that the prime factorization of ¡
p
k
¢
(p−k)!k! must contain a p, so it
follows that the prime factorization of ¡
p
k
¢
contains a p. Thus ¡
p
k
¢
is a multiple
of p, so p divides ¡
p
k
¢
. ■
23. If n ∈ N then ¡
2n
n
¢
is even.
Proof. By definition, ¡
2n
n
¢
is the number of n-element subsets of a set A with 2n
elements. For each subset X ⊆ A with |X| = n, the complement X is a different
set, but it also has 2n − n = n elements. Imagine listing out all the n-elements
subset of a set A. It could be done in such a way that the list has form
X1, X1, X2, X2, X3, X3, X4, X4, X5, X5 ...
This list has an even number of items, for they are grouped in pairs. Thus ¡
2n
n
¢
is even. ■
259
25. If a,b, c ∈ N and c ≤ b ≤ a then ¡
a
b
¢¡b
c
¢
=
¡
a
b−c
¢¡a−b+c
c
¢
.
Proof. Assume a,b, c ∈ N with c ≤ b ≤ a. Then we have ¡
a
b
¢¡b
c
¢
=
a!
(a−b)!b!
b!
(b−c)!c!
=
a!
(a−b+c)!(a−b)!
(a−b+c)!
(b−c)!c!
=
a!
(b−c)!(a−b+c)!
(a−b+c)!
(a−b)!c!
=
¡
a
b−c
¢¡a−b+c
c
¢
. ■
27. Suppose a,b ∈ N. If gcd(a,b) > 1, then b | a or b is not prime.
Proof. Suppose gcd(a,b) > 1. Let c = gcd(a,b) > 1. Then since c is a divisor of
both a and b, we have a = cx and b = c y for integers x and y. We divide into
two cases according to whether or not b is prime.
Case I. Suppose b is prime. Then the above equation b = c y with c > 1 forces
c = b and y = 1. Then a = cx becomes a = bx, which means b | a. We conclude
that the statement “b | a or b is not prime,” is true.
Case II. Suppose b is not prime. Then the statement “b | a or b is not prime,”
is automatically true. ■
Chapter 5 Exercises
1. Proposition Suppose n ∈ Z. If n
2
is even, then n is even.
Proof. (Contrapositive) Suppose n is not even. Then n is odd, so n = 2a+1 for
some integer a, by definition of an odd number. Thus n
2 = (2a+1)2 = 4a
2+4a+1 =
2(2a
2 +2a)+1. Consequently n
2 = 2b+1, where b is the integer 2a
2 +2a, so n
2
is
odd. Therefore n
2
is not even. ■
3. Proposition Suppose a,b ∈ Z. If a
2
(b
2 −2b) is odd, then a and b are odd.
Proof. (Contrapositive) Suppose it is not the case that a and b are odd. Then,
by DeMorgan’s law, at least one of a and b is even. Let us look at these cases
separately.
Case 1. Suppose a is even. Then a = 2c for some integer c. Thus a
2
(b
2 −2b)
= (2c)
2
(b
2 −2b) = 2(2c
2
(b
2 −2b)), which is even.
Case 2. Suppose b is even. Then b = 2c for some integer c. Thus a
2
(b
2 −2b)
= a
2
((2c)
2 −2(2c)) = 2(a
2
(2c
2 −2c)), which is even.
(A third case involving a and b both even is unnecessary, for either of the two
cases above cover this case.) Thus in either case a
2
(b
2 −2b) is even, so it is not
odd. ■
5. Proposition Suppose x ∈ R. If x
2 +5x < 0 then x < 0.
Proof. (Contrapositive) Suppose it is not the case that x < 0, so x ≥ 0. Then
neither x
2 nor 5x is negative, so x
2+5x ≥ 0. Thus it is not true that x
2+5x < 0. ■
260 Solutions
7. Proposition Suppose a,b ∈ Z. If both ab and a+ b are even, then both a and
b are even.
Proof. (Contrapositive) Suppose it is not the case that both a and b are even.
Then at least one of them is odd. There are three cases to consider.
Case 1. Suppose a is even and b is odd. Then there are integers c and
d for which a = 2c and b = 2d + 1. Then ab = 2c(2d + 1), which is even; and
a + b = 2c +2d +1 = 2(c + d)+1, which is odd. Thus it is not the case that both
ab and a+ b are even.
Case 2. Suppose a is odd and b is even. Then there are integers c and d for
which a = 2c +1 and b = 2d. Then ab = (2c +1)(2d) = 2(d(2c +1)), which is even;
and a + b = 2c +1+2d = 2(c + d)+1, which is odd. Thus it is not the case that
both ab and a+ b are even.
Case 3. Suppose a is odd and b is odd. Then there are integers c and d for
which a = 2c + 1 and b = 2d + 1. Then ab = (2c + 1)(2d + 1) = 4cd + 2c + 2d + 1 =
2(2cd + c + d) + 1, which is odd; and a + b = 2c + 1 + 2d + 1 = 2(c + d + 1), which is
even. Thus it is not the case that both ab and a+ b are even.
These cases show that it is not the case that ab and a + b are both even. (Note
that unlike Exercise 3 above, we really did need all three cases here, for each
case involved specific parities for both a and b.) ■
9. Proposition Suppose n ∈ Z. If 3 - n
2
, then 3 - n.
Proof. (Contrapositive) Suppose it is not the case that 3 - n, so 3 | n. This means
that n = 3a for some integer a. Consequently n
2 = 9a
2
, from which we get
n
2 = 3(3a
2
). This shows that there in an integer b = 3a
2
for which n
2 = 3b, which
means 3 | n
2
. Therefore it is not the case that 3 - n
2
. ■
11. Proposition Suppose x, y ∈ Z. If x
2
(y+3) is even, then x is even or y is odd.
Proof. (Contrapositive) Suppose it is not the case that x is even or y is odd.
Using DeMorgan’s law, this means x is not even and y is not odd, which is to
say x is odd and y is even. Thus there are integers a and b for which x = 2a +1
and y = 2b. Consequently x
2
(y + 3) = (2a + 1)2
(2b + 3) = (4a
2 + 4a + 1)(2b + 3) =
8a
2b +8ab +2b +12a
2 +12a+3 = 8a
2b +8ab +2b +12a
2 +12a+2+1 =
2(4a
2b+4ab+ b+6a
2 +6a+1)+1. This shows x
2
(y+3) = 2c+1 for c = 4a
2b+4ab+
b +6a
2 +6a+1 ∈ Z. Consequently, x
2
(y+3) is not even. ■
13. Proposition Suppose x ∈ R. If x
5 +7x
3 +5x ≥ x
4 + x
2 +8, then x ≥ 0.
Proof. (Contrapositive) Suppose it is not true that x ≥ 0. Then x < 0, that is
x is negative. Consequently, the expressions x
5
, 7x
3 and 5x are all negative
(note the odd powers) so x
5 + 7x
3 + 5x < 0. Similarly the terms x
4
, x
2
, and 8
are all positive (note the even powers), so 0 < x
4 + x
2 + 8. From this we get
x
5 +7x
3 +5x < x
4 + x
2 +8, so it is not true that x
5 +7x
3 +5x ≥ x
4 + x
2 +8. ■
261
15. Proposition Suppose x ∈ Z. If x
3 −1 is even, then x is odd.
Proof. (Contrapositive) Suppose x is not odd. Thus x is even, so x = 2a for some
integer a. Then x
3 −1 = (2a)
3 −1 = 8a
3 −1 = 8a
3 −2+1 = 2(4a
3 −1)+1. Therefore
x
3 −1 = 2b +1 where b = 4a
3 −1 ∈ Z, so x
3 −1 is odd. Thus x
3 −1 is not even. ■
17. Proposition If n is odd, then 8 | (n
2 −1).
Proof. (Direct) Suppose n is odd, so n = 2a+1 for some integer a. Then n
2 −1 =
(2a+1)2 −1 = 4a
2 +4a = 4(a
2 +a) = 4a(a+1). So far we have n
2 −1 = 4a(a+1), but
we want a factor of 8, not 4. But notice that one of a or a+1 must be even, so
a(a +1) is even and hence a(a +1) = 2c for some integer c. Now we have n
2 −1 =
4a(a+1) = 4(2c) = 8c. But n
2 −1 = 8c means 8 | (n
2 −1). ■
19. Proposition Let a,b ∈ Z and n ∈ N. If a ≡ b (mod n) and a ≡ c (mod n), then
c ≡ b (mod n).
Proof. (Direct) Suppose a ≡ b (mod n) and a ≡ c (mod n).
This means n | (a− b) and n | (a− c).
Thus there are integers d and e for which a− b = nd and a− c = ne.
Subtracting the second equation from the first gives c − b = nd − ne.
Thus c − b = n(d − e), so n | (c − b) by definition of divisibility.
Therefore c ≡ b (mod n) by definition of congruence modulo n. ■
21. Proposition Let a,b ∈ Z and n ∈ N. If a ≡ b (mod n), then a
3 ≡ b
3
(mod n).
Proof. (Direct) Suppose a ≡ b (mod n). This means n | (a − b), so there is an
integer c for which a− b = nc. Then:
a− b = nc
(a− b)(a
2 + ab + b
2
) = nc(a
2 + ab + b
2
)
a
3 + a
2
b + ab2 − ba2 − ab2 − b
3 = nc(a
2 + ab + b
2
)
a
3 − b
3 = nc(a
2 + ab + b
2
).
Since a
2 + ab + b
2 ∈ Z, the equation a
3 − b
3 = nc(a
2 + ab + b
2
) implies n | (a
3 − b
3
),
and therefore a
3 ≡ b
3
(mod n). ■
23. Proposition Let a,b, c ∈ Z and n ∈ N. If a ≡ b (mod n), then ca ≡ cb (mod n).
Proof. (Direct) Suppose a ≡ b (mod n). This means n | (a − b), so there is an
integer d for which a−b = nd. Multiply both sides of this by c to get ac−bc = ndc.
Consequently, there is an integer e = dc for which ac − bc = ne, so n | (ac − bc)
and consequently ac ≡ bc (mod n). ■
25. If n ∈ N and 2
n −1 is prime, then n is prime.
Proof. Assume n is not prime. Write n = ab for some a,b > 1. Then 2
n − 1 =
2
ab−1 =
¡
2
b−1
¢¡2
ab−b+2
ab−2b+2
ab−3b+···+2
ab−ab¢
. Hence 2
n−1 is composite. ■
262 Solutions
27. If a ≡ 0 (mod 4) or a ≡ 1 (mod 4) then ¡
a
2
¢
is even.
Proof. We prove this directly. Assume a ≡ 0 (mod 4). Then ¡
a
2
¢
=
a(a−1)
2
. Since
a = 4k for some k ∈ N, we have ¡
a
2
¢
=
4k(4k−1)
2
= 2k(4k −1). Hence ¡
a
2
¢
is even.
Now assume a ≡ 1 (mod 4). Then a = 4k+1 for some k ∈ N. Hence ¡
a
2
¢
=
(4k+1)(4k)
2
=
2k(4k +1). Hence, ¡
a
2
¢
is even. This proves the result. ■
29. If integers a and b are not both zero, then gcd(a,b) = gcd(a− b,b).
Proof. (Direct) Suppose integers a and b are not both zero. Let d = gcd(a,b).
Because d is a divisor of both a and b, we have a = dx and b = d y for some
integers x and y. Then a − b = dx − d y = d(x − y), so it follows that d is also a
common divisor of a − b and b. Therefore it can’t be greater than the greatest
common divisor of a− b and b, which is to say gcd(a,b) = d ≤ gcd(a− b,b).
Now let e = gcd(a − b,b). Then e divides both a − b and b, that is, a − b = ex and
b = e y for integers x and y. Then a = (a− b)+ b = ex+ e y = e(x+ y), so now we see
that e is a divisor of both a and b. Thus it is not more than their greatest
common divisor, that is, gcd(a− b,b) = e ≤ gcd(a,b).
The above two paragraphs have given gcd(a,b) ≤ gcd(a− b,b) and gcd(a− b,b) ≤
gcd(a,b). Thus gcd(a,b) = gcd(a− b,b).
■
31. Suppose the division algorithm applied to a and b yields a = qb + r. Then
gcd(a,b) = gcd(r,b).
Proof. Suppose a = qb+r. Let d = gcd(a,b), so d is a common divisor of a and b;
thus a = dx and b = d y for some integers x and y. Then dx = a = qb + r = qd y+ r,
hence dx = qd y+ r, and so r = dx− qd y = d(x − q y). Thus d is a divisor of r (and
also of b), so gcd(a,b) = d ≤ gcd(r,b).
On the other hand, let e = gcd(r,b), so r = ex and b = e y for some integers x and
y. Then a = qb + r = qe y+ ex = e(q y+ x). Hence e is a divisor of a (and of course
also of b) so gcd(r,b) = e ≤ gcd(a,b).
We’ve now shown gcd(a,b) ≤ gcd(r,b) and gcd(r,b) ≤ gcd(a,b), so gcd(r,b) = gcd(a,b).
■
Chapter 6 Exercises
1. Suppose n is an integer. If n is odd, then n
2
is odd.
Proof. Suppose for the sake of contradiction that n is odd and n
2
is not odd.
Then n
2
is even. Now, since n is odd, we have n = 2a + 1 for some integer a.
Thus n
2 = (2a +1)2 = 4a
2 +4a +1 = 2(2a
2 +2a)+1. This shows n
2 = 2b +1, where
b is the integer b = 2a
2 + 2a. Therefore we have n
2
is odd and n
2
is even, a
contradiction. ■
263
3. Prove that p3
2 is irrational.
Proof. Suppose for the sake of contradiction that p3
2 is not irrational. Therefore
it is rational, so there exist integers a and b for which p3
2 =
a
b
. Let us assume
that this fraction is reduced, so a and b are not both even. Now we have
p3
2
3
=
¡
a
b
¢3
, which gives 2 =
a
3
b
3
, or 2b
3 = a
3
. From this we see that a
3
is even,
from which we deduce that a is even. (For if a were odd, then a
3 = (2c +1)3 =
8c
3 +12c
2 +6c +1 = 2(4c
3 +6c
2 +3c)+1 would be odd, not even.) Since a is even,
it follows that a = 2d for some integer d. The equation 2b
3 = a
3
from above then
becomes 2b
3 = (2d)
3
, or 2b
3 = 8d
3
. Dividing by 2, we get b
3 = 4d
3
, and it follows
that b
3
is even. Thus b is even also. (Using the same argument we used when
a
3 was even.) At this point we have discovered that both a and b are even,
contradicting the fact (observed above) that the a and b are not both even. ■
Here is an alternative proof.
Proof. Suppose for the sake of contradiction that p3
2 is not irrational. Therefore
there exist integers a and b for which p3
2 =
a
b
. Cubing both sides, we get 2 =
a
3
b
3
.
From this, a
3 = b
3 + b
3
, which contradicts Fermat’s last theorem. ■
5. Prove that p
3 is irrational.
Proof. Suppose for the sake of contradiction that p
3 is not irrational. Therefore
it is rational, so there exist integers a and b for which p
3 =
a
b
. Let us assume
that this fraction is reduced, so a and b have no common factor. Notice that
p
3
2
=
¡
a
b
¢2
, so 3 =
a
2
b
2
, or 3b
2 = a
2
. This means 3 | a
2
.
Now we are going to show that if a ∈ Z and 3 | a
2
, then 3 | a. (This is a proofwithin-a-proof.)
We will use contrapositive proof to prove this conditional
statement. Suppose 3 - a. Then there is a remainder of either 1 or 2 when 3 is
divided into a.
Case 1. There is a remainder of 1 when 3 is divided into a. Then a = 3m+1
for some integer m. Consequently, a
2 = 9m2 +6m+1 = 3(3m2 +2m)+1, and this
means 3 divides into a
2 with a remainder of 1. Thus 3 - a
2
.
Case 2. There is a remainder of 2 when 3 is divided into a. Then a = 3m+2
for some integer m. Consequently, a
2 = 9m2 + 12m + 4 = 9m2 + 12m + 3 + 1 =
3(3m2+4m+1)+1, and this means 3 divides into a
2 with a remainder of 1. Thus
3 - a
2
.
In either case we have 3 - a
2
, so we’ve shown 3 - a implies 3 - a
2
. Therefore, if
3 | a
2
, then 3 | a.
Now go back to 3 | a
2
in the first paragraph. This combined with the result of
the second paragraph implies 3 | a, so a = 3d for some integer d. Now also in the
first paragraph we had 3b
2 = a
2
, which now becomes 3b
2 = (3d)
2 or 3b
2 = 9d
2
, so
b
2 = 3d
2
. But this means 3 | b
2
, and the second paragraph implies 3 | b. Thus
we have concluded that 3 | a and 3 | b, but this contradicts the fact that the
fraction a
b
is reduced. ■
264 Solutions
7. If a,b ∈ Z, then a
2 −4b −3 6= 0.
Proof. Suppose for the sake of contradiction that a,b ∈ Z but a
2−4b−3 = 0. Then
we have a
2 = 4b+3 = 2(2b+1)+1, which means a
2
is odd. Therefore a is odd also,
so a = 2c +1 for some integer c. Plugging this back into a
2 −4b −3 = 0 gives us
(2c +1)2 −4b −3 = 0
4c
2 +4c +1−4b −3 = 0
4c
2 +4c −4b = 2
2c
2 +2c −2b = 1
2(c
2 + c − b) = 1.
From this last equation, we see that 1 is an even number, a contradiction. ■
9. Suppose a,b ∈ R and a 6= 0. If a is rational and ab is irrational, then b is
irrational.
Proof. Suppose for the sake of contradiction that a is rational and ab is irrational
and b is not irrational. Thus we have a and b rational, and ab irrational.
Since a and b are rational, we know there are integers c,d, e, f for which a =
c
d
and b =
e
f
. Then ab =
ce
d f , and since both ce and d f are integers, it follows
that ab is rational. But this is a contradiction because we started out with ab
irrational. ■
11. There exist no integers a and b for which 18a+6b = 1.
Proof. Suppose for the sake of contradiction that there do exist integers a
and b for which 18a + 6b = 1. Then 1 = 2(9a + 3b), which means 1 is even, a
contradiction. ■
13. For every x ∈ [π/2,π], sinx−cos x ≥ 1.
Proof. Suppose for the sake of contradiction that x ∈ [π/2,π], but sinx−cos x < 1.
Since x ∈ [π/2,π], we know sinx ≥ 0 and cos x ≤ 0, so sinx − cos x ≥ 0. Therefore
we have 0 ≤ sinx − cos x < 1. Now the square of any number between 0 and
1 is still a number between 0 and 1, so we have 0 ≤ (sinx − cos x)
2 < 1, or 0 ≤
sin2
x−2sinxcos x+cos2
x < 1. Using the fact that sin2
x+cos2
x = 1, this becomes
0 ≤ −2sinxcos x+1 < 1. Subtracting 1, we obtain −2sinxcos x < 0. But above we
remarked that sinx ≥ 0 and cos x ≤ 0, and hence −2sinxcos x ≥ 0. We now have
the contradiction −2sinxcos x < 0 and −2sinxcos x ≥ 0. ■
15. If b ∈ Z and b - k for every k ∈ N, then b = 0.
Proof. Suppose for the sake of contradiction that b ∈ Z and b - k for every k ∈ N,
but b 6= 0.
Case 1. Suppose b > 0. Then b ∈ N, so b|b, contradicting b - k for every k ∈ N.
Case 2. Suppose b < 0. Then −b ∈ N, so b|(−b), again a contradiction ■
265
17. For every n ∈ Z, 4 - (n
2 +2).
Proof. Assume there exists n ∈ Z with 4 | (n
2 +2). Then for some k ∈ Z, 4k = n
2 +2
or 2k = n
2 + 2(1 − k). If n is odd, this means 2k is odd, and we’ve reached a
contradiction. If n is even then n = 2 j and we get k = 2 j
2 +1− k for some j ∈ Z.
Hence 2(k − j
2
) = 1, so 1 is even, a contradiction. ■
Remark. It is fairly easy to see that two more than a perfect square is always
either 2 (mod 4) or 3 (mod 4). This would end the proof immediately.
19. The product of 5 consecutive integers is a multiple of 120.
Proof. Given any collection of 5 consecutive integers, at least one must be a
multiple of two, at least one must be a multiple of three, at least one must be
a multiple of four and at least one must be a multiple of 5. Hence the product
is a multiple of 5·4·3·2 = 120. In particular, the product is a multiple of 60. ■
21. Hints for Exercises 20–23. For Exercises 20, first show that the equation
a
2 + b
2 = 3c
2 has no solutions (other than the trivial solution (a,b, c) = (0,0,0))
in the integers. To do this, investigate the remainders of a sum of squares
(mod 4). After you’ve done this, prove that the only solution is indeed the trivial
solution.
Now, assume that the equation x
2 + y
2 −3 = 0 has a rational solution. Use the
definition of rational numbers to yield a contradiction.
Chapter 7 Exercises
1. Suppose x ∈ Z. Then x is even if and only if 3x+5 is odd.
Proof. We first use direct proof to show that if x is even, then 3x + 5 is odd.
Suppose x is even. Then x = 2n for some integer n. Thus 3x + 5 = 3(2n) + 5 =
6n +5 = 6n +4 +1 = 2(3n +2) +1. Thus 3x +5 is odd because it has form 2k +1,
where k = 3n+2 ∈ Z.
Conversely, we need to show that if 3x + 5 is odd, then x is even. We will
prove this using contrapositive proof. Suppose x is not even. Then x is odd, so
x = 2n+1 for some integer n. Thus 3x+5 = 3(2n+1)+5 = 6n+8 = 2(3n+4). This
means says 3x+5 is twice the integer 3n+4, so 3x+5 is even, not odd. ■
3. Given an integer a, then a
3 + a
2 + a is even if and only if a is even.
Proof. First we will prove that if a
3 +a
2 +a is even then a is even. This is done
with contrapositive proof. Suppose a is not even. Then a is odd, so there is an
integer n for which a = 2n+1. Then
a
3 + a
2 + a = (2n+1)3 +(2n+1)2 +(2n+1)
= 8n
3 +12n
2 +6n+1+4n
2 +4n+1+2n+1
= 8n
3 +16n
2 +12n+2+1
= 2(4n
3 +8n
2 +6n+1)+1.
266 Solutions
This expresses a
3 + a
2 + a as twice an integer plus 1, so a
3 + a
2 + a is odd, not
even. We have now shown that if a
3 + a
2 + a is even then a is even.
Conversely, we need to show that if a is even, then a
3+a
2+a is even. We will use
direct proof. Suppose a is even, so a = 2n for some integer n. Then a
3 + a
2 + a =
(2n)
3 +(2n)
2 +2n = 8n
3 +4n
2 +2n = 2(4n
3 +2n
2 + n). Therefore, a
3 + a
2 + a is even
because it’s twice an integer. ■
5. An integer a is odd if and only if a
3
is odd.
Proof. Suppose that a is odd. Then a = 2n + 1 for some integer n, and a
3 =
(2n+1)3 = 8n
3 +12n
2 +6n+1 = 2(4n
3 +6n
2 +3n)+1. This shows that a
3
is twice
an integer, plus 1, so a
3
is odd. Thus we’ve proved that if a is odd then a
3
is
odd.
Conversely we need to show that if a
3
is odd, then a is odd. For this we employ
contrapositive proof. Suppose a is not odd. Thus a is even, so a = 2n for some
integer n. Then a
3 = (2n)
3 = 8n
3 = 2(4n
3
) is even (not odd). ■
7. Suppose x, y ∈ R. Then (x+ y)
2 = x
2 + y
2
if and only if x = 0 or y = 0.
Proof. First we prove with direct proof that if (x + y)
2 = x
2 + y
2
, then x = 0 or
y = 0. Suppose (x+ y)
2 = x
2+ y
2
. From this we get x
2+2x y+ y
2 = x
2+ y
2
, so 2x y = 0,
and hence x y = 0. Thus x = 0 or y = 0.
Conversely, we need to show that if x = 0 or y = 0, then (x + y)
2 = x
2 + y
2
. This
will be done with cases.
Case 1. If x = 0 then (x+ y)
2 = (0+ y)
2 = y
2 = 0
2 + y
2 = x
2 + y
2
.
Case 2. If y = 0 then (x+ y)
2 = (x+0)2 = x
2 = x
2 +0
2 = x
2 + y
2
.
Either way, we have (x+ y)
2 = x
2 + y
2
. ■
9. Suppose a ∈ Z. Prove that 14 | a if and only if 7 | a and 2 | a.
Proof. First we prove that if 14 | a, then 7 | a and 2 | a. Direct proof is used.
Suppose 14 | a. This means a = 14m for some integer m. Therefore a = 7(2m),
which means 7 | a, and also a = 2(7m), which means 2 | a. Thus 7 | a and 2 | a.
Conversely, we need to prove that if 7 | a and 2 | a, then 14 | a. Once again direct
proof if used. Suppose 7 | a and 2 | a. Since 2 | a it follows that a = 2m for some
integer m, and that in turn implies that a is even. Since 7 | a it follows that
a = 7n for some integer n. Now, since a is known to be even, and a = 7n, it
follows that n is even (if it were odd, then a = 7n would be odd). Thus n = 2p for
an appropriate integer p, and plugging n = 2p back into a = 7n gives a = 7(2p),
so a = 14p. Therefore 14 | a. ■
267
11. Suppose a,b ∈ Z. Prove that (a−3)b
2
is even if and only if a is odd or b is even.
Proof. First we will prove that if (a −3)b
2
is even, then a is odd or b is even.
For this we use contrapositive proof. Suppose it is not the case that a is
odd or b is even. Then by DeMorgan’s law, a is even and b is odd. Thus
there are integers m and n for which a = 2m and b = 2n + 1. Now observe
(a−3)b
2 = (2m−3)(2n+1)2 = (2m−3)(4n
2+4n+1) = 8mn2+8mn+2m−12n
2−12n−3 =
8mn2 + 8mn + 2m − 12n
2 − 12n − 4 + 1 = 2(4mn2 + 4mn + m − 6n
2 − 6n − 2) + 1. This
shows (a−3)b
2
is odd, so it’s not even.
Conversely, we need to show that if a is odd or b is even, then (a −3)b
2
is even.
For this we use direct proof, with cases.
Case 1. Suppose a is odd. Then a = 2m+1 for some integer m. Thus (a−3)b
2 =
(2m+1−3)b
2 = (2m−2)b
2 = 2(m−1)b
2
. Thus in this case (a−3)b
2
is even.
Case 2. Suppose b is even. Then b = 2n for some integer n. Thus (a −3)b
2 =
(a−3)(2n)
2 = (a−3)4n
2 = 2(a−3)2n
2 =. Thus in this case (a−3)b
2
is even.
Therefore, in any event, (a−3)b
2
is even. ■
13. Suppose a,b ∈ Z. If a+ b is odd, then a
2 + b
2
is odd.
Hint: Use direct proof. Suppose a + b is odd. Argue that this means a and b
have opposite parity. Then use cases.
15. Suppose a,b ∈ Z. Prove that a+ b is even if and only if a and b have the same
parity.
Proof. First we will show that if a+b is even, then a and b have the same parity.
For this we use contrapositive proof. Suppose it is not the case that a and b
have the same parity. Then one of a and b is even and the other is odd. Without
loss of generality, let’s say that a is even and b is odd. Thus there are integers
m and n for which a = 2m and b = 2n +1. Then a + b = 2m+2n +1 = 2(m+ n)+1,
so a+ b is odd, not even.
Conversely, we need to show that if a and b have the same parity, then a + b is
even. For this, we use direct proof with cases. Suppose a and b have the same
parity.
Case 1. Both a and b are even. Then there are integers m and n for which
a = 2m and b = 2n, so a+ b = 2m+2n = 2(m+ n) is clearly even.
Case 2. Both a and b are odd. Then there are integers m and n for which
a = 2m+1 and b = 2n+1, so a+ b = 2m+1+2n+1 = 2(m+ n+1) is clearly even.
Either way, a+ b is even. This completes the proof. ■
17. There is a prime number between 90 and 100.
Proof. Simply observe that 97 is prime. ■
268 Solutions
19. If n ∈ N, then 2
0 +2
1 +2
2 +2
3 +2
4 +··· +2
n = 2
n+1 −1.
Proof. We use direct proof. Suppose n ∈ N. Let S be the number
S = 2
0 +2
1 +2
2 +2
3 +2
4 +··· +2
n−1 +2
n
. (1)
In what follows, we will solve for S and show S = 2
n+1 − 1. Multiplying both
sides of (1) by 2 gives
2S = 2
1 +2
2 +2
3 +2
4 +2
5 +··· +2
n +2
n+1
. (2)
Now subtract Equation (1) from Equation (2) to obtain 2S − S = −2
0 + 2
n+1
,
which simplifies to S = 2
n+1 − 1. Combining this with Equation (1) produces
2
0 +2
1 +2
2 +2
3 +2
4 +··· +2
n = 2
n+1 −1, so the proof is complete. ■
21. Every real solution of x
3 + x+3 = 0 is irrational.
Proof. Suppose for the sake of contradiction that this polynomial has a rational
solution a
b
. We may assume that this fraction is fully reduced, so a and b are
not both even. We have ¡
a
b
¢3
+
a
b
+3 = 0. Clearing the denominator gives
a
3 + ab2 +3b
3 = 0.
Consider two cases: First, if both a and b are odd, the left-hand side is a sum
of three odds, which is odd, meaning 0 is odd, a contradiction. Second, if one
of a and b is odd and the other is even, then the middle term of a
3 + ab2 +3b
3
is even, while a
3 and 3b
2 have opposite parity. Then a
3 + ab2 +3b
3
is the sum
of two evens and an odd, which is odd, again contradicting the fact that 0 is
even. ■
23. Suppose a,b and c are integers. If a | b and a | (b
2 − c), then a | c.
Proof. (Direct) Suppose a | b and a | (b
2 − c). This means that b = ad and
b
2 − c = ae for some integers d and e. Squaring the first equation produces
b
2 = a
2d
2
. Subtracting b
2 − c = ae from b
2 = a
2d
2 gives c = a
2d
2 − ae = a(ad2 − e).
As ad2 − e ∈ Z, it follows that a | c. ■
25. If p > 1 is an integer and n - p for each integer n for which 2 ≤ n ≤
p
p, then p is
prime.
Proof. (Contrapositive) Suppose that p is not prime, so it factors as p = mn for
1 < m,n < p.
Observe that it is not the case that both m >
p
p and n >
p
p, because if this were
true the inequalities would multiply to give mn >
p
p
p
p = p, which contradicts
p = mn.
Therefore m ≤
p
p or n ≤
p
p. Without loss of generality, say n ≤
p
p. Then the
equation p = mn gives n | p, with 1 < n ≤
p
p. Therefore it is not true that n - p
for each integer n for which 2 ≤ n ≤
p
p. ■
269
27. Suppose a,b ∈ Z. If a
2 + b
2
is a perfect square, then a and b are not both odd.
Proof. (Contradiction) Suppose a
2 +b
2
is a perfect square, and a and b are both
odd. As a
2 +b
2
is a perfect square, say c is the integer for which c
2 = a
2 +b
2
. As
a and b are odd, we have a = 2m+1 and b = 2n+1 for integers m and n. Then
c
2 = a
2 + b
2 = (2m+1)2 +(2n+1)2 = 4(m2 + n
2 + mn)+2.
This is even, so c is even also; let c = 2k. Now the above equation results in
(2k)
2 = 4(m2 +n
2 +mn)+2, which simplifies to 2k
2 = 2(m2 +n
2 +mn)+1. Thus 2k
2
is both even and odd, a contradiction. ■
29. If a | bc and gcd(a,b) = 1, then a | c.
Proof. (Direct) Suppose a | bc and gcd(a,b) = 1. The fact that a | bc means bc = az
for some integer z. The fact that gcd(a,b) = 1 means that ax + b y = 1 for some
integers x and y (by Proposition 7.1 on page 126). From this we get acx+bc y = c;
substituting bc = az yields acx+az y = c, that is, a(cx+z y) = c. Therefore a | c. ■
31. If n ∈ Z, then gcd(n,n+1) = 1.
Proof. Suppose d is a positive integer that is a common divisor of n and n+1.
Then n = dx and n +1 = d y for integers x and y. Then 1 = (n +1)− n = d y− dx =
d(y − x). Now, 1 = d(y − x) is only possible if d = ±1 and y − x = ±1. Thus the
greatest common divisor of n and n+1 can be no greater than 1. But 1 does
divide both n and n+1, so gcd(n,n+1) = 1. ■
33. If n ∈ Z, then gcd(2n+1,4n
2 +1) = 1.
Proof. Note that 4n
2 +1 = (2n +1)(2n −1)+2. Therefore, it suffices to show that
gcd(2n + 1,(2n + 1)(2n − 1) + 2) = 1. Let d be a common positive divisor of both
2n+1 and (2n+1)(2n−1)+2, so 2n+1 = dx and (2n+1)(2n−1)+2 = d y for integers
x and y. Substituting the first equation into the second gives dx(2n−1)+2 = d y,
so 2 = d y− dx(2n −1) = d(y−2nx − x). This means d divides 2, so d equals 1 or
2. But the equation 2n +1 = dx means d must be odd. Therefore d = 1, that is,
gcd(2n+1,(2n+1)(2n−1)+2) = 1. ■
35. Suppose a,b ∈ N. Then a = gcd(a,b) if and only if a | b.
Proof. Suppose a = gcd(a,b). This means a is a divisor of both a and b. In
particular a | b.
Conversely, suppose a | b. Then a divides both a and b, so a ≤ gcd(a,b). On the
other hand, since gcd(a,b) divides a, we have a = gcd(a,b)· x for some integer x.
As all integers involved are positive, it follows that a ≥ gcd(a,b).
It has been established that a ≤ gcd(a,b) and a ≥ gcd(a,b). Thus a = gcd(a,b). ■
270 Solutions
Chapter 8 Exercises
1. Prove that {12n : n ∈ Z} ⊆ {2n : n ∈ Z}∩{3n : n ∈ Z}.
Proof. Suppose a ∈ {12n : n ∈ Z}. This means a = 12n for some n ∈ Z. Therefore
a = 2(6n) and a = 3(4n). From a = 2(6n), it follows that a is multiple of 2, so
a ∈ {2n : n ∈ Z}. From a = 3(4n), it follows that a is multiple of 3, so a ∈ {3n : n ∈ Z}.
Thus by definition of the intersection of two sets, we have a ∈ {2n : n ∈ Z} ∩
{3n : n ∈ Z}. Thus {12n : n ∈ Z} ⊆ {2n : n ∈ Z}∩{3n : n ∈ Z}. ■
3. If k ∈ Z, then {n ∈ Z : n | k} ⊆
©
n ∈ Z : n | k
2
ª
.
Proof. Suppose k ∈ Z. We now need to show {n ∈ Z : n | k} ⊆
©
n ∈ Z : n | k
2
ª
.
Suppose a ∈ {n ∈ Z : n | k}. Then it follows that a | k, so there is an integer c for
which k = ac. Then k
2 = a
2
c
2
. Therefore k
2 = a(ac2
), and from this the definition
of divisibility gives a | k
2
. But a | k
2 means that a ∈
©
n ∈ Z : n | k
2
ª
. We have now
shown {n ∈ Z : n | k} ⊆
©
n ∈ Z : n | k
2
ª
. ■
5. If p and q are integers, then {pn : n ∈ N}∩{qn : n ∈ N} 6= ;.
Proof. Suppose p and q are integers. Consider the integer pq. Observe that
pq ∈ {pn : n ∈ N} and pq ∈ {qn : n ∈ N}, so pq ∈ {pn : n ∈ N} ∩ {qn : n ∈ N}. Therefore
{pn : n ∈ N}∩{qn : n ∈ N} 6= ;. ■
7. Suppose A,B and C are sets. If B ⊆ C, then A ×B ⊆ A ×C.
Proof. This is a conditional statement, and we’ll prove it with direct proof.
Suppose B ⊆ C. (Now we need to prove A ×B ⊆ A ×C.)
Suppose (a,b) ∈ A×B. Then by definition of the Cartesian product we have a ∈ A
and b ∈ B. But since b ∈ B and B ⊆ C, we have b ∈ C. Since a ∈ A and b ∈ C, it
follows that (a,b) ∈ A ×C. Now we’ve shown (a,b) ∈ A ×B implies (a,b) ∈ A ×C, so
A ×B ⊆ A ×C.
In summary, we’ve shown that if B ⊆ C, then A ×B ⊆ A ×C. This completes the
proof. ■
9. If A,B and C are sets then A ∩(B ∪C) = (A ∩B)∪(A ∩C).
Proof. We use the distributive law P ∧(Q ∨ R) = (P ∧Q)∨(P ∧ R) from page 50.
A ∩(B ∪C) = {x : x ∈ A ∧ x ∈ B ∪C} (def. of intersection)
= {x : x ∈ A ∧ (x ∈ B ∨ x ∈ C)} (def. of union)
=
©
x :
¡
x ∈ A ∧ x ∈ B
¢
∨
¡
x ∈ A ∧ x ∈ C
¢ª (distributive law)
= {x : (x ∈ A ∩B) ∨ (x ∈ A ∩C)} (def. of intersection)
= (A ∩B)∪(A ∩C) (def. of union)
The proof is complete. ■
271
11. If A and B are sets in a universal set U, then A ∪B = A ∩B.
Proof. Just observe the following sequence of equalities.
A ∪B = U −(A ∪B) (def. of complement)
= {x : (x ∈ U)∧(x ∉ A ∪B)} (def. of −)
= {x : (x ∈ U)∧ ∼ (x ∈ A ∪B)}
= {x : (x ∈ U)∧ ∼ ((x ∈ A)∨(x ∈ B))} (def. of ∪)
= {x : (x ∈ U)∧(∼ (x ∈ A)∧ ∼ (x ∈ B))} (DeMorgan)
= {x : (x ∈ U)∧(x ∉ A)∧(x ∉ B)}
= {x : (x ∈ U)∧(x ∈ U)∧(x ∉ A)∧(x ∉ B)} (x ∈ U) = (x ∈ U)∧(x ∈ U)
= {x : ((x ∈ U)∧(x ∉ A))∧((x ∈ U)∧(x ∉ B))} (regroup)
= {x : (x ∈ U)∧(x ∉ A)}∩{x : (x ∈ U)∧(x ∉ B)} (def. of ∩)
= (U − A)∩(U −B) (def. of −)
= A ∩B (def. of complement)
The proof is complete. ■
13. If A,B and C are sets, then A −(B ∪C) = (A −B)∩(A −C).
Proof. Just observe the following sequence of equalities.
A −(B ∪C) = {x : (x ∈ A)∧(x ∉ B ∪C)} (def. of −)
= {x : (x ∈ A)∧ ∼ (x ∈ B ∪C)}
= {x : (x ∈ A)∧ ∼ ((x ∈ B)∨(x ∈ C))} (def. of ∪)
= {x : (x ∈ A)∧(∼ (x ∈ B)∧ ∼ (x ∈ C))} (DeMorgan)
= {x : (x ∈ A)∧(x ∉ B)∧(x ∉ C)}
= {x : (x ∈ A)∧(x ∈ A)∧(x ∉ B)∧(x ∉ C)} (x ∈ A) = (x ∈ A)∧(x ∈ A)
= {x : ((x ∈ A)∧(x ∉ B))∧((x ∈ A)∧(x ∉ C))} (regroup)
= {x : (x ∈ A)∧(x ∉ B)}∩{x : (x ∈ A)∧(x ∉ C)} (def. of ∩)
= (A −B)∩(A −C) (def. of −)
The proof is complete. ■
15. If A,B and C are sets, then (A ∩B)−C = (A −C)∩(B −C).
Proof. Just observe the following sequence of equalities.
(A ∩B)−C = {x : (x ∈ A ∩B)∧(x ∉ C)} (def. of −)
= {x : (x ∈ A)∧(x ∈ B)∧(x ∉ C)} (def. of ∩)
= {x : (x ∈ A)∧(x ∉ C)∧(x ∈ B)∧(x ∉ C)} (regroup)
= {x : ((x ∈ A)∧(x ∉ C))∧((x ∈ B)∧(x ∉ C))} (regroup)
= {x : (x ∈ A)∧(x ∉ C)}∩{x : (x ∈ B)∧(x ∉ C)} (def. of ∩)
= (A −C)∩(B −C) (def. of ∩)
The proof is complete. ■
17. If A,B and C are sets, then A ×(B ∩C) = (A ×B)∩(A ×C).
Proof. See Example 8.12. ■
272 Solutions
19. Prove that {9
n
: n ∈ Z} ⊆ {3
n
: n ∈ Z}, but {9
n
: n ∈ Z} 6= {3
n
: n ∈ Z}.
Proof. Suppose a ∈ {9
n
: n ∈ Z}. This means a = 9
n
for some integer n ∈ Z. Thus
a = 9
n = (32
)
n = 3
2n
. This shows a is an integer power of 3, so a ∈ {3
n
: n ∈ Z}.
Therefore a ∈ {9
n
: n ∈ Z} implies a ∈ {3
n
: n ∈ Z}, so {9
n
: n ∈ Z} ⊆ {3
n
: n ∈ Z}.
But notice {9
n
: n ∈ Z} 6= {3
n
: n ∈ Z} as 3 ∈ {3
n
: n ∈ Z}, but 3 ∉ {9
n
: n ∈ Z}. ■
21. Suppose A and B are sets. Prove A ⊆ B if and only if A −B = ;.
Proof. First we will prove that if A ⊆ B, then A −B = ;. Contrapositive proof is
used. Suppose that A −B 6= ;. Thus there is an element a ∈ A −B, which means
a ∈ A but a ∉ B. Since not every element of A is in B, we have A 6⊆ B.
Conversely, we will prove that if A −B = ;, then A ⊆ B. Again, contrapositive
proof is used. Suppose A 6⊆ B. This means that it is not the case that every
element of A is an element of B, so there is an element a ∈ A with a ∉ B.
Therefore we have a ∈ A −B, so A −B 6= ;. ■
23. For each a ∈ R, let Aa =
©
(x,a(x
2 −1)) ∈ R
2
: x ∈ R
ª
. Prove that \
a∈R
Aa = {(−1,0),(1,0))}.
Proof. First we will show that {(−1,0),(1,0))} ⊆
\
a∈R
Aa. Notice that for any a ∈ R,
we have (−1,0) ∈ Aa because Aa contains the ordered pair (−1,a((−1)2−1) = (−1,0).
Similarly (1,0) ∈ Aa. Thus each element of {(−1,0),(1,0))} belongs to every set
Aa, so every element of \
a∈R
Aa, so {(−1,0),(1,0))} ⊆
\
a∈R
Aa.
Now we will show \
a∈R
Aa ⊆ {(−1,0),(1,0))}. Suppose (c,d) ∈
\
a∈R
Aa. This means
(c,d) is in every set Aa. In particular (c,d) ∈ A0 =
©
(x,0(x
2 −1)) : x ∈ R
ª
= {(x,0) : x ∈ R}.
It follows that d = 0. Then also we have (c,d) = (c,0) ∈ A1 =
©
(x,1(x
2 −1)) : x ∈ R
ª
= ©
(x, x
2 −1) : x ∈ R
ª
. Therefore (c,0) has the form (c, c
2 −1), that is (c,0) = (c, c
2 −1).
From this we get c
2 − 1 = 0, so c = ±1. Therefore (c,d) = (1,0) or (c,d) = (−1,0),
so (c,d) ∈ {(−1,0),(1,0))}. This completes the demonstration that (c,d) ∈
\
a∈R
Aa
implies (c,d) ∈ {(−1,0),(1,0))}, so it follows that \
a∈R
Aa ⊆ {(−1,0),(1,0))}.
Now it’s been shown that {(−1,0),(1,0))} ⊆
\
a∈R
Aa and \
a∈R
Aa ⊆ {(−1,0),(1,0))}, so it
follows that \
a∈R
Aa = {(−1,0),(1,0))}. ■
25. Suppose A,B,C and D are sets. Prove that (A ×B)∪(C ×D) ⊆ (A ∪C)×(B ∪D).
Proof. Suppose (a,b) ∈ (A ×B)∪(C ×D).
By definition of union, this means (a,b) ∈ (A ×B) or (a,b) ∈ (C ×D).
We examine these two cases individually.
Case 1. Suppose (a,b) ∈ (A × B). By definition of ×, it follows that a ∈ A and
b ∈ B. From this, it follows from the definition of ∪ that a ∈ A ∪C and b ∈ B ∪D.
Again from the definition of ×, we get (a,b) ∈ (A ∪C)×(B ∪D).
273
Case 2. Suppose (a,b) ∈ (C × D). By definition of ×, it follows that a ∈ C and
b ∈ D. From this, it follows from the definition of ∪ that a ∈ A ∪C and b ∈ B ∪D.
Again from the definition of ×, we get (a,b) ∈ (A ∪C)×(B ∪D).
In either case, we obtained (a,b) ∈ (A ∪C)×(B ∪D),
so we’ve proved that (a,b) ∈ (A ×B)∪(C ×D) implies (a,b) ∈ (A ∪C)×(B ∪D).
Therefore (A ×B)∪(C ×D) ⊆ (A ∪C)×(B ∪D). ■
27. Prove {12a+4b : a,b ∈ Z} = {4c : c ∈ Z}.
Proof. First we show {12a+4b : a,b ∈ Z} ⊆ {4c : c ∈ Z}. Suppose x ∈ {12a+4b : a,b ∈ Z}.
Then x = 12a+4b for some integers a and b. From this we get x = 4(3a+ b), so
x = 4c where c is the integer 3a+b. Consequently x ∈ {4c : c ∈ Z}. This establishes
that {12a+4b : a,b ∈ Z} ⊆ {4c : c ∈ Z}.
Next we show {4c : c ∈ Z} ⊆ {12a+4b : a,b ∈ Z}. Suppose x ∈ {4c : c ∈ Z}. Then x = 4c
for some c ∈ Z. Thus x = (12 + 4(−2))c = 12c + 4(−2c), and since c and −2c are
integers we have x ∈ {12a+4b : a,b ∈ Z}.
This proves that {12a+4b : a,b ∈ Z} = {4c : c ∈ Z}. ■
29. Suppose A 6= ;. Prove that A ×B ⊆ A ×C, if and only if B ⊆ C.
Proof. First we will prove that if A×B ⊆ A×C, then B ⊆ C. Using contrapositive,
suppose that B 6⊆ C. This means there is an element b ∈ B with b ∉ C. Since
A 6= ;, there exists an element a ∈ A. Now consider the ordered pair (a,b). Note
that (a,b) ∈ A ×B, but (a,b) 6∈ A ×C. This means A ×B 6⊆ A ×C.
Conversely, we will now show that if B ⊆ C, then A ×B ⊆ A ×C. We use direct
proof. Suppose B ⊆ C. Assume that (a,b) ∈ A × B. This means a ∈ A and b ∈ B.
But, as B ⊆ C, we also have b ∈ C. From a ∈ A and b ∈ C, we get (a,b) ∈ A × C.
We’ve now shown (a,b) ∈ A ×B implies (a,b) ∈ A ×C, so A ×B ⊆ A ×C. ■
31. Suppose B 6= ; and A ×B ⊆ B ×C. Prove A ⊆ C.
Proof. Suppose B 6= ; and A × B ⊆ B × C. In what follows, we show that A ⊆ C.
Let x ∈ A. Because B is not empty, it contains some element b. Observe that
(x,b) ∈ A × B. But as A × B ⊆ B × C, we also have (x,b) ∈ B × C, so, in particular,
x ∈ B. As x ∈ A and x ∈ B, we have (x, x) ∈ A × B. But as A × B ⊆ B × C, it follows
that (x, x) ∈ B ×C. This implies x ∈ C.
Now we’ve shown x ∈ A implies x ∈ C, so A ⊆ C. ■
Chapter 9 Exercises
1. If x, y ∈ R, then |x+ y| = |x| +|y|.
This is false.
Disproof: Here is a counterexample: Let x = 1 and y = −1. Then |x+ y| = 0 and
|x| +|y| = 2, so it’s not true that |x+ y| = |x| +|y|.
274 Solutions
3. If n ∈ Z and n
5 − n is even, then n is even.
This is false.
Disproof: Here is a counterexample: Let n = 3. Then n
5 − n = 3
5 −3 = 240, but
n is not even.
5. If A, B,C and D are sets, then (A ×B)∪(C ×D) = (A ∪C)×(B ∪D).
This is false.
Disproof: Here is a counterexample: Let A = {1,2}, B = {1,2}, C = {2,3} and
D = {2,3}. Then (A×B)∪(C×D) = {(1,1),(1,2),(2,1),(2,2)}∪{(2,2),(2,3),(3,2),(3,3)} =
{(1,1),(1,2),(2,1),(2,2),(2,3),(3,2),(3,3)}. Also (A ∪ C) × (B ∪ D) = {1,2,3} × {1,2,3}=
{(1,1),(1,2),(1,3),(2,1),(2,2),(2,3),(3,1),(3,2),(3,3)}, so you can see that (A × B) ∪
(C ×D) 6= (A ∪C)×(B ∪D).
7. If A, B and C are sets, and A ×C = B ×C, then A = B.
This is false.
Disproof: Here is a counterexample: Let A = {1}, B = {2} and C = ;. Then
A ×C = B ×C = ;, but A 6= B.
9. If A and B are sets, then P(A)−P(B) ⊆ P(A −B).
This is false.
Disproof: Here is a counterexample: Let A = {1,2} and B = {1}. Then P(A)−
P(B) = {;,{1},{2},{1,2}}−{;,{1}} = {{2},{1,2}}. Also P(A −B) = P({2}) = {;,{2}}. In
this example we have P(A)−P(B) 6⊆ P(A −B).
11. If a,b ∈ N, then a+ b < ab.
This is false.
Disproof: Here is a counterexample: Let a = 1 and b = 1. Then a + b = 2 and
ab = 1, so it’s not true that a+ b < ab.
13. There exists a set X for which R ⊆ X and ; ∈ X. This is true.
Proof. Simply let X = R∪{;}. If x ∈ R, then x ∈ R∪{;} = X, so R ⊆ X. Likewise,
; ∈ R∪{;} = X because ; ∈ {;}. ■
15. Every odd integer is the sum of three odd integers. This is true.
Proof. Suppose n is odd. Then n = n +1+(−1), and therefore n is the sum of
three odd integers. ■
17. For all sets A and B, if A −B = ;, then B 6= ;.
This is false.
Disproof: Here is a counterexample: Just let A = ; and B = ;. Then A −B = ;,
but it’s not true that B 6= ;.
19. For every r, s ∈ Q with r < s, there is an irrational number u for which r < u < s.
This is true.
Proof. (Direct) Suppose r, s ∈ Q with r < s. Consider the number u = r +
p
2
s−r
2
.
In what follows we will show that u is irrational and r < u < s. Certainly since
275
s− r is positive, it follows that r < r +
p
2
s−r
2
= u. Also, since p
2 < 2 we have
u = r +
p
2
s− r
2
< r +2
s− r
2
= s,
and therefore u < s. Thus we can conclude r < u < s.
Now we just need to show that u is irrational. Suppose for the sake of contradiction
that u is rational. Then u =
a
b
for some integers a and b. Since r and s
are rational, we have r =
c
d
and s =
e
f
for some c,d, e, f ∈ Z. Now we have
u = r +
p
2
s− r
2
a
b
=
c
d
+
p
2
e
f
−
c
d
2
ad − bc
bd
=
p
2
ed − c f
2d f
(ad − bc)2d f
bd(ed − c f )
=
p
2
This expresses p
2 as a quotient of two integers, so p
2 is rational, a contradiction.
Thus u is irrational.
In summary, we have produced an irrational number u with r < u < s, so the
proof is complete. ■
21. There exist two prime numbers p and q for which p − q = 97.
This statement is false.
Disproof: Suppose for the sake of contradiction that this is true. Let p and
q be prime numbers for which p − q = 97. Now, since their difference is odd, p
and q must have opposite parity, so one of p and q is even and the other is
odd. But there exists only one even prime number (namely 2), so either p = 2
or q = 2. If p = 2, then p − q = 97 implies q = 2 − 97 = −95, which is not prime.
On the other hand if q = 2, then p − q = 97 implies p = 99, but that’s not prime
either. Thus one of p or q is not prime, a contradiction.
23. If x, y ∈ R and x
3 < y
3
, then x < y. This is true.
Proof. (Contrapositive) Suppose x ≥ y. We need to show x
3 ≥ y
3
.
Case 1. Suppose x and y have opposite signs, that is one of x and y is positive
and the other is negative. Then since x ≥ y, x is positive and y is negative.
Then, since the powers are odd, x
3
is positive and y
3
is negative, so x
3 ≥ y
3
.
Case 2. Suppose x and y do not have opposite signs. Then x
2 + x y+ y
2 ≥ 0 and
also x− y ≥ 0 because x ≥ y. Thus we have x
3 − y
3 = (x− y)(x
2 + x y+ y
2
) ≥ 0. From
this we get x
3 − y
3 ≥ 0, so x
3 ≥ y
3
.
In either case we have x
3 ≥ y
3
. ■
276 Solutions
25. For all a,b, c ∈ Z, if a | bc, then a | b or a | c.
This is false.
Disproof: Let a = 6, b = 3 and c = 4. Note that a | bc, but a - b and a - c.
27. The equation x
2 = 2
x has three real solutions.
Proof. By inspection, the numbers x = 2 and x = 4 are two solutions of this
equation. But there is a third solution. Let m be the real number for which
m2
m =
1
2
. Then negative number x = −2m is a solution, as follows.
x
2 = (−2m)
2 = 4m2 = 4
µ
m2
m
2m
¶2
= 4
Ã
1
2
2m
!2
=
1
2
2m
= 2
−2m = 2
x
.
Therefore we have three solutions 2, 4 and m. ■
29. If x, y ∈ R and |x+ y| = |x− y|, then y = 0.
This is false.
Disproof: Let x = 0 and y = 1. Then |x+ y| = |x− y|, but y = 1.
31. No number appears in Pascal’s triangle more than four times.
Disproof: The number 120 appears six times. Check that ¡
10
3
¢
=
¡
10
7
¢
=
¡
16
2
¢
= ¡
16
14
¢
=
¡
120
1
¢
=
¡
120
119
¢
= 120.
33. Suppose f (x) = a0 + a1x+ a2x
2 +··· + anx
n
is a polynomial of degree 1 or greater,
and for which each coefficient ai is in N. Then there is an n ∈ N for which the
integer f (n) is not prime.
Proof. (Outline) Note that, because the coefficients are all positive and the
degree is greater than 1, we have f (1) > 1. Let b = f (1) > 1. Now, the polynomial
f (x) − b has a root 1, so f (x) − b = (x − 1)g(x) for some polynomial g. Then
f (x) = (x − 1)g(x) + b. Now note that f (b + 1) = bg(b) + b = b(g(b) + 1). If we can
now show that g(b) + 1 is an integer, then we have a nontrivial factoring
f (b +1) = b(g(b)+1), and f (b +1) is not prime. To complete the proof, use the
fact that f (x)− b = (x−1)g(x) has integer coefficients, and deduce that g(x) must
also have integer coefficients. ■
Chapter 10 Exercises
1. For every integer n ∈ N, it follows that 1+2+3+4+··· + n =
n
2 + n
2
.
Proof. We will prove this with mathematical induction.
(1) Observe that if n = 1, this statement is 1 =
1
2 +1
2
, which is obviously true.
277
(2) Consider any integer k ≥ 1. We must show that Sk implies Sk+1. In other
words, we must show that if 1+2+3+4+··· + k =
k
2+k
2
is true, then
1+2+3+4+··· + k +(k +1) =
(k +1)2 +(k +1)
2
is also true. We use direct proof.
Suppose k ≥ 1 and 1+2+3+4+··· + k =
k
2+k
2
. Observe that
1+2+3+4+··· + k +(k +1) =
(1+2+3+4+··· + k)+(k +1) =
k
2 + k
2
+(k +1) =
k
2 + k +2(k +1)
2
=
k
2 +2k +1 + k +1
2
=
(k +1)2 +(k +1)
2
.
Therefore we have shown that 1+2+3+4+··· + k +(k +1) =
(k+1)2+(k+1)
2
. ■
3. For every integer n ∈ N, it follows that 1
3 +2
3 +3
3 +4
3 +··· + n
3 =
n
2
(n+1)2
4
.
Proof. We will prove this with mathematical induction.
(1) When n = 1 the statement is 1
3 =
1
2
(1+1)2
4
=
4
4
= 1, which is true.
(2) Now assume the statement is true for some integer n = k ≥ 1, that is assume
1
3 +2
3 +3
3 +4
3 +··· + k
3 =
k
2
(k+1)2
4
. Observe that this implies the statement is
true for n = k +1.
1
3 +2
3 +3
3 +4
3 +··· + k
3 +(k +1)3 =
(13 +2
3 +3
3 +4
3 +··· + k
3
)+(k +1)3 =
k
2
(k +1)2
4
+(k +1)3 =
k
2
(k +1)2
4
+
4(k +1)3
4
=
k
2
(k +1)2 +4(k +1)3
4
=
(k +1)2
(k
2 +4(k +1)1
)
4
=
(k +1)2
(k
2 +4k +4)
4
=
(k +1)2
(k +2)2
4
=
(k +1)2
((k +1)+1)2
4
Therefore 1
3 +2
3 +3
3 +4
3 + ··· + k
3 +(k +1)3 =
(k+1)2
((k+1)+1)2
4
, which means the
statement is true for n = k +1. ■
278 Solutions
5. If n ∈ N, then 2
1 +2
2 +2
3 +··· +2
n = 2
n+1 −2.
Proof. The proof is by mathematical induction.
(1) When n = 1, this statement is 2
1 = 2
1+1 −2, or 2 = 4−2, which is true.
(2) Now assume the statement is true for some integer n = k ≥ 1, that is assume
2
1 +2
2 +2
3 +··· +2
k = 2
k+1 −2. Observe this implies that the statement is true
for n = k +1, as follows:
2
1 +2
2 +2
3 +··· +2
k +2
k+1 =
(21 +2
2 +2
3 +··· +2
k
)+2
k+1 =
2
k+1 −2+2
k+1 = 2·2
k+1 −2
= 2
k+2 −2
= 2
(k+1)+1 −2
Thus we have 2
1 +2
2 +2
3 +··· +2
k +2
k+1 = 2
(k+1)+1 −2, so the statement is true
for n = k +1.
Thus the result follows by mathematical induction. ■
7. If n ∈ N, then 1·3+2·4+3·5+4·6+··· + n(n+2) =
n(n+1)(2n+7)
6
.
Proof. The proof is by mathematical induction.
(1) When n = 1, we have 1·3 =
1(1+1)(2+7)
6
, which is the true statement 3 =
18
6
.
(2) Now assume the statement is true for some integer n = k ≥ 1, that is assume
1·3+2·4+3·5+4·6+··· + k(k +2) =
k(k+1)(2k+7)
6
. Now observe that
1·3+2·4+3·5+4·6+··· + k(k +2)+(k +1)((k +1)+2) =
(1·3+2·4+3·5+4·6+··· + k(k +2))+(k +1)((k +1)+2) =
k(k +1)(2k +7)
6
+(k +1)((k +1)+2) =
k(k +1)(2k +7)
6
+
6(k +1)(k +3)
6
=
k(k +1)(2k +7)+6(k +1)(k +3)
6
=
(k +1)(k(2k +7)+6(k +3))
6
=
(k +1)(2k
2 +13k +18)
6
=
(k +1)(k +2)(2k +9)
6
=
(k +1)((k +1)+1)(2(k +1)+7)
6
Thus we have 1·3+2·4+3·5+4·6+···+k(k+2)+(k+1)((k+1)+2) =
(k+1)((k+1)+1)(2(k+1)+7)
6
,
and this means the statement is true for n = k +1.
Thus the result follows by mathematical induction. ■
279
9. For any integer n ≥ 0, it follows that 24 | (52n −1).
Proof. The proof is by mathematical induction.
(1) For n = 0, the statement is 24 | (52·0 −1). This is 24 | 0, which is true.
(2) Now assume the statement is true for some integer n = k ≥ 1, that is assume
24 | (52k −1). This means 5
2k −1 = 24a for some integer a, and from this we
get 5
2k = 24a+1. Now observe that
5
2(k+1) −1 =
5
2k+2 −1 =
5
2
5
2k −1 =
5
2
(24a+1)−1 =
25(24a+1)−1 =
25·24a+25−1 = 24(25a+1).
This shows 5
2(k+1) −1 = 24(25a+1), which means 24 | 5
2(k+1) −1.
This completes the proof by mathematical induction. ■
11. For any integer n ≥ 0, it follows that 3 | (n
3 +5n+6).
Proof. The proof is by mathematical induction.
(1) When n = 0, the statement is 3 | (03 +5·0+6), or 3 | 6, which is true.
(2) Now assume the statement is true for some integer n = k ≥ 0, that is assume
3 | (k
3 + 5k + 6). This means k
3 + 5k + 6 = 3a for some integer a. We need to
show that 3 | ((k +1)3 +5(k +1)+6). Observe that
(k +1)3 +5(k +1)+6 = k
3 +3k
2 +3k +1+5k +5+6
= (k
3 +5k +6)+3k
2 +3k +6
= 3a+3k
2 +3k +6
= 3(a+ k
2 + k +2).
Thus we have deduced (k +1)3 −(k +1) = 3(a+ k
2 + k +2). Since a+ k
2 + k +2 is
an integer, it follows that 3 | ((k +1)3 +5(k +1)+6).
It follows by mathematical induction that 3 | (n
3 +5n+6) for every n ≥ 0. ■
13. For any integer n ≥ 0, it follows that 6 | (n
3 − n).
Proof. The proof is by mathematical induction.
(1) When n = 0, the statement is 6 | (03 −0), or 6 | 0, which is true.
280 Solutions
(2) Now assume the statement is true for some integer n = k ≥ 0, that is, assume
6 | (k
3 − k). This means k
3 − k = 6a for some integer a. We need to show that
6 | ((k +1)3 −(k +1)). Observe that
(k +1)3 −(k +1) = k
3 +3k
2 +3k +1− k −1
= (k
3 − k)+3k
2 +3k
= 6a+3k
2 +3k
= 6a+3k(k +1).
Thus we have deduced (k +1)3 −(k +1) = 6a+3k(k +1). Since one of k or (k +1)
must be even, it follows that k(k +1) is even, so k(k +1) = 2b for some integer
b. Consequently (k + 1)3 − (k + 1) = 6a + 3k(k + 1) = 6a + 3(2b) = 6(a + b). Since
(k +1)3 −(k +1) = 6(a+ b) it follows that 6 | ((k +1)3 −(k +1)).
Thus the result follows by mathematical induction. ■
15. If n ∈ N, then 1
1·2
+
1
2·3
+
1
3·4
+
1
4·5
+··· + 1
n(n+1) = 1−
1
n+1
.
Proof. The proof is by mathematical induction.
(1) When n = 1, the statement is 1
1(1+1) = 1−
1
1+1
, which simplifies to 1
2
=
1
2
.
(2) Now assume the statement is true for some integer n = k ≥ 1, that is assume
1
1·2
+
1
2·3
+
1
3·4
+
1
4·5
+··· + 1
k(k+1) = 1−
1
k+1
. Next we show that the statement for
n = k +1 is true. Observe that
1
1·2
+
1
2·3
+
1
3·4
+
1
4·5
+··· +
1
k(k +1)
+
1
(k +1)((k +1)+1)
=
µ
1
1·2
+
1
2·3
+
1
3·4
+
1
4·5
+··· +
1
k(k +1) ¶
+
1
(k +1)(k +2)
=
µ
1−
1
k +1
¶
+
1
(k +1)(k +2)
=
1−
1
k +1
+
1
(k +1)(k +2)
=
1−
k +2
(k +1)(k +2)
+
1
(k +1)(k +2)
=
1−
k +1
(k +1)(k +2)
=
1−
1
k +2
=
1−
1
(k +1)+1
.
This establishes 1
1·2
+
1
2·3
+
1
3·4
+
1
4·5
+···+ 1
(k+1)((k+1)+1
= 1−
1
(k+1)+1
, which is to say
that the statement is true for n = k +1.
This completes the proof by mathematical induction. ■
281
17. Suppose A1, A2,... An are sets in some universal set U, and n ≥ 2. Prove that
A1 ∩ A2 ∩··· ∩ An = A1 ∪ A2 ∪··· ∪ An.
Proof. The proof is by strong induction.
(1) When n = 2 the statement is A1 ∩ A2 = A1 ∪ A2. This is not an entirely
obvious statement, so we have to prove it. Observe that
A1 ∩ A2 = {x : (x ∈ U)∧(x ∉ A1 ∩ A2)} (definition of complement)
= {x : (x ∈ U)∧ ∼ (x ∈ A1 ∩ A2)}
= {x : (x ∈ U)∧ ∼ ((x ∈ A1)∧(x ∈ A2))} (definition of ∩)
= {x : (x ∈ U)∧(∼ (x ∈ A1)∨ ∼ (x ∈ A2))} (DeMorgan)
= {x : (x ∈ U)∧((x ∉ A1)∨(x ∉ A2))}
= {x : (x ∈ U)∧(x ∉ A1)∨(x ∈ U)∧(x ∉ A2)} (distributive prop.)
= {x : ((x ∈ U)∧(x ∉ A1))}∪{x : ((x ∈ U)∧(x ∉ A2))} (def. of ∪)
= A1 ∪ A2 (definition of complement)
(2) Let k ≥ 2. Assume the statement is true if it involves k or fewer sets. Then
A1 ∩ A2 ∩··· ∩ Ak−1 ∩ Ak ∩ Ak+1 =
A1 ∩ A2 ∩··· ∩ Ak−1 ∩(Ak ∩ Ak+1) = A1 ∪ A2 ∪··· ∪ Ak−1 ∪ Ak ∩ Ak+1
= A1 ∪ A2 ∪··· ∪ Ak−1 ∪ Ak ∪ Ak+1
Thus the statement is true when it involves k +1 sets.
This completes the proof by strong induction. ■
19. Prove Pn
k=1
1/k
2 ≤ 2−1/n for every n.
Proof. This clearly holds for n = 1. Assume it holds for some n ≥ 1. Then
Pn+1
k=1
1/k
2 ≤ 2−1/n+1/(n+1)2 = 2−
(n+1)2−n
n(n+1)2 ≤ 2−1/(n+1). The proof is complete. ■
21. If n ∈ N, then 1
1
+
1
2
+
1
3
+··· + 1
2
n ≥ 1+
n
2
.
Proof. If n = 1, the result is obvious.
Assume the proposition holds for some n > 1. Then
1
1
+
1
2
+
1
3
+··· +
1
2
n+1
=
µ
1
1
+
1
2
+
1
3
+··· +
1
2
n
¶
+
µ
1
2
n +1
+
1
2
n +2
+
1
2
n +3
+··· +
1
2
n+1
¶
≥
³
1+
n
2
´
+
µ
1
2
n +1
+
1
2
n +2
+
1
2
n +3
+··· +
1
2
n+1
¶
.
Now, the sum ³
1
2
n+1
+
1
2
n+2
+
1
2
n+3
+··· + 1
2
n+1
´
on the right has 2
n+1−2
n = 2
n
terms,
all greater than or equal to 1
2
n+1
, so the sum is greater than 2
n 1
2
n+1 =
1
2
. Therefore
we get 1
1
+
1
2
+
1
3
+ ··· + 1
2
n+1 ≥
¡
1+
n
2
¢
+
³
1
2
n+1
+
1
2
n+2
+
1
2
n+3
+··· + 1
2
n+1
´
≥
¡
1+
n
2
¢
+
1
2
=
1+
n+1
2
. This means the result is true for n+1, so the theorem is proved. ■
282 Solutions
23. Use induction to prove the binomial theorem (x+ y)
n =
Pn
i=0
¡
n
i
¢
x
n−i
y
i
.
Proof. Notice that when n = 1, the formula is (x + y)
1 =
¡
1
0
¢
x
1
y
0 +
¡
1
1
¢
x
0
y
1 = x + y,
which is true.
Now assume the theorem is true for some n > 1. We will show that this implies
that it is true for the power n+1. Just observe that
(x+ y)
n+1 = (x+ y)(x+ y)
n
= (x+ y)
Xn
i=0
Ã
n
i
!
x
n−i
y
i
=
Xn
i=0
Ã
n
i
!
x
(n+1)−i
y
i +
Xn
i=0
Ã
n
i
!
x
n−i
y
i+1
=
Xn
i=0
"Ãn
i
!
+
Ã
n
i −1
!# x
(n+1)−i
y
i + y
n+1
=
Xn
i=0
Ã
n+1
i
!
x
(n+1)−i
y
i +
Ã
n+1
n+1
!
y
n+1
=
nX+1
i=0
Ã
n+1
i
!
x
(n+1)−i
y
i
.
This shows that the formula is true for (x+ y)
n+1
, so the theorem is proved. ■
25. Concerning the Fibonacci sequence, prove that F1+F2+F3+F4+...+Fn = Fn+2−1.
Proof. The proof is by induction.
(1) When n = 1 the statement is F1 = F1+2 −1 = F3 −1 = 2−1 = 1, which is true.
Also when n = 2 the statement is F1 +F2 = F2+2 −1 = F4 −1 = 3−1 = 2, which is
true, as F1 + F2 = 1+1 = 2.
(2) Now assume k ≥ 1 and F1 + F2 + F3 + F4 +...+ Fk = Fk+2 −1. We need to show
F1 + F2 + F3 + F4 +...+ Fk + Fk+1 = Fk+3 −1. Observe that
F1 + F2 + F3 + F4 +...+ Fk + Fk+1 =
(F1 + F2 + F3 + F4 +...+ Fk)+ Fk+1 =
Fk+2 −1+ +Fk+1 = (Fk+1 + Fk+2)−1
= Fk+3 −1.
This completes the proof by induction. ■
27. Concerning the Fibonacci sequence, prove that F1 + F3 +··· + F2n−1 = F2n.
Proof. If n = 1, the result is immediate. Assume for some n > 1 we have
Pn
i=1
F2i−1 = F2n. Then Pn+1
i=1
F2i−1 = F2n+1+
Pn
i=1
F2i−1 = F2n+1+F2n = F2n+2 = F2(n+1)
as desired. ■
283
29. Prove that ¡
n
0
¢
+
¡
n−1
1
¢
+
¡
n−2
2
¢
+
¡
n−3
3
¢
+··· +¡
1
n−1
¢
+
¡
0
n
¢
= Fn+1.
Proof. (Strong Induction) For n = 1 this is ¡
1
0
¢
+
¡
0
1
¢
= 1+0 = 1 = F2 = F1+1. Thus
the assertion is true when n = 1.
Now fix n and assume that ¡
k
0
¢
+
¡
k−1
1
¢
+
¡
k−2
2
¢
+
¡
k−3
3
¢
+···+ ¡
1
k−1
¢
+
¡
0
k
¢
= Fk+1 whenever
k < n. In what follows we use the identity ¡
n
k
¢
=
¡
n−1
k−1
¢
+
¡
n−1
k
¢
. We also often use
¡
a
b
¢
= 0 whenever it is untrue that 0 ≤ b ≤ a.
Ã
n
0
!
+
Ã
n−1
1
!
+
Ã
n−2
2
!
+··· +Ã
1
n−1
!
+
Ã
0
n
!
=
Ã
n
0
!
+
Ã
n−1
1
!
+
Ã
n−2
2
!
+··· +Ã
1
n−1
!
=
Ã
n−1
−1
!
+
Ã
n−1
0
!
+
Ã
n−2
0
!
+
Ã
n−2
1
!
+
Ã
n−3
1
!
+
Ã
n−3
2
!
+··· +Ã
0
n−1
!
+
Ã
0
n
!
=
Ã
n−1
0
!
+
Ã
n−2
0
!
+
Ã
n−2
1
!
+
Ã
n−3
1
!
+
Ã
n−3
2
!
+··· +Ã
0
n−1
!
+
Ã
0
n
!
=
"Ãn−1
0
!
+
Ã
n−2
1
!
+··· +Ã
0
n−1
!#+
"Ãn−2
0
!
+
Ã
n−3
1
!
+··· +Ã
0
n−2
!#
= Fn + Fn−1 = Fn
This completes the proof. ■
31. Prove that Pn
k=0
¡
k
r
¢
=
¡
n+1
r+1
¢
, where r ∈ N.
Hint: Use induction on the integer n. After doing the basis step, break up the
expression ¡
k
r
¢
as ¡
k
r
¢
=
¡
k−1
r−1
¢
+
¡
k−1
r
¢
. Then regroup, use the induction hypothesis,
and recombine using the above identity.
33. Suppose that n infinitely long straight lines lie on the plane in such a way that
no two are parallel, and no three intersect at a single point. Show that this
arrangement divides the plane into n
2+n+2
2
regions.
Proof. The proof is by induction. For the basis step, suppose n = 1. Then there
is one line, and it clearly divides the plane into 2 regions, one on either side of
the line. As 2 =
1
2+1+2
2
=
n
2+n+2
2
, the formula is correct when n = 1.
Now suppose there are n+1 lines on the plane, and that the formula is correct
for when there are n lines on the plane. Single out one of the n+1 lines on the
plane, and call it `. Remove line `, so that there are now n lines on the plane.
By the induction hypothesis, these n lines
divide the plane into n
2+n+2
2
regions. Now add
line ` back. Doing this adds an additional
n + 1 regions. (The diagram illustrates the
case where n + 1 = 5. Without `, there are
n = 4 lines. Adding ` back produces n+1 = 5
new regions.)
`
1
2
3
4
5
284 Solutions
Thus, with n+1 lines there are all together (n+1)+
n
2+n+2
2
regions. Observe
(n+1)+
n
2 + n+2
2
=
2n+2+ n
2 + n+2
2
=
(n+1)2 +(n+1)+2
2
.
Thus, with n + 1 lines, we have (n+1)2+(n+1)+2
2
regions, which means that the
formula is true for when there are n + 1 lines. We have shown that if the
formula is true for n lines, it is also true for n +1 lines. This completes the
proof by induction. ■
35. If n,k ∈ N, and n is even and k is odd, then ¡
n
k
¢
is even.
Proof. Notice that if k is not a value between 0 and n, then ¡
n
k
¢
= 0 is even; thus
from here on we can assume that 0 < k < n. We will use strong induction.
For the basis case, notice that the assertion is true for the even values n = 2
and n = 4:
¡
2
1
¢
= 2;
¡
4
1
¢
= 4;
¡
4
3
¢
= 4 (even in each case).
Now fix and even n assume that ¡m
k
¢
is even whenever m is even, k is odd, and
m < n. Using the identity ¡
n
k
¢
=
¡
n−1
k−1
¢
+
¡
n−1
k
¢
three times, we get
Ã
n
k
!
=
Ã
n−1
k −1
!
+
Ã
n−1
k
!
=
Ã
n−2
k −2
!
+
Ã
n−2
k −1
!
+
Ã
n−2
k −1
!
+
Ã
n−2
k
!
=
Ã
n−2
k −2
!
+2
Ã
n−2
k −1
!
+
Ã
n−2
k
!
.
Now, n−2 is even, and k and k −2 are odd. By the inductive hypothesis, the
outer terms of the above expression are even, and the middle is clearly even;
thus we have expressed ¡
n
k
¢
as the sum of three even integers, so it is even. ■
Chapter 11 Exercises
Section 11.0 Exercises
1. Let A = {0,1,2,3,4,5}. Write out the relation R that expresses > on A. Then
illustrate it with a diagram.
R =
©
(5,4),(5,3),(5,3),(5,3),(5,1),(5,0),(4,3),(4,2),(4,1),
(4,0),(3,2),(3,1),(3,0),(2,1),(2,0),(1,0)ª
0
2 1
3
4 5
3. Let A = {0,1,2,3,4,5}. Write out the relation R that expresses ≥ on A. Then
illustrate it with a diagram.
285
R =
©
(5,5),(5,4),(5,3),(5,2),(5,1),(5,0),
(4,4),(4,3),(4,2),(4,1),(4,0),
(3,3),(3,2),(3,1),(3,0),
(2,2),(2,1),(2,0),(1,1),(1,0),(0,0)ª
0
2 1
3
4 5
5. The following diagram represents a relation R on a set A. Write the sets A
and R. Answer: A = {0,1,2,3,4,5}; R = {(3,3),(4,3),(4,2),(1,2),(2,5),(5,0)}
7. Write the relation < on the set A = Z as a subset R of Z×Z. This is an infinite
set, so you will have to use set-builder notation.
Answer: R = {(x, y) ∈ Z×Z : y− x ∈ N}
9. How many different relations are there on the set A =
©
1,2,3,4,5,6
ª
?
Consider forming a relation R ⊆ A × A on A. For each ordered pair (x, y) ∈ A × A,
we have two choices: we can either include (x, y) in R or not include it. There
are 6·6 = 36 ordered pairs in A × A. By the multiplication principle, there are
thus 2
36 different subsets R and hence also this many relations on A.
11. Answer: 2
(|A|
2
) 13. Answer: 6= 15. Answer: ≡ (mod 3)
Section 11.1 Exercises
1. Consider the relation R = {(a,a),(b,b),(c, c),(d,d),(a,b),(b,a)} on the set A = {a,b, c,d}.
Which of the properties reflexive, symmetric and transitive does R possess and
why? If a property does not hold, say why.
This is reflexive because (x, x) ∈ R (i.e., xRx )for every x ∈ A.
It is symmetric because it is impossible to find an (x, y) ∈ R for which (y, x) ∉ R.
It is transitive because (xR y∧ yRz) ⇒ xRz always holds.
3. Consider the relation R = {(a,b),(a, c),(c,b),(b, c)} on the set A = {a,b, c}. Which
of the properties reflexive, symmetric and transitive does R possess and why?
If a property does not hold, say why.
This is not reflexive because (a,a) ∉ R (for example).
It is not symmetric because (a,b) ∈ R but (b,a) ∉ R.
It is not transitive because cRb and bRc are true, but cRc is false.
5. Consider the relation R =
©
(0,0),(
p
2,0),(0,
p
2),(
p
2,
p
2)ª
on R. Say whether this
relation is reflexive, symmetric and transitive. If a property does not hold, say
why.
This is not reflexive because (1,1) ∉ R (for example).
It is symmetric because it is impossible to find an (x, y) ∈ R for which (y, x) ∉ R.
It is transitive because (xR y∧ yRz) ⇒ xRz always holds.
7. There are 16 possible different relations R on the set A = {a,b}. Describe all of
them. (A picture for each one will suffice, but don’t forget to label the nodes.)
Which ones are reflexive? Symmetric? Transitive?
286 Solutions
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
a
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
b
Only the four in the right column are reflexive. Only the eight in the first and
fourth rows are symmetric. All of them are transitive except the first three
on the fourth row.
9. Define a relation on Z by declaring xR y if and only if x and y have the same
parity. Say whether this relation is reflexive, symmetric and transitive. If a
property does not hold, say why. What familiar relation is this?
This is reflexive because xRx since x always has the same parity as x.
It is symmetric because if x and y have the same parity, then y and x must
have the same parity (that is, xR y ⇒ yRx).
It is transitive because if x and y have the same parity and y and z have the
same parity, then x and z must have the same parity. (That is (xR y∧yRz) ⇒ xRz
always holds.)
The relation is congruence modulo 2.
11. Suppose A = {a,b, c,d} and R = {(a,a),(b,b),(c, c),(d,d)}. Say whether this relation
is reflexive, symmetric and transitive. If a property does not hold, say why.
This is reflexive because (x, x) ∈ R for every x ∈ A.
It is symmetric because it is impossible to find an (x, y) ∈ R for which (y, x) ∉ R.
It is transitive because (xR y∧ yRz) ⇒ xRz always holds.
(For example (aRa∧ aRa) ⇒ aRa is true, etc.)
13. Consider the relation R = {(x, y) ∈ R×R : x− y ∈ Z} on R. Prove that this relation
is reflexive and symmetric, and transitive.
Proof. In this relation, xR y means x− y ∈ Z.
To see that R is reflexive, take any x ∈ R and observe that x− x = 0 ∈ Z, so xRx.
Therefore R is reflexive.
To see that R is symmetric, we need to prove xR y ⇒ yRx for all x, y ∈ R. We
use direct proof. Suppose xR y. This means x − y ∈ Z. Then it follows that
−(x− y) = y− x is also in Z. But y− x ∈ Z means yRx. We’ve shown xR y implies
yRx, so R is symmetric.
To see that R is transitive, we need to prove (xR y ∧ yRz) ⇒ xRz is always
true. We prove this conditional statement with direct proof. Suppose xR y and
yRz. Since xR y, we know x − y ∈ Z. Since yRz, we know y − z ∈ Z. Thus x − y
and y− z are both integers; by adding these integers we get another integer
(x− y)+(y− z) = x− z. Thus x− z ∈ Z, and this means xRz. We’ve now shown that
if xR y and yRz, then xRz. Therefore R is transitive. ■
287
15. Prove or disprove: If a relation is symmetric and transitive, then it is also
reflexive.
This is false. For a counterexample, consider the relation R = {(a,a),(a,b),(b,a),(b,b)}
on the set A = {a,b, c}. This is symmetric and transitive but it is not reflexive.
17. Define a relation ∼ on Z as x ∼ y if and only if |x − y| ≤ 1. Say whether ∼ is
reflexive, symmetric and transitive.
This is reflexive because |x− x| = 0 ≤ 1 for all integers x. It is symmetric because
x ∼ y if and only if |x− y| ≤ 1, if and only if |y−x| ≤ 1, if and only if y ∼ x. It is not
transitive because, for example, 0 ∼ 1 and 1 ∼ 2, but is not the case that 0 ∼ 2.
Section 11.2 Exercises
1. Let A = {1,2,3,4,5,6}, and consider the following equivalence relation on A: R =
{(1,1),(2,2),(3,3),(4,4),(5,5),(6,6),(2,3),(3,2),(4,5),(5,4),(4,6),(6,4),(5,6),(6,5)}. List
the equivalence classes of R.
The equivalence classes are: [1] = {1}; [2] = [3] = {2,3}; [4] = [5] = [6] = {4,5,6}.
3. Let A = {a,b, c,d, e}. Suppose R is an equivalence relation on A. Suppose R has
three equivalence classes. Also aRd and bRc. Write out R as a set.
Answer: R = {(a,a),(b,b),(c, c),(d,d),(e, e),(a,d),(d,a),(b, c),(c,b)}.
5. There are two different equivalence relations on the set A = {a,b}. Describe
them all. Diagrams will suffice.
Answer: R = {(a,a),(b,b)} and R = {(a,a),(b,b),(a,b),(b,a)}
7. Define a relation R on Z as xR y if and only if 3x −5y is even. Prove R is an
equivalence relation. Describe its equivalence classes.
To prove that R is an equivalence relation, we must show it’s reflexive, symmetric
and transitive.
The relation R is reflexive for the following reason. If x ∈ Z, then 3x−5x = −2x
is even. But then since 3x−5x is even, we have xRx. Thus R is reflexive.
To see that R is symmetric, suppose xR y. We must show yRx. Since xR y, we
know 3x−5y is even, so 3x−5y = 2a for some integer a. Now reason as follows:
3x−5y = 2a
3x−5y+8y−8x = 2a+8y−8x
3y−5x = 2(a+4y−4x).
From this it follows that 3y−5x is even, so yRx. We’ve now shown xR y implies
yRx, so R is symmetric.
To prove that R is transitive, assume that xR y and yRz. (We will show that this
implies xRz.) Since xR y and yRz, it follows that 3x−5y and 3y−5z are both even,
so 3x−5y = 2a and 3y−5z = 2b for some integers a and b. Adding these equations,
we get (3x − 5y) + (3y − 5z) = 2a + 2b, and this simplifies to 3x − 5z = 2(a + b + y).
288 Solutions
Therefore 3x −5z is even, so xRz. We’ve now shown that if xR y and yRz, then
xRz, so R is transitive.
We’ve now shown that R is reflexive, symmetric and transitive, so it is an
equivalence relation.
The completes the first part of the problem. Now we move on the second part.
To find the equivalence classes, first note that
[0] = {x ∈ Z : xR0} = {x ∈ Z : 3x−5·0 is even} = {x ∈ Z : 3x is even} = {x ∈ Z : x is even}.
Thus the equivalence class [0] consists of all even integers. Next, note that
[1] = {x ∈ Z : xR1} = {x ∈ Z : 3x−5·1 is even} = {x ∈ Z : 3x−5 is even} =
©
x ∈ Z : x is oddª
.
Thus the equivalence class [1] consists of all odd integers.
Consequently there are just two equivalence classes {...,−4,−2,0,2,4,...} and
{...,−3,−1,1,3,5,...}.
9. Define a relation R on Z as xR y if and only if 4 | (x + 3y). Prove R is an
equivalence relation. Describe its equivalence classes.
This is reflexive, because for any x ∈ Z we have 4 | (x+3x), so xRx.
To prove that R is symmetric, suppose xR y. Then 4 | (x + 3y), so x + 3y = 4a
for some integer a. Multiplying by 3, we get 3x + 9y = 12a, which becomes
y+3x = 12a −8y. Then y+3x = 4(3a −2y), so 4 | (y+3x), hence yRx. Thus we’ve
shown xR y implies yRx, so R is symmetric.
To prove transitivity, suppose xR y and yRz. Then 4|(x +3y) and 4|(y+3z), so
x+3y = 4a and y+3z = 4b for some integers a and b. Adding these two equations
produces x+4y+3z = 4a+4b, or x+3z = 4a+4b −4y = 4(a+ b − y). Consequently
4|(x+3z), so xRz, and R is transitive.
As R is reflexive, symmetric and transitive, it is an equivalence relation.
Now let’s compute its equivalence classes.
[0] = {x ∈ Z : xR0} = {x ∈ Z : 4 | (x+3·0)} = {x ∈ Z : 4 | x} = {...−4,0,4,8,12,16...}
[1] = {x ∈ Z : xR1} = {x ∈ Z : 4 | (x+3·1)} = {x ∈ Z : 4 | (x+3)} = {...−3,1,5,9,13,17...}
[2] = {x ∈ Z : xR2} = {x ∈ Z : 4 | (x+3·2)} = {x ∈ Z : 4 | (x+6)} = {...−2,2,6,10,14,18...}
[3] = {x ∈ Z : xR3} = {x ∈ Z : 4 | (x+3·3)} = {x ∈ Z : 4 | (x+9)} = {...−1,3,7,11,15,19...}
11. Prove or disprove: If R is an equivalence relation on an infinite set A, then R
has infinitely many equivalence classes.
This is False. Counterexample: consider the relation of congruence modulo 2.
It is a relation on the infinite set Z, but it has only two equivalence classes.
13. Answer: m|A| 15. Answer: 15
289
Section 11.3 Exercises
1. List all the partitions of the set A = {a,b}. Compare your answer to the answer
to Exercise 5 of Section 11.2.
There are just two partitions {{a},{b}} and {{a,b}}. These correspond to the two
equivalence relations R1 = {(a,a),(b,b)} and R2 = {(a,a),(a,b),(b,a),(b,b)}, respectively,
on A.
3. Describe the partition of Z resulting from the equivalence relation ≡ (mod 4).
Answer: The partition is {[0],[1],[2],[3]} =
©
{...,−4,0,4,8,12,...},{...,−3,1,5,9,13,...}, {...,−2,2,4,6,10,14,...}, {...,−1,3,7,11,15,...}
ª
5. Answer: Congruence modulo 2, or “same parity.”
Section 11.4 Exercises
1. Write the addition and multiplication tables for Z2.
+ [0] [1]
[0] [0] [1]
[1] [1] [0]
· [0] [1]
[0] [0] [0]
[1] [0] [1]
3. Write the addition and multiplication tables for Z4.
+ [0] [1] [2] [3]
[0] [0] [1] [2] [3]
[1] [1] [2] [3] [0]
[2] [2] [3] [0] [1]
[3] [3] [0] [1] [2]
· [0] [1] [2] [3]
[0] [0] [0] [0] [0]
[1] [0] [1] [2] [3]
[2] [0] [2] [0] [2]
[3] [0] [3] [2] [1]
5. Suppose [a],[b] ∈ Z5 and [a]·[b] = [0]. Is it necessarily true that either [a] = [0]
or [b] = [0]?
The multiplication table for Z5 is shown in Section 11.4. In the body of that
table, the only place that [0] occurs is in the first row or the first column. That
row and column are both headed by [0]. It follows that if [a]·[b] = [0], then
either [a] or [b] must be [0].
7. Do the following calculations in Z9, in each case expressing your answer as [a]
with 0 ≤ a ≤ 8.
(a) [8]+[8] = [7] (b) [24]+[11] = [8] (c) [21]·[15] = [0] (d) [8]·[8] = [1]
290 Solutions
Chapter 12 Exercises
Section 12.1 Exercises
1. Suppose A = {0,1,2,3,4}, B = {2,3,4,5} and f = {(0,3),(1,3),(2,4),(3,2),(4,2)}. State
the domain and range of f . Find f (2) and f (1).
Domain is A; Range is {2,3,4}; f (2) = 4; f (1) = 3.
3. There are four different functions f : {a,b} → {0,1}. List them all. Diagrams will
suffice.
f1 = {(a,0),(b,0)} f2 = {(a,1),(b,0)}, f3 = {(a,0),(b,1)} f4 = {(a,1),(b,1)}
5. Give an example of a relation from {a,b, c,d} to {d, e} that is not a function.
One example is {(a,d),(a, e),(b,d),(c,d),(d,d)}.
7. Consider the set f = {(x, y) ∈ Z×Z : 3x+ y = 4}. Is this a function from Z to Z?
Explain.
Yes, since 3x+ y = 4 if and only if y = 4−3x, this is the function f : Z → Z defined
as f (x) = 4−3x.
9. Consider the set f =
©
(x
2
, x) : x ∈ R
ª
. Is this a function from R to R? Explain.
No. This is not a function. Observe that f contains the ordered pairs (4,2) and
(4,−2). Thus the real number 4 occurs as the first coordinate of more than one
element of f .
11. Is the set θ = {(X,|X|) : X ⊆ Z5} a function? If so, what is its domain and range?
Yes, this is a function. The domain is P(Z5). The range is {0,1,2,3,4,5}.
Section 12.2 Exercises
1. Let A = {1,2,3,4} and B = {a,b, c}. Give an example of a function f : A → B that
is neither injective nor surjective.
Consider f = {(1,a),(2,a),(3,a),(4,a)}.
Then f is not injective because f (1) = f (2).
Also f is not surjective because it sends no element of A to the element c ∈ B.
3. Consider the cosine function cos : R → R. Decide whether this function is injective
and whether it is surjective. What if it had been defined as cos : R → [−1,1]?
The function cos : R → R is not injective because, for example, cos(0) = cos(2π). It
is not surjective because if b = 5 ∈ R (for example), there is no real number for
which cos(x) = b. The function cos : R → [−1,1] is surjective. but not injective.
5. A function f : Z → Z is defined as f (n) = 2n+1. Verify whether this function is
injective and whether it is surjective.
This function is injective. To see this, suppose m,n ∈ Z and f (m) = f (n).
This means 2m+1 = 2n+1, from which we get 2m = 2n, and then m = n.
Thus f is injective.
This function is not surjective. To see this notice that f (n) is odd for all
n ∈ Z. So given the (even) number 2 in the codomain Z, there is no n with
f (n) = 2.
291
7. A function f : Z × Z → Z is defined as f ((m,n)) = 2n − 4m. Verify whether this
function is injective and whether it is surjective.
This is not injective because (0,2) 6= (−1,0), yet f ((0,2)) = f ((−1,0)) = 4. This is
not surjective because f ((m,n)) = 2n −4m = 2(n −2m) is always even. If b ∈ Z
is odd, then f ((m,n)) 6= b, for all (m,n) ∈ Z×Z.
9. Prove that the function f : R−{2} → R−{5} defined by f (x) =
5x+1
x−2
is bijective.
Proof. First, let’s check that f is injective. Suppose f (x) = f (y). Then
5x+1
x−2
=
5y+1
y−2
(5x+1)(y−2) = (5y+1)(x−2)
5x y−10x+ y−2 = 5yx−10y+ x−2
−10x+ y = −10y+ x
11y = 11x
y = x.
Since f (x) = f (y) implies x = y, it follows that f is injective.
Next, let’s check that f is surjective. For this, take an arbitrary element
b ∈ R−{5}. We want to see if there is an x ∈ R−{2} for which f (x) = b, or 5x+1
x−2
= b.
Solving this for x, we get:
5x+1 = b(x−2)
5x+1 = bx −2b
5x− xb = −2b −1
x(5− b) = −2b −1.
Since we have assumed b ∈ R − {5}, the term (5 − b) is not zero, and we can
divide with impunity to get x =
−2b −1
5− b
. This is an x for which f (x) = b, so f is
surjective.
Since f is both injective and surjective, it is bijective. ■
11. Consider the function θ : {0,1}×N → Z defined as θ(a,b) = (−1)ab. Is θ injective?
Is it surjective? Explain.
First we show that θ is injective. Suppose θ(a,b) = θ(c,d). Then (−1)ab = (−1)cd.
As b and d are both in N, they are both positive. Then because (−1)ab = (−1)cd,
it follows that (−1)a and (−1)c have the same sign. Since each of (−1)a and (−1)c
equals ±1, we have (−1)a = (−1)c
, so then (−1)ab = (−1)cd implies b = d. But also
(−1)a = (−1)c means a and c have the same parity, and because a, c ∈ {0,1}, it
follows a = c. Thus (a,b) = (c,d), so θ is injective.
Next note that θ is not surjective because θ(a,b) = (−1)ab is either positive or
negative, but never zero. Therefore there exist no element (a,b) ∈ {0,1}×N for
which θ(a,b) = 0 ∈ Z.
292 Solutions
13. Consider the function f : R
2 → R
2 defined by the formula f (x, y) = (x y, x
3
). Is f
injective? Is it surjective?
Notice that f (0,1) = (0,0) and f (0,0) = (0,0), so f is not injective. To show that f
is also not surjective, we will show that it’s impossible to find an ordered pair
(x, y) with f (x, y) = (1,0). If there were such a pair, then f (x, y) = (x y, x
3
) = (1,0),
which yields x y = 1 and x
3 = 0. From x
3 = 0 we get x = 0, so x y = 0, a contradiction.
15. This question concerns functions f : {A,B,C,D,E,F,G} → {1,2,3,4,5,6,7}. How
many such functions are there? How many of these functions are injective?
How many are surjective? How many are bijective?
Function f can described as a list (f (A), f (B), f (C), f (D), f (E), f (F), f (G)), where
there are seven choices for each entry. By the multiplication principle, the total
number of functions f is 7
7 = 823543.
If f is injective, then this list can’t have any repetition, so there are 7! = 5040
injective functions. Since any injective function sends the seven elements of the
domain to seven distinct elements of the codomain, all of the injective functions
are surjective, and vice versa. Thus there are 5040 surjective functions and
5040 bijective functions.
17. This question concerns functions f : {A,B,C,D,E,F,G} → {1,2}. How many such
functions are there? How many of these functions are injective? How many
are surjective? How many are bijective?
Function f can described as a list (f (A), f (B), f (C), f (D), f (E), f (F), f (G)), where
there are two choices for each entry. Therefore the total number of functions
is 2
7 = 128. It is impossible for any function to send all seven elements of
{A,B,C,D,E,F,G} to seven distinct elements of {1,2}, so none of these 128
functions is injective, hence none are bijective.
How many are surjective? Only two of the 128 functions are not surjective, and
they are the “constant” functions {(A,1),(B,1),(C,1),(D,1),(E,1),(F,1),(G,1)} and
{(A,2),(B,2),(C,2),(D,2),(E,2),(F,2),(G,2)}. So there are 126 surjective functions.
Section 12.3 Exercises
1. If 6 integers are chosen at random, at least two will have the same remainder
when divided by 5.
Proof. Write Z as follows: Z =
S4
j=0
{5k + j : k ∈ Z}. This is a partition of Z into 5
sets. If six integers are picked at random, by the pigeonhole principle, at least
two will be in the same set. However, each set corresponds to the remainder
of a number after being divided by 5 (for example, {5k +1 : k ∈ Z} are all those
integers that leave a remainder of 1 after being divided by 5). ■
3. Given any six positive integers, there are two for which their sum or difference
is divisible by 9.
Proof. If for two of the integers n,m we had n ≡ m (mod 9), then n−m ≡ 0 (mod 9),
and we would be done. Thus assume this is not the case. Observe that the
293
only two element subsets of positive integers that sum to 9 are {1,8},{2,7},{3,6},
and {4,5}. However, since at least five of the six integers must have distinct
remainders from 1, 2, ..., 8 it follows from the pigeonhole principle that two
integers n,m are in the same set. Hence n+ m ≡ 0 (mod 9) as desired. ■
5. Prove that any set of 7 distinct natural numbers contains a pair of numbers
whose sum or difference is divisible by 10.
Proof. Let S = {a1,a2,a3,a4,a5,a6,a7} be any set of 7 natural numbers. Let’s say
that a1 < a2 < a3 < ··· < a7. Consider the set
A = {a1 − a2, a1 − a3, a1 − a4, a1 − a5, a1 − a6, a1 − a7,
a1 + a2, a1 + a3, a1 + a4, a1 + a5, a1 + a6, a1 + a7}
Thus |A| = 12. Now let B = {0,1,2,3,4,5,6,7,8,9}, so |B| = 10. Let f : A → B be the
function for which f (n) equals the last digit of n. (That is f (97) = 7, f (12) = 2,
f (230) = 0, etc.) Then, since |A| > |B|, the pigeonhole principle guarantees that
f is not injective. Thus A contains elements a1 ± ai and a1 ± aj for which
f (a1 ±ai) = f (a1 ±aj). This means the last digit of a1 ±ai is the same as the last
digit of a1 ±aj
. Thus the last digit of the difference (a1 ±ai)−(a1 ±aj) = ±ai ±aj
is 0. Hence ±ai ± aj is a sum or difference of elements of S that is divisible by
10. ■
Section 12.4 Exercises
1. Suppose A = {5,6,8}, B = {0,1}, C = {1,2,3}. Let f : A → B be the function f =
{(5,1),(6,0),(8,1)}, and g : B → C be g = {(0,1),(1,1)}. Find g ◦ f .
g ◦ f = {(5,1),(6,1),(8,1)}
3. Suppose A = {1,2,3}. Let f : A → A be the function f = {(1,2),(2,2),(3,1)}, and let
g : A → A be the function g = {(1,3),(2,1),(3,2)}. Find g ◦ f and f ◦ g.
g ◦ f = {(1,1),(2,1),(3,3)}; f ◦ g = {(1,1),(2,2),(3,2)}.
5. Consider the functions f , g : R → R defined as f (x) =
p3
x+1 and g(x) = x
3
. Find
the formulas for g ◦ f and f ◦ g.
g ◦ f (x) = x+1; f ◦ g(x) =
p3
x
3 +1
7. Consider the functions f , g : Z × Z → Z × Z defined as f (m,n) = (mn,m2
) and
g(m,n) = (m+1,m+ n). Find the formulas for g ◦ f and f ◦ g.
Note g ◦ f (m,n) = g(f (m,n)) = g(mn,m2
) = (mn+1,mn+ m2
).
Thus g ◦ f (m,n) = (mn+1,mn+ m2
).
Note f ◦ g(m,n) = f (g(m,n)) = f (m+1,m+ n) = ((m+1)(m+ n),(m+1)2
).
Thus f ◦ g(m,n) = (m2 + mn+ m+ n,m2 +2m+1).
9. Consider the functions f : Z×Z → Z defined as f (m,n) = m+ n and g : Z → Z×Z
defined as g(m) = (m,m). Find the formulas for g ◦ f and f ◦ g.
g ◦ f (m,n) = (m+ n,m+ n)
f ◦ g(m) = 2m
294 Solutions
Section 12.5 Exercises
1. Check that the function f : Z → Z defined by f (n) = 6 − n is bijective. Then
compute f
−1
.
It is injective as follows. Suppose f (m) = f (n). Then 6− m = 6− n, which reduces
to m = n.
It is surjective as follows. If b ∈ Z, then f (6− b) = 6−(6− b) = b.
Inverse: f
−1
(n) = 6− n.
3. Let B = {2
n
: n ∈ Z} =
©
...,
1
4
,
1
2
,1,2,4,8,...ª
. Show that the function f : Z → B defined
as f (n) = 2
n
is bijective. Then find f
−1
.
It is injective as follows. Suppose f (m) = f (n), which means 2
m = 2
n
. Taking
log2 of both sides gives log2
(2m) = log2
(2n
), which simplifies to m = n.
The function f is surjective as follows. Suppose b ∈ B. By definition of B this
means b = 2
n
for some n ∈ Z. Then f (n) = 2
n = b.
Inverse: f
−1
(n) = log2
(n).
5. The function f : R → R defined as f (x) = πx− e is bijective. Find its inverse.
Inverse: f
−1
(x) =
x+ e
π
.
7. Show that the function f : R
2 → R
2 defined by the formula f ((x, y) = ((x
2 +1)y, x
3
)
is bijective. Then find its inverse.
First we prove the function is injective. Assume f (x1, y1) = f (x2, y2). Then
(x
2
1
+1)y1 = (x
2
2
+1)y2 and x
3
1
= x
3
2
. Since the real-valued function f (x) = x
3
is oneto-one,
it follows that x1 = x2. Since x1 = x2, and x
2
1
+1 > 0 we may divide both
sides of (x
2
1
+1)y1 = (x
2
1
+1)y2 by (x
2
1
+1) to get y1 = y2. Hence (x1, y1) = (x2, y2).
Now we prove the function is surjective. Let (a,b) ∈ R
2
. Set x = b
1/3 and y =
a/(b
2/3 +1). Then f (x, y) = ((b
2/3 +1) a
b
2/3+1
,(b
1/3)
3
) = (a,b). It now follows that f is
bijective.
Finally, we compute the inverse. Write f (x, y) = (u,v). Interchange variables to
get (x, y) = f (u,v) = ((u
2 +1)v,u
3
). Thus x = (u
2 +1)v and y = u
3
. Hence u = y
1/3 and
v =
x
y
2/3+1
. Therefore f
−1
(x, y) = (u,v) =
³
y
1/3
,
x
y
2/3+1
´
.
9. Consider the function f : R×N → N×R defined as f (x, y) = (y,3x y). Check that
this is bijective; find its inverse.
To see that this is injective, suppose f (a,b) = f (c,d). This means (b,3ab) =
(d,3cd). Since the first coordinates must be equal, we get b = d. As the second
coordinates are equal, we get 3ab = 3dc, which becomes 3ab = 3bc. Note that,
from the definition of f , b ∈ N, so b 6= 0. Thus we can divide both sides of
3ab = 3bc by the non-zero quantity 3b to get a = c. Now we have a = c and b = d,
so (a,b) = (c,d). It follows that f is injective.
Next we check that f is surjective. Given any (b, c) in the codomain N×R, notice
that (
c
3b
,b) belongs to the domain R×N, and f (
c
3b
,b) = (b, c). Thus f is surjective.
As it is both injective and surjective, it is bijective; thus the inverse exists.
To find the inverse, recall that we obtained f (
c
3b
,b) = (b, c). Then f
−1
f (
c
3b
,b) =
f
−1
(b, c), which reduces to (
c
3b
,b) = f
−1
(b, c). Replacing b and c with x and y,
respectively, we get f
−1
(x, y) = (
y
3x
, x).
295
Section 12.6 Exercises
1. Consider the function f : R → R defined as f (x) = x
2 + 3. Find f ([−3,5]) and
f
−1
([12,19]). Answers: f ([−3,5]) = [3,28]; f
−1
([12,19]) = [−4,−3]∪[3,4].
3. This problem concerns functions f : {1,2,3,4,5,6,7} → {0,1,2,3,4}. How many
such functions have the property that |f
−1
({3})| = 3? Answer: 4
4
¡
7
3
¢
.
5. Consider a function f : A → B and a subset X ⊆ A. We observed in Section 12.6
that f
−1
(f (X)) 6= X in general. However X ⊆ f
−1
(f (X)) is always true. Prove this.
Proof. Suppose a ∈ X. Thus f (a) ∈ {f (x) : x ∈ X} = f (X), that is f (a) ∈ f (X). Now,
by definition of preimage, we have f
−1
(f (X)) = {x ∈ A : f (x) ∈ f (X)}. Since a ∈ A
and f (a) ∈ f (X), it follows that a ∈ f
−1
(f (X)). This proves X ⊆ f
−1
(f (X)). ■
7. Given a function f : A → B and subsets W, X ⊆ A, prove f (W ∩ X) ⊆ f (W)∩ f (X).
Proof. Suppose b ∈ f (W ∩ X). This means b ∈ {f (x) : x ∈ W ∩ X}, that is b = f (a)
for some a ∈ W ∩ X. Since a ∈ W we have b = f (a) ∈ {f (x) : x ∈ W} = f (W). Since
a ∈ X we have b = f (a) ∈ {f (x) : x ∈ X} = f (X). Thus b is in both f (W) and f (X), so
b ∈ f (W)∩ f (X). This completes the proof that f (W ∩ X) ⊆ f (W)∩ f (X). ■
9. Given a function f : A → B and subsets W, X ⊆ A, prove f (W ∪ X) = f (W)∪ f (X).
Proof. First we will show f (W ∪ X) ⊆ f (W) ∪ f (X). Suppose b ∈ f (W ∪ X). This
means b ∈ {f (x) : x ∈ W ∪ X}, that is, b = f (a) for some a ∈ W ∪ X. Thus a ∈ W
or a ∈ X. If a ∈ W, then b = f (a) ∈ {f (x) : x ∈ W} = f (W). If a ∈ X, then b = f (a) ∈
{f (x) : x ∈ X} = f (X). Thus b is in f (W) or f (X), so b ∈ f (W)∪ f (X). This completes
the proof that f (W ∪ X) ⊆ f (W)∪ f (X).
Next we will show f (W)∪ f (X) ⊆ f (W ∪ X). Suppose b ∈ f (W)∪ f (X). This means
b ∈ f (W) or b ∈ f (X). If b ∈ f (W), then b = f (a) for some a ∈ W. If b ∈ f (X), then
b = f (a) for some a ∈ X. Either way, b = f (a) for some a that is in W or X. That
is, b = f (a) for some a ∈ W ∪ X. But this means b ∈ f (W ∪ X). This completes the
proof that f (W)∪ f (X) ⊆ f (W ∪ X).
The previous two paragraphs show f (W ∪ X) = f (W)∪ f (X). ■
11. Given f : A → B and subsets Y,Z ⊆ B, prove f
−1
(Y ∪ Z) = f
−1
(Y)∪ f
−1
(Z).
Proof. First we will show f
−1
(Y ∪ Z) ⊆ f
−1
(Y) ∪ f
−1
(Z). Suppose a ∈ f
−1
(Y ∪ Z).
By Definition 12.9, this means f (a) ∈ Y ∪ Z. Thus, f (a) ∈ Y or f (a) ∈ Z. If
f (a) ∈ Y, then a ∈ f
−1
(Y), by Definition 12.9. Similarly, if f (a) ∈ Z, then a ∈
f
−1
(Z). Hence a ∈ f
−1
(Y) or a ∈ f
−1
(Z), so a ∈ f
−1
(Y) ∪ f
−1
(Z). Consequently
f
−1
(Y ∪ Z) ⊆ f
−1
(Y)∪ f
−1
(Z).
Next we show f
−1
(Y) ∪ f
−1
(Z) ⊆ f
−1
(Y ∪ Z). Suppose a ∈ f
−1
(Y) ∪ f
−1
(Z). This
means a ∈ f
−1
(Y) or a ∈ f
−1
(Z). Hence, by Definition 12.9, f (a) ∈ Y or f (a) ∈ Z,
which means f (a) ∈ Y ∪Z. But by Definition 12.9, f (a) ∈ Y ∪Z means a ∈ f
−1
(Y ∪Z).
Consequently f
−1
(Y)∪ f
−1
(Z) ⊆ f
−1
(Y ∪ Z).
The previous two paragraphs show f
−1
(Y ∪ Z) = f
−1
(Y)∪ f
−1
(Z). ■
296 Solutions
13. Let f : A → B be a function, and X ⊆ A. Prove or disprove: f
¡
f
−1
(f (X))¢
= f (X).
Proof. First we will show f
¡
f
−1
(f (X))¢
⊆ f (X). Suppose y ∈ f
¡
f
−1
(f (X))¢
. By
definition of image, this means y = f (x) for some x ∈ f
−1
(f (X)). But by definition
of preimage, x ∈ f
−1
(f (X)) means f (x) ∈ f (X). Thus we have y = f (x) ∈ f (X), as
desired.
Next we show f (X) ⊆ f
¡
f
−1
(f (X))¢
. Suppose y ∈ f (X). This means y = f (x) for
some x ∈ X. Then f (x) = y ∈ f (X), which means x ∈ f
−1
(f (X)). Then by definition
of image, f (x) ∈ f (f
−1
(f (X))). Now we have y = f (x) ∈ f (f
−1
(f (X))), as desired.
The previous two paragraphs show f
¡
f
−1
(f (X))¢
= f (X). ■
Chapter 13 Exercises
Section 13.1 Exercises
1. R and (0,∞)
Observe that the function f (x) = e
x
sends R to (0,∞). It is injective because
f (x) = f (y) implies e
x = e
y
, and taking ln of both sides gives x = y. It is surjective
because if b ∈ (0,∞), then f (ln(b)) = b. Therefore, because of the bijection
f : R → (0,∞), it follows that |R| = |(0,∞)|.
3. R and (0,1)
Observe that the function 1
π
f (x) = cot−1
(x) sends R to (0,1). It is injective and
surjective by elementary trigonometry. Therefore, because of the bijection
f : R → (0,1), it follows that |R| = |(0,1)|.
5. A = {3k : k ∈ Z} and B = {7k : k ∈ Z}
Observe that the function f (x) =
7
3
x sends A to B. It is injective because
f (x) = f (y) implies 7
3
x =
7
3
y, and multiplying both sides by 3
7
gives x = y. It is
surjective because if b ∈ B, then b = 7k for some integer k. Then 3k ∈ A, and
f (3k) = 7k = b. Therefore, because of the bijection f : A → B, it follows that
|A| = |B|.
7. Z and S =
©
...,
1
8
,
1
4
,
1
2
,1,2,4,8,16,...ª
Observe that the function f : Z → S defined as f (n) = 2
n
is bijective: It is injective
because f (m) = f (n) implies 2
m = 2
n
, and taking log2 of both sides produces m = n.
It is surjective because any element b of S has form b = 2
n
for some integer n,
and therefore f (n) = 2
n = b. Because of the bijection f : Z → S, it follows that
|Z| = |S|.
9. {0,1}×N and N
Consider the function f : {0,1}×N → N defined as f (a,n) = 2n−a. This is injective
because if f (a,n) = f (b,m), then 2n−a = 2m−b. Now if a were unequal to b, one
of a or b would be 0 and the other would be 1, and one side of 2n− a = 2m− b
would be odd and the other even, a contradiction. Therefore a = b. Then
2n − a = 2m− b becomes 2n − a = 2m− a; add a to both sides and divide by 2 to
get m = n. Thus we have a = b and m = n, so (a,n) = (b,m), so f is injective.
297
To see that f is surjective, take any b ∈ N. If b is even, then b = 2n for some
integer n, and f (0,n) = 2n −0 = b. If b is odd, then b = 2n +1 for some integer n.
Then f (1,n +1) = 2(n +1)−1 = 2n +1 = b. Therefore f is surjective. Then f is a
bijection, so |{0,1}×N| = |N|.
11. [0,1] and (0,1)
Proof. Consider the subset X =
©
1
n
: n ∈ N
ª
⊆ [0,1]. Let f : [0,1] → [0,1) be defined
as f (x) = x if x ∈ [0,1]− X and f (
1
n
) =
1
n+1
for any 1
n
∈ X. It is easy to check that
f is a bijection. Next let Y =
©
1−
1
n
: n ∈ N
ª
⊆ [0,1), and define g : [0,1) → (0,1) as
g(x) = x if x ∈ [0,1)−Y and g(1−
1
n
) = 1−
1
n+1
for any 1−
1
n
∈ Y. As in the case of f , it
is easy to check that g is a bijection. Therefore the composition g◦ f : [0,1] → (0,1)
is a bijection. (See Theorem 12.2.) We conclude that |[0,1]| = |(0,1)|. ■
13. P(N) and P(Z)
Outline: By Exercise 18 of Section 12.2, we have a bijection f : N → Z defined as
f (n) =
(−1)n
(2n−1)+1
4
. Now define a function Φ : P(N) → P(Z) as Φ(X) = {f (x) :
x ∈ X}. Check that Φ is a bijection.
15. Find a formula for the bijection f in Example 13.2.
Hint: Consider the function f from Exercise 18 of Section 12.2.
Section 13.2 Exercises
1. Prove that the set A = {ln(n) : n ∈ N} ⊆ R is countably infinite.
Just note that its elements can be written in infinite list form as ln(1),ln(2),ln(3),···.
Thus A is countably infinite.
3. Prove that the set A = {(5n,−3n) : n ∈ Z} is countably infinite.
Consider the function f : Z → A defined as f (n) = (5n,−3n). This is clearly
surjective, and it is injective because f (n) = f (m) gives (5n,−3n) = (5m,−3m), so
5n = 5m, hence m = n. Thus, because f is surjective, |Z| = |A|, and |A| = |Z| = ℵ0.
Therefore A is countably infinite.
5. Prove or disprove: There exists a countably infinite subset of the set of irrational
numbers.
This is true. Just consider the set consisting of the irrational numbers
π
1
,
π
2
,
π
3
,
π
4
,···.
7. Prove or disprove: The set Q
100 is countably infinite.
This is true. Note Q
100 = Q×Q× ··· ×Q (100 times), and since Q is countably
infinite, it follows from the corollary of Theorem 13.5 that this product is
countably infinite.
9. Prove or disprove: The set {0,1}×N is countably infinite.
This is true. Note that {0,1}×N can be written in infinite list form as
(0,1),(1,1),(0,2),(1,2),(0,3),(1,3),(0,4),(1,4),···. Thus the set is countably infinite.
298 Solutions
11. Partition N into 8 countably infinite sets.
For each i ∈ {1,2,3,4,5,6,7,8}, let Xi be those natural numbers that are congruent
to i modulo 8, that is,
X1 = {1,9,17,25,33,...}
X2 = {2,10,18,26,34,...}
X3 = {3,11,19,27,35,...}
X4 = {4,12,20,28,36,...}
X5 = {5,13,21,29,37,...}
X6 = {6,14,22,30,38,...}
X7 = {7,15,13,31,39,...}
X8 = {8,16,24,32,40,...}
13. If A = {X ⊂ N : X is finite}, then |A| = ℵ0.
Proof. This is true. To show this we will describe how to arrange the items of
A in an infinite list X1, X2, X3, X4,....
For each natural number n, let pn be the nth prime number. Thus p1 = 2,
p2 = 3, p3 = 5, p4 = 7, p5 = 11, and so on. Now consider any element X ∈ A. If
X 6= ;, then X = {n1,n2,n3,...,nk}, where k = |X| and ni ∈ N for each 1 ≤ i ≤ k.
Define a function f : A → N ∪ {0} as follows: f ({n1,n2,n3,...,nk}) = pn1 pn2
··· pnk
.
For example, f ({1,2,3}) = p1p2p3 = 2·3·5 = 30, and f ({3,5}) = p3p5 = 5·11 = 55, etc.
Also, we should not forget that ; ∈ A, and we define f (;) = 0.
Note that f : A → N ∪ {0} is an injection: Let X = {n1,n2,n3,...,nk} and Y =
{m1,m2,m3,...,m`}, and X 6= Y. Then there is an integer a that belongs to
one of X or Y but not the other. Then the prime factorization of one of the
numbers f (X) and f (Y) uses the prime number pa but the prime factorization
of the other does not use pa. It follows that f (X) 6= f (Y) by the fundamental
theorem of arithmetic. Thus f is injective.
So each set X ∈ A is associated with an integer f (X) ≥ 0, and no two different
sets are associated with the same number. Thus we can list the elements in
X ∈ A in increasing order of the numbers f (X). The list begins as
;, {1}, {2}, {3}, {1,2}, {4}, {1,3}, {5}, {6}, {1,4}, {2,3}, {7},...
It follows that A is countably infinite. ■
15. Hint: Use the fundamental theorem of arithmetic.
Section 13.3 Exercises
1. Suppose B is an uncountable set and A is a set. Given that there is a surjective
function f : A → B, what can be said about the cardinality of A?
299
The set A must be uncountable, as follows. For each b ∈ B, let ab be an
element of A for which f (ab) = b. (Such an element must exist because f is
surjective.) Now form the set U = {ab : b ∈ B}. Then the function f : U → B is
bijective, by construction. Then since B is uncountable, so is U. Therefore U is
an uncountable subset of A, so A is uncountable by Theorem 13.9.
3. Prove or disprove: If A is uncountable, then |A| = |R|.
This is false. Let A = P(R). Then A is uncountable, and by Theorem 13.7,
|R| < |P(R)| = |A|.
5. Prove or disprove: The set {0,1}×R is uncountable.
This is true. To see why, first note that the function f : R → {0}×R defined as
f (x) = (0, x) is a bijection. Thus |R| = |{0}×R|, and since R is uncountable, so is
{0}×R. Then {0}×R is an uncountable subset of the set {0,1}×R, so {0,1}×R is
uncountable by Theorem 13.9.
7. Prove or disprove: If A ⊆ B and A is countably infinite and B is uncountable,
then B − A is uncountable.
This is true. To see why, suppose to the contrary that B− A is countably infinite.
Then B = A ∪(B − A) is a union of countably infinite sets, and thus countable,
by Theorem 13.6. This contradicts the fact that B is uncountable.
Exercises for Section 13.4
1. Show that if A ⊆ B and there is an injection g : B → A, then |A| = |B|.
Just note that the map f : A → B defined as f (x) = x is an injection. Now apply
the Cantor-Bernstein-Schröeder theorem.
3. Let F be the set of all functions N →
©
0,1}. Show that |R| = |F|.
Because |R| = |P(N)|, it suffices to show that |F| = |P(N)|. To do this, we will
exhibit a bijection f : F → P(N). Define f as follows. Given a function ϕ ∈ F,
let f (ϕ) = {n ∈ N : ϕ(n) = 1}. To see that f is injective, suppose f (ϕ) = f (θ). Then
{n ∈ N : ϕ(n) = 1} = {n ∈ N : θ(n) = 1}. Put X = {n ∈ N : ϕ(n) = 1}. Now we see that if
n ∈ X, then ϕ(n) = 1 = θ(n). And if n ∈ N− X, then ϕ(n) = 0 = θ(n). Consequently
ϕ(n) = θ(n) for any n ∈ N, so ϕ = θ. Thus f is injective. To see that f is surjective,
take any X ∈ P(N). Consider the function ϕ ∈ F for which ϕ(n) = 1 if n ∈ X and
ϕ(n) = 0 if n ∉ X. Then f (ϕ) = X, so f is surjective.
5. Consider the subset B =
©
(x, y) : x
2 + y
2 ≤ 1
ª
⊆ R
2
. Show that |B| = |R
2
|.
This will follow from the Cantor-Bernstein-Schröeder theorem provided that
we can find injections f : B → R
2 and g : R
2 → B. The function f : B → R
2 defined
as f (x, y) = (x, y) is clearly injective. For g : R
2 → B, consider the function
g(x, y) =
µ
x
2 + y
2
x
2 + y
2 +1
x,
x
2 + y
2
x
2 + y
2 +1
y
¶
.
Verify that this is an injective function g : R
2 → B.
300 Solutions
7. Prove or disprove: If there is a injection f : A → B and a surjection g : A → B,
then there is a bijection h : A → B.
This is true. Here is an outline of a proof. Define a function g
0
: B → A as
follows. For each b ∈ B, choose an element xb ∈ g
−1
({x}). (That is, choose an
element xb ∈ A for which g(xb) = b.) Now let g
0
: B → A be the function defined
as g
0
(b) = xb. Check that g
0
is injective and apply the the Cantor-BernsteinSchröeder
theorem.
2 CHAPTER 1. DIVISIBILITY
Multiplication also follows from associativity. Assume that d | n so that n=dk. Thenan=a(dk)=(ad)kshowsthatad|ak.
For Cancellation, assume that a ̸= 0 and that ad | an. Then there is a k such that an = (ad)k. We will show that n = dk. Assume first that a > 0. By the Trichotomy Property from the first Appendix, either n > dk or n = dk or n < dk. If n > dk then we have that an > a(dk) = (ad)k, which contradicts this paragraph’s assumption that an = (ad)k. If n < dk then an < a(dk) = (ad)k, also contradicting the assumption. Therefore n = dk, and so d | n. The argument for the a < 0 case is similar.
To verify Linearity, suppose that d | n and d | m so that n = dk1 and m = dk2 for k1, k2 ∈ Z. Then an + bm = a(dk1) + b(dk2) = d(ak1 + bk2) shows that d | (an + bm).
Finally, for Comparison, assume that d,n > 0 and d | n. Then n = dk for some k. Observe that k is positive because the other two are positive. By Trichotomy,eitherd<nord=nord>n. Wewillshowthatthed>ncase is not possible. Assume that d > n. Then dk > nk follows by one of the first Appendix’s Properties of Inequalities. But that gives n > nk, which means that n·1 > n·k despite that fact that k is positive and so 1 ≤ k. This is impossible because it violates the same Property of Inequalities. qed
1.3 Definition An integer n is even (or has even parity) if it is divisible by 2 and is odd (or is of odd parity) otherwise.
1.4Lemma Recallthat|a|equalsaifa≥0andequals−aifa<0. (1) Ifd|athen−d|aandd|−a.
(2) Ifd|athend||a|
(3) The largest positive integer that divides a nonzero number a is |a|.
proof. For (1), if d | a then a = dk for some k. It follows that a = (−d)(−k) and since −d and −k are also integers, this shows that −d | a. It also follows that −a = (−k)d, and so d | −a.
For (2), suppose first that a is nonnegative. Then |a| = a and so if d | a then d | |a|. Next suppose that a is negative. Since |a| = −a for negative a, and since (1) shows that d | −a, and d therefore divides |a|.
For (3), first note that |a| actually divides a: in the a ≥ 0 case |a| | a because in this case |a| = a and we know that a | a, while in the a < 0 case we have that a = |a|(−1), so that |a| is indeed a factor of a. We finish by showing that |a| is maximal among the divisors of a. Suppose that d is a positive number that divides a. Then a = dk for some k, and also −a = d(−k). Thus d | |a|, whether a is positive or negative. So by the Comparison property of Theorem 1.2, we have that d ≤ |a|. qed
Chapter 2
Prime Numbers
2.1 Definition An integer p ≥ 2 is prime if it has no positive divisors other than 1 and itself. An integer greater than or equal to 2 that is not prime is composite .
Note that 1 is neither prime nor composite.
2.2 Lemma An integer n ≥ 2 is composite if and only if it has factors a and
b such that 1 < a < n and 1 < b < n.
proof. Let n ≥ 2. The ‘if’ direction is obvious. For ‘only if’, assume that n is composite. Then it has a positive integer factor a such that a ̸= 1, a ̸= n. This means that there is a b with n = ab. Since n and a are positive, so is b. Hence 1≤aand1≤b. ByTheorem1.2,a≤nandb≤n. Sincea̸=1anda̸=nwe have1<a<n. Ifb=1thena=n,whichisnotpossible,sob̸=1. Ifb=n then a = 1, which is also not possible. So 1 < b < n, finishing this half of the argument. qed
2.3Lemma Ifn>1thenthereisaprimepsuchthatp|n.
proof. Let S denote the set of all integers greater than 1 that have no prime divisor. We must show that S is empty.
If S is not empty then by the Well-Ordering Property it has a smallest member; call it m. Now m > 1 and has no prime divisor. Then m cannot be prime (as every number is a divisor of itself). Hence m is composite. Therefore byLemma2.2,m=abwhere1<a<mand1<b<m. Since1<a<m,the factoraisnotamemberofS. Soamusthaveaprimedivisorp. Thenp|a and a | m, so by Theorem 1.2, p | m. This contradicts the assumption that m has no prime divisor. So the set S must be empty. qed
2.4 Theorem (Euclid’s Theorem) There are infinitely many primes. proof. Assume, to get a contradiction, that there are only a finitely many
primes p1 = 2, p2 = 3, . . . , pn. Consider the number N = p1p2 · · · pn + 1. 3
4 CHAPTER 2. PRIME NUMBERS
Since p1 ≥ 2, clearly N ≥ 2. So by Lemma 2.3, N has a prime divisor p. That prime must be one of p1, . . . , pn since that list was assumed to be exhaustive. However, observe that the equation
N =pi(p1p2···pi−1pi+1···pn)+1
along with 0 ≤ 1 < pi shows by Lemma 3.2 that n is not divisible by pi. This is a contradiction; it follows that the assumption that there are only finitely many primes is not true. qed
2.5 Remark Eucild’s Theorem, and its proof, is often cited as an example of the beauty of Mathematics.
2.6 Theorem If n > 1 is composite then n has a prime divisor p ≤ √n. proof. Letn>1becomposite. Thenn=abwhere1<a<nand1<b<n.
We can use Theorem 2.6 to help compute whether an integer is prime. Given n>1,weneedonlytrytodivideitbyallprimesp≤√n. Ifnoneofthese divides n then n must be prime.
√√
2.7 Example Consider the number 97. Note that 97 < 100 = 10. The primes less than 10 are 2, 3, 5, and 7. None of these divides 97, and so 97 is prime.
 Weclaimthatatleastoneofaorbislessthanorequalto
a > √n and b > √n, and hence n = ab > √n · √n = n, which is impossible.
√
 n. Forifnotthen
    Suppose, without loss of generality, that a ≤ √n. Since 1 < a, by Lemma 2.3 there is a prime p such that p | a. Hence, by Transitivity in Theorem 1.2, since a|nwehavep|n. ByComparisoninTheorem1.2,sincep|awehave p ≤ a ≤ √n. qed
     
Chapter 3
Division
3.1 Theorem Where a and b > 0 are integers, there are integers q and r, called the quotient and the remainder on division of a by b, satisfying these two conditions.
a = bq + r 0 ≤ r < b Further, those integers are unique.
Note that this result has two parts. One part is that the theorem says there exists a quotient and remainder satisfying the conditions. The second part is that the quotient, remainder pair are unique: no other pair of numbers satisfies those conditions.
proof. To verify that for any a and b > 0 there exists an appropriate quotient and remainder we need only produce suitable numbers. Consider these.
q =   a   r = a − bq b
Obviously a = bq + r, so these satisfy the first condition. To finish the existence half of this proof, we need only check that 0 ≤ r < b. The Floor Lemma from the Some Properties of R appendix gives
a − 1 <  a  ≤ a. bbb
Multiply all of the terms of this inequality by −b. Since b is positive, −b is negative, and so the direction of the inequality is reversed.
b − a > −b   a   ≥ −a b
Add a to all three terms of the inequality and replace ⌊a/b⌋ by q to get b > a − bq ≥ 0.
Since r = a − bq this shows that 0 ≤ r < b. 5
     
6 CHAPTER 3. DIVISION We still must prove that q and r are unique. Assume that there are two
quotient, remainder pairs
a=bq1 +r1 with 0≤r1 <b
and
a=bq2 +r2 with 0≤r2 <b.
Subtracting
0=a−a=(bq1 +r1)−(bq2 +r2)=b(q1 −q2)+(r1 −r2)
implies that
(3.1) r2 − r1 = b(q1 − q2).
We must show that the two pairs are equal, that r1 = r2 and q1 = q2. To obtain a contradiction, suppose otherwise. First suppose that r1 ̸= r2. Then one must be larger than the other; without loss of generality assume that r2 > r1. Then
0≤r1 <r2 <b
and so r2 − r1 < b. But (3.1) shows that b divides r2 − r1 and by the Comparison property of Theorem 1.2 this implies that b ≤ r2 − r1. This is the desired contradiction and so we conclude that r1 = r2. With that, from equation 3.1 we have 0 = b(q1−q2). Since b > 0, this gives that q1−q2 = 0 and so q1 = q2. qed
3.2 Corollary The number d divides the number n if and only if on division of n by d the remainder is 0.
proof. Iftheremainderis0thenn=dq+0=dqshowsthatd|n. Forthe other half, if d | n then for some k we have n = dk = dk + 0 (with 0 ≤ 0 < d) and the fact that the quotient, remainder pair is uniqus shows that k and 0 must be the quotient and the remainder. qed
That corollary says that Theorem 3.1 generalizes the results on divisibility. For instance, fix b = 3. Then, given a, instead of only being able to say that a is divisible or not, we can give a finer description: a leaves a remainder of 0 (this is the case where b | a), or 1, or 2.
3.3 Definition For b > 0 define a mod b = r where r is the remainder when a is divided by b.
For example 23mod7 = 2 since 23 = 7·3+2 and −4mod5 = 1 since −4 = 5 · (−1) + 1.
Chapter 4
Greatest Common Divisor
4.1 Definition An integer is a common divisor of two others if it divides both of them.
We write C(a, b) for the set of numbers that are common divisors of a and b. 4.2 Definition The greatest common divisor of two nonzero integers a and b,
gcd(a, b), is the largest integer that divides both, except that gcd(0, 0) = 0. The exception is there because every number divides zero, and so we specially
define gcd(0, 0) to be a convienent value.
4.3 Example The set of common divisors of 18 and 30 is
C(18, 30) = {−1, 1, −2, 2, −3, 3, −6, 6}.
So, gcd(18, 30) = 6.
4.4 Lemma gcd(a, b) = gcd(b, a).
proof. Clearly the two sets C(a,b) and C(b,a) are equal. It follows that their largest elements are equal, that is, that gcd(a, b) = gcd(b, a). qed
4.5 Lemma gcd(a, b) = gcd(|a|, |b|).
proof. If a = 0 and b = 0 then |a| = a and |b| = b, and so in this case gcd(a, b) = gcd(|a|, |b|). Suppose that one of a or b is not 0. Lemma 1.4 shows that d | a ⇔ d | |a|. It follows that the two sets C(a,b) and C(|a|,|b|) are the same set. So the largest member of that set, the greatest common divisor of a and b, is also the greatest common divisor of |a| and |b|. qed
4.6 Lemma If a ̸= 0 or b ̸= 0, then gcd(a, b) exists and satisfies 0 < gcd(a, b) ≤ min{|a|, |b|}.
7
8 CHAPTER 4. GREATEST COMMON DIVISOR
proof. Note that gcd(a,b) is the largest integer in the set C(a,b). Since 1 | a and 1 | b we know that 1 ∈ C(a, b). So the greatest common divisor must be at least 1, and is therefore positive. On the other hand, if d ∈ C(a, b) then d | |a| and d | |b|, so d is no larger than |a| and no larger than |b|. Thus, d is at most the minimum of |a| and |b|. qed
4.7 Example The above results give that
gcd(48, 732) = gcd(−48, 732) = gcd(−48, −732) = gcd(48, −732).
We also know that 0 < gcd(48, 732) ≤ 48. Since if d = gcd(48, 732) then d | 48, to find d we need check only for positive divisors of 48 that also divide 732.
4.8Remark Observethatthefirsttwolemmas,whichdrawconclusionsabout the properties of the gcd operator, preceed Lemma 4.6, which shows that the gcd exists.
If two numbers have a greatest common divisor of 1 then they have no nontivial common factors.
4.9 Definition Two numbers are relatively prime if they have a greatest com- mon divisor of 1.
Although the relatively prime relationship is symmetric—if gcd(a,b) = 1 then gcd(b, a) = 1 — we sometimes state it as “a is relatively prime to b.”
4.10 Lemma If g = gcd(a, b) then gcd(a/g, b/g) = 1.
proof. The greatest common divisor of a/g and b/g must exist, by the prior result. Let gcd(a/g, b/g) = k. Then k is a divisor of both a/g and b/g so there are numbers ja and jb such that jak = a/g and jbk = b/g. Therefore ja(kg) = a andjb(kg)=b,andsokgisacommondivisorofaandb. Ifk>1thiswould be a contradiction, because then kg > g but g is the greatest common divisor. Therefore k = 1. qed
Chapter 5 Bezout’s Lemma
5.1 Definition A number c is a linear combination of the numbers a and b if c = as + bt for some s and t.
5.2Example Thenumberc=16isalinearcombinationofa=2andb=5 sincetakings=3andt=2gives16=3·2+2·5. Thenumber21isnota linear combination of 2 and 4, since in the equation 21 = s·2+t·4 the right side is divisible by 2 while the left is not. (That is, there are no integers s and t; we can solve the equation with rational numbers that are not integral.)
Thus, the Linearity statement in the Divisibility Properties theorem says that if d divides a and b, then d divides all linear combinations of a and b. So the greatest common divisor, gcd(a, b), divides every member of L(a, b), the set of all linear combinations of a and b. The next result says that gcd(a, b) is itself a member of L(a, b).
5.3 Lemma (Bezout’s Lemma) The greatest common divisor of two num- bers is a linear combination of those two: for all integers a and b there exist integers s and t such that gcd(a, b) = sa + tb.
proof. If a and b are 0 then s and t may be anything since gcd(0,0) = 0 = s·0+t·0. So we may assume that a ̸= 0 or b ̸= 0. Consider the set L(a,b) = {na + mb : n, m ∈ Z} of all linear combinations of a and b.
Denote the set of positive members of L(a, b) by L+(a, b). Note that L(a, b) contains a, −a, b and −b, and since a ̸= 0 or b ̸= 0, at least one of these four numbers is positive. Therefore L+(a,b) is not empty. Because of this, by the Well-Ordering Property for N, we know that the set L+(a, b) contains a smallest positive integer; call it d. We will show that d is the greatest common divisor of a and b. That will finish the argument because d is a linear combination of a and b as it is a member of L.
Since d ∈ L+(a,b) we have d = sa+tb for some integers s and t. Let g = gcd(a, b). Then g | a and g | b, so by the Linearity property of Theorem 1.2, we have g | (sa+tb), that is, g | d. Since g and d are positive, by the Comparision property of Theorem 1.2, we have that g ≤ d.
9
10 CHAPTER 5. BEZOUT’S LEMMA
If we show that d is a common divisor of a and b, then we will have that d ≤ g (as g is the greatest of the common divisors), and so we will have shown thatg=d. Toshowthatd|a,writea=dq+rwhere0≤r<dandcompute
r = a − dq = a − (sa + tb)q = (1 − sq)a + (−tq)b.
to conclude that r ∈ L(a, b). Thus, if r were to be strictly greater than 0 then r would be a member of L+(a,b). But this cannot be, since r is strictly less than d and d is the smallest integer in L+(a,b). So we must have that r = 0. That is,a=dq,andhenced|a. Asimilarargumentshowsthatd|b. Thus,dis indeed a common divisor of a and b, and d = g = gcd(a, b). qed
5.4 Example Notice that 1 = gcd(2, 3) and 1 = (−1)2 + 1 · 3. Notice also that 1 = 2 · 2 + (−1)3. So the numbers s and t in Bezout’s Lemma are uniquely determined. In fact, as we will see later that for each pair a, b there are infinitely many s and t.
5.5 Corollary The set L(a, b) of all linear combinations of a and b equals the set of multiples of gcd(a, b).
proof. We observed above that any member of L(a, b) is a multiple of gcd(a, b). For the converse, consider the multiple k·gcd(a, b), apply Bezout’s Lemma to get s,t ∈ Z so that gcd(a,b) = sa+tb, and substitute: k·gcd(a,b) = k·(sa+tb) = (ks)a + (kt)b. qed
5.6Lemma Ifa|bcandaisrelativelyprimetobthena|c.
proof. Since gcd(a, b) = 1, by Bezout’s Lemma there are coefficients s and t such that 1 = as+bt. Multiply both sides by c to get c = cas+cbt = a(cs)+(bc)t. Note that a | a(cs) and that a | bc by assumption, so Theorem 1.2 gives that a divides the linear combination a(cs) + (bc)t = c. qed
Observe that 6 | (4 · 9) but 6   4 and 6   9 (6 is not relatively prime to 4, and is also not relatively prime to 9). Thus the condition of relative primality is needed in that lemma.
We can completely characterize L(a, b).
5.7 Lemma Fix a,b ∈ Z. If gcd(a,b) | c then the equation sa+tb = c has
infinitely many solution pairs s, t, which have the form
s = s0 − j · (b/d), t = t0 + j · (a/d) j ∈ Z
where s0 , t0 is any particular solution pair.
proof. First assume that a solution pair s0,t0 exists, to show that any pair of
numbers of that form also solve the equation. Plug them into the equation. (s0−j(b/d))·a+(t0+j(a/d))·b = (s0a+t0b)+j(−(ab/d)+(ab/d)) = s0a+t0b = c
11
To finish we must show that pairs of the stated type are the only solutions.. Suppose that s and t also solve the equation: sa + tb = c. Subtracting gives (s − s0)a + (t − t0)b = 0, that is,
(∗) (s−s0)a=(t0 −t)b.
Divide by g = gcd(a, b) on both sides to get (s − s0)(a/g) = (t0 − t)(b/g), which shows that b/g divides (s − s0)(a/g). By Lemma 4.10, gcd(a/g, b/g) = 1 and so the prior result, Lemma 5.6, shows that b/g divides s − s0. Thus, for this solution pair s, t, there is a j ∈ Z such that j · (b/g) = s − s0, that is, s has the form s = s0 − j(b/g). With that form for s, substituting into equation (∗) gives that ((s0 − j(b/g)) − s0)a = −ja(b/g) equals (t0 − t)b. Dividing both sides by b and rearranging gives that t = t0 + j(a/g). qed
12 CHAPTER 5. BEZOUT’S LEMMA
Chapter 6
The Euclidean Algorithm
We can efficiently compute the greatest common divisor of two numbers.
We first reduce the problem. Since gcd(a, b) = gcd(|a|, |b|) (and gcd(0, 0) = 0), we need only give a method to compute gcd(a, b) where a and b are nonneg- ative. And, since gcd(a, b) = gcd(b, a), it is enough for us to give a method for
a ≥ b ≥ 0.
6.1 Lemma If a > 0 then gcd(a,0) = a.
proof. Since every integer divides 0, C(a, 0) is just the set of divisors of a. The largest divisor of a is |a|. Since a is positive, |a| = a, and so gcd(a, 0) = a. qed
The prior lemma reduces the problem of computing gcd(a,b) to the case where a ≥ b > 0.
6.2 Lemma If a > 0 then gcd(a,a) = a.
proof. Obviously, a is a common divisor. By Lemma 4.6, gcd(a, a) ≤ |a| and
since a is positive, |a| = a. So a is the greatest common divisor. qed We have now reduced the problem to the case a > b > 0. The central result
is next.
6.3Lemma Leta>b>0.Ifa=bq+r,thengcd(a,b)=gcd(b,r).
proof. It suffices to show that the two sets C(a, b) and C(b, r) are equal, because then they must have the same greatest member. To show that the sets are equal we will show that they have the same members.
First,supposethatd∈C(a,b),sothatd|aandd|b. Notethatr=a−bq. ByTheorem1.2(3)wehavethatd|r. Thusd|bandd|r,andsod∈C(b,r). We have shown that any member of C(a,b) is a member of C(b,r), that is, that C(a, b) ⊆ C(b, r).
For the other containment, assume that d ∈ C(b,r) so that d | b and d | r. Since a = bq+r, Theorem 1.2(3) applies again to shows that d | a. So d | a and
d | b, and therefore d ∈ C(a, b).
qed
13
14 CHAPTER 6. THE EUCLIDEAN ALGORITHM
The Euclidean Algorithm uses Lemma 6.3 to compute the greatest common divisor of two numbers. Rather introduce a computer language in which to give algorithm, we will illustrate it with an example.
6.4 Example Compute gcd(803, 154).
gcd(803, 154) = gcd(154, 33) gcd(154, 33) = gcd(33, 22) gcd(33, 22) = gcd(22, 11)
gcd(22, 11) = gcd(11, 0) gcd(11, 0) = 11
since 803 = 154 · 5 + 33 since 154 = 33 · 4 + 22 since 33 = 22 · 1 + 11
since 22 = 11 · 1 + 0
Hence gcd(803, 154) = 11.
6.5 Remark This method is much faster than finding C(a,b) and can find
gcd’s of quite large numbers.
Recall that Bezout’s Lemma asserts that given a and b there exists two numbers s and t such that gcd(a, b) = s · a + t · b. We can use Euclid’s Algorithm to find s and t by tracing through the steps, in reverse.
6.6 Example Express gcd(803, 154) as a linear combination of 803 and 154.
11 = 33 + 22 · (−1)
= 33 + (154 − 33 · 4) · (−1) = 154 · (−1) + 33 · 5
= 154 · (−1) + (803 − 154 · 5) · 5 = 803 · 5 + 154 · (−26)
Chapter 7
The Fundamental Theorem
7.1 Theorem (Fundamental Theorem of Arithmetic) Every number greater than 1 factors into a product of primes n = p1p2 · · · ps. Further, writ- ing the primes in ascending order p1 ≤ p2 ≤ · · · ≤ ps makes the factorization unique.
Some of the primes in the product may be equal. For instance, 12 = 2·2·3 = 22·3. So the Fundamental Theorem is sometimes stated as: every number greater than 1 can be factored uniquely as a product of powers of primes.
7.2 Example 600=2·2·2·3·5·5=23 ·3·52
We will break the proof of the Fundamental Theorem into a sequence of
Lemmas.
7.3Lemma(Euclid’sLemma) Ifpisaprimeandp|ab,thenp|aorp|b.
proof. Assume that p | ab. If p | a then we are done, so suppose that it does not. Letd=gcd(p,a). Notethatd>0,andthatd|pandd|a. Sinced|p wehavethatd=1ord=p. Ifd=pthenp|a,whichweassumedwasnot true. Sowemusthaved=1. Hencegcd(p,a)=1andp|ab. SobyLemma5.6, p | b. qed
7.4 Lemma Let p be prime. Let a1,a2,...,an, n ≥ 1, be integers. If p | a1a2 ···an, then p | ai for at least one i ∈ {1,2,...,n}.
proof. We use induction on n. For the n = 1 base case the result is clear. For the inductive step, assume the inductive hypothesis: that the lemma holdsfornsuchthat1≤n≤k. Wemustshowthatitholdsforn=k+1. Assume that p is prime and that p | a1a2 ···akak+1. Write a1a2 ···ak as a, and ak+1 asb. Thenp|aorp|bbyLemma7.3. Ifp|a=a1···ak thenbythe induction hypothesis, p | ai for some i ∈ {1,...,k}. If p | b then p | ak+1. So we can say that p | ai for some i ∈ {1,2,...,k+1}. This verifies the lemma for n = k + 1. Hence by mathematical induction, it holds for all n ≥ 1. qed
15
16 CHAPTER 7. THE FUNDAMENTAL THEOREM 7.5Lemma(FundamentalTheorem,Existence) Ifn>1thenthereexist
primes p1,...,ps, where s ≥ 1, such that n = p1p2 ···ps and p1 ≤ p2 ≤ ··· ≤ ps.
proof. We will use induction on n. The base step is n = 2: in this case, since 2isprimewecantakes=1andp1 =2.
For the inductive step, assume the hypothesis that the lemma holds for 2≤n≤k;wewillshowthatitholdsforn=k+1. Ifk+1isprimethens=1 andp1 =k+1. Ifk+1iscompositethenwritek+1=abwhere1<a<k+1 and 1 < b < k+1. By the induction hypothesis there are primes p1,...,pu and q1,...,qv such that a = p1 ···pu and b = q1 ···qv. This gives that k + 1 is a product of primes
k + 1 = ab = p1 p2 · · · pu q1 q2 · · · qv ,
where s = u + v. Reorder the primes into ascending order, if necessary.
The base step and the inductive step together give us that the statement is true for all n > 1. qed
7.6 Lemma (Fundamental Theorem, Uniqueness) If n = p1p2 · · · ps for s≥1withp1 ≤p2 ≤···≤ps,andalson=q1q2···qt fort≥1with q1 ≤q2 ≤···≤qt,thent=s,andpi =qi forallibetween1ands.
proof. The proof is by induction on s. In the s = 1 base case, n = p1 is prime and we have p1 = q1q2 ···qt. Now, t must be 1 or else this is a factorization of the prime p1, and therefore p1 = q1.
Now assume the inductive hypothesis that the result holds for all s with 1 ≤ s ≤ k. We must show that the result then holds for s = k+1. Assume that n = p1p2 ···pkpk+1 where p1 ≤ p2 ≤ ··· ≤ pk+1, and also n = q1q2 ···qt where q1 ≤ q2 ≤ ··· ≤ qt. Clearly pk+1 | n, so pk+1 | q1 ···qt. Euclid’s Lemma then gives that pk+1 divides some qi. That implies that pk+1 = qi, or else pk+1 would be a non-1 divisor of the prime qi, which is impossible. Hence pk+1 = qi ≤ qt.
A similar argument shows that qt = pj ≤ pk+1. Therefore pk+1 = qt. To finish, cancel pk+1 = qt from the two sides of this equation.
p1p2 ···pkpk+1 = q1q2 ···qt−1qt
Now the induction hypothesis applies: k = t−1 and pi = qi for i = 1,...,t−1. So the lemma holds also in the s = k + 1 case, and so by mathematical induction it holds for all s ≥ 1. qed
7.7 Remark Unique factorization gives an alternative, conceptually simpler, way to find the greatest common divisor of two numbers. For example, 600 = 23 ·31 ·52 ·70 and 252 = 22 ·32 ·50 ·7. Now, 23 divides both number. So does 31, but 32 does not divide both. Also, the highest power of 5 dividing both numbers is 50, and similarly the highest power of 7 that works for both is 70. So gcd(600, 252) = 22 · 31 · 50 · 70 = 24. In general, we can find the greatest common divisor of two numbers factoring, then taking the minimum power of 2, times the minimum power of 3, etc.
17
The difficulty with this method is that we must factor the numbers. But factorization is very difficult! That is, for numbers that are large, factoring is slow while the Euclidean algorithm is relatively fast.
18 CHAPTER 7. THE FUNDAMENTAL THEOREM
Chapter 8
Distribution of Primes
The Sieve of Eratosthenes is an ancient method to find primes. To find the primes less than n, list the numbers from 2 to n − 1. The smallest number, 2, is prime. Cross off all proper multiples of 2 (that is, the even numbers greater than 2). The smallest number remaining, 3, is prime. Cross off all proper multiples of 3, that is, 6, 9, etc. (some of them have already been eliminated). The smallest remaining number, 5, is prime. Cross off all proper multiples of 5. Continue this process until the list is exhausted.
Here is what is left when the sieve filters out the nonprimes less than 100.
 00 01 02 03 04 05 06 07 02357
08 09
19 29
59
79 89
   10 11 20 3031 40 41 50 6061 70 71 80
90
13 23
43 53
73 83
17
37 47
67
97
        Obviously, the columns with even numbers and the columns with multiples of 5 are empty (except for 2 and 5) but this is an artifact of the fact that the rows of the table are 10 = 2 · 5 wide. Other than that, at first glance no pattern is apparent.
8.1 Theorem (Wilson’s Theorem) There are arbitrarily long gaps between primes: for any positive integer n there is a sequence of n consecutive composite integers.
proof. Given n ≥ 1, consider a = (n+1)!+2. We will show that all of the numbers a,a+1,...,a+(n−1) are composite.
19
20 CHAPTER 8. DISTRIBUTION OF PRIMES
Since n+1 ≥ 2, clearly 2 | (n+1)!. Hence 2 | (n+1)!+2. Since (n+1)!+2 > 2, we therefore have that a = (n + 1)! + 2 is composite. We will finish by showing that the i-th number in the sequence, a + i where 0 ≤ i ≤ n − 1, is composite. Because 2 ≤ i+2 ≤ n+1, we have that (i+2) | (n+1)!. Hence i+2 | a+i = (n+1)!+(i+2). Because a+i > i+2 > 1, we have that a+i is composite. qed
8.2Definition Foranypositiverealnumberx,thenumberofprimeslessthan or equal to x is π(x).
For example, π(10) = 4.
The next result was first conjectured in 1793 by by Gauss, on the basis of numerical evidence like that in the table above. It was, however, not proved until over 100 years later, by Hadamard and Vall ́ee Poussin. The proof is beyond the scope of this course.
8.3 Theorem (The Prime Number Theorem)
lim π(x) = 1. x→∞ (x/ln(x))
 Here is a table of values of π(10i) second set of values have been rounded
π(x) 102   25 103   168 104   1229 105   9592 106   78498 107   664579 108   5761455 109   50847534 1010   455052511
and 10i/ln(10i) for i = 2,...,10 (the to the nearest integer).
round(x/ ln(x)) 22 145 1086 8686 72382 620421 5428681 48254942 434294482
 x
 This table has been continued up to 1021, but mathematicians are still working on finding the value of π(1022). Of course, computing the approximations are easy, but finding the exact value of π(1022) is hard.
Chapter 9
Fermat Primes and Mersenne Primes
A formula that produces the primes would be nice. Historically, lacking such a formula, mathematicians have looked for formulas that at least produce only primes.
In 1640 Fermat noted that the numbers in this list n01234
Fn=2(2n)+1 3 5 17 257 65,537
are all prime. He conjectured that Fn is always prime. Numbers of the form
22n + 1 are called Fermat numbers.
9.1Lemma Leta>1andn>1. Ifan+1isprimethenaisevenandn=2k
for some k ≥ 1.
proof. We first show that n is even. Suppose otherwise, and recall the well-
known factorization.
an −1=(a−1)(an−1 +an−2 +···+a+1)
Replace a by −a.
(−a)n −1=(−a−1) (−a)n−1 +(−a)n−2 +···+(−a)+1 
If the exponent n is odd then n−1 is even, n−2 is odd, etc. So we have (−a)n = −an, (−a)n−1 = an−1, (−a)n−2 = −an−2, etc., and the factorization becomes
−(an +1)=−(a+1) an−1 −an−2 +···−a+1 .
Then changing the sign of both sides gives (an + 1) = (a + 1)(an−1 − an−2 + · · · − a + 1). But with n ≥ 2, we have 1 < a + 1 < an + 1. This shows that if n is odd and a > 1, then an + 1 is not prime.
21
   
22 CHAPTER 9. FERMAT PRIMES AND MERSENNE PRIMES
Soniseven. Writen=2s·twheretisodd. Thenifan+1isprimewe have (a2s )t + 1 is prime. But by what we just showed this cannot be prime if t isoddandt≥2. Sowemusthavet=1andthereforen=2s.
Also, an + 1 prime implies that a is even since if a is odd then so is an, and in consequence an + 1 would be even. But the only even prime is 2, adnd we are assuming that a > 1 and so we have a ≥ 2, which implies that so an + 1 ≥ 3. qed
9.2 Definition A prime number of the form Fn = 2(2n) +1, n ≥ 0, is a Fermat prime .
Euler showed that Fermat number next on the table, F5 = 4, 294, 967, 297, is composite.
As n increases, the Fn’s increase in size very rapidly, and are not easy to check for primality. We know that Fn is composite for all n such that 5 ≤ n ≤ 30, and a large number of other values of n including 382447 (the largest one that I know). Many researchers now conjecture that Fn is composite for n ≥ 5. So Fermat’s original thought that Fn is always prime is badly mistaken.
Mathematicians have also looked for formulas that produce many primes. That is, we can guess that numbers of various special forms are disproportion- ately prime. One form that has historically been of interest is are the Mersenne numbers Mn = 2n − 1.
n 2 3 5 7 13 f(n) 3 7 31 127 8191
All of the numbers on the second row are prime. Note that 24 − 1 is not prime, so this is not supposed to be a formula that gives only primes.
9.3Lemma Leta>1andn>1. Ifan−1isprimethena=2andnisprime.
proof. Consider again an −1 = (a−1)(an−1 +···+a+1) Note that if a > 2 and n > 1 then a − 1 > 1 and an−1 + · · · + a + 1 > a + 1 > 3 so both factors are greater then 1, and therefore an − 1 is not prime. Hence if an − 1 is prime then we must have a = 2.
Now suppose 2n − 1 is prime. We claim that n is prime. For, if not, then n=stwhere1<s<nand1<t<n. Then2n−1=2st−1=(2s)t−1is prime. But we just showed that if an − 1 is prime then we must have a = 2. So we must have 2s = 2, and hence s = 1 and t = n. Therefore n is not composite, that is, n is prime. qed
9.4 Corollary If Mn is prime, then n is prime.
proof. This is immediate from Lemma 9.3. qed
At first it was thought that Mp = 2p − 1 is prime whenever p is prime. But in 1536, Hudalricus Regius showed that M11 = 211 − 1 = 2047 is not prime: 2047 = 23 · 89.
   
23 9.5 Definition A prime number of the form Mn = 2n −1, n ≥ 2, is a Mersenne
prime .
People continue to work on determining which Mp’s are prime. To date (2003-Dec-09), we know that 2p − 1 is prime if p is one of the following 40 primes: 2, 3, 5, 7, 13, 17, 19, 31, 61, 89, 107, 127, 521, 607, 1279, 2203, 2281, 3217, 4253, 4423, 9689, 9941, 11213, 19937, 21701, 23209, 44497, 86243, 110503, 132049, 216091, 756839, 859433, 1257787, 1398269, 2976221, 3021377, 6972593, 13466917, and 20996011.
The first number with more than a thousand digits known to be prime was M4253. The largest number on that list was found on 2003-Nov-17. This number has 6, 320, 430 digits. It was found as part of the Great Internet Mersenne Prime Search (GIMPS). (You can participate in this search by setting a program to run at times when your computer is not busy; see Chris Caldwell’s page for more about this.) Later we will see a connection between Mersenne primes and perfect numbers.
One reason that we know so much about Mersenne primes is that the follow- ing test makes it easier to check whether or not Mp is prime when p is a large prime.
9.6 Theorem (The Lucas-Lehmer Mersenne Prime Test) Let p be an odd prime. Define the sequence r1, r2, r3, . . . , rp−1 by the rules r1 = 4, and for k ≥ 2,
rk = (rk2−1 − 2) mod Mp. Then Mp is prime if and only if rp−1 = 0.
The proof of this is beyond the scope of this book. 9.7Example Letp=5.ThenMp=M5=31.
r1 = 4
r2 =(42 −2)mod31=14mod31=14 r3 =(142 −2)mod31=194mod31=8 r4 =(82 −2)mod31=62mod31=0
Hence by the Lucas-Lehmer test, M5 = 31 is prime.
9.8 Remark Note that the Lucas-Lehmer test for Mp = 2p − 1 takes only p − 1 steps. On the other hand, if we try to prove that Mp is prime by testing all primes less than or equal to  Mp then must consider about 2(p/2) steps. This is much larger, in general, than p.
No one knows whether there are infinitely many Mersenne primes.
 
24 CHAPTER 9. FERMAT PRIMES AND MERSENNE PRIMES
Chapter 10
The Functions σ and τ
10.1 Definition Where n is a positive integer, τ(n) is the number of positive divisors of n.
10.2 Example The number 12 = 3 · 22 has positive divisors 1, 2, 3, 4, 6, 12, and so τ(12) = 6.
10.3 Definition Where n is a positive integer, σ(n) is the sum of the positive divisors of n.
A positive divisor d of n is a proper divisor if d < n. The sum of all proper divisors of n is σ∗(n).
Note that if n ≥ 2 then σ∗(n) = σ(n) − n.
10.4Example σ(12)=1+2+3+4+6+12=28,σ∗(12)=16.
10.5 Definition A number n > 1 is perfect if σ∗(n) = n.
10.6Example Thefirstperfectnumberis6becauseitsproperdivisorsare1, 2 and 3.
10.7 Theorem Consider the prime factorization n = pe1 pe2 · · · per . 12r
(1) τ(n)=(e1 +1)(e2 +1)···(er +1)
pe2+1 −1 · 2
pe1+1 −1 (2)σ(n)= 1
per+1 −1 ··· r
   p1 − 1
10.8Example Ifn=72=23·32 thenτ(72)=(3+1)(2+1)=12and
σ(72) = 24 − 1 · 33 − 1 = 15 · 13 = 195. 2−1 3−1
Proof of item (1). From the Fundamental Theorem of Arithmetic, if d is a factor
ofnthentheprimefactorsofdcomefromthoseofn. Henced|niffd=
pf1pf2 ···pfr where for each i, 0 ≤ f ≤ e . There are (e +1)(e +1)···(e +1) 12rii12r
choices for the exponents f1, f2, . . . , fr. qed 25
p2 − 1
pr − 1
  
26 CHAPTER 10. THE FUNCTIONS σ AND τ Our proof of the second item requires two preliminary results.
10.9Lemma Supposethatn=ab,wherea>0,b>0andgcd(a,b)=1. Then σ(n) = σ(a)σ(b).
proof. Since a and b have only 1 as a common factor, the Fundamental The- orem of Arithmetic, shows that d | n only when d factors into d = d1d2 where d1 | a and d2 | b. That is, the divisors of ab are products of the divisors of a with the divisors of b. Let the divisors of a be 1,a1,...,as and the divisors of b be 1,b1,...,bt. These are the divisors of n = ab.
1,b1,b2,...,bt
a1 ·1,a1 ·b1,a1 ·b2,...,a1 ·bt a2 ·1,a2 ·b1,a2 ·b2,...,a2 ·bt
.
as ·1,as ·b1,as ·b2,...,as ·bt
This list has no repetitions because, as gcd(a,b) = 1, if aibj = akbl then ai = ak and bj = bl. Therefore to find σ(b) we can sum the rows
1 + b1 + · · · + bt = σ(b) a11+a1b1 +···+a1bt =a1σ(b)
.
as ·1+asb1 +···+asbt =asσ(b)
and add those partial sums
σ(n) = σ(b) + a1σ(b) + a2σ(b) + · · · + a3σ(b) =(1+a1 +a2 +···+as)σ(b)
= σ(a)σ(b)
to get the required result. qed 10.10Lemma Ifpisaprimeandk≥0then
σ(pk)= pk+1 −1. p−1
 proof. Since p is prime, the divisors of pk are 1, p, p2, . . . , pk. Hence σ(pk)=1+p+p2 +···+pk = pk+1 −1
 p−1
follows from the formula for the sum of a geometric series. qed
27 Proof of item (2). Let n = pe1pe2 ···per. This proof is by induction on the
12r
number of prime factors r. In the r = 1 base case we have n = pe1 and the
1
result follows from Lemma 10.10.
For the inductive step, the inductive hypothesis is that the statment is true
when 1 ≤ r ≤ k. Consider the r = k+1 case: n = pe1 ···pekpek+1 where the 1 kk+1
primes are distinct. Let a = pe1 ···pek and b = pek+1. Clearly gcd(a,b) = 1. 1k k+1
Lemma 10.9 applies to give that σ(n) = σ(a)σ(b). The inductive hypothesis and Lemma 10.10 give
 pe1+1 − 1  σ(a)= 1
and therefore
 pek+1 − 1  ··· k
pek+1+1 − 1 σ(b)= k+1
   p1 − 1
pk − 1
pk+1 − 1
σ(n) =
 pe1+1 − 1  1
p1 − 1
· · ·
 pek+1+1 − 1  k+1
pk+1 − 1
  as desired. So the result holds for r = k + 1, and that implies that the theorem is true for all integers by the principle of mathematical induction. qed
28 CHAPTER 10. THE FUNCTIONS σ AND τ
Chapter 11
Perfect Numbers and Mersenne Primes
A search for perfect numbers up to 10, 000 finds only these.
6=2·3
28 = 22 · 7 496=24 ·31 8128 = 26 · 127
Note that 3 = 22 − 1, 7 = 23 − 1, 31 = 25 − 1, and 127 = 27 − 1 are Mersenne primes. We can conjecture that all perfect numbers fit this pattern. This chapter discusses to what extent this is known to be true.
11.1 Theorem If 2p − 1 is a Mersenne prime then 2p−1 · (2p − 1) is perfect. proof. Write q = 2p−1 and n = 2p−1q. Since q is odd and prime, Theorem 10.7
gives that σ(n) is
 p−1    2p−1  q2−1  p p p
σ 2 q = 2−1 q−1 =(2 −1)(q+1)=(2 −1)2 =2n. That is, σ(n) = 2n, and so n is perfect. qed
  11.2 Theorem If n is even and perfect then there is a Mersenne prime 2p − 1 such that n = 2p−1(2p − 1).
proof. Suppose that n is even and perfect. Factor out all of the 2’s to get n=2k·qwithqanoddnumber,andk≥1sinceniseven. Sinceqisodd, gcd(2k,q) = 1 and so by Lemmas 10.9 and 10.10 we have σ(n) = σ(2k)σ(q) = (2k+1 − 1)σ(q). Thus, as n is perfect,
2k+1q = 2n = σ(n) = (2k+1 − 1)σ(q). 29
30 CHAPTER 11. PERFECT NUMBERS AND MERSENNE PRIMES Now substituting σ(q) = σ∗(q) + q, into the prior displayed equation gives
that is
This implies that
2k+1q = (2k+1 − 1)(σ∗(q) + q) 2k+1q = (2k+1 − 1)σ∗(q) + 2k+1q − q
(∗) σ∗(q)(2k+1 − 1) = q.
Soσ∗(q)isadivisorofq. Sincek≥1wehavethat2k+1−1≥4−1=3. So σ∗(q) is a proper divisor of q. But σ∗(q) is the sum of all of the proper divisors of q. This can only happen if q has only one proper divisor, that is, it implies that q is prime and so σ∗(q) = 1. Then equation (∗) shows that q = 2k+1 − 1. So q is a Mersenne prime and k+1 = p is prime. Therefore n = 2p−1 ·(2p −1), as desired. qed
11.3Corollary Thereisaone-to-onecorrespondencebetweentheevenperfect numbers and the Mersenne primes.
Here are two questions that remain open: (i) Are there infinitely many even perfect numbers? (ii) Are there any odd perfect numbers? (We know that if an odd perfect number exists, then it must be greater than 1050.)
Chapter 12
Congruences
12.1 Definition Let m ≥ 0. We we say that the numbers a and b are congruent modulo m, denoted a ≡ b (mod m), if a and b leave the same remainder when divided by m. The number m is the modulus of the congruence. The notation a ̸≡ b (mod m) means that they are not congruent.
12.2 Lemma The numbers a and b are congruent modulo m if and only if m | (a − b), and also if and only if m | (b − a).
proof. Writea=mqa+ra andb=mqb+rb forsomeqa,qb,ra,andrb,with 0 ≤ ra, rb < m. Subtracting gives a − b = m(qa − qb) + (ra − rb). Observe that the restrictions on the remainders imply that −m < ra − rb < m, and so ra − rb isnotamultipleofmunlessra−rb =0.
If a and b are congruent modulo m then ra = rb, which implies that a − b = m(qa − qb), which in turn gives that a − b is a multiple of m.
The implications in the prior paragraph reverse: if a − b is a multiple of m then in the equation a−b = m(qa −qb)+(ra −rb) we must have that ra −rb = 0 by the observation in the first paragraph, and therefore ra = rb.
The b − a statement is proved similarly. qed 12.3 Examples
1. 25≡1 (mod4)since4|24
2. 25̸≡2 (mod4)since4 23
3. 1≡−3 (mod4)since4|4
4. a≡b (mod1)foralla,b
5. a≡b (mod0)⇐⇒a=bforalla,b
Do not confuse the use of mod in Definition 12.1 a≡b (modm)ifm|a−b
31
32 CHAPTER 12. CONGRUENCES
with that of Definition 3.3.
a mod b = r where r is the remainder when a is divided by b
The two are related but not identitical.
12.4 Example One difference between the two is that 25 ≡ 5 (mod 4) is true while 25 = 5 mod 4 is false (it asserts that 25 = 1).
The ‘mod’ in a ≡ b (mod m) defines a binary relation, a relationship between two things. The ‘mod’ in a mod b is a binary operation, just as addition or multiplication are binary operations. Thus,
a≡b (modm)⇐⇒amodm=bmodm. Thatis,ifm>0anda≡r (modm)where0≤r<mthenamodm=r.
Expressions such as
x=2 42 = 16
x2 +2x=sin(x)+3 are equations. By analogy, expressions such as
x≡2 (mod16) 25≡5 (mod5)
x3+2x≡6x2+3 (mod27)
The next two theorems show that congruences and equations share many
are called congruences. properties.
12.5 Theorem Congruence is an equivalence relation: for all a, b, c, and m > 0 we have
(1) (Reflexivity property) a ≡ a (mod m)
(2) (Symmetry property) a ≡ b (mod m) ⇒ b ≡ a (mod m)
(3) (Transitivity property) a ≡ b (modm) and b ≡ c (modm) ⇒ a ≡ c
(mod m)
proof. For reflexivity: on division by m, any number leaves the same remainder as itself.
For symmetry, if a leaves the same remainder as b, then b leaves the same remainder as a.
For transitivity, assume that a leaves the same remainder as b on division by m, and that b leaves the same remainder as c. The all three leave the same remainder as each other, and in particular a leaves the same remainder as c. qed
33
Below we will consider polynomials f(x) = anxn +an−1xn−1 +· · ·+a1x+a0. We will assume that the coefficients an, . . . , a0 are integers and that x also represents an integer variable. Here the degree of the polynomial is an integer n ≥ 0.
12.6Theorem Ifa≡b(modm)andc≡d(modm),then (1) a+c≡b+d (modm)anda−c≡b−d (modm)
(2) ac ≡ bd (mod m)
(3) an ≡bn (modm)foralln≥1
(4) f(a) ≡ f(b) (mod m) for all polynomials f(x) with integer coefficients.
Proof of (1). Since a − c = a + (−c), it suffices to prove only the addition case. By assumption m | a − b and m | c − d. By linearity of the ‘divides’ relation, m | (a−b)+(c−d), that is m | (a+c)−(b+d). Hence a+c ≡ b+d (mod m). qed
Proof of (2). Since m | a−b and m | c−d, by linearity m | c(a−b)+b(c−d). Now, c(a−b)+b(c−d) = ca−bd, hence m | ca−bd, and so ca ≡ bd (mod m), as desired. qed
Proof of (3). We prove this by induction on n. If n = 1, the result is true by the assumption that a ≡ b (mod m). Assume that the result holds for n = 1,...,k. Then we have ak ≡ bk (mod m). This, together with a ≡ b (mod m) using property (2) above, gives that aak ≡ bbk (mod m). Hence ak+1 ≡ bk+1 (mod m) and the result holds in the n = k + 1 case. So the result holds for all n ≥ 1, by induction. qed
Proof of (4). Let f(x) = cnxn + ··· + c1x + c0. We prove by induction on the degree of the polynomial n that if a ≡ b (mod m) then cnan +···+c0 ≡ cnbn + ··· + c0 (mod m). For the degree n = 0 base case, by the reflexivity of congruence we have that c0 ≡ c0 (mod m).
For the induction assume that the result holds for n = k. Then we have (∗) ckak +···+c1a+c0 ≡ckbk +···+c1b+c0 (modm).
By item (3) above we have ak+1 ≡ bk+1 (mod m). Since ck+1 ≡ ck+1 (mod m), using item (2) above we have
(∗∗) ck+1ak+1 ≡ ck+1bk+1 (mod m).
Now we can apply Theorem 15.3 (1) to (∗) and (∗∗) to obtain
ck+1ak+1 + ckak + · · · + c0 ≡ ck+1bk+1 + ckbk + · · · + c0 (mod m).
So by induction the result holds for all n ≥ 0.
qed
12.7 Example (From [1].) The first five Fermat numbers 3, 5, 17, 257, and
65, 537 are prime. We will use congruences to show that F5 = 232 + 1 is divisible by 641 and is therefore not prime.
34 CHAPTER 12. CONGRUENCES
Everyone knows that 22 = 4, 24 = 16, and 28 = 256. Also, 216 = (28)2 = 2562 = 65, 536. A straightforward division shows that 65, 536 ≡ 154 (mod 641). Next, for 232, we have that (216)2 ≡ (154)2 (mod 641). That is, 232 ≡ 23, 716 (mod 641). Since an easy division finds that 23, 716 ≡ 640 (mod 641), and 640 ≡ −1 (mod 641), we have that 232 ≡ −1 (mod 641). Hence 232 +1 ≡ 0 (mod 641), and so . 641 | 232 +1, as claimed. Clearly 232 +1 ̸= 641, so 232 +1
is composite.
The work done here did not require us to find the value of 232 + 1 =
4,294,967,297 and divide it by 641; instead the calculations were with much smaller numbers.
Chapter 13
Divisibility Tests
Elementary school children know how to tell if a number is even, or divisible by 5, by looking at the least significant digit.
13.1 Theorem If a number a has the decimal representation an−110n−1 + an−210n−2 +···+a110+a0 then
(1) amod2=a0 mod2 (2) amod5=a0 mod5
proof. Consider f(x) = an−1xn−1 +···+a1x+a0. Note that 10 ≡ 0 (mod 2). So by Theorem 12.6
an−110n−1+···+a110+a0 ≡an−10n−1+···+a10+a0 (mod2). That is, a ≡ a0 (mod 2); this proves item (1). Since 10 ≡ 0 (mod 5) also, the
proof of item (2) is similar. qed
13.2 Example Thus, the number 1457 is odd because 7 is odd: 1457 mod 2 = 7mod2 = 1. And on division by 5 it leaves a remainder of 1457mod5 = 7 mod 5 = 2.
13.3 Theorem Where a = an−110n−1 + an−210n−2 + · · · + a110 + a0 is the decimal representation,
(1) amod3=(an−1 +···+a0)mod3
(2) amod9=(an−1 +···+a0)mod9
(3) amod11=(a0 −a1 +a2 −a3 +···)mod11.
proof. Note that 10 ≡ 1 (mod 3). Theorem 12.6 gives an−110n−1+···+a110+a0 ≡an−11n−1+···+a11+a0
(mod3)
and so a ≡ an−1 +···+a1 +a0 (mod 3). This proves item (1). Since 10 ≡ 1 (mod 9) also, the proof of item (2) is similar.
35
36 CHAPTER 13. DIVISIBILITY TESTS
For item (3), note that 10 ≡ −1 (mod 11) so
an−110n−1+···+a110+a0 ≡an−1(−1)n−1+···+a1(−1)+a0 (mod11).
Thatis,a≡a0−a1+a2−··· (mod11). qed
13.4 Example Consider 1457 again. For divisibility by 3 we have 1457 mod 3 = (1+4+5+7)mod3 = 17mod3 = 8mod3 = 2. As for 9, we get 1457mod9=(1+4+5+7)mod9=17mod9=8mod9=8. Finally,for11, the calculation is 1457 mod 11 = 7 − 5 + 4 − 1 mod 11 = 5 mod 11 = 5.
Note that m | a ⇔ amodm = 0 so from the prior two results we obtain immediately the following.
13.5 Corollary Let a = an−110n−1 + an−210n−2 + · · · + a110 + a0. (1) 2|a⇔a0 =0,2,4,6or8
(2) 5|a⇔a0 =0or5
(3) 3|a⇔3|a0 +a1 +···+an−1
(4) 9|a⇔9|a0 +a1 +···+an−1
(5) 11|a⇔11|a0 −a1 +a2 −a3 +···.
13.6 Theorem Let a = ar10r +···+a2102 +a110+a0 be the decimal repre- sentation, so that we write a as the sequence arar−1 · · · a1a0. Then
(1) 7|a⇔7|ar···a1−2a0.
(2) 13|a⇔13|ar···a1−9a0
(where ar · · · a1 is the sequence representing (a − a0 )/10).
proof. For item (1), let c = ar ···a1 so that a = 10c+a0. Since gcd(7,−2) = 1 we have that 7 | a ⇔ 7 | −2a. Consequently, consider −2a = −20c − 2a0. Because 1 ≡ −20 (mod 7), we have that −2a ≡ c − 2a0 (mod 7). Therefore, 7|−2a⇔7|c−2a0. Itfollowsthat7|a⇔7|c−2a0,whichiswhatwe wanted to prove.
The proof of item (2) is similar. qed 13.7 Example We can test whether 7 divides 2481.
7 | 2481 ⇔ 7 | 248 − 2 ⇔ 7 | 246 ⇔ 7 | 24 − 12 ⇔ 7 | 12 Since 7   12 we have that 7   2481.
13.8 Example The number 12987 is divisible by 13 because
13 | 12987 ⇔ 13 | 1298 − 63 ⇔ 13 | 1235 ⇔ 13 | 123 − 45 ⇔ 13 | 78
and 13 · 6 = 78.
Chapter 14
More Properties of Congruences
Theorem 12.6 provides some laws of algebra for ≡. A typical algebra problem is to solve for an unknown; for instance, we can look for x such that 2x ≡ 7 mod 15.
14.1 Theorem Let m ≥ 2. If a and m are relatively prime then there exists auniqueintegera∗ suchthataa∗ ≡1 (modm)and0<a∗ <m.
proof. Assume that gcd(a, m) = 1. Bezout’s Lemma applies to give an s and t such that as+mt = 1. Hence as−1 = m(−t), that is, m | as−1 and so as≡1(modm). Accordingly,leta∗ =smodmsothat0<a∗ <m. Then a∗ ≡s (modm)soaa∗ ≡1 (modm).
To show uniqueness, assume that ac ≡ 1 (mod m) and 0 < c < m. Then ac ≡ aa∗ (mod m). Multiply both sides of this congruence on the left by c and use the fact that ca ≡ 1 (mod m) to obtain c ≡ a∗ (mod m). Because both are in [0 .. m), it follows that c = a∗. qed
We call a∗ the inverse of a modulo m. Note that we do not denote a∗ by a−1 here since we keep that symbol for the usual meaning of inverse.
14.2 Remark The proof shows that Blankinship’s Method will compute the inverse of a, when it exists. But for small m we may find a∗ by trial and error. For example, take m = 15 and a = 2. We can check each possibility: 2 · 0 ̸≡ 1 (mod15),2·1̸≡1 (mod15),...,2·8≡1 (mod15). Sowecantake2∗ =8.
Notethatwemaywellhaveca≡1modmwithc̸=aifc≡a∗ (modm) andc>morc<0. Forinstance,8·2≡1mod15andalso23·2≡1mod15. So the inverse is unique only if we specify that 0 < a∗ < m.
The converse of Theorem 14.1 holds.
14.3 Theorem Let m > 0. If ab ≡ 1 (mod m) then both a and b are relatively prime to m.
37
38 CHAPTER 14. MORE PROPERTIES OF CONGRUENCES
proof. Ifab≡1 (modm),thenm|ab−1. Soab−1=mtforsomet. Hence, ab + m(−t) = 1.
The proof of Bezout’s Lemma, Lemma 5.3, shows that gcd(a, m) is the small- est positive linear combination of a and m. The last paragraph shows that there is a combination that adds to 1. Since no combination can be positive and smaller than 1, we have that gcd(a,m) = 1. The case of gcd(b,m) is similar.. qed
14.4 Corollary A number a has an inverse modulo m if and only if a and m are relatively prime.
The second paragraph of Theorem 14.1 uses a technique that is worth iso- lating.
14.5 Theorem (Cancellation) Let m > 0. If gcd(c, m) = 1 then ca ≡ cb (modm)⇒a≡b (modm).
proof. If gcd(c, m) = 1 then it has an inverse c∗ modulo m, such that c∗c ≡ 1 (mod m). Since ca ≡ cb (mod m) by Theorem 12.6, c∗ca ≡ c∗cb (mod m). But c∗c≡1 (modm)soc∗ca≡a (modm)andc∗cb≡b (modm). Byreflexivity and transitivity this yields a ≡ b (mod m). qed
Although in general we cannot cancel if gcd(c,m) > 1, the next result is some consolation.
14.6Theorem Ifc>0andm>0thena≡b (modm)⇔ca≡cb (modcm). proof. The congruence a ≡ b (mod m) is true if and only if m | (a − b) holds,
which in turn holds if and only if cm | (ca − cb). qed 14.7Theorem Fixm>0andletd=gcd(c,m).Thenca≡cb(modm)⇒
a ≡ b (mod m/d).
proof. Since d = gcd(c, m), the equations c = d(c/d) and m = d(m/d) involve
integers. Rewriting ca ≡ cb (mod m) gives
d c a ≡ d c b (mod d m ).
   ddd
By Theorem 14.6 we have
 c a ≡  c b (mod m).
   ddd
Since d = gcd(c,m), we have that gcd(c/d,m/d) = 1 and so by cancellation,
Theorem 14.5, a ≡ b (mod m/d). qed
14.8 Theorem If m > 0 and a ≡ b (mod m) then gcd(a,m) = gcd(b,m).
proof. Let da = gcd(m, a) and db = gcd(m, b). Since a ≡ b (mod m) we have a−b=mtforsomet. Rewritethatasa=mt+bandnotethatdb |mand db |b,sodb |a. Thus,db isacommondivisorofmanda,andsodb ≤da. A similar argument gives that da ≤ db, and therefore db = da. qed
39 14.9 Corollary Fix m > 0. If a ≡ b (mod m) then a has an inverse modulo
m if and only if b does also.
proof. Immediate. qed
40 CHAPTER 14. MORE PROPERTIES OF CONGRUENCES
Chapter 15 Residue Classes
The work that we’ve seen shows that if a ≡ b (mod m) then the two numbers a and b, while not necessarily equal, are in some ways alike.
15.1 Definition Fix m > 0. The residue class class of a modulo m (or congruence class, or equivalence class of a modulo m) is [a] = {x | x ≡ a (mod m)}, the set of all integers congruent to a modulo m.
Note that, by definition, [a] is a set.
[a] = {mq + a | q ∈ Z} = {. . . , −2m + a, −m + a, a, m + a, 2m + a, . . . }
Note also that [a] depends on m and so it would be more accurate to write [a]m instead, but this would be cumbersome.
15.2Theorem Ifm>0then[a]=[b]⇔a≡b(modm).
proof. First assume that [a] = [b]. Note that a ∈ [a] because a ≡ a (mod m). And, because [a] = [b], we have a ∈ [b]. By definition of [b], then a ≡ b (mod m). For the implication the other way, assume that a ≡ b (mod m), aiming to prove that the sets [a] and [b] are equal. To prove that the sets are equal, we will prove that every element of the first is a member of the second, and vice versa. Supposethatx∈[a],sothatx≡a (modm). Sincea≡b (modm),by transitivity of equivalence, x ≡ b (mod m), and so x ∈ [b]. The argument to show that if x ∈ [b] then x ∈ [a] is similar. qed
15.3 Theorem Given m > 0. For every a there is a unique r ∈ [0 .. m) such that [a] = [r].
proof. Let r = a mod m so that 0 ≤ r < m, and a ≡ r (mod m), and by Theorem 15.2, [a] = [r]. To prove that r is unique, suppose that [a] = [r′], where 0 ≤ r′ < m. By Theorem 15.2, this implies that a ≡ r′ (mod m). This, together withtherestrictionthat0≤r′ <m,impliesthatr′ =amodm=r. qed
15.4 Theorem Given m > 0, there are exactly m distinct residue classes modulo m, namely, [0], [1],. . . , and [m − 1].
41
42 CHAPTER 15. RESIDUE CLASSES
proof. By Theorem 15.3 we know that every residue class [a] is equal to one of [0], or [1], . . . , or [m − 1]. So any residue classes is in this list. These residue classesaredistinct:if0≤r1 <mand0≤r2 <mand[r1]=[r2]thenbythe uniqueness part of Theorem 15.3 we must have r1 = r2. qed
15.5 Definition Any element x ∈ [a] is a class representative. The element of [a] that is in [0..m) is the principle class representative or principle residue.
Chapter 16
Zm and Complete Residue Systems
Throughout this section we assume a fixed modulus m > 0.
16.1 Definition The set {[a] | a ∈ Z} of all residue classes modulo m is
denoted Zm.
Recall that in a set, the order in which elements appear does not matter, and repeat elements collapse: the set {0, 2, 3, 1} and the set {2, 0, 2, 3, 1, 4, 1} are equal. So, while at first glance Zm may seem to have infinitely many elements Zm ={...,[−2],[−1],[0],[1],[2],...},Theorem15.4showsthataftertherepeats collapse Zm = {[0], [1], . . . , [m − 1]}, and so instead Zm has exactly m elements.
16.2 Example Fix m = 4. Then [1] = {...,−7,−3,1,5,...}, and so all of theseclassesareequal: ···=[−7]=[−3]=[1]=[5]=···. Wecould,therefore, instead of Z4 = {[0], [1], [2], [3]}, write Z4 = {[8], [5], [−6], [11]}.
16.3 Definition A set of m integers {a0, a1, . . . , am−1} is a complete residue system modulo m (or a complete set of representatives for Zm) if the set Zm equals the set {[a0], [a1], . . . , [am−1]}.
16.4 Example These are complete residue systems modulo 5.
1. {0,1,2,3,4}
2. {−2,−1,0,1,2}
3. {−9,14,12,10,8}
4. {0 + 5n1,1 + 5n2,2 + 5n3,3 + 5n4,4 + 5n4}, where n1,n2,n3,n4,n5 may be any integers.
For each m > 0 there are infinitely many distinct complete residue systems modulo m. In particular, {0, 1, . . . , m−1} is the set of least nonnegative residues modulo m.
43
44 CHAPTER 16. ZM AND COMPLETE RESIDUE SYSTEMS
16.5 Theorem Fix m > 0. If m = 2k then {0,1,2,...,k − 1,k,−(k − 1),...,−2,−1} is a complete residue system modulo m. If m = 2k + 1, then {0,1,2,...,k,−k,...,−2,−1} is a complete residue system modulo m.
proof. If m = 2k, then since Zm = {[0],[1],...,[k],[k+1],...,[k+i],[k+k−1]}, it suffices to note that [k+i] = [k+i−2k] = [−k+i] = [−(k−i)]. So [k + 1] = [−(k − 1)], [k + 2] = [−(k − 2)], . . . , [k + k − 1] = [−1], as desired.
In the n = 2k+1 case, [k+i] = [−(2k+1)+k+i] = [−k+i+1] = [−(k−i+1)] so [k + 1] = [−k], [k + 2] = [−(k − 1)], . . . , [2k] = [−1], as desired. qed
16.6 Definition The complete residue system modulo m given in the prior theorem is the least absolute residue system modulo m.
16.7 Example Where m = 232, the least absolute residue system is {−(231 −1),−(231 −2),...,−2,−1,0,1,2,...,231}.
Chapter 17
Addition and Multiplication
in Zm
In this chapter we show how to define addition and multiplication of residue classes modulo m. With respect to these binary operations Zm is a ring as defined in Appendix A.
17.1 Definition For [a],[b] ∈ Zm, the sum of the residue class [a] and the residue class [b] is the residue class [a + b]. The product of the residue class [a] and the residue class [b] is the residue class [ab]. That is,
[a] + [b] = [a + b] [a][b] = [ab].
17.2 Example For m = 5 we have [2] + [3] = [5] and [2][3] = [6]. Note that since 5 ≡ 0 (mod 5) and 6 ≡ 1 (mod 5) we can also write [2] + [3] = [0] and [2][3] = [1].
We must check that these binary operations are well defined. That is, since a residue class can have many representatives, we must check that the results of an operation do not depend on the representatives chosen for that operation.
For example, fix m = 5 and consider [7] + [11]. We know that the residue classes [7] and the residue class [2] are equal, and also that [11] = [21]. Therefore for the binary operations to make sense we must have that [7] + [11] = [2] + [21]. In this case, [7] + [11] = [18] and [2] + [21] = [23], and [18] = [23] so this one example is fine.
17.3Theorem Theresultsofthesumandproductofresidueclassesdoesnot depend on the choice of class representative: for any modulus m > 0, if [a] = [b] and [c] = [d] then [a] + [c] = [b] + [d] and [a][c] = [b][d].
proof. This follows immediately from Theorem 12.6. qed When performing addition and multiplication in Zm, we may at any time
change class representatives, rewriting [a] by [a′], where a ≡ a′ (mod m). 45
46 CHAPTER 17. ADDITION AND MULTIPLICATION IN ZM
17.4 Example Take m = 151 and consider the calculation [150][149]. Then 150 ≡ −1 (mod 151) and 149 ≡ −2 (mod 151), and so [150][149] = [−1][−2] = [2], an easier calculation.
When working with Zm it is often useful to write all residue classes in the least nonnegative residue system, as we do in constructing the following addition and multiplication tables for Z4.
+ [0] [1] [2] [3] [0] [0] [1] [2] [3] [1] [1] [2] [3] [0] [2] [2] [3] [0] [1] [3] [3] [0] [1] [2]
· [0] [1] [2] [3] [0] [0] [0] [0] [0] [1] [0] [1] [2] [3] [2] [0] [2] [0] [2] [3] [0] [3] [2] [1]
          Notice that we have reduced results of the sum and product to keep the repre- sentative in [0 .. 4). That is, in constructing those tables we follow the alogrithm that resclassa + [b] = [(a + b) mod m] and [a][b] = [(ab) mod m].
This leads to an alternative way to define Zm and addition and multiplication in Zm. For clarity we will use different notation.
17.5Definition Form>0,letJm betheset={0,1,2,...,m−1}endowed with two binary operations: for a,b ∈ Jm, let a⊕b = (a+b)modm and a ⊙ b = (ab) mod m.
Here are the addition and multiplication tables for J4.
⊕0123 ⊙0123 00123 00000 11230 10123 22301 20202 33012 30321
17.6 Remark The precise expression of the intuition that Jm with ⊕ and ⊙ is just like Zm with addition and multiplication is to say that the two are “isomorphic.” In this book we will leave the idea as informal.
17.7 Example Let’s solve the congruence 272x ≡ 901 (mod 9). Using residue classes modulo 9 we see that this congruence is equivalent to [272x] = [901], which is equivalent to [272][x] = [901]. That is equivalent to [2][x] = [1]. We know [x] ∈ {[0],[1],...,[8]}, so by trial and error we see that x = 5 is a solution.
          
Chapter 18
The Group of Units
This is the multiplication table for Z6.
⊙ [0] [1] [2] [3] [4] [5] [0] [0] [0] [0] [0] [0] [0] [1] [0] [1] [2] [3] [4] [5] [2] [0] [2] [4] [0] [2] [4] [3] [0] [3] [0] [3] [0] [3] [4] [0] [4] [2] [0] [4] [2] [5] [0] [5] [4] [3] [2] [1]
Note that some rows, and some columns, contain all of the members of Z6 while others do not. We can state that as: for some [a], [b] ∈ Z6 the equation [a] ⊙ x = [b] has no solution x.
18.1Example Theequation[5]⊙x=[3]hasthesolutionx=[3].Infact,for any [b] ∈ Z6, the equation [5] ⊙ x = [b] has a solution. However, the equation [4] ⊙ x = [1] has no solution.
18.2 Definition Let m > 0. A residue class [a] ∈ Zm is a unit if there is another residue class [b] ∈ Zm such that [a] ⊙ [b] = [1]. In this case [a] and [b] are said to be inverses of each other in Zm.
18.3 Theorem Let m > 0. A residue class [a] ∈ Zm is a unit if and only if gcd(a, m) = 1.
proof. Let [a] be a unit. Then there is a [b] such that [a] ⊙ [b] = [1]. Hence [ab] = [1] and so ab ≡ 1 (mod m). Thus, by Theorem 14.3, gcd(a, m) = 1.
To prove the converse, let gcd(a,m) = 1. By Theorem 14.1 there is an integer a∗ such that aa∗ ≡ 1 (mod m). Hence [aa∗] = [1]. So [a] ⊙ [a∗] = [1] and we can take b = a∗. qed
Note that from Theorem 14.8 if [a] = [b]—that is, if a ≡ b (mod m)—then gcd(a, m) = 1 ⇔ gcd(b, m) = 1. So, in checking whether or not a residue class is a unit we can use any representative of the class.
47
       
48 CHAPTER 18. THE GROUP OF UNITS 18.4 Theorem For m > 0, the set of units in Zm is the set of residue classes
{[i]|1≤i≤mand gcd(i,m)=1}.
proof. If[a]∈Zm then[a]=[i],where0≤i≤m−1,soforeachm>0we need only consider residue classes with representatives in the interval [0 .. m).
If m = 1 then Zm consists of a single residue class Z1 = {[0]} = {[1]}. Since [1] ⊙ [1] = [1], we have that this single class [1] is a unit.
If m > 1 then gcd(0, m) = m ̸= 1 and gcd(m, m) = m ̸= 1, but gcd(i, m) = 1 for 1 ≤ i ≤ m. So the theorem follows from Theorem 18.3. qed
18.5 Definition The set of all units in Zm, the group of units, is denoted Um. (See Appendix A for the definition of a group.)
18.6 Example Here are the first few Um’s. i123456
Ui {[1]} {[1]} {[1], [2]} {[1], [3]} {[1], [2], [3], [4]} {[1], [5]} 18.7 Theorem The set of units Um has these properties.
1. (Closure) If [a] and [b] are members of Um then the product [a][b] is also a member of Um.
2. (Associativity) For all [a], [b], [c] in Um we have ([a] ⊙ [b]) ⊙ [c] = [a] ⊙ ([b] ⊙ [c]).
3. (Existence of an identity) [1] ⊙ [a] = [a] ⊙ [1] = [a] for all [a] ∈ Um.
4. (Existence of inverses) For each [a] ∈ Um there is a [a]∗ ∈ Um such that
[a] ⊙ [a]∗ = [1].
5. (Commutativity) For all [a], [b] ∈ Um , we have that [a] ⊙ [b] = [b] ⊙ [a].
18.8 Example Theorem 18.3 shows that
U15 ={[1],[2],[4],[7],[8],[11],[13],[14]}
= {[1], [2], [4], [7], [−7], [−4], [−2], [−1]}.
Rather than list the entire multiplication table, we just show the inverse of each
element.
r [1] [2] [4] [7] [8] [11] [13] [14] inverse of r   [1] [8] [4] [13] [2] [11] [7] [14]
18.9 Theorem Let m > 0 and fix [a], [b] ∈ Um. Then the equation [a] ⊙ x resclassb has a unique solution x ∈ Um.
proof. To see that it has a solution, consider [a]∗ ⊙[b]. By the closure property, that is an element of Um. Also, [a]⊙([a]∗ ⊙[b]) = ([a]⊙[a]∗)⊙[b] = [1]⊙[b] = [1 · b] = [b], as required (the first equality follows by the associative property).
To see that the solution is unique, suppose that x,x′ ∈ Um are such that [a]⊙x = [b] and also [a]⊙x′ = [b]. Then [a]⊙x = [a]⊙x′. Multiplying both sides of that equation by the inverse [a]∗ gives [a]∗ ⊙ ([a] ⊙ x) = ([a]∗ ⊙ [a]) ⊙ x = [1]⊙x=xontheleft,andx′ ontheright. Sothetwoareequal. qed
     
49 18.10 Definition If X is a set, the cardinality |X| is the number of elements
in X.
18.11 Example |{1}| = 1, |{0,1,3,9}| = 4, |Zm| = m if m > 0.
18.12 Definition If m ≥ 1 then the Euler phi function (or the totient) is φ(m) = |{i ∈ Z | 1 ≤ i ≤ m and gcd(i, m) = 1}|.
18.13 Example Here are the first few values of φ. i123456
φ(i) 1 1 2 2 4 2 Compare this to the table in Example 18.6.
18.14 Corollary If m > 0 then |Um| = φ(m).
Note that if p is any prime then φ(p) = p − 1.
In general, though, φ(m) is not easy to calculate. However, computing φ(m) is easy once we know the prime factorization of m.
18.15 Theorem Fix a, b > 0. If gcd(a, b) = 1 then φ(ab) = φ(a)φ(b). 18.16Theorem Ifpisprimeandn>0thenφ(pn)=pn−pn−1.
18.17 Theorem Let p1,p2,...,pk be distinct primes and let n1,n2,...,nk be positive integers. Then
φ(pn1pn2 ···pnk)= pn1 −pn1−1 ··· pnk −pnk−1 . 12k11kk
The proofs of Theorem 18.15 and Theorem 18.17 are routine arguments by induction on n, and are left as exercises.
Proof of Theorem 18.16. We want to count the number of elements in the set A = {1,2,...,pn} that are relatively prime to pn. Let B be the set of elements of A that are not relatively prime, that is, that have a factor greater than 1 in common with pn. The nuber p is prime, so the only factors of pn are 1,p,...,pn, and hence b = pk for some k. It follows that if a number b is an element of B thenithastheformb=kpforsome1≤k≤pn−1. Thatis,Bisasubsetof this set: {p, 2p, 3p, . . . , kp, . . . , pn−1p}. But obviously every element of that set is not relatively prime to pn, so in fact B equals that set.
The number of elements in A is |A| = pn and the number in B is |B| = pn−1, so the number of elements of A that are not in B is pn − pn−1. qed
18.18 Example φ(12)=φ(22 ·3)=(22 −21)(31 −30)=2·2=4
18.19 Example φ(9000) = φ(23 · 53 · 32) = (23 − 22)(53 − 52)(32 − 31) = 4 · 100 · 6 = 2400
   
50 CHAPTER 18. THE GROUP OF UNITS
Chapter 19
The Chinese Remainder Theorem
19.1 Definition A linear congruence has the form ax ≡ b (mod n) where x is a variable.
19.2 Example The linear congruence 2x ≡ 1 (mod 3) is solved by x = 2 because 2 · 2 = 4 ≡ 1 (mod 3). The solution set of that congruence is {...,2,5,8,11,...}.
19.3 Example The congruence 4x ≡ 1 (mod 2) has no solution, because 4x is even, and so is not congruent to 1, modulo 2.
19.4 Lemma Fix a modulus m and a number a. The congruence ax ≡ b (mod m) has a solution if an only if gcd(a, m) | b. If a solution x0 does exist then, where d = gcd(a, b), the set of solutions is
{...,x0 +(−m/d),x0,x0 +(m/d),x0 +(2m/d),x0 +(3m/d),...}
the residue class [x0] modulo m/d.
proof. The existence of an x solving ax ≡ b (mod m) is equivalent to the existence of a k such that ax − b = km, which in turn is equivalent to the equivalence of a k such that xa + (−k)m = b. With that, this result is a restatement of Lemma 5.7. qed
One generalization of Lemma 19.4 is to consider systems of linear congru- ences. In 1247, Ch’in Chiu-Shao published a solution for a special case of that problem. We first need a preliminary result.
19.5Lemma Ifgcd(a,b)=1andcisanumbersuchthata|candb|cthen ab | c
51
52 CHAPTER 19. THE CHINESE REMAINDER THEOREM
proof. Because a | c and b | c there are numbers ka, kb such that kaa = c and kbb = c. By Bezout’s Lemma, there are s and t such that as + bt = 1. Multiply by c to get cas + cbt = c. Substitution gives (kbb)as + (kaa)bt = c. Then ab divides the left side of the equation and so ab must divide the right side, c. qed
19.6Theorem(ChineseRemainderTheorem) Supposethatm1,...,mn are pairwise relatively prime (that is, gcd(mi,mj) = 1 whenever i ̸= j). Then the system of congruences
x ≡ a1 x ≡ a2
.
x ≡ an
has a unique solution modulo m1m2 . . . mn.
proof. Let M = m1m2 ...mn and for i ∈ {1,...,n} let Mi = M/mi = m1m2 . . . mi−imi+1 . . . mn. Observe that gcd(Mi, mi) = 1 and so Lemma 19.4 says that the linear congruence Mix ≡ 1 (mod mi) has a set of solutions that is a single congruence class [xi] modulo mi.
Now consider the number
s0 =a1M1x1 +a2M2x2 +···+anMnxn.
We claim that s0 solves the system. For, consider the i-th congruence x ≡ ai (mod mi). Because mi divides Mj when i ̸= j, we have that s0 ≡ aiMixi (mod mi). Since xi was chosen because of the property that Mixi ≡ 1 (mod mi), wehavethats0 ≡ai·1≡ai (modmi),asclaimed.
To finish we must show that the solution is unique modulo M. Suppose that x also solves the system, so that for each i ∈ {1,...,n} we have that x ≡ ai ≡ x0 (mod mi). Restated, for each i we have that ni | (x − x0).
We can now show that m1m2 ...mn | (x−x0). We have that gcd(m1,m2) = 1 and m1 | (x−x0) and m2 | (x−x0), so the prior lemma applies and we conclude that m1m2 | (x − x0). In this way, we can build up to the entire product m1 . . . mn. qed
(mod m1) (mod m2)
(mod mn)
Chapter 20
Fermat’s Little Theorem
20.1 Definition For [a] ∈ Um, the powers of the residue class are given by [a]1 = [a], [a]2 = [a][a], etc.
20.2Lemma If[a]∈Um then[a]n ∈Um forn≥1,and[a]n =[an].
proof. We will check this by induction on n. The n = 1 base case is trivial: [a]1 = [a] = [a1], and by assumption [a] ∈ Um. For the inductive step, suppose that[a]k =[ak]∈Um fork≥1andconsiderthek+1-stpower.
[a]k+1 = [a]k[a] = [ak][a] = [aka] = [ak+1]
By induction the theorem holds for all n ≥ 1. qed
20.3Theorem(Euler’sTheorem) Ifm>0,andaisrelativelyprimetom, then aφ(m) ≡ 1 (mod m).
proof. Form>0,wehavethatgcd(a,m)=1ifandonlyif[a]∈Um. Theprior result gives that an ≡ 1 (mod m) ⇐⇒ [an] = [1] ⇐⇒ [a]n = [1]. Therefore, Euler’s Theorem is equivalent to the following: if m > 0 and [a] ∈ Um then [a]φ(m) = [1].
We will write X1, X2, . . . , Xφ(m) for the residue classes in Um.
We first show that if X ∈ Um then the set O = {XX1,XX2,...,XXφ(m)} equals the set Um. Containment one way is easy: any member of O is a member of Um by the closure property of Theorem 18.7. For containment the other way, consider Xi ∈ Um,and note that Theorem 18.9 shows that the equation X⊙x=Xi hasasolutionx=Xj forsomej,soXi =XXj isanelementofO.
Next, for any X ∈ Um consider the product XX1XX2 · · · XXφ(m). The associative property says that we can parenthesize this term in any way, and the prior paragraph then gives that the product is (XX1)(XX2) · · · (XXφ(m)) = X1X2 · · · Xφ(m).
Finally, let A = X1X2 · · · Xφ(m), and for any X ∈ Um consider Xφ(m)A. The commutative property of Theorem 18.7 gives that
Xφ(m)A=Xφ(m)X1X2···Xφ(m) =(XX1)(XX2)···(XXφ(m)). 53
54 CHAPTER 20. FERMAT’S LITTLE THEOREM The prior paragraph then shows that Xφ(m)A = A.
Multiplying both sides of that equation by the inverse A∗ of A gives (Xφ(m)A)A∗ = Xφ(m)(AA∗) = Xφ(m)[1] = Xφ(m)
on the left and AA∗ = [1] on the right, as desired. qed
20.4 Example Fix m = 12. The positive integers a < m with gcd(a, m) = 1 are 1, 5, 7 and 11, and so φ(m) = 4. We will check Euler’s result for all four.
First, 14 ≡ 1 (mod 12) is clear. Next, 52 ≡ 1 (mod 12) since 12 | 25−1, and so 54 ≡ (52)2 ≡ 12 (mod 12). From that one, and because 7 ≡ −5 (mod 12) and4iseven,74 ≡54 (mod12)≡1 (mod12). And,fourth,11≡−1 (mod12) and again since 4 is even we have that 114 ≡ (−1)4 (mod 12) ≡ 1 (mod 12).
20.5 Theorem (Fermat’s Little Theorem) If p is prime, and a is relatively prime to p, then ap−1 ≡ 1 (mod p).
proof. Where p is prime, φ(p) = p − 1. qed
20.6 Example Fermat’s Little Theorem can simplify the computation of an modpwherepisprime. Recallthatifan ≡r (modp)where0≤r<p,then an mod p = r. We can do two things to simplify the computation: (i) replace a by a mod p, and (ii) replace n by n mod (p − 1).
Suppose that we want to calculate 12347865435 mod 11 Note that 1234 ≡ −1+2−3+4 (mod 11), that is, 1234 ≡ 2 (mod 11). Since gcd(2,11) = 1 we have that 210 ≡ 1 (mod 11). Now 7865435 = (786543) · 10 + 5 so
27865435 ≡ 2(786543)·10+5 (mod 11) ≡  210 786543 · 25 (mod 11)
≡ 1786543 · 25 (mod 11) ≡ 25 (mod 11),
and 25 = 32 ≡ 10 (mod 11). Hence, 12347865435 ≡ 10 (mod 11). It follows that 12347865435 mod 11 = 10.
20.7 Remark Fermat’s theorem is called “little ” as a contrast with Fermat’s Last Theorem, which states that xn + yn = zn has no solutions x, y, z ∈ N when n > 2. For many years this was the most famous unsolved problem in Mathematics, until it was proved by Andrew Wiles in 1995, over 350 years after it was first mentioned by Fermat. Fermat’s Little Theorem is much easier to prove, but has more far-reaching consequences for applications to cryptography and secure transmission of data on the Internet.
Chapter 21
Probabilistic Primality Tests
Fermat’s Little Theorem says that if p is prime and 1 ≤ a ≤ p−1, then ap−1 ≡ 1 (mod p). It has this converse.
21.1Theorem Ifm≥2andforallasuchthat1≤a≤m−1wehave am−1 ≡ 1 (mod m) then m must be prime.
proof. If the hypothesis holds, then for all a with 1 ≤ a ≤ m − 1, we know that a has an inverse modulo m, namely, am−2. By Theorem 18.3, this says thatforall1≤a≤m−1wehavethatgcd(a,m)=1. Butthismeansthatm is prime, because if not then we would have m = ab with 1 < a,b < m, which would mean gcd(a, m) = a > 1. qed
Therefore, one way to check that a number m is prime would be to check that 1m−1 ≡ 1 (mod m), and that 2m−1 ≡ 1 (mod m), ..., and that m−1m−1 ≡ 1 (mod m).
This check is a lot of work, but it does have an advantage. Consider m = 63. Note that 26 = 64 ≡ 1 (mod 63) and raising both sides to the 10-th power gives 260 ≡ 1 (mod 63). Multiplying both sides by 22 yields the conclusion that 262 ≡4 (mod63). Since4̸≡1 (mod63)wehavethat262 ̸≡1 (mod63). This tells us, without factoring 63, that 63 is not prime.
On the other hand, knowing only that 2m−1 ≡ 1 (mod m) is not enough to show that m is prime. For instance, 2m−1 ≡ 1 (mod m) for the composite number m = 341.
Nonetheless, consider only the base b = 2. There are 455,052,511 odd primes p ≤ 1010, all of which satisfy 2p−1 ≡ 1 (mod p). There are only 14,884 com- posite numbers 2 < m ≤ 1010 that satisfy 2m−1 ≡ 1 (mod m). Thus, for a randome number m with 2 < m ≤ 1010, if m satisfies 2m−1 ≡ 1 (mod m) then the probability that m is prime is
455, 052, 511 ≈ .999967292. 455, 052, 511 + 14, 884
55
 
56 CHAPTER 21. PROBABILISTIC PRIMALITY TESTS
In other words, if we find that 2m−1 ≡ 1 (mod m), then it is highly likely (but not a certainty) that m is prime, at least when m ≤ 1010. Thus we are led to the following algorithm (expressed in the syntax of Maple).
 > is_prob_prime:=proc(n)
     if n <=1  or Power(2,n-1) mod n <> 1 then
        return "not prime";
     else
        return "probably prime";
     end if;
end proc:
What happens if we use 3 instead of 2 in the above probabilistic primality test? Or, better yet, what if we evaluate am−1 mod m for several different a’s? The number of primes less than 106 is 78,498. The number of numbers m ≤ 106 that are composite and such that 2m−1 ≡ 1 (mod m) is 245. The number of numbers m ≤ 106 that are composite and such that both 2m−1 ≡ 1 (mod m) and 3m−1 ≡ 1 (mod m) is 66. The number of numbers m ≤ 106 that are composite and such that am−1 ≡ 1 (mod m) where a is any of the first thirteenprimesis0. (Ifm>106 andam−1 ≡1 (modm)forallainthesetof the first thirteen prime then it is highly likely, but not certain, that m is prime.) That is, if we check for primality by using the scheme of this chapter then we may possibly find out early that the number is not prime, having done very little work. Otherwise, as we work our way through bases a ∈ [1..m), calculating whether am−1 ≡ 1 (mod m), we gain confidence that m is prime. This is the
Solovay-Strassen pseudoprimality test.
In practice, there are better probabilistic primality tests than the one de-
scribed here. For instance, the built-in Maple procedure isprime is a very sophisticated probabilistic primality test. So far no one has found an integer n for which isprime(n) gives the wrong answer.
Chapter 22
Representations in Other Bases
22.1 Definition Let b ≥ 2 and n > 0. The base b representation of n is n = [ak,ak−1,...,a1,a0]b forsomek≥0,wheren=akbk+ak−1bk−1+···+a1b+a0 andai ∈{0,1,...,b−1}fori=0,1,...,k.
22.2Example (1) 267=[5,3,1]7,since267=5·72+3·7+1
(2) 147 = [1,0,0,1,0,0,1,1]2, since 147 = 1·27 +0·26 +0·25 +1·24 +0·
23 +0·22 +1·2+1
(3) 4879=[4,8,7,9]10,since4879=4·103+8·102+7·10+9
(4) 10705679 = [A,3,5,B,0,F]16, since 10705679 = 10 · 165 + 3 · 164 + 5 ·
163 +11·162 +0·16+15
Observe that a number’s base 10 representation is just its ordinary one.
The representations are said to be in binary if b = 2, in ternary if b = 3, inoctalifb=8,indecimalifb=10,andinhexadecimalifb=16. Ifbis understood, especially if b = 10, we write akak−1 · · · a1a0, without the subscript base. In the case of b = 16, which is used frequently in computer science, for the ai of 10, 11, 12, 13, 14 and 15 we use A, B, C, D, E and F, respectively.
For a fixed base b > 2, the numbers ai’s the digits of the base b representa- tion. In the binary case, the ai’s are bits, a shortening of “binary digits”.
22.3 Theorem If b ≥ 2 then every n > 0 has a unique base b representation. proof. To show that a representation exists, iterate the Division Algorithm:
n = bq0 + r0 q0 =bq1 +r1 q1 =bq2 +r2
0 ≤ r0 < b 0≤r1 <b 0≤r2 <b
.
qk =bqk+1 +rk+1 0≤rk+1 <b.
57
58 CHAPTER 22. REPRESENTATIONS IN OTHER BASES
Note that n > q0 > q1 > ··· > qk. This shows that iteration of the Division Algorithm cannot go on forever, and we must eventually obtain ql = 0 for some l, so that ql−1 = b · 0 + rl. We claim that the desired representation is n = [rl, rl−1, . . . , r0]. For, note that n = bq0 + r0 and q0 = bq1 + r1, and hence n = b(bq1 +r1)+r0 = b2q1 +br1 +r0. Continuing in this way we find that n = bl+1ql + blrl + · · · + br1 + r0. And, since ql = 0 we have
(∗) n=blrl +···+br1 +r0,
which shows that n = [rl,...,r1,r0]b.
To see that this representation is unique, note that from equation (∗) we
have
n=b bl−1rl +···+r1 +r0, 0≤r0 <b.
Because r0 is uniquely determined by n, so is the quotient q = bl−1rl + · · · + r1. A similar argument shows that r1 is uniquely determined. Continuing in this wayweseethatallthedigitsrl,rl−1,...,r0 areuniquelydetermined. qed
22.4 Example We find the base 7 representation of 1,749.
1749 = 249 · 7 + 6 249 = 35 · 7 + 4 35 = 5 · 7 + 0
5=0·7+5
22.5 Example This finds the binary representation of 137.
137 = 2 · 68 + 1 68 = 2 · 34 + 0 34 = 2 · 17 + 0 17 = 2 · 8 + 1
8=2·4+0 4=2·2+0 2=2·1+0 1=2·0+1
Therefore 137 = [1, 0, 0, 0, 1, 0, 0, 1]2.
22.6Remark Wecansometimes“eyeball”therepresentationinanotherbase of a small number. For instance, we can see how to represent n = 137 in binary, without the machinery of the proof. Note that 21 = 2, 22 = 4, 23 = 8, 24 = 16, 25 =32,26 =64,27 =128,and28 =256. Byeye,wespotthatthevalueclosest to 137 but not greater than it is 27, and we compute that 137 − 27 = 9. The power of 2 closest to it but not above 9 is 23, and 9−23 = 1. Finally, 1 is a power of 2, since 1 = 20. Therefore 137 = 27+23+20 and so 137 = [1,0,0,0,1,0,0,1]2.
Hence 1749 = [5,0,4,6]7.
Chapter 23
Computation of aN mod m
Some Number Theory work involves computing with large numbers. Since com- puter multiplication of numbers is a slow operation (relative to computer addi- tion), we can ask: where n is any positive integer, what is the smallest number of multiplications required to compute an?
For instance, the naive way to calculate 28 is to do seven multiplications.
22 =2·2=4
23 =2·4=8
24 =2·8=16
25 =2·16=32 26 =2·32=64 27 =2·64=128 28 =2·128=256
In general, computing an by this naive method requires n − 1 multiplications. But we can compute 28 with only three multiplications
22 =2·2=4
24 = 22 2 =4·4=16
28 = 24 2 =16·16=256
If the exponent has the form n = 2k then this successive squaring method 59
60 CHAPTER 23. COMPUTATION OF AN MOD M requires only k-many multiplications.
a2 = a · a
a22 =(a2)2=a2·a2 a23 =(a22)2 =a22 ·a22
.
a2k = (a2k−1 )2 = a2k−1 · a2k−1
This is quite a savings because if n = 2k then k is generally much smaller than n−1, just as 3 is smaller than 7.
This is the foundation of the binary method to compute an. It is best explained by example.
23.1 Example To compute 315, first express the exponent in binary 15 = 23+22+2+1=[1,1,1,1]2.Thus,315=323 ·322 ·32·3.
Next, we use successive squaring to get the factors in that expansion of 315.
32 =3·3=9
322 =9·9=81
323 =81·81=6561
Putting those factors together
3·32 =3·9=27 (3·32)·322 =27·81=2187
(3·32 ·322)323 =2187·6561=14348907
gives that 315 = 14348907. This took just six multiplications, while the naive method would have taken fourteen. (Finding the binary representation of 15 took some extra effort, but not much.)
23.2 Theorem Computing xn using the binary method requires ⌊lg(n)⌋ divi- sions and at most 2⌊lg(n)⌋ multiplications.
proof. If n = [ar,...,a0]2 and ar = 1 then 2r ≤ n ≤< 2r+1. By the familiar properties of any logarithm, lg(2r) ≤ lg(n) < lg(2r+1). Since lg2(2x) = x this gives r ≤ lg(n) < r + 1, hence r = ⌊lg(n)⌋. Note that r is the number of times we need to divide to get n’s binary representation n = [ar,...,a0]2.
To compute the powers x,x2,x22,...,x2r by successive squaring requires r = ⌊lg(n)⌋ multiplications and similarly to compute the product
x2r ·xar−12r−1 ···xa12 ·xa0
requires r multiplications. So after obtaining the binary representation we need at most 2r = 2⌊lg(n)⌋ multiplications. qed
61
Note that if we count an application of the Division Algorithm and a mul- tiplication as having the same cost then the above tells us that we need at most 3⌊lg(n)⌋ operations to compute xn. So, for example, if n = 106, then 3⌊lg(n)⌋ = 57.
To compute an mod m, we use the binary method of exponentiation, with the added refinement that after every multiplication we reduce modulo m. This keeps the products from getting too big for our computer or calculator.
23.3 Example We compute 315 mod 10:
32 =3·3=9≡9 (mod10) 34 =9·9=81≡1 (mod10) 38 ≡1·1≡1≡1 (mod10)
and so 315 = 38 · 34 · 32 · 31 ≡ 1 · 1 · 9 · 3 = 27 ≡ 7 (mod 10).
Note that 315 ≡ 7 (mod 10). In Example 23.1 we calculated that 315 =
14348907 which is clearly congruent to 7 mod 10, but the multiplications there were not so easy.
23.4 Example To find 2644 mod 645, we first get the binary representation 644 = [1,0,1,0,0,0,0,1,0,0]2. That is, 644 = 29 +27 +22 = 512+128+4. By successive squaring and reducing modulo 645 we get
22 =2·2=4≡4 (mod645)
24 ≡4·4=16≡16 (mod645)
28 ≡16·16=256≡256 (mod645)
216 ≡256·256=65,536≡391 (mod645) 232 ≡391·391=152,881≡16 (mod645) 264 ≡16·16=256≡256 (mod645)
2128 ≡256·256=65,536≡391 (mod645) 2256 ≡391·391=152,881≡16 (mod645) 2512 ≡16·16=256≡256 (mod645).
Now 2644 = 2512 ·2128 ·24, and hence 2644 ≡ 256·391·16 (mod645). So 256·391 = 100099 ≡ 121 (mod 645) and 121·16 = 1936 ≡ 1 (mod 645). Hence 2644 mod 645 = 1.
62 CHAPTER 23. COMPUTATION OF AN MOD M
Chapter 24
Public Key Cryptosystems
Everyone has tried secret codes. A common one is the Caesar cipher: the sender and the recipient agree in advance to express letters as numbers (1 for A, 2 for B, etc.) and also agree to use an encoding that offsets the message; for instance f(n) = (n + 13) mod 26 offsets the letters by 13. The sender then, in place of transmitting the number n, will transmit f(n)—instead of A, the sender will transmit K, the thirteenth letter. This code is very easy to break, but nonetheless notice that there is a general encryption/decryption scheme of sending offset letters, and within that scheme it relies on the single secret key, the 13.
In 1976, W Diffie and M Hellman proposed a new kind of cryptographic system where there are two keys. A message encrypted with the first key can be decrypted with the second, and a message encrypted with the second key can be decrypted with the first. We will first illustrate some advantages of such a system and then give one way to produce such key pairs.
24.1 Example If two people, Alice and Bob, want to have private commu- nications then each can take a key. Bob alone can read Alice’s messages, and Alice alone can read Bob’s.
24.2Example Alicecankeeponekeyasecret,andpublishtheotherkeyina public place such as the Internet. Then people who receive an encrypted message that claims to be from Alice can get Alice’s public key and try to decrypt the message. If the result is sensible text, then Alice must have been the one who encrypted it, since she kept her other key private. This is authentication; her message has been digitally signed.
Also, people who want to send a message to Alice in private can encrypt it with her public key. Only she can decrypt it, using her private key.
24.3 Example Key pairs can be used to do things that seem impossible. Suppose that Alice and Bob want to settle a dispute by flipping a coin, but they must do so over the Internet. Each person will flip separately, and they agree that Alice wins if the two coins come out the same while Bob wins if they are
63
64 CHAPTER 24. PUBLIC KEY CRYPTOSYSTEMS
different. However they do not trust each other and so they cannot just email each other the results. How can they agree if neither will believe the other?
Each person generates a key pair. Each then sends the other the message with “heads” or “tails” encrypted using one of their two keys. After that, each person publishes their other key, the one that they did not use to encrypt. The other person can now decrypt the message they’ve received — they are sure that they are not being cheated because they now have the other person’s outcome, albiet encrypted (and the key pairs have the property that finding a new key pair that makes the message decrypt the other way is essentially impossible).
Implicit in these examples are a number of technical requirements on key pairs: from either key we should not be able to find the other, we should not be able to decrypt the message by just trying every possible key, etc. These techni- cal requirements have been met by a number of schemes. The most important is RSA, due to R Rivest, A Shamir, and L Adelman in 1977 [11]. This chapter outlines its number-theoretic underpinning.
Assume that our message has been converted to an integer in the set Jm = {0, 1, 2, . . . , m − 1} where m is some positive integer to be determined. (For example, we can take the file as a collection of bits and interpret it as a num- ber written in binary.) Generally this is a large integer. We will require two functions:
E:Jm→Jm(Eforencipher) and D:Jm→Jm(Dfordecipher). By ‘encipher’ and ‘decipher’ we only mean that D(E(x)) = x for all x ∈ Jm.
We first need two statements about congruences.
24.4 Lemma Let m1,m2 ∈ Z+ be relatively prime. Then a ≡ b (mod m1)
anda≡b (modm2)ifandonlyifa≡b (modm1m2).
proof. One direction is easy: if a ≡ b (mod m1m2) then there is k ∈ Z such thata−b=k(m1m2). Rewritingthatasa−b=(km1)m2 showsthata−bis a multiple of m2 and so a ≡ b (mod m2). The other equivalence is similar.
Ifa≡b(modm1)anda≡b(modm2)thentherearek1,k2 ∈Zsuch that a − b = k1m1 and a − b = k2m2. Therefore k1m1 = k2m2. This shows that m1 | k2m2. As m1 is relatively prime to m2, Lemma 5.6 then gives that m1 | k2. Writing k2 = km1 for some k, and substituting into the earlier equation a−b = k2m2 gives that a−b = km1m2. Therefore a−b | m1m2 and so a ≡ b (mod m1m2). qed
24.5 Lemma Let p and q be two distinct primes and let m = pq. Suppose that e and d are positive integers that are inverses of each other modulo φ(m). Thenxed ≡x (modm)forallx.
proof. By Theorem 18.17, φ(m) = (p − 1)(q − 1). Since ed ≡ 1 (mod φ(m)) we have that ed−1 = kφ(m) = k(p−1)(q−1) for some k. Note the k > 0 unless ed = 1, in which case the theorem is obvious. So we have
(∗) ed = kφ(m) + 1 = k(p − 1)(q − 1) + 1
65
for some k > 0.
We will show that xed ≡ x (mod b) for all x. There are two cases. For
the first case, if gcd(x, p) = 1 then by Fermat’s Little Theorem we have that xp−1 ≡ 1 (mod p). Raising both sides of the congruence to the power (q − 1)k gives x(p−1)(q−1)k ≡ 1 (mod p). Then multiplying by x gives x(p−1)(q−1)k+1 ≡ x (mod p). That is, by (∗)
(∗∗) xed ≡ x (mod p).
For the second case, the gcd(x, p) = p case, the relation (∗∗) is obvious, since then x ≡ 0 (mod p).
A similar argument proves that xed ≡ x (mod q) for all x. So by Lemma 24.4 and the fact that gcd(p,q) = 1, we have that xed ≡ x (mod m) for all x. qed
24.6 Theorem Let p and q be two distinct primes, let m = pq, and suppose that e and d are positive integers that are inverses of each other modulo φ(m). WhereJm ={0,1,2,...,m−1},defineE:Jm →Jm andD:Jm →Jm by
E(x)=xe modm and D(x)=xd modm. Then E and D are inverse functions.
proof. It suffices to show that D(E(x)) = x for all x ∈ Jm. Suppose that x∈Jm,andthatE(x)=xe modm=r1,andalsothatD(r1)=r1d modm= r2. Wemustshowthatr2 =x. Sincexemodm=r1 weknowthatxe ≡r1 (mod m). Hence xed ≡ r1d (mod m). We also know that r1d ≡ r2 (mod m) and hence xed ≡ r2 (mod m). By Lemma 24.5, xed ≡ x (mod m) so we have that x ≡ r2 (mod m). Since both x and r2 are elements of Jm, both are the principle residue: x = r2. qed
66 CHAPTER 24. PUBLIC KEY CRYPTOSYSTEMS
Appendix A
Proof by Induction
Most of the proof methods used in mathematics are instinctive to a person with a talent for the work. This section covers a method, the method of Mathematical Induction that is not.
As with all proofs, we will have some assertion to prove. Each assertion will say that something is true for all integers. Thus, we can denote the assertion P(n). Our first example is the proof that for all n, if n ≥ 5 then 2n > 5n.
P(n): n≥5⇔2n >5n
An argument by induction involves two steps. In the base step we show that P is true for some first integer. Typically, that is a straightforward verification. For our example, we show that P(5) is true by just checking that 25 = 32 is indeed greater than 5 · 5 = 25, which of course it is.
The second step is called the inductive step. We must show that if P(5), ..., P(k) are all true then P(k+1) is also true.
At the end of the proof we will show why this suffices. For the moment note only that we are not asserting that P (5), . . . , P (k) are in fact all true (as that would be assuming the thing that we are to prove); instead we are proving that if they are true then P (k + 1) follows.
To prove this if-then statement, take the inductive hypothesis that P(5), ..., P(k) hold. Then, by the hypothesis that P(k) is true we have 2k > 5k, and Multiplying both sides by 2 gives 2k+1 > 10k We are trying to prove that 2k+1 >5(k+1)soifwecanshow10k≥5k+5thenwewillbedone. Because k ≥ 5, we have that 5k ≥ 5 and therefore 10k = 5k + 5k ≥ 5k + 5 = 5(k + 1). We have therefore established P (k + 1) follows from the inductive hypothesis, as 2k+1 > 10k ≥ 5(k + 1). That ends the inductive step.
To see why the two steps togehter prove the assertion, note that we have checked the statement for 5. To see it is true for 6, note that in the inductive step we proved that P (5) ⇔ P (6). To see that the statement is true for 7, note that we have proved in the inductive step that P (5) and P (6) ⇔ P (7) (and the
67
68 APPENDIX A. PROOF BY INDUCTION
prior sentence shows that P (6) holds). In this way we can see that the statement is true for all numbers n ≥ 5.
Here is an induction proof that is more streamlined, more like the ones given elsewhere in the book..
1.1 Proposition If n ≥ 5 then 2n > 5n.
proof. We prove the proposition by induction on the variable n. Ifn=5thenwehave25 >5·5or32>25,whichistrue.
Next, assume the hypothesis that 2n > 5n for 5 ≤ n ≤ k. Taking n = k gives
that 2k > 5k. Multiplying both sides by 2 gives 2k+1 > 10k. Now 10k = 5k+5k andk≥5,andso5k≥5. Hence10k=5k+5k≥5k+5=5(k+1). Itfollows that 2k+1 > 10k ≥ 5(k + 1) and therefore 2k+1 > 5(k + 1).
Hence by mathematical induction we conclude that 2n > 5n for n ≥ 5. qed
Appendix B
Axioms for Z
The set of natural numbers is N = {0, 1, 2, 3, · · · }. The set of integers includes the natural numbers and the negative integers Z = {...,−2,−1,0,1,2,···}. We sometimes want to restrict our attention to the positive integers Z+ = {1, 2, · · · }.
The rational numbers include all of the fractions Q = {n/m | n,m ∈ Z and m ̸= 0}. The real numbers R enlarge that set with the irrationals (which are too hard to precisely describe here). Note that Z+ ⊂ N ⊂ Z ⊂ Q ⊂ R.
In the first chapter we rely on some particularly important properties of Z, the axioms.
1.Ifa,b∈Z,thena+b,a−bandab∈Z. (Thatis,Zisclosedunder addition, subtraction and multiplication.)
2. Ifa∈Zthenthereisnox∈Zsuchthata<x<a+1.
3. Ifa,b∈Zandab=1,theneithera=b=1ora=b=−1.
4. LawsofExponentsForn,m∈Nanda,b∈Rwithaandbnotboth0we have (an)m = anm and (ab)n = anbn. and anam = an+m.
5. Properties of Inequalities: For a, b, c in R the following hold: if a < b and b < c, then a < c, and if a < b then a + c < b + c, and if a < b and 0 < c then ac < bc, and if a < b and c < 0 then bc < ac, and finally, given a and b, one and only one of a = b , a < b, b < a holds.
6. Well-Ordering Property Every non-empty subset of N contains a least element.
7. Mathematical Induction Let P(n) be a statement concerning the integer variable n. Let n0 be any fixed integer. Then P(n) is true for all integers n ≥ n0 if both of the following statements hold: (the base step) P(n) is true for n = n0, and (the inductive step) whenever P(n) is true for n0 ≤n≤kthenP(n)istrueforn=k+1.
69
70 APPENDIX B. AXIOMS FOR Z
Appendix C
Some Properties of R
3.1 Definition Where x ∈ R, the floor (or greatest integer) ⌊x⌋ is the largest integer less than or equal to x. Its ceiling ⌈x⌉ is the least integer greater than or equal to x.
For example, ⌊3.1⌋ = 3 and ⌈3.1⌉ = 4, ⌊3⌋ = 3 and ⌈3⌉ = 3, and ⌊−3.1⌋ = −4 and ⌈−3.1⌉ = −3.
From that definition we immediately have that ⌊x⌋ = max{n ∈ Z | n ≤ x}, andthatn=⌊x⌋⇐⇒n≤x<n+1. Fromthiswehavealsothat⌊x⌋≤xand that ⌊x⌋ = x ⇐⇒ x ∈ Z.
3.2 Lemma (Floor Lemma) Where x is real, x − 1 < ⌊x⌋ ≤ x.
proof. Let n = ⌊x⌋. Then by the above comments, we have n ≤ x < n+1. This gives immediately that ⌊x⌋ ≤ x, as already noted above. It also gives that x < n + 1 which implies that x − 1 < n, that is, that x − 1 < ⌊x⌋. qed
3.3 Definition The decimal representation of a positive integer a is given by a=an−1an−2···a1a0 where
a=an−110n−1 +an−210n−2 +···+a110+a0
and the digits an−1,an−2,...,a1,a0 are in the set {0,1,2,3,4,5,6,7,8,9}, with an−1 ̸= 0. This representation shows that a is, with respect to base 10, an n digit number (or is n digits long).
71
72 APPENDIX C. SOME PROPERTIES OF R